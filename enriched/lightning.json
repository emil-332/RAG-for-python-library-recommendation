{
  "name": "lightning",
  "summary": "The Deep Learning framework to train, deploy, and ship AI products Lightning fast.",
  "language": "python",
  "tags": [
    "dev",
    "math",
    "ml"
  ],
  "chunks": [
    {
      "chunk_id": "lightning::chunk_0",
      "text": "Looking for GPUs?\nOver 340,000 developers use [Lightning Cloud](https://lightning.ai/?utm_source=ptl_readme&utm_medium=referral&utm_campaign=ptl_readme) - purpose-built for PyTorch and PyTorch Lightning.\n\nWhy PyTorch Lightning?   \n\nTraining models in plain PyTorch is tedious and error-prone - you have to manually handle things like backprop, mixed precision, multi-GPU, and distributed training, often rewriting code for every new project. PyTorch Lightning organizes PyTorch code to automate those complexities so you can focus on your model and data, while keeping full control and scaling from CPU to multi-node without changing your core code. But if you want control of those things, you can still opt into [expert-level control](#lightning-fabric-expert-control).   \n\nFun analogy: If PyTorch is Javascript, PyTorch Lightning is ReactJS or NextJS.\n\nLightning has 2 core packages\n\n[PyTorch Lightning: Train and deploy PyTorch at scale](#why-pytorch-lightning).\n\n[Lightning Fabric: Expert control](#lightning-fabric-expert-control).\n\nLightning gives you granular control over how much abstraction you want to add over PyTorch.\n\n&nbsp;\n\nQuick start\nInstall Lightning:\n\nPyTorch Lightning example\nDefine the training workflow. Here's a toy example ([explore real examples](https://lightning.ai/lightning-ai/studios?view=public&section=featured&query=pytorch+lightning&utm_source=ptl_readme&utm_medium=referral&utm_campaign=ptl_readme)):\n\nRun the model on your terminal\n\n&nbsp;\n\nConvert from PyTorch to PyTorch Lightning\n\nPyTorch Lightning is just organized PyTorch - Lightning disentangles PyTorch code to decouple the science from the engineering.\n\n&nbsp;\n\n----\n\nExamples\nExplore various types of training possible with PyTorch Lightning. Pretrain and finetune ANY kind of model to perform ANY task like classification, segmentation, summarization and more:    \n\nTask\n------\n\n______________________________________________________________________\n\nAdvanced features\n\nLightning has over [40+ advanced features](https://lightning.ai/docs/pytorch/stable/common/trainer.html?utm_source=ptl_readme&utm_medium=referral&utm_campaign=ptl_readme#trainer-flags)\ndesigned for professional AI research at scale.\n\nHere are some examples:\n\n  \n\n  Train on 1000s of GPUs without code changes\n\n  Train on other accelerators like TPUs without code changes\n\n  16-bit precision\n\n  Experiment managers\n\nEarly Stopping\n\n  Checkpointing\n\n  Export to torchscript (JIT) (production use)\n\n  Export to ONNX (production use)\n\n______________________________________________________________________\n\nAdvantages over unstructured PyTorch\n\n- Models become hardware agnostic\n- Code is clear to read because engineering code is abstracted away\n- Easier to reproduce\n- Make fewer mistakes because lightning handles the tricky engineering\n- Keeps all the flexibility (LightningModules are still PyTorch modules), but removes a ton of boilerplate\n- Lightning has dozens of integrations with popular machine learning tools.\n- [Tested rigorously with every new PR](https://github.com/Lightning-AI/lightning/tree/master/tests). We test every combination of PyTorch and Python supported versions, every OS, multi GPUs and even TPUs.\n- Minimal running speed overhead (about 300 ms per epoch compared with pure PyTorch).\n\n______________________________________________________________________\n\n______________________________________________________________________\n\n&nbsp;\n&nbsp;\n\nLightning Fabric: Expert control\n\nRun on any device at any scale with expert-level control over PyTorch training loop and scaling strategy. You can even write your own Trainer.\n\nFabric is designed for the most complex models like foundation model scaling, LLMs, diffusion, transformers, reinforcement learning, active learning. Of any size.\n\nWhat to change\nResulting Fabric Code (copy me!)\n\nKey features\n\n  Easily switch from running on CPU to GPU (Apple Silicon, CUDA, â€¦), TPU, multi-GPU or even multi-node training\n\n  Use state-of-the-art distributed training strategies (DDP, FSDP, DeepSpeed) and mixed precision out of the box\n\n  All the device logic boilerplate is handled for you\n\n  Build your own custom Trainer using Fabric primitives for training checkpointing, logging, and more\n\nYou can find a more extensive example in our [examples](examples/fabric/build_your_own_trainer)\n\n______________________________________________________________________\n\n______________________________________________________________________\n\n&nbsp;\n&nbsp;\n\nExamples\n\nSelf-supervised Learning\n\nConvolutional Architectures\n\n- [GPT-2](https://lightning-bolts.readthedocs.io/en/stable/models/convolutional.html#gpt-2)\n- [UNet](https://lightning-bolts.readthedocs.io/en/stable/models/convolutional.html#unet)\n\nReinforcement Learning\n\nGANs\n\n- [Basic GAN](https://lightning-bolts.readthedocs.io/en/stable/models/gans.html#basic-gan)\n- [DCGAN](https://lightning-bolts.readthedocs.io/en/stable/models/gans.html#dcgan)\n\nClassic ML\n\n- [Logistic Regression](https://lightning-bolts.readthedocs.io/en/stable/models/classic_ml.html#logistic-regression)\n- [Linear Regression](https://lightning-bolts.readthedocs.io/en/stable/models/classic_ml.html#linear-regression)\n\n&nbsp;\n&nbsp;\n\nContinuous Integration\n\nLightning is rigorously tested across multiple CPUs, GPUs and TPUs and against major Python and PyTorch versions.\n\n\\*Codecov is > 90%+ but build delays may show less\n\n  Current build statuses\n\nSystem / PyTorch ver.\n:--------------------------------:\nLinux py3.9 \\[GPUs\\]\nLinux (multiple Python versions)\nOSX (multiple Python versions)\nWindows (multiple Python versions)\n\n&nbsp;\n&nbsp;\n\nCommunity\n\nThe lightning community is maintained by\n\n- [10+ core contributors](https://lightning.ai/docs/pytorch/latest/community/governance.html) who are all a mix of professional engineers, Research Scientists, and Ph.D. students from top AI labs.\n- 800+ community contributors.\n\nWant to help us build Lightning and reduce boilerplate for thousands of researchers? [Learn how to make your first contribution here](https://lightning.ai/docs/pytorch/stable/generated/CONTRIBUTING.html?utm_source=ptl_readme&utm_medium=referral&utm_campaign=ptl_readme)\n\nLightning is also part of the [PyTorch ecosystem](https://pytorch.org/ecosystem/) which requires projects to have solid testing, documentation and support.\n\nAsking for help\n\nIf you have any questions please:\n\n1. [Read the docs](https://lightning.ai/docs?utm_source=ptl_readme&utm_medium=referral&utm_campaign=ptl_readme).\n1. [Search through existing Discussions](https://github.com/Lightning-AI/lightning/discussions), or [add a new question](https://github.com/Lightning-AI/lightning/discussions/new)\n1. [Join our discord](https://discord.com/invite/tfXFetEZxv).",
      "word_count": 692
    }
  ]
}