{
  "name": "correctionlib",
  "summary": "A generic correction library",
  "language": "python",
  "tags": [
    "data",
    "math",
    "web"
  ],
  "chunks": [
    {
      "chunk_id": "correctionlib::chunk_0",
      "text": "correctionlib\n\n[![PyPI version][pypi-version]][pypi-link]\n[![PyPI platforms][pypi-platforms]][pypi-link]\n\nIntroduction\nThe purpose of this library is to provide a well-structured JSON data format for a\nwide variety of ad-hoc correction factors encountered in a typical HEP analysis and\na companion evaluation tool suitable for use in C++ and python programs.\nHere we restrict our definition of correction factors to a class of functions with\nscalar inputs that produce a scalar output.\n\nIn python, the function signature is:\n\nIn C++, the evaluator implements this currently as:\n\nThe supported function classes include:\n\n  * multi-dimensional binned lookups;\n  * binned lookups pointing to multi-argument formulas with a restricted\n  * categorical (string or integer enumeration) maps;\n  * input transforms (updating one input value in place); and\n  * compositions of the above.\n\nEach function type is represented by a \"node\" in a call graph and holds all\nof its parameters in a JSON structure, described by the JSON schema.\nPossible future extension nodes might include weigted sums (which, when composed with\nthe others, could represent a BDT) and perhaps simple MLPs.\n\nThe tool should provide:\n\n  * standardized, versioned [JSON schemas](https://json-schema.org/);\n  * forward-porting tools (to migrate data written in older schema versions); and\n  * a well-optimized C++ evaluator and python bindings (with numpy vectorization support).\n\nThis tool will definitely not provide:\n\n  * support for `TLorentzVector` or other object-type inputs (such tools should be written\n\nFormula support currently includes a mostly-complete subset of the ROOT library `TFormula` class,\nand is implemented in a threadsafe standalone manner. The parsing grammar is formally defined\nand parsed through the use of a header-only [PEG parser library](https://github.com/yhirose/cpp-peglib).\nThe supported features mirror CMSSW's [reco::formulaEvaluator](https://github.com/cms-sw/cmssw/pull/11516)\nand fully passes the test suite for that utility with the purposeful exception of the `TMath::` namespace.\nThe python bindings may be able to call into [numexpr](https://numexpr.readthedocs.io/en/latest/user_guide.html),\nthough, due to the tree-like structure of the corrections, it may prove difficult to exploit vectorization\nat levels other than the entrypoint.\n\nDetailed instructions for installing and using this package are provided in the [documentation][rtd-link].\n\nCreating new corrections\n\nA demo/tutorial of the features is available in the [documentation][rtd-link] and also available interactively\non [binder](https://mybinder.org/v2/gh/cms-nanoAOD/correctionlib/HEAD?labpath=binder%2Fcorrectionlib_tutorial.ipynb)\n\nThe `correctionlib.schemav2` module provides a helpful framework for defining correction objects\nand `correctionlib.convert` includes select conversion routines for common types. Nodes can be type-checked as they are\nconstructed using the [parse_obj](https://pydantic-docs.helpmanual.io/usage/models/#helper-functions)\nclass method or by directly constructing them using keyword arguments.\n\nDeveloping\nSee CONTRIBUTING.md",
      "word_count": 392
    }
  ],
  "usage_description": "This library is used to provide a standardized and structured format for handling ad-hoc correction factors in High-Energy Physics (HEP) analysis, allowing developers to easily create and evaluate complex corrections. With this library, developers can implement a range of correction functions, including binned lookups, formula evaluations, and categorical mappings, making it easier to manage data correction in HEP analysis applications."
}