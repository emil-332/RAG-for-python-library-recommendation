{
  "name": "llama-index-core",
  "summary": "Interface between LLMs and your data",
  "language": "python",
  "tags": [
    "math",
    "ml"
  ],
  "chunks": [
    {
      "chunk_id": "llama-index-core::chunk_0",
      "text": "LlamaIndex Core\n\nThe core python package to the LlamaIndex library. Core classes and abstractions\nrepresent the foundational building blocks for LLM applications, most notably,\nRAG. Such building blocks include abstractions for LLMs, Vector Stores, Embeddings,\nStorage, Callables and several others.\n\nWe've designed the core library so that it can be easily extended through subclasses.\nBuilding LLM applications with LlamaIndex thus involves building with LlamaIndex\ncore as well as with the LlamaIndex [integrations](https://github.com/run-llama/llama_index/tree/main/llama-index-integrations) needed for your application.",
      "word_count": 76
    }
  ],
  "usage_description": "This library is used to provide a core interface between Large Language Models (LLMs) and various data storage and processing systems, enabling the development of LLM applications. It serves as the foundational building block for RAG and other LLM applications, offering extensible classes and abstractions for tasks such as embedding, storage, and model interactions."
}