{
  "name": "sentence-transformers",
  "summary": "Embeddings, Retrieval, and Reranking",
  "language": "python",
  "tags": [
    "math",
    "ml"
  ],
  "chunks": [
    {
      "chunk_id": "sentence-transformers::chunk_0",
      "text": "Sentence Transformers: Embeddings, Retrieval, and Reranking\n\nThis framework provides an easy method to compute embeddings for accessing, using, and training state-of-the-art embedding and reranker models. It can be used to compute embeddings using Sentence Transformer models ([quickstart](https://sbert.net/docs/quickstart.html#sentence-transformer)), to calculate similarity scores using Cross-Encoder (a.k.a. reranker) models ([quickstart](https://sbert.net/docs/quickstart.html#cross-encoder)) or to generate sparse embeddings using Sparse Encoder models ([quickstart](https://sbert.net/docs/quickstart.html#sparse-encoder)). This unlocks a wide range of applications, including [semantic search](https://sbert.net/examples/applications/semantic-search/README.html), [semantic textual similarity](https://sbert.net/docs/sentence_transformer/usage/semantic_textual_similarity.html), and [paraphrase mining](https://sbert.net/examples/applications/paraphrase-mining/README.html).\n\nA wide selection of over [15,000 pre-trained Sentence Transformers models](https://huggingface.co/models?library=sentence-transformers) are available for immediate use on  Hugging Face, including many of the state-of-the-art models from the [Massive Text Embeddings Benchmark (MTEB) leaderboard](https://huggingface.co/spaces/mteb/leaderboard). Additionally, it is easy to train or finetune your own [embedding models](https://sbert.net/docs/sentence_transformer/training_overview.html), [reranker models](https://sbert.net/docs/cross_encoder/training_overview.html) or [sparse encoder models](https://sbert.net/docs/sparse_encoder/training_overview.html) using Sentence Transformers, enabling you to create custom models for your specific use cases.\n\nFor the **full documentation**, see **[www.SBERT.net](https://www.sbert.net)**.\n\nInstallation\n\nWe recommend **Python 3.10+**, **[PyTorch 1.11.0+](https://pytorch.org/get-started/locally/)**, and **[transformers v4.34.0+](https://github.com/huggingface/transformers)**.\n\n**Install with pip**\n\n**Install with conda**\n\n**Install from sources**\n\nAlternatively, you can also clone the latest version from the [repository](https://github.com/huggingface/sentence-transformers) and install it directly from the source code:\n\n**PyTorch with CUDA**\n\nIf you want to use a GPU / CUDA, you must install PyTorch with the matching CUDA Version. Follow\n[PyTorch - Get Started](https://pytorch.org/get-started/locally/) for further details how to install PyTorch.\n\nGetting Started\n\nSee [Quickstart](https://www.sbert.net/docs/quickstart.html) in our documentation.\n\nEmbedding Models\n\nFirst download a pretrained embedding a.k.a. Sentence Transformer model.\n\nThen provide some texts to the model.\n\nAnd that's already it. We now have numpy arrays with the embeddings, one for each text. We can use these to compute similarities.\n\nReranker Models\n\nFirst download a pretrained reranker a.k.a. Cross Encoder model.\n\nThen provide some texts to the model.\n\nAnd we're good to go. You can also use [`model.rank`](https://sbert.net/docs/package_reference/cross_encoder/cross_encoder.html#sentence_transformers.cross_encoder.CrossEncoder.rank) to avoid having to perform the reranking manually:\n\nSparse Encoder Models\n\nFirst download a pretrained sparse embedding a.k.a. Sparse Encoder model.\n\nPre-Trained Models\n\nWe provide a large list of pretrained models for more than 100 languages. Some models are general purpose models, while others produce embeddings for specific use cases.\n\nTraining\n\nThis framework allows you to fine-tune your own sentence embedding methods, so that you get task-specific sentence embeddings. You have various options to choose from in order to get perfect sentence embeddings for your specific task.\n\n- Embedding Models\n  - [Sentence Transformer > Training Overview](https://www.sbert.net/docs/sentence_transformer/training_overview.html)\n  - [Sentence Transformer > Training Examples](https://www.sbert.net/docs/sentence_transformer/training/examples.html) or [training examples on GitHub](https://github.com/huggingface/sentence-transformers/tree/main/examples/sentence_transformer/training).\n- Reranker Models\n  - [Cross Encoder > Training Overview](https://www.sbert.net/docs/cross_encoder/training_overview.html)\n  - [Cross Encoder > Training Examples](https://www.sbert.net/docs/cross_encoder/training/examples.html) or [training examples on GitHub](https://github.com/huggingface/sentence-transformers/tree/main/examples/cross_encoder/training).\n- Sparse Embedding Models\n  - [Sparse Encoder > Training Overview](https://www.sbert.net/docs/sparse_encoder/training_overview.html)\n  - [Sparse Encoder > Training Examples](https://www.sbert.net/docs/sparse_encoder/training/examples.html) or [training examples on GitHub](https://github.com/huggingface/sentence-transformers/tree/main/examples/sparse_encoder/training).\n\nSome highlights across the different types of training are:\n\n- Support of various transformer networks including BERT, RoBERTa, XLM-R, DistilBERT, Electra, BART, ...\n- Multi-Lingual and multi-task learning\n- Evaluation during training to find optimal model\n- [20+ loss functions](https://www.sbert.net/docs/package_reference/sentence_transformer/losses.html) for embedding models, [10+ loss functions](https://www.sbert.net/docs/package_reference/cross_encoder/losses.html) for reranker models and [10+ loss functions](https://www.sbert.net/docs/package_reference/sparse_encoder/losses.html) for sparse embedding models, allowing you to tune models specifically for semantic search, paraphrase mining, semantic similarity comparison, clustering, triplet loss, contrastive loss, etc.\n\nApplication Examples\n\nYou can use this framework for:\n\n- **Computing Sentence Embeddings**\n\n  - [Dense Embeddings](https://www.sbert.net/examples/sentence_transformer/applications/computing-embeddings/README.html)\n  - [Sparse Embeddings](https://www.sbert.net/examples/sparse_encoder/applications/computing_embeddings/README.html)\n\n- **Semantic Textual Similarity**\n\n  - [Dense STS](https://www.sbert.net/docs/sentence_transformer/usage/semantic_textual_similarity.html)\n  - [Sparse STS](https://www.sbert.net/examples/sparse_encoder/applications/semantic_textual_similarity/README.html)\n\n- **Semantic Search**\n\n  - [Dense Search](https://www.sbert.net/examples/sentence_transformer/applications/semantic-search/README.html)\n  - [Sparse Search](https://www.sbert.net/examples/sparse_encoder/applications/semantic_search/README.html)\n\n- **Retrieve & Re-Rank**\n\n  - [Dense only Retrieval](https://www.sbert.net/examples/sentence_transformer/applications/retrieve_rerank/README.html)\n  - [Sparse/Dense/Hybrid Retrieval](https://www.sbert.net/examples/sentence_transformer/applications/retrieve_rerank/README.html)\n\n- [Clustering](https://www.sbert.net/examples/sentence_transformer/applications/clustering/README.html)\n\n- [Paraphrase Mining](https://www.sbert.net/examples/sentence_transformer/applications/paraphrase-mining/README.html)\n\n- [Translated Sentence Mining](https://www.sbert.net/examples/sentence_transformer/applications/parallel-sentence-mining/README.html)\n\n- [Multilingual Image Search, Clustering & Duplicate Detection](https://www.sbert.net/examples/sentence_transformer/applications/image-search/README.html)\n\nand many more use-cases.\n\nFor all examples, see [examples/sentence_transformer/applications](https://github.com/huggingface/sentence-transformers/tree/main/examples/sentence_transformer/applications).\n\nDevelopment setup\n\nAfter cloning the repo (or a fork) to your machine, in a virtual environment, run:\n\nTo test your changes, run:\n\nCiting & Authors\n\nIf you find this repository helpful, feel free to cite our publication [Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks](https://huggingface.co/papers/1908.10084):\n\nIf you use one of the multilingual models, feel free to cite our publication [Making Monolingual Sentence Embeddings Multilingual using Knowledge Distillation](https://huggingface.co/papers/2004.09813):\n\nPlease have a look at [Publications](https://www.sbert.net/docs/publications.html) for our different publications that are integrated into SentenceTransformers.\n\nMaintainers\n\nMaintainer: [Tom Aarsen](https://github.com/tomaarsen),  Hugging Face\n\nDon't hesitate to open an issue if something is broken (and it shouldn't be) or if you have further questions.\n\n---\n\nThis project was originally developed by the [Ubiquitous Knowledge Processing (UKP) Lab](https://www.ukp.tu-darmstadt.de/) at TU Darmstadt. We're grateful for their foundational work and continued contributions to the field.\n\n> This repository contains experimental software and is published for the sole purpose of giving additional background details on the respective publication.",
      "word_count": 745
    }
  ],
  "usage_description": "This library is used to compute embeddings for state-of-the-art models and calculate similarity scores using various encoder types. With this library, developers can easily implement applications such as semantic search and reranking in their projects."
}