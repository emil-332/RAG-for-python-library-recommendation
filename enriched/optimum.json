{
  "name": "optimum",
  "summary": "Optimum Library is an extension of the Hugging Face Transformers library, providing a framework to integrate third-party libraries from Hardware Partners and interface with their specific functionality.",
  "language": "python",
  "tags": [
    "cli",
    "math",
    "ml",
    "web"
  ],
  "chunks": [
    {
      "chunk_id": "optimum::chunk_0",
      "text": "Installation\n\nOptimum can be installed using `pip` as follows:\n\nIf you'd like to use the accelerator-specific features of Optimum, you can check the documentation and install the required dependencies according to the table below:\n\nAccelerator\n:----------------------------------------------------------------------------------\n[ONNX](https://huggingface.co/docs/optimum-onnx/en/index)\n[ONNX Runtime](https://huggingface.co/docs/optimum-onnx/onnxruntime/overview)\n[ONNX Runtime GPU](https://huggingface.co/docs/optimum-onnx/onnxruntime/overview)\n[Intel Neural Compressor](https://huggingface.co/docs/optimum/intel/index)\n[OpenVINO](https://huggingface.co/docs/optimum/intel/index)\n[IPEX](https://huggingface.co/docs/optimum/intel/index)\n[NVIDIA TensorRT-LLM](https://huggingface.co/docs/optimum/main/en/nvidia_overview)\n[AMD Instinct GPUs and Ryzen AI NPU](https://huggingface.co/docs/optimum/amd/index)\n[AWS Trainum & Inferentia](https://huggingface.co/docs/optimum-neuron/index)\n[Intel Gaudi Accelerators (HPU)](https://huggingface.co/docs/optimum/habana/index)\n[FuriosaAI](https://huggingface.co/docs/optimum/furiosa/index)\n\nThe `--upgrade --upgrade-strategy eager` option is needed to ensure the different packages are upgraded to the latest possible version.\n\nTo install from source:\n\nFor the accelerator-specific features, append `optimum[accelerator_type]` to the above command:\n\nAccelerated Inference\n\nOptimum provides multiple tools to export and run optimized models on various ecosystems:\n\nThe [export](https://huggingface.co/docs/optimum/exporters/overview) and optimizations can be done both programmatically and with a command line.\n\nONNX + ONNX Runtime\n\n ONNX integration was moved to [`optimum-onnx`](https://github.com/huggingface/optimum-onnx) so make sure to follow the installation instructions \n\nBefore you begin, make sure you have all the necessary libraries installed :\n\nIt is possible to export Transformers, Diffusers, Sentence Transformers and Timm models to the [ONNX](https://onnx.ai/) format and perform graph optimization as well as quantization easily.\n\nFor more information on the ONNX export, please check the [documentation](https://huggingface.co/docs/optimum-onnx/en/onnx/usage_guides/export_a_model).\n\nOnce the model is exported to the ONNX format, we provide Python classes enabling you to run the exported ONNX model in a seamless manner using [ONNX Runtime](https://onnxruntime.ai/) in the backend.\n\nFor this make sure you have ONNX Runtime installed, fore more information check out the [installation instructions](https://onnxruntime.ai/docs/install/).\n\nMore details on how to run ONNX models with `ORTModelForXXX` classes [here](https://huggingface.co/docs/optimum-onnx/en/onnxruntime/usage_guides/models).\n\nIntel (OpenVINO + Neural Compressor + IPEX)\n\nBefore you begin, make sure you have all the necessary [libraries installed](https://huggingface.co/docs/optimum/main/en/intel/installation).\n\nYou can find more information on the different integration in our [documentation](https://huggingface.co/docs/optimum/main/en/intel/index) and in the examples of [`optimum-intel`](https://github.com/huggingface/optimum-intel).\n\nExecuTorch\n\nBefore you begin, make sure you have all the necessary libraries installed :\n\nUsers can export Transformers models to [ExecuTorch](https://github.com/pytorch/executorch) and run inference on edge devices within PyTorch's ecosystem.\n\nFor more information about export Transformers to ExecuTorch, please check the doc for [Optimum-ExecuTorch](https://huggingface.co/docs/optimum-executorch/guides/export).\n\nQuanto\n\n[Quanto](https://github.com/huggingface/optimum-quanto) is a pytorch quantization backend which allows you to quantize a model either using the python API or the `optimum-cli`.\n\nYou can see more details and [examples](https://github.com/huggingface/optimum-quanto/tree/main/examples) in the [Quanto](https://github.com/huggingface/optimum-quanto) repository.\n\nAccelerated training\n\nOptimum provides wrappers around the original Transformers [Trainer](https://huggingface.co/docs/transformers/main_classes/trainer) to enable training on powerful hardware easily.\nWe support many providers:\n\n- [Intel Gaudi Accelerators (HPU)](https://huggingface.co/docs/optimum/main/en/habana/usage_guides/accelerate_training) enabling optimal performance on first-gen Gaudi, Gaudi2 and Gaudi3.\n- [AWS Trainium](https://huggingface.co/docs/optimum-neuron/training_tutorials/sft_lora_finetune_llm) for accelerated training on Trn1 and Trn1n instances.\n- ONNX Runtime (optimized for GPUs).\n\nIntel Gaudi Accelerators\n\nBefore you begin, make sure you have all the necessary libraries installed :\n\nYou can find examples in the [documentation](https://huggingface.co/docs/optimum/habana/quickstart) and in the [examples](https://github.com/huggingface/optimum-habana/tree/main/examples).\n\nAWS Trainium\n\nBefore you begin, make sure you have all the necessary libraries installed :\n\nYou can find examples in the [documentation](https://huggingface.co/docs/optimum-neuron/index) and in the [tutorials](https://huggingface.co/docs/optimum-neuron/tutorials/fine_tune_bert).",
      "word_count": 473
    }
  ],
  "usage_description": "Here is a 2-sentence summary of what a developer can achieve with this library:\n\nThis library is used to integrate third-party hardware libraries from partner companies, enabling developers to harness the unique functionality and capabilities of these accelerators in their projects. With Optimum, developers can tap into specialized hardware features such as GPU acceleration, neural compression, and more, to optimize performance and efficiency in AI and machine learning applications."
}