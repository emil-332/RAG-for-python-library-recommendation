{
  "name": "optuna",
  "summary": "A hyperparameter optimization framework",
  "language": "python",
  "tags": [
    "math",
    "ml",
    "visualization",
    "web"
  ],
  "chunks": [
    {
      "chunk_id": "optuna::chunk_0",
      "text": "Optuna: A hyperparameter optimization framework\n\n(https://www.python.org)\n(https://pypi.python.org/pypi/optuna)\n(https://anaconda.org/conda-forge/optuna)\n(https://github.com/optuna/optuna)\n(https://optuna.readthedocs.io/en/stable/)\n(https://codecov.io/gh/optuna/optuna)\n\n:link: [**Website**](https://optuna.org/)\n:page_with_curl: [**Docs**](https://optuna.readthedocs.io/en/stable/)\n:gear: [**Install Guide**](https://optuna.readthedocs.io/en/stable/installation.html)\n:pencil: [**Tutorial**](https://optuna.readthedocs.io/en/stable/tutorial/index.html)\n:bulb: [**Examples**](https://github.com/optuna/optuna-examples)\n[**Twitter**](https://twitter.com/OptunaAutoML)\n[**LinkedIn**](https://www.linkedin.com/showcase/optuna/)\n[**Medium**](https://medium.com/optuna)\n\n*Optuna* is an automatic hyperparameter optimization software framework, particularly designed\nfor machine learning. It features an imperative, *define-by-run* style user API. Thanks to our\n*define-by-run* API, the code written with Optuna enjoys high modularity, and the user of\nOptuna can dynamically construct the search spaces for the hyperparameters.\n\n:loudspeaker: News\nHelp us create the next version of Optuna!\n\nOptuna 5.0 Roadmap published for review. Please take a look at [the planned improvements to Optuna](https://medium.com/optuna/optuna-v5-roadmap-ac7d6935a878), and share your feedback in [the github issues](https://github.com/optuna/optuna/labels/v5). PR contributions also welcome!\n\nPlease take a few minutes to fill in [this survey](https://forms.gle/wVwLCQ9g6st6AXuq9), and let us know how you use Optuna now and what improvements you'd like.\nAll questions are optional. ‍♂️\n\n* **Oct 28, 2025**: A new article [AutoSampler: Full Support for Multi-Objective & Constrained Optimization](https://medium.com/optuna/autosampler-full-support-for-multi-objective-constrained-optimization-c1c4fc957ba2) has been published.\n* **Sep 22, 2025**: A new article [[Optuna v4.5] Gaussian Process-Based Sampler (GPSampler) Can Now Perform Constrained Multi-Objective Optimization](https://medium.com/optuna/optuna-v4-5-81e78d8e077a) has been published.\n* **Jun 16, 2025**: Optuna 4.4.0 has been released! Check out [the release blog](https://medium.com/optuna/announcing-optuna-4-4-ece661493126).\n* **May 26, 2025**: Optuna 5.0 roadmap has been published! See [the blog](https://medium.com/optuna/optuna-v5-roadmap-ac7d6935a878) for more details.\n* **Apr 14, 2025**: Optuna 4.3.0 is out! Check out [the release note](https://github.com/optuna/optuna/releases/tag/v4.3.0) for details.\n* **Mar 24, 2025**: A new article [Distributed Optimization in Optuna and gRPC Storage Proxy](https://medium.com/optuna/distributed-optimization-in-optuna-and-grpc-storage-proxy-08db83f1d608) has been published.\n\n:fire: Key Features\n\nOptuna has modern functionalities as follows:\n\n- [Lightweight, versatile, and platform agnostic architecture](https://optuna.readthedocs.io/en/stable/tutorial/10_key_features/001_first.html)\n  - Handle a wide variety of tasks with a simple installation that has few requirements.\n- [Pythonic search spaces](https://optuna.readthedocs.io/en/stable/tutorial/10_key_features/002_configurations.html)\n  - Define search spaces using familiar Python syntax including conditionals and loops.\n- [Efficient optimization algorithms](https://optuna.readthedocs.io/en/stable/tutorial/10_key_features/003_efficient_optimization_algorithms.html)\n  - Adopt state-of-the-art algorithms for sampling hyperparameters and efficiently pruning unpromising trials.\n- [Easy parallelization](https://optuna.readthedocs.io/en/stable/tutorial/10_key_features/004_distributed.html)\n  - Scale studies to tens or hundreds of workers with little or no changes to the code.\n- [Quick visualization](https://optuna.readthedocs.io/en/stable/tutorial/10_key_features/005_visualization.html)\n  - Inspect optimization histories from a variety of plotting functions.\n\nBasic Concepts\n\nWe use the terms *study* and *trial* as follows:\n\n- Study: optimization based on an objective function\n- Trial: a single execution of the objective function\n\nPlease refer to the sample code below. The goal of a *study* is to find out the optimal set of\nhyperparameter values (e.g., `regressor` and `svr_c`) through multiple *trials* (e.g.,\n`n_trials=100`). Optuna is a framework designed for automation and acceleration of\noptimization *studies*.\n\nSample code with scikit-learn\n\n(http://colab.research.google.com/github/optuna/optuna-examples/blob/main/quickstart.ipynb)\n\n> [!NOTE]\n> More examples can be found in [optuna/optuna-examples](https://github.com/optuna/optuna-examples).\n>\n> The examples cover diverse problem setups such as multi-objective optimization, constrained optimization, pruning, and distributed optimization.\n\nInstallation\n\nOptuna is available at [the Python Package Index](https://pypi.org/project/optuna/) and on [Anaconda Cloud](https://anaconda.org/conda-forge/optuna).\n\n> [!IMPORTANT]\n> Optuna supports Python 3.9 or newer.\n>\n> Also, we provide Optuna docker images on [DockerHub](https://hub.docker.com/r/optuna/optuna).\n\nIntegrations\n\nOptuna has integration features with various third-party libraries. Integrations can be found in [optuna/optuna-integration](https://github.com/optuna/optuna-integration) and the document is available [here](https://optuna-integration.readthedocs.io/en/stable/index.html).\n\nSupported integration libraries\n\n* [Catboost](https://github.com/optuna/optuna-examples/tree/main/catboost/catboost_pruning.py)\n* [Dask](https://github.com/optuna/optuna-examples/tree/main/dask/dask_simple.py)\n* [fastai](https://github.com/optuna/optuna-examples/tree/main/fastai/fastai_simple.py)\n* [Keras](https://github.com/optuna/optuna-examples/tree/main/keras/keras_integration.py)\n* [LightGBM](https://github.com/optuna/optuna-examples/tree/main/lightgbm/lightgbm_integration.py)\n* [MLflow](https://github.com/optuna/optuna-examples/tree/main/mlflow/keras_mlflow.py)\n* [PyTorch](https://github.com/optuna/optuna-examples/tree/main/pytorch/pytorch_simple.py)\n* [PyTorch Ignite](https://github.com/optuna/optuna-examples/tree/main/pytorch/pytorch_ignite_simple.py)\n* [PyTorch Lightning](https://github.com/optuna/optuna-examples/tree/main/pytorch/pytorch_lightning_simple.py)\n* [TensorBoard](https://github.com/optuna/optuna-examples/tree/main/tensorboard/tensorboard_simple.py)\n* [TensorFlow](https://github.com/optuna/optuna-examples/tree/main/tensorflow/tensorflow_estimator_integration.py)\n* [tf.keras](https://github.com/optuna/optuna-examples/tree/main/tfkeras/tfkeras_integration.py)\n* [Weights & Biases](https://github.com/optuna/optuna-examples/tree/main/wandb/wandb_integration.py)\n* [XGBoost](https://github.com/optuna/optuna-examples/tree/main/xgboost/xgboost_integration.py)\n\nWeb Dashboard\n\n[Optuna Dashboard](https://github.com/optuna/optuna-dashboard) is a real-time web dashboard for Optuna.\nYou can check the optimization history, hyperparameter importance, etc. in graphs and tables.\nYou don't need to create a Python script to call [Optuna's visualization](https://optuna.readthedocs.io/en/stable/reference/visualization/index.html) functions.\nFeature requests and bug reports are welcome!\n\n`optuna-dashboard` can be installed via pip:\n\n> [!TIP]\n> Please check out the convenience of Optuna Dashboard using the sample code below.\n\nSample code to launch Optuna Dashboard\n\nSave the following code as `optimize_toy.py`.\n\nThen try the commands below:\n\nOptunaHub\n\n[OptunaHub](https://hub.optuna.org/) is a feature-sharing platform for Optuna.\nYou can use the registered features and publish your packages.\n\nUse registered features\n\n`optunahub` can be installed via pip:\n\nYou can load registered module with `optunahub.load_module`.\n\nFor more details, please refer to [the optunahub documentation](https://optuna.github.io/optunahub/).\n\nPublish your packages\n\nYou can publish your package via [optunahub-registry](https://github.com/optuna/optunahub-registry).\nSee the [Tutorials for Contributors](https://optuna.github.io/optunahub/tutorials_for_contributors.html) in OptunaHub.\n\nCommunication\n\n- [GitHub Discussions] for questions.\n- [GitHub Issues] for bug reports and feature requests.\n\nContribution\n\nAny contributions to Optuna are more than welcome!\n\nIf you are new to Optuna, please check the [good first issues](https://github.com/optuna/optuna/labels/good%20first%20issue). They are relatively simple, well-defined, and often good starting points for you to get familiar with the contribution workflow and other developers.\n\nIf you already have contributed to Optuna, we recommend the other [contribution-welcome issues](https://github.com/optuna/optuna/labels/contribution-welcome).\n\nFor general guidelines on how to contribute to the project, take a look at [CONTRIBUTING.md](./CONTRIBUTING.md).\n\nReference\n\nIf you use Optuna in one of your research projects, please cite [our KDD paper](https://doi.org/10.1145/3292500.3330701) \"Optuna: A Next-generation Hyperparameter Optimization Framework\":\n\nBibTeX\n\nLicense\n\nMIT License (see [LICENSE](./LICENSE)).\n\nOptuna uses the codes from SciPy and fdlibm projects (see [LICENSE_THIRD_PARTY](./LICENSE_THIRD_PARTY)).",
      "word_count": 799
    }
  ],
  "usage_description": "This library is used to automatically optimize the hyperparameters of machine learning models, allowing developers to improve their model's performance without manually tuning parameters. With Optuna, developers can easily find the optimal combination of hyperparameters for their specific use case, streamlining the development and deployment process."
}