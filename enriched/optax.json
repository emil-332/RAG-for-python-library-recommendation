{
  "name": "optax",
  "summary": "A gradient processing and optimization library in JAX.",
  "language": "python",
  "tags": [
    "dev",
    "math",
    "ml",
    "web"
  ],
  "chunks": [
    {
      "chunk_id": "optax::chunk_0",
      "text": "Optax\n\n(http://optax.readthedocs.io)\n\nIntroduction\n\nOptax is a gradient processing and optimization library for JAX.\n\nOptax is designed to facilitate research by providing building blocks\nthat can be easily recombined in custom ways.\n\nOur goals are to\n\n*   Provide simple, well-tested, efficient implementations of core components.\n*   Improve research productivity by enabling to easily combine low-level\n*   Accelerate adoption of new ideas by making it easy for anyone to contribute.\n\nWe favor focusing on small composable building blocks that can be effectively\ncombined into custom solutions. Others may build upon these basic components\nin more complicated abstractions. Whenever reasonable, implementations prioritize\nreadability and structuring code to match standard equations, over code reuse.\n\nAn initial prototype of this library was made available in JAX's experimental\nfolder as `jax.experimental.optix`. Given the wide adoption across DeepMind\nof `optix`, and after a few iterations on the API, `optix` was eventually moved\nout of `experimental` as a standalone open-source library, and renamed `optax`.\n\nDocumentation on Optax can be found at [optax.readthedocs.io](https://optax.readthedocs.io/).\n\nInstallation\n\nYou can install the latest released version of Optax from PyPI via:\n\nor you can install the latest development version from GitHub:\n\nQuickstart\n\nOptax contains implementations of [many popular optimizers](https://optax.readthedocs.io/en/latest/api/optimizers.html) and\n[loss functions](https://optax.readthedocs.io/en/latest/api/losses.html).\nFor example, the following code snippet uses the Adam optimizer from `optax.adam`\nand the mean squared error from `optax.l2_loss`. We initialize the optimizer\nstate using the `init` function and `params` of the model.\n\nTo write the update loop we need a loss function that can be differentiated by\nJax (with `jax.grad` in this\nexample) to obtain the gradients.\n\nThe gradients are then converted via `optimizer.update` to obtain the updates\nthat should be applied to the current parameters to obtain the new ones.\n`optax.apply_updates` is a convenience utility to do this.\n\nYou can continue the quick start in [the Optax  Getting started notebook.](https://github.com/google-deepmind/optax/blob/main/docs/getting_started.ipynb)\n\nDevelopment\n\nWe welcome new contributors.\n\nSource code\n\nYou can check the latest sources with the following command.\n\nTesting\n\nTo run the tests, please execute the following script.\n\nDocumentation\n\nTo build the documentation, first ensure that all the dependencies are installed.\n\nThen, execute the following.\n\nBenchmarks\nIf you feel lost in the crowd of available optimizers for deep learning, there\nexist some extensive benchmarks:\n\n[Benchmarking Neural Network Training Algorithms, Dahl G. et al, 2023](https://arxiv.org/pdf/2306.07179),\n\n[Descending through a Crowded Valley â€” Benchmarking Deep Learning Optimizers, Schmidt R. et al, 2021](https://proceedings.mlr.press/v139/schmidt21a).\n\nIf you are interested in developing your own benchmark for some tasks,\nconsider the following framework\n\n[Benchopt: Reproducible, efficient and collaborative optimization benchmarks, Moreau T. et al, 2022](https://arxiv.org/abs/2206.13424).\n\nFinally, if you are searching for some recommendations on tuning optimizers,\nconsider taking a look at\n\n[Deep Learning Tuning Playbook, Godbole V. et al, 2023](https://github.com/google-research/tuning_playbook).\n\nCiting Optax\n\nThis repository is part of the DeepMind JAX Ecosystem, to cite Optax\nplease use the citation:",
      "word_count": 459
    }
  ],
  "usage_description": "This library is used to streamline the development of custom optimization algorithms by providing a set of well-tested and efficient building blocks for gradient processing in JAX, enabling researchers to easily combine low-level components into customized solutions. With optax, developers can focus on implementing novel ideas without having to reinvent basic components, accelerating the adoption of new research concepts and improving overall productivity."
}