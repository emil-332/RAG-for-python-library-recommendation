{
  "name": "stringzilla",
  "summary": "Search, hash, sort, and process strings faster via SWAR and SIMD",
  "language": "python",
  "tags": [
    "data",
    "dev",
    "math",
    "ui",
    "web"
  ],
  "chunks": [
    {
      "chunk_id": "stringzilla::chunk_0",
      "text": "StringZilla\n\nStrings are the first fundamental data type every programming language implements in software rather than hardware, so dedicated CPU instructions are rare - and the few that exist are hardly ideal.\nThat's why most languages lean on the C standard library (libc) for their string operations, which, despite its name, ships its hottest code in hand-tuned assembly.\nIt does exploit SIMD, but it isn't perfect.\n1️⃣ Even on ubiquitous hardware - over a billion 64-bit ARM CPUs - routines such as `strstr` and `memmem` top out at roughly one-third of available throughput.\n2️⃣ SIMD coverage is uneven: fast forward scans don't guarantee speedy reverse searches, hashing and case-mapping is not even part of the standard.\n3️⃣ Many higher-level languages can't rely on libc at all because their strings aren't NUL-terminated - or may even contain embedded zeroes.\nThat's why StringZilla exists: predictable, high performance on every modern platform, OS, and programming language.\n\n(https://github.com/ashvardanian/stringzilla)\n(https://crates.io/crates/stringzilla)\n\n(https://github.com/ashvardanian/StringZilla/actions/workflows/release.yml)\n(https://github.com/ashvardanian/StringZilla/actions/workflows/release.yml)\n(https://github.com/ashvardanian/StringZilla/actions/workflows/release.yml)\n-->\n\nStringZilla is the GodZilla of string libraries, using [SIMD][faq-simd] and [SWAR][faq-swar] to accelerate binary and UTF-8 string operations on modern CPUs and GPUs.\nIt delivers up to __10x higher CPU throughput in C, C++, Rust, Python__, and other languages, and can be __100x faster than existing GPU kernels__, covering a broad range of functionality.\nIt __accelerates exact and fuzzy string matching, hashing, edit distance computations, sorting, provides allocation-free lazily-evaluated smart-iterators, and even random-string generators__.\n\n-  __[C](#basic-usage-with-c-99-and-newer):__ Upgrade LibC's `` to ``  in C 99\n-  __[C++](#basic-usage-with-c-11-and-newer):__ Upgrade STL's `` to `` in C++ 11\n-  __[CUDA](#cuda):__ Process in-bulk with `` in CUDA C++ 17\n-  __[Python](#quick-start-python):__ Upgrade your `str` to faster `Str`\n-  __[Rust](#quick-start-rust):__ Use the `StringZilla` traits crate\n-  __[Go](#quick-start-golang):__ Use the `StringZilla` cGo module\n-  __[Swift](#quick-start-swift):__ Use the `String+StringZilla` extension\n-  __[JavaScript](#quick-start-javascript):__ Use the `StringZilla` library\n-  __[Shell][faq-shell]__: Accelerate common CLI tools with `sz-` prefix\n-  Researcher? Jump to [Algorithms & Design Decisions](#algorithms--design-decisions)\n-  Thinking to contribute? Look for [\"good first issues\"][first-issues]\n-  And check the [guide](https://github.com/ashvardanian/StringZilla/blob/main/CONTRIBUTING.md) to set up the environment\n- Want more bindings or features? Let [me](https://github.com/ashvardanian) know!\n\n__Who is this for?__\n\n- For data-engineers parsing large datasets, like the [CommonCrawl](https://commoncrawl.org/), [RedPajama](https://github.com/togethercomputer/RedPajama-Data), or [LAION](https://laion.ai/blog/laion-5b/).\n- For software engineers optimizing strings in their apps and services.\n- For bioinformaticians and search engineers looking for edit-distances for [USearch](https://github.com/unum-cloud/usearch).\n- For [DBMS][faq-dbms] devs, optimizing `LIKE`, `ORDER BY`, and `GROUP BY` operations.\n- For hardware designers, needing a SWAR baseline for string-processing functionality.\n- For students studying SIMD/SWAR applications to non-data-parallel operations.\n\nPerformance\n\nMost StringZilla modules ship ready-to-run benchmarks for C, C++, Python, and more.\nGrab them from `./scripts`, and see [`CONTRIBUTING.md`](CONTRIBUTING.md) for instructions.\nOn CPUs that permit misaligned loads, even the 64-bit SWAR baseline outruns both libc and the STL.\nFor wider head-to-heads against Rust and Python favorites, browse the __[StringWars][stringwars]__ repository.\nTo inspect collision resistance and distribution shapes for our hashers, see __[HashEvals][hashevals]__.",
      "word_count": 475
    },
    {
      "chunk_id": "stringzilla::chunk_1",
      "text": "> Most benchmarks were conducted on a 1 GB English text corpus, with an average word length of 6 characters.\n> The code was compiled with GCC 12, using `glibc` v2.35.\n> The benchmarks were performed on Arm-based Graviton3 AWS `c7g` instances and `r7iz` Intel Sapphire Rapids.\n> Most modern Arm-based 64-bit CPUs will have similar relative speedups.\n> Variance within x86 CPUs will be larger.\n> For CUDA benchmarks, the Nvidia H100 GPUs were used.\n> 1 Unlike other libraries, LibC requires strings to be NULL-terminated.\n> 2 Six whitespaces in the ASCII set are: ` \\t\\n\\v\\f\\r`. Python's and other standard libraries have specialized functions for those.\n> 3 All modulo operations were conducted with `uint8_t` to allow compilers more optimization opportunities.\n> The C++ STL and StringZilla benchmarks used a 64-bit [Mersenne Twister][faq-mersenne-twister] as the generator.\n> For C, C++, and StringZilla, an in-place update of the string was used.\n> In Python every string had to be allocated as a new object, which makes it less fair.\n> 4 Contrary to the popular opinion, Python's default `sorted` function works faster than the C and C++ standard libraries.\n> That holds for large lists or tuples of strings, but fails as soon as you need more complex logic, like sorting dictionaries by a string key, or producing the \"sorted order\" permutation.\n> The latter is very common in database engines and is most similar to `numpy.argsort`.\n> The current StringZilla solution can be at least 4x faster without loss of generality.\n> 5 Most Python libraries for strings are also implemented in C.\n> 6 Unlike the rest of BioPython, the alignment score computation is [implemented in C](https://github.com/biopython/biopython/blob/master/Bio/Align/_pairwisealigner.c).\n\nFunctionality\n\nStringZilla is compatible with most modern CPUs, and provides a broad range of functionality.\nIt's split into 2 layers:\n\n1. StringZilla: single-header C library and C++ wrapper for high-performance string operations.\n2. StringZillas: parallel CPU/GPU backends used for large-batch operations and accelerators.\n\nHaving a second C++/CUDA layer greatly simplifies the implementation of similarity scoring and fingerprinting functions, which would otherwise require too much error-prone boilerplate code in pure C.\nBoth layers are designed to be extremely portable:\n\n- [x] across both little-endian and big-endian architectures.\n- [x] across 32-bit and 64-bit hardware architectures.\n- [x] across operating systems and compilers.\n- [x] across ASCII and UTF-8 encoded inputs.\n\nNot all features are available across all bindings.\nConsider contributing if you need a feature that's not yet implemented.\n\nMaturity\n:-----------------------------\nSubstring Search\nCharacter Set Search\nSorting & Sequence Operations\nLazy Ranges, Compressed Arrays\nOne-Shot & Streaming Hashes\nCryptographic Hashes\nSmall String Class\nRandom String Generation\nUnicode Case Folding\nCase-Insensitive UTF-8 Search\nTR29 Word Boundary Detection\nParallel Similarity Scoring\nParallel Rolling Fingerprints\n\n>  parts are used in production.\n>  parts are in beta.\n>  parts are under active development, and are likely to break in subsequent releases.\n>  are implemented.\n> ⚪ are considered.\n>  are not intended.\n\nQuick Start: Python",
      "word_count": 490
    },
    {
      "chunk_id": "stringzilla::chunk_2",
      "text": "Python bindings are available on PyPI for Python 3.8+, and can be installed with `pip`.\n\nYou can immediately check the installed version and the used hardware capabilities with following commands:\n\nBasic Usage\n\nIf you've ever used the Python `str`, `bytes`, `bytearray`, or `memoryview` classes, you'll know what to expect.\nStringZilla's `Str` class is a hybrid of the above, providing a `str`-like interface to byte arrays.\n\nThe `File` class memory-maps a file from persistent storage without loading its copy into RAM.\nThe contents of that file would remain immutable, and the mapping can be shared by multiple Python processes simultaneously.\nA standard dataset pre-processing use case would be to map a sizable textual dataset like Common Crawl into memory, spawn child processes, and split the job between them.\n\nBasic Operations\n\n- Length: `len(text) -> int`\n- Indexing: `text[42] -> str`\n- Slicing: `text[42:46] -> Str`\n- Substring check: `'substring' in text -> bool`\n- Hashing: `hash(text) -> int`\n- String conversion: `str(text) -> str`\n\nAdvanced Operations\n\nIt's important to note that the last function's behavior is slightly different from Python's `str.splitlines`.\nThe [native version][faq-splitlines] matches `\\n`, `\\r`, `\\v` or `\\x0b`, `\\f` or `\\x0c`, `\\x1c`, `\\x1d`, `\\x1e`, `\\x85`, `\\r\\n`, `\\u2028`, `\\u2029`, including 3x two-byte-long runes.\nThe StringZilla version matches only `\\n`, `\\v`, `\\f`, `\\r`, `\\x1c`, `\\x1d`, `\\x1e`, `\\x85`, avoiding two-byte-long runes.\n\nCharacter Set Operations\n\nPython strings don't natively support character set operations.\nThis forces people to use regular expressions, which are slow and hard to read.\nTo avoid the need for `re.finditer`, StringZilla provides the following interfaces:\n\nStringZilla also provides string trimming functions and random string generation:\n\nYou can also transform the string using Look-Up Tables (LUTs), mapping it to a different character set.\nThis would result in a copy - `str` for `str` inputs and `bytes` for other types.\n\nFor efficiency reasons, pass the LUT as a string or bytes object, not as a dictionary.\nThis can be useful in high-throughput applications dealing with binary data, including bioinformatics and image processing.\nHere is an example:\n\nHash\n\nSingle-shot and incremental hashing are both supported:\n\nSHA-256 Checksums\n\nSHA-256 cryptographic checksums are also available for single-shot and incremental hashing:\n\nStringZilla integrates seamlessly with memory-mapped files for efficient large file processing.\nThe traditional approach with `hashlib`:\n\nCan be simplified with StringZilla:\n\nBoth output the same digest: `7278165ce01a4ac1e8806c97f32feae908036ca3d910f5177d2cf375e20aeae1`.\nOpenSSL (powering `hashlib`) has faster Assembly kernels, but StringZilla avoids file I/O overhead with memory mapping and skips Python's abstraction layers:\n\n- OpenSSL-backed `hashlib.sha256`: 12.6s\n- StringZilla end-to-end: 4.0s — __3× faster!__\n\nUnicode Case-Folding and Case-Insensitive Search\n\nStringZilla implements both Unicode Case Folding and Case-Insensitive UTF-8 Search.\nUnlike most libraries only capable of lower-casing ASCII-represented English alphabet, StringZilla covers over 1M+ codepoints.\nThe case-folding API expects the output buffer to be at least 3× larger than the input, to accommodate for the worst-case character expansions scenarios.\n\nThe case-insensitive search returns the byte offset of the match, handling expansions correctly.\n\nCollection-Level Operations",
      "word_count": 483
    },
    {
      "chunk_id": "stringzilla::chunk_3",
      "text": "Once split into a `Strs` object, you can sort, shuffle, and reorganize the slices with minimal memory footprint.\nIf all the chunks are located in consecutive memory regions, the memory overhead can be as low as 4 bytes per chunk.\n\nWorking on [RedPajama][redpajama], addressing 20 billion annotated English documents, one will need only 160 GB of RAM instead of terabytes.\nOnce loaded, the data will be memory-mapped, and can be reused between multiple Python processes without copies.\nAnd of course, you can use slices to navigate the dataset and shard it between multiple workers.\n\nIterators and Memory Efficiency\n\nPython's operations like `split()` and `readlines()` immediately materialize a `list` of copied parts.\nThis can be very memory-inefficient for large datasets.\nStringZilla saves a lot of memory by viewing existing memory regions as substrings, but even more memory can be saved by using lazily evaluated iterators.\n\nStringZilla can easily be 10x more memory efficient than native Python classes for tokenization.\nWith lazy operations, it practically becomes free.\n\nLow-Level Python API\n\nAside from calling the methods on the `Str` and `Strs` classes, you can also call the global functions directly on `str` and `bytes` instances.\nAssuming StringZilla CPython bindings are implemented [without any intermediate tools like SWIG or PyBind](https://ashvardanian.com/posts/pybind11-cpython-tutorial/), the call latency should be similar to native classes.\n\nSimilarity Scores\n\nStringZilla exposes high-performance, batch-oriented similarity via the `stringzillas` module. \nUse `DeviceScope` to pick hardware and optionally limit capabilities per engine.\n\nNote, that this computes byte-level distances.\nFor UTF-8 codepoints, use a different engine class:\n\nFor alignment scoring provide a 256×256 substitution matrix using NumPy:\n\nSeveral Python libraries provide edit distance computation.\nMost are implemented in C but may be slower than StringZilla on large inputs.\nFor proteins ~10k chars, 100 pairs:\n\n- [JellyFish](https://github.com/jamesturk/jellyfish): 62.3s\n- [EditDistance](https://github.com/roy-ht/editdistance): 32.9s\n- StringZilla: __0.8s__\n\nUsing the same proteins for Needleman-Wunsch alignment scores:\n\n- [BioPython](https://github.com/biopython/biopython): 25.8s\n- StringZilla: __7.8s__\n\n§ Example converting from BioPython to StringZilla.\n\nRolling Fingerprints\n\nMinHashing is a common technique for Information Retrieval, producing compact representations of large documents.\nFor $D$ hash-functions and a text of length $L$, in the worst case it involves computing $O(D \\cdot L)$ hashes.\n\nSerialization\n\nFilesystem\n\nSimilar to how `File` can be used to read a large file, other interfaces can be used to dump strings to disk faster.\nThe `Str` class has `write_to` to write the string to a file, and `offset_within` to obtain integer offsets of substring view in larger string for navigation.\n\nPyArrow\n\nA `Str` is easy to cast to [PyArrow](https://arrow.apache.org/docs/python/arrays.html#string-and-binary-types) buffers.\n\nAnd only slightly harder to convert in reverse direction:\n\nThat means you can convert `Str` to `pyarrow.Buffer` and `Strs` to `pyarrow.Array` without extra copies.\nFor more details on the tape-like layouts, refer to the [StringTape](https://github.com/ashvardanian/StringTape) repository.\n\nQuick Start: C/C++\n\nThe C library is header-only, so you can just copy the `stringzilla.h` header into your project.\nSame applies to C++, where you would copy the `stringzilla.hpp` header.\nAlternatively, add it as a submodule, and include it in your build system.\n\nOr using a pure CMake approach:",
      "word_count": 500
    },
    {
      "chunk_id": "stringzilla::chunk_4",
      "text": "Last, but not the least, you can also install it as a library, and link against it.\nThis approach is worse for inlining, but brings [dynamic runtime dispatch](#dynamic-dispatch) for the most advanced CPU features.\n\nBasic Usage with C 99 and Newer\n\nThere is a stable C 99 interface, where all function names are prefixed with `sz_`.\nMost interfaces are well documented, and come with self-explanatory names and examples.\nIn some cases, hardware specific overloads are available, like `sz_find_skylake` or `sz_find_neon`.\nBoth are companions of the `sz_find`, first for x86 CPUs with AVX-512 support, and second for Arm NEON-capable CPUs.\n\n§ Mapping from LibC to StringZilla.\n\nBy design, StringZilla has a couple of notable differences from LibC:\n\n1. all strings are expected to have a length, and are not necessarily null-terminated.\n2. every operations has a reverse order counterpart.\n\nThat way `sz_find` and `sz_rfind` are similar to `strstr` and `strrstr` in LibC.\nSimilarly, `sz_find_byte` and `sz_rfind_byte` replace `memchr` and `memrchr`.\nThe `sz_find_byteset` maps to `strspn` and `strcspn`, while `sz_rfind_byteset` has no sibling in LibC.\n\nBasic Usage with C++ 11 and Newer\n\nThere is a stable C++ 11 interface available in the `ashvardanian::stringzilla` namespace.\nIt comes with two STL-like classes: `string_view` and `string`.\nThe first is a non-owning view of a string, and the second is a mutable string with a [Small String Optimization][faq-sso].\n\nStringZilla also provides string literals for automatic type resolution, [similar to STL][stl-literal]:\n\nUnicode Case-Folding and Case-Insensitive Search\n\nStringZilla implements both Unicode Case Folding and Case-Insensitive UTF-8 Search.\nUnlike most libraries only capable of lower-casing ASCII-represented English alphabet, StringZilla covers over 1M+ codepoints.\nThe case-folding API expects the output buffer to be at least 3× larger than the input, to accommodate for the worst-case character expansions scenarios.\n\nThe case-insensitive search API returns a pointer to the start of the first relevant glyph in the haystack, or `NULL` if not found.\nIt outputs the length of the matched haystack substring in bytes, and accepts a metadata structure to speed up repeated searches for the same needle.\n\nSame functionality is available in C++:\n\nSimilarity Scores\n\nStringZilla exposes high-performance, batch-oriented similarity via the `stringzillas/stringzillas.h` header. \nUse `szs_device_scope_t` to pick hardware and optionally limit capabilities per engine.\n\nTo target a different device, use the appropriate `szs_device_scope_init_{cpu_cores,gpu_device}` function.\nWhen dealing with GPU backends, make sure to use the \"unified memory\" allocators exposed as `szs_unified_{alloc,free}`.\nSimilar stable C ABIs are exposed for other workloads as well.\n\n- UTF-8: `szs_levenshtein_distances_utf8_{sequence,u32tape,u64tape}`\n- Needleman-Wunsch: `szs_needleman_wunsch_scores_{sequence,u32tape,u64tape}`\n- Smith-Waterman: `szs_smith_waterman_scores_{sequence,u32tape,u64tape}`\n\nMoreover, in C++ codebases one can tap into the raw templates implementing that functionality, customizing them with custom executors, SIMD plugins, etc.\nFor that include `stringzillas/similarities.hpp` for C++ and `stringzillas/similarities.cuh` for CUDA.\n\nAll of the potentially failing StringZillas' interfaces return error codes, and none raise C++ exceptions.\nParallelism is enabled at both collection-level and within individual pairs of large inputs.\n\nRolling Fingerprints\n\nStringZilla exposes parallel fingerprinting (Min-Hashes or Count-Min-Sketches) via the `stringzillas/stringzillas.h` header. \nUse `szs_device_scope_t` to pick hardware and optionally limit capabilities per engine.",
      "word_count": 494
    },
    {
      "chunk_id": "stringzilla::chunk_5",
      "text": "Moreover, in C++ codebases one can tap into the raw templates implementing that functionality, customizing them with custom executors, SIMD plugins, etc.\nFor that include `stringzillas/fingerprints.hpp` for C++ and `stringzillas/fingerprints.cuh` for CUDA.\n\nCUDA\n\nStringZilla provides CUDA C++ templates for composable string batch-processing operations.\nDifferent GPUs have varying warp sizes, shared memory capacities, and register counts, affecting algorithm selection, so it's important to query the `gpu_specs_t` via `gpu_specs_fetch`.\nFor memory management, ensure that you use GPU-visible' unified memory` exposed in an STL-compatible manner as a `unified_alloc` template class.\nFor error handling, `cuda_status_t` extends the traditional `status_t` with GPU-specific information.\nIt's implicitly convertible to `status_t`, so you can use it in places expecting a `status_t`.\n\nMost algorithms can load-balance both a large number of small strings and a small number of large strings.\nStill, with large H100-scale GPUs, it's best to submit thousands of inputs at once.\n\nMemory Ownership and Small String Optimization\n\nMost operations in StringZilla don't assume any memory ownership.\nBut in addition to the read-only search-like operations StringZilla provides a minimalistic C and C++ implementations for a memory owning string \"class\".\nLike other efficient string implementations, it uses the [Small String Optimization][faq-sso] (SSO) to avoid heap allocations for short strings.\n\nAs one can see, a short string can be kept on the stack, if it fits within `internal.chars` array.\nBefore 2015 GCC string implementation was just 8 bytes, and could only fit 7 characters.\nDifferent STL implementations today have different thresholds for the Small String Optimization.\nSimilar to GCC, StringZilla is 32 bytes in size, and similar to Clang it can fit 22 characters on stack.\nOur layout might be preferential, if you want to avoid branches.\nIf you use a different compiler, you may want to check its SSO buffer size with a [simple Gist](https://gist.github.com/ashvardanian/c197f15732d9855c4e070797adf17b21).\n\n`libstdc++` in  GCC 13\n:--------------\nString `sizeof`\nInner Capacity\n\nThis design has been since ported to many high-level programming languages.\nSwift, for example, [can store 15 bytes](https://developer.apple.com/documentation/swift/substring/withutf8(_:)#discussion) in the `String` instance itself.\nStringZilla implements SSO at the C level, providing the `sz_string_t` union and a simple API for primary operations.\n\nUnlike the conventional C strings, the `sz_string_t` is allowed to contain null characters.\nTo safely print those, pass the `string_length` to `printf` as well.\n\nWhat's Wrong with the C Standard Library?\n\nStringZilla is not a drop-in replacement for the C Standard Library.\nIt's designed to be a safer and more modern alternative.\nConceptually:\n\n1. LibC strings are expected to be null-terminated, so to use the efficient LibC implementations on slices of larger strings, you'd have to copy them, which is more expensive than the original string operation.\n2. LibC functionality is asymmetric - you can find the first and the last occurrence of a character within a string, but you can't find the last occurrence of a substring.\n3. LibC function names are typically very short and cryptic.\n4. LibC lacks crucial functionality like hashing and doesn't provide primitives for less critical but relevant operations like fuzzy matching.",
      "word_count": 495
    },
    {
      "chunk_id": "stringzilla::chunk_6",
      "text": "Something has to be said about its support for UTF-8.\nAside from a single-byte `char` type, LibC provides `wchar_t`:\n\n- The size of `wchar_t` is not consistent across platforms. On Windows, it's typically 16 bits (suitable for UTF-16), while on Unix-like systems, it's usually 32 bits (suitable for UTF-32). This inconsistency can lead to portability issues when writing cross-platform code.\n- `wchar_t` is designed to represent wide characters in a fixed-width format (UTF-16 or UTF-32). In contrast, UTF-8 is a variable-length encoding, where each character can take from 1 to 4 bytes. This fundamental difference means that `wchar_t` and UTF-8 are incompatible.\n\nStringZilla [partially addresses those issues](#unicode-utf-8-and-wide-characters).\n\nWhat's Wrong with the C++ Standard Library?\n\nC++ Code\n:-----------------------------------\n`\"Loose\"s.replace(2, 2, \"vath\"s, 1)`\n`\"Loose\"s.replace(2, 2, \"vath\", 1)`\n\nStringZilla is designed to be a drop-in replacement for the C++ Standard Templates Library.\nThat said, some of the design decisions of STL strings are highly controversial, error-prone, and expensive.\nMost notably:\n\n1. Argument order for `replace`, `insert`, `erase` and similar functions is impossible to guess.\n2. Bounds-checking exceptions for `substr`-like functions are only thrown for one side of the range.\n3. Returning string copies in `substr`-like functions results in absurd volume of allocations.\n4. Incremental construction via `push_back`-like functions goes through too many branches.\n5. Inconsistency between `string` and `string_view` methods, like the lack of `remove_prefix` and `remove_suffix`.\n\nCheck the following set of asserts validating the `std::string` specification.\nIt's not realistic to expect the average developer to remember the [14 overloads of `std::string::replace`][stl-replace].\n\nTo avoid those issues, StringZilla provides an alternative consistent interface.\nIt supports signed arguments, and doesn't have more than 3 arguments per function or\nThe standard API and our alternative can be conditionally disabled with `SZ_SAFETY_OVER_COMPATIBILITY=1`.\nWhen it's enabled, the _~~subjectively~~_ risky overloads from the Standard will be disabled.\n\nAssuming StringZilla is a header-only library you can use the full API in some translation units and gradually transition to safer restricted API in others.\nBonus - all the bound checking is branchless, so it has a constant cost and won't hurt your branch predictor.\n\nBeyond the C++ Standard Library - Learning from Python\n\nPython is arguably the most popular programming language for data science.\nIn part, that's due to the simplicity of its standard interfaces.\nStringZilla brings some of that functionality to C++.\n\n- Content checks: `isalnum`, `isalpha`, `isascii`, `isdigit`, `islower`, `isspace`, `isupper`.\n- Trimming character sets: `lstrip`, `rstrip`, `strip`.\n- Trimming string matches: `remove_prefix`, `remove_suffix`.\n- Ranges of search results: `splitlines`, `split`, `rsplit`.\n- Number of non-overlapping substring matches: `count`.\n- Partitioning: `partition`, `rpartition`.\n\nFor example, when parsing documents, it is often useful to split it into substrings.\nMost often, after that, you would compute the length of the skipped part, the offset and the length of the remaining part.\nThis results in a lot of pointer arithmetic and is error-prone.\nStringZilla provides a convenient `partition` function, which returns a tuple of three string views, making the code cleaner.",
      "word_count": 489
    },
    {
      "chunk_id": "stringzilla::chunk_7",
      "text": "Combining those with the `split` function, one can easily parse a CSV file or HTTP headers.\n\nSome other extensions are not present in the Python standard library either.\nLet's go through the C++ functionality category by category.\n\n- [Splits and Ranges](#splits-and-ranges).\n- [Concatenating Strings without Allocations](#concatenating-strings-without-allocations).\n- [Random Generation](#random-generation).\n- [Edit Distances and Fuzzy Search](#levenshtein-edit-distance-and-alignment-scores).\n\nSome of the StringZilla interfaces are not available even Python's native `str` class.\nHere is a sneak peek of the most useful ones.\n\nSplits and Ranges\n\nOne of the most common use cases is to split a string into a collection of substrings.\nWhich would often result in [StackOverflow lookups][so-split] and snippets like the one below.\n\nThose allocate memory for each string and the temporary vectors.\nEach allocation can be orders of magnitude more expensive, than even serial `for`-loop over characters.\nTo avoid those, StringZilla provides lazily-evaluated ranges, compatible with the [Range-v3][range-v3] library.\n\nConcatenating Strings without Allocations\n\nAnother common string operation is concatenation.\nThe STL provides `std::string::operator+` and `std::string::append`, but those are not very efficient, if multiple invocations are performed.\n\nThe efficient approach would be to pre-allocate the memory and copy the strings into it.\n\nThat's mouthful and error-prone.\nStringZilla provides a more convenient `concatenate` function, which takes a variadic number of arguments.\nIt also overrides the `operator|` to concatenate strings lazily, without any allocations.\n\nRandom Generation\n\nSoftware developers often need to generate random strings for testing purposes.\nThe STL provides `std::generate` and `std::random_device`, that can be used with StringZilla.\n\nMouthful and slow.\nStringZilla provides a C native method - `sz_fill_random` and a convenient C++ wrapper - `sz::generate`.\nSimilar to Python it also defines the commonly used character sets.\n\nBulk Replacements\n\nIn text processing, it's often necessary to replace all occurrences of a specific substring or set of characters within a string.\nStandard library functions may not offer the most efficient or convenient methods for performing bulk replacements, especially when dealing with large strings or performance-critical applications.\n\n- `haystack.replace_all(needle_string, replacement_string)`\n- `haystack.replace_all(sz::byteset(\"\"), replacement_string)`\n- `haystack.try_replace_all(needle_string, replacement_string)`\n- `haystack.try_replace_all(sz::byteset(\"\"), replacement_string)`\n- `haystack.lookup(sz::look_up_table::identity())`\n- `haystack.lookup(sz::look_up_table::identity(), haystack.data())`\n\nSorting in C and C++\n\nLibC provides `qsort` and STL provides `std::sort`.\nBoth have their quirks.\nThe LibC standard has no way to pass a context to the comparison function, that's only possible with platform-specific extensions.\nThose have [different arguments order](https://stackoverflow.com/a/39561369) on every OS.\n\nC++ generic algorithm is not perfect either.\nThere is no guarantee in the standard that `std::sort` won't allocate any memory.\nIf you are running on embedded, in real-time or on 100+ CPU cores per node, you may want to avoid that.\nStringZilla doesn't solve the general case, but hopes to improve the performance for strings.\nUse `sz_sequence_argsort`, or the high-level `sz::argsort`, which can be used sort any collection of elements convertible to `sz::string_view`.\n\nStandard C++ Containers with String Keys\n\nThe C++ Standard Templates Library provides several associative containers, often used with string keys.",
      "word_count": 476
    },
    {
      "chunk_id": "stringzilla::chunk_8",
      "text": "The performance of those containers is often limited by the performance of the string keys, especially on reads.\nStringZilla can be used to accelerate containers with `std::string` keys, by overriding the default comparator and hash functions.\n\nAlternatively, a better approach would be to use the `sz::string` class as a key.\nThe right hash function and comparator would be automatically selected and the performance gains would be more noticeable if the keys are short.\n\nCompilation Settings and Debugging\n\n__`SZ_DEBUG`__:\n\n> For maximal performance, the C library does not perform any bounds checking in Release builds.\n> In C++, bounds checking happens only in places where the STL `std::string` would do it.\n> If you want to enable more aggressive bounds-checking, define `SZ_DEBUG` before including the header.\n> If not explicitly set, it will be inferred from the build type.\n\n__`SZ_USE_GOLDMONT`, `SZ_USE_WESTMERE`, `SZ_USE_HASWELL`, `SZ_USE_SKYLAKE`, `SZ_USE_ICE`, `SZ_USE_NEON`, `SZ_USE_NEON_AES`, `SZ_USE_NEON_SHA`, `SZ_USE_SVE`, `SZ_USE_SVE2`, `SZ_USE_SVE2_AES`__:\n\n> One can explicitly disable certain families of SIMD instructions for compatibility purposes.\n> Default values are inferred at compile time depending on compiler support (for dynamic dispatch) and the target architecture (for static dispatch).\n\n__`SZ_USE_CUDA`, `SZ_USE_KEPLER`, `SZ_USE_HOPPER`__:\n\n> One can explicitly disable certain families of PTX instructions for compatibility purposes.\n> Default values are inferred at compile time depending on compiler support (for dynamic dispatch) and the target architecture (for static dispatch).\n\n__`SZ_ENFORCE_SVE_OVER_NEON`__:\n\n> SVE and SVE2 are expected to supersede NEON on ARM architectures.\n> Still, oftentimes the equivalent SVE kernels are slower due to equally small register files and higher complexity of the instructions.\n> By default, when both SVE and NEON are available, SVE is used selectively only for the algorithms that benefit from it.\n> If you want to enforce SVE usage everywhere, define this flag.\n\n__`SZ_DYNAMIC_DISPATCH`__:\n\n> By default, StringZilla is a header-only library.\n> But if you are running on different generations of devices, it makes sense to pre-compile the library for all supported generations at once, and dispatch at runtime.\n> This flag does just that and is used to produce the `stringzilla.so` shared library, as well as the Python bindings.\n\n__`SZ_USE_MISALIGNED_LOADS`__:\n\n> Default is platform-dependent: enabled on x86 (where unaligned accesses are fast), disabled on others by default.\n> When enabled, many byte-level operations use word-sized loads, which can significantly accelerate the serial (SWAR) backend.\n> Consider enabling it explicitly if you are targeting platforms that support fast unaligned loads.\n\n__`SZ_AVOID_LIBC`__ and __`SZ_OVERRIDE_LIBC`__:\n\n> When using the C header-only library one can disable the use of LibC.\n> This may affect the type resolution system on obscure hardware platforms. \n> Moreover, one may let `stringzilla` override the common symbols like the `memcpy` and `memset` with its own implementations.\n> In that case you can use the [`LD_PRELOAD` trick][ld-preload-trick] to prioritize its symbols over the ones from the LibC and accelerate existing string-heavy applications without recompiling them.\n> It also adds a layer of security, as the `stringzilla` isn't [undefined for NULL inputs][redhat-memcpy-ub] like `memcpy(NULL, NULL, 0)`.\n\n__`SZ_AVOID_STL`__ and __`SZ_SAFETY_OVER_COMPATIBILITY`__:",
      "word_count": 497
    },
    {
      "chunk_id": "stringzilla::chunk_9",
      "text": "> When using the C++ interface one can disable implicit conversions from `std::string` to `sz::string` and back.\n> If not needed, the `` and `` headers will be excluded, reducing compilation time.\n> Moreover, if STL compatibility is a low priority, one can make the API safer by disabling the overloads, which are subjectively error prone.\n\n__`STRINGZILLA_BUILD_SHARED`, `STRINGZILLA_BUILD_TEST`, `STRINGZILLA_BUILD_BENCHMARK`, `STRINGZILLA_TARGET_ARCH`__ for CMake users:\n\n> When compiling the tests and benchmarks, you can explicitly set the target hardware architecture.\n> It's synonymous to GCC's `-march` flag and is used to enable/disable the appropriate instruction sets.\n> You can also disable the shared library build, if you don't need it.\n\nQuick Start: Rust\n\nStringZilla is available as a Rust crate, with documentation available on [docs.rs/stringzilla](https://docs.rs/stringzilla).\nYou can immediately check the installed version and the used hardware capabilities with following commands:\n\nTo use the latest crate release in your project, add the following to your `Cargo.toml`:\n\nOr if you want to use the latest pre-release version from the repository:\n\nOnce installed, all of the functionality is available through the `stringzilla` namespace.\nMany interfaces will look familiar to the users of the `memchr` Rust crate.\n\nIt also provides no constraints on the size of the character set, while `memchr` allows only 1, 2, or 3 characters.\nIn addition to global functions, `stringzilla` provides a `StringZilla` extension trait:\n\nHash\n\nSingle-shot and incremental hashing are both supported:\n\nTo use StringZilla with `std::collections`:\n\nSHA-256 Checksums\n\nSHA-256 cryptographic checksums are available:\n\nUnicode Case-Folding and Case-Insensitive Search\n\nStringZilla implements both Unicode Case Folding and Case-Insensitive UTF-8 Search.\nUnlike most libraries only capable of lower-casing ASCII-represented English alphabet, StringZilla covers over 1M+ codepoints.\nThe case-folding API expects the output buffer to be at least 3× larger than the input, to accommodate for the worst-case character expansions scenarios.\n\nThe case-insensitive search returns `Some((offset, matched_length))` or `None`.\nThe `matched_length` may differ from needle length due to expansions.\n\nSimilarity Scores\n\nStringZilla exposes high-performance, batch-oriented similarity via the `szs` module.\nUse `DeviceScope` to pick hardware and optionally limit capabilities per engine.\n\nNote, that this computes byte-level distances.\nFor UTF-8 codepoints, use a different engine class:\n\nSimilarly, for variable substitution costs, also pass in a a weights matrix:\n\nOr for local alignment scores:\n\nFor high-performance applications, use the [StringTape](https://github.com/ashvardanian/StringTape) crate to pass strings to `compute_into` methods without extra memory allocations:\n\nRolling Fingerprints\n\nMinHashing is a common technique for Information Retrieval, producing compact representations of large documents.\nFor $D$ hash-functions and a text of length $L$, in the worst case it involves computing $O(D \\cdot L)$ hashes.\n\nFor zero-copy processing with [StringTape](https://github.com/ashvardanian/StringTape) format and unified memory:\n\nQuick Start: JavaScript\n\nInstall the Node.js package and use zero-copy `Buffer` APIs.\n\nUnicode Case-Folding and Case-Insensitive Search\n\nStringZilla provides full Unicode case folding (including expansions like `ß → ss`, ligatures like `ﬁ → fi`, and special folds like `µ → μ`, `K → k`)\nand a case-insensitive substring search that accounts for those expansions.\n\nHash\n\nSingle-shot and incremental hashing are both supported:\n\nSHA-256 Checksums\n\nSHA-256 cryptographic checksums are available:",
      "word_count": 499
    },
    {
      "chunk_id": "stringzilla::chunk_10",
      "text": "Quick Start: Swift\n\nStringZilla can be added as a dependency in the Swift Package Manager.\nIn your `Package.swift` file, add the following:\n\nThe package currently covers only the most basic functionality, but is planned to be extended to cover the full C++ API.\n\nUnicode Case-Folding and Case-Insensitive Search\n\nHash\n\nStringZilla provides high-performance hashing for Swift strings:\n\nSHA-256 Checksums\n\nSHA-256 cryptographic checksums are available:\n\nQuick Start: GoLang\n\nAdd the Go binding as a module dependency:\n\nBuild the shared C library once, then ensure your runtime can locate it (Linux shown):\n\nUse finders (substring, bytes, and sets):\n\nUnicode Case-Folding and Case-Insensitive Search\n\nHash\n\nSingle-shot and incremental hashing are both supported.\nThe `Hasher` type implements Go's standard `hash.Hash64` and `io.Writer` interfaces:\n\nSHA-256 Checksums\n\nSHA-256 cryptographic checksums are available.\nThe `Sha256` type implements Go's standard `hash.Hash` and `io.Writer` interfaces:\n\nAlgorithms & Design Decisions\n\nStringZilla aims to optimize some of the slowest string operations.\nSome popular operations, however, like equality comparisons and relative order checking, almost always complete on some of the very first bytes in either string.\nIn such operations vectorization is almost useless, unless huge and very similar strings are considered.\nStringZilla implements those operations as well, but won't result in substantial speedups.\nWhere vectorization stops being effective, parallelism takes over with the new layered cake architecture:\n\n- StringZilla C library w/out dependencies\n- StringZillas parallel extensions:\n  - Parallel C++ algorithms built with [Fork Union](https://github.com/ashvardanian/fork_union)\n  - Parallel CUDA algorithms for Nvidia GPUs\n  - Parallel ROCm algorithms for AMD GPUs\n\nExact Substring Search\n\nSubstring search algorithms are generally divided into: comparison-based, automaton-based, and bit-parallel.\nDifferent families are effective for different alphabet sizes and needle lengths.\nThe more operations are needed per-character - the more effective SIMD would be.\nThe longer the needle - the more effective the skip-tables are.\nStringZilla uses different exact substring search algorithms for different needle lengths and backends:\n\n- When no SIMD is available - SWAR (SIMD Within A Register) algorithms are used on 64-bit words.\n- Boyer-Moore-Horspool (BMH) algorithm with Raita heuristic variation for longer needles.\n- SIMD backends compare characters at multiple strategically chosen offsets within the needle to reduce degeneracy.\n\nOn very short needles, especially 1-4 characters long, brute force with SIMD is the fastest solution.\nOn mid-length needles, bit-parallel algorithms are effective, as the character masks fit into 32-bit or 64-bit words.\nEither way, if the needle is under 64-bytes long, on haystack traversal we will still fetch every CPU cache line.\nSo the only way to improve performance is to reduce the number of comparisons.\n\n> For 2-byte needles, see `sz_find_2byte_serial_` in `include/stringzilla/find.h`:\n\nGoing beyond that, to long needles, Boyer-Moore (BM) and its variants are often the best choice.\nIt has two tables: the good-suffix shift and the bad-character shift.\nCommon choice is to use the simplified BMH algorithm, which only uses the bad-character shift table, reducing the pre-processing time.\nWe do the same for mid-length needles up to 256 bytes long.\nThat way the stack-allocated shift table remains small.",
      "word_count": 495
    },
    {
      "chunk_id": "stringzilla::chunk_11",
      "text": "> For mid-length needles (≤256 bytes), see `sz_find_horspool_upto_256bytes_serial_` in `include/stringzilla/find.h`:\n\nIn the C++ Standards Library, the `std::string::find` function uses the BMH algorithm with Raita's heuristic.\nBefore comparing the entire string, it matches the first, last, and the middle character.\nVery practical, but can be slow for repetitive characters.\nBoth SWAR and SIMD backends of StringZilla have a cheap pre-processing step, where we locate unique characters.\nThis makes the library a lot more practical when dealing with non-English corpora.\n\n> The offset selection heuristic is implemented in `sz_locate_needle_anomalies_` in `include/stringzilla/find.h`:\n\nAll those, still, have $O(hn)$ worst case complexity.\nTo guarantee $O(h)$ worst case time complexity, the Apostolico-Giancarlo (AG) algorithm adds an additional skip-table.\nPreprocessing phase is $O(n+sigma)$ in time and space.\nOn traversal, performs from $(h/n)$ to $(3h/2)$ comparisons.\nIt however, isn't practical on modern CPUs.\nA simpler idea, the Galil-rule might be a more relevant optimizations, if many matches must be found.\n\nOther algorithms previously considered and deprecated:\n\n- Apostolico-Giancarlo algorithm for longer needles. _Control-flow is too complex for efficient vectorization._\n- Shift-Or-based Bitap algorithm for short needles. _Slower than SWAR._\n- Horspool-style bad-character check in SIMD backends. _Effective only for very long needles, and very uneven character distributions between the needle and the haystack. Faster \"character-in-set\" check needed to generalize._\n\n> § Reading materials.\n> [Exact String Matching Algorithms in Java](https://www-igm.univ-mlv.fr/~lecroq/string).\n> [SIMD-friendly algorithms for substring searching](http://0x80.pl/articles/simd-strfind.html).\n\nExact Multiple Substring Search\n\nFew algorithms for multiple substring search are known.\nMost are based on the Aho-Corasick automaton, which is a generalization of the KMP algorithm.\nThe naive implementation, however:\n\n- Allocates disjoint memory for each Trie node and Automaton state.\n- Requires a lot of pointer chasing, limiting speculative execution.\n- Has a lot of branches and conditional moves, which are hard to predict.\n- Matches text a character at a time, which is slow on modern CPUs.\n\nThere are several ways to improve the original algorithm.\nOne is to use sparse DFA representation, which is more cache-friendly, but would require extra processing to navigate state transitions.\n\nLevenshtein Edit Distance\n\nLevenshtein distance is the best known edit-distance for strings, that checks, how many insertions, deletions, and substitutions are needed to transform one string to another.\nIt's extensively used in approximate string-matching, spell-checking, and bioinformatics.\n\nThe computational cost of the Levenshtein distance is $O(n * m)$, where $n$ and $m$ are the lengths of the string arguments.\nTo compute that, the naive approach requires $O(n * m)$ space to store the \"Levenshtein matrix\", the bottom-right corner of which will contain the Levenshtein distance.\nThe algorithm producing the matrix has been simultaneously studied/discovered by the Soviet mathematicians Vladimir Levenshtein in 1965, Taras Vintsyuk in 1968, and American computer scientists - Robert Wagner, David Sankoff, Michael J. Fischer in the following years.\nSeveral optimizations are known:",
      "word_count": 463
    },
    {
      "chunk_id": "stringzilla::chunk_12",
      "text": "1. __Space Optimization__: The matrix can be computed in $O(min(n,m))$ space, by only storing the last two rows of the matrix.\n2. __Divide and Conquer__: Hirschberg's algorithm can be applied to decompose the computation into subtasks.\n3. __Automata__: Levenshtein automata can be effective, if one of the strings doesn't change, and is a subject to many comparisons.\n4. __Shift-Or__: Bit-parallel algorithms transpose the matrix into a bit-matrix, and perform bitwise operations on it.\n\nThe last approach is quite powerful and performant, and is used by the great [RapidFuzz][rapidfuzz] library.\nIt's less known, than the others, derived from the Baeza-Yates-Gonnet algorithm, extended to bounded edit-distance search by Manber and Wu in 1990s, and further extended by Gene Myers in 1999 and Heikki Hyyro between 2002 and 2004.\n\nStringZilla focuses on a different approach, extensively used in Unum's internal combinatorial optimization libraries.\nIt doesn't change the number of trivial operations, but performs them in a different order, removing the data dependency, that occurs when computing the insertion costs.\nStringZilla __evaluates diagonals instead of rows__, exploiting the fact that all cells within a diagonal are independent, and can be computed in parallel.\nWe'll store 3 diagonals instead of the 2 rows, and each consecutive diagonal will be computed from the previous two.\nSubstitution costs will come from the sooner diagonal, while insertion and deletion costs will come from the later diagonal.\n\nRow-by-Row Algorithm\nComputing row 4:\n\n∅  0  1  2  3  4  5\n P  1  ░  ░  ░  ░  ░\n Q  2  ■  ■  ■  ■  ■\n R  3  ■  ■  □  →  .\n S  4  .  .  .  .  .\n T  5  .  .  .  .  .\n\nAnti-Diagonal Algorithm\nComputing diagonal 5:\n\n∅  0  1  2  3  4  5\n P  1  ░  ░  ■  ■  □\n Q  2  ░  ■  ■  □  ↘\n R  3  ■  ■  □  ↘  .\n S  4  ■  □  ↘  .  .\n T  5  □  ↘  .  .  .\n\nLegend:\n0,1,2,3... = initialization constants &nbsp;&nbsp;\n░ = cells processed and forgotten &nbsp;&nbsp;\n■ = stored cells &nbsp;&nbsp;\n□ = computing in parallel &nbsp;&nbsp;\n→ ↘ = movement direction &nbsp;&nbsp;\n. = cells to compute later\n\nThis results in much better vectorization for intra-core parallelism and potentially multi-core evaluation of a single request.\nMoreover, it's easy to generalize to weighted edit-distances, where the cost of a substitution between two characters may not be the same for all pairs, often used in bioinformatics.\n\n> § Reading materials.\n> [Faster Levenshtein Distances with a SIMD-friendly Traversal Order](https://ashvardanian.com/posts/levenshtein-diagonal).\n\nNeedleman-Wunsch and Smith-Waterman Scores for Bioinformatics\n\nThe field of bioinformatics studies various representations of biological structures.\nThe \"primary\" representations are generally strings over sparse alphabets:",
      "word_count": 442
    },
    {
      "chunk_id": "stringzilla::chunk_13",
      "text": "- [DNA][faq-dna] sequences, where the alphabet is {A, C, G, T}, ranging from ~100 characters for short reads to 3 billion for the human genome.\n- [RNA][faq-rna] sequences, where the alphabet is {A, C, G, U}, ranging from ~50 characters for tRNA to thousands for mRNA.\n- [Proteins][faq-protein], where the alphabet is made of 22 amino acids, ranging from 2 characters for [dipeptide][faq-dipeptide] to 35,000 for [Titin][faq-titin], the longest protein.\n\nThe shorter the representation, the more often researchers may want to use custom substitution matrices.\nMeaning that the cost of a substitution between two characters may not be the same for all pairs.\nIn the general case the serial algorithm is supposed to work for arbitrary substitution costs for each of 256×256 possible character pairs.\nThat lookup table, however, is too large to fit into CPU registers, so instead, the upcoming design focuses on 32×32 substitution matrices, which fit into 1 KB with single-byte \"error costs\".\nThat said, most [BLOSUM][faq-blosum] and [PAM][faq-pam] substitution matrices only contain 4-bit values, so they can be packed even further.\n\nNext design goals:\n\n- [ ] Needleman-Wunsch Automata\n\nMemory Copying, Fills, and Moves\n\nA lot has been written about the time computers spend copying memory and how that operation is implemented in LibC.\nInterestingly, the operation can still be improved, as most Assembly implementations use outdated instructions.\nEven performance-oriented STL replacements, like Meta's [Folly v2024.09.23 focus on AVX2](https://github.com/facebook/folly/blob/main/folly/memset.S), and don't take advantage of the new masked instructions in AVX-512 or SVE.\n\nIn AVX-512, StringZilla uses non-temporal stores to avoid cache pollution, when dealing with very large strings.\nMoreover, it handles the unaligned head and the tails of the `target` buffer separately, ensuring that writes in big copies are always aligned to cache-line boundaries.\nThat's true for both AVX2 and AVX-512 backends.\n\nStringZilla also contains \"drafts\" of smarter, but less efficient algorithms, that minimize the number of unaligned loads, performing shuffles and permutations.\nThat's a topic for future research, as the performance gains are not yet satisfactory.\n\n> § Reading materials.\n> [`memset` benchmarks](https://github.com/nadavrot/memset_benchmark?tab=readme-ov-file) by Nadav Rotem.\n> [Cache Associativity](https://en.algorithmica.org/hpc/cpu-cache/associativity/) by Sergey Slotin.\n\nHashing\n\nStringZilla implements a high-performance 64-bit hash function inspired by the \"AquaHash\", \"aHash\", and \"GxHash\" design and optimized for modern CPU architectures.\nThe algorithm utilizes AES encryption rounds combined with shuffle-and-add operations to achieve exceptional mixing properties while maintaining consistent output across platforms.\nIt passes the rigorous SMHasher test suite, including the `--extra` flag with no collisions.\n\nThe core algorithm operates on a dual-state design:\n\n- __AES State__: Initialized with seed `XOR`-ed against π constants.\n- __Sum State__: Accumulates shuffled input data with a permutation.\n\nFor strings ≤64 bytes, a minimal state processes data in 16-byte blocks.\nLonger strings employ a 4× wider state (512 bits) that processes 64-byte chunks, maximizing throughput on modern superscalar CPUs.\nThe algorithm can be expressed in pseudocode as:",
      "word_count": 471
    },
    {
      "chunk_id": "stringzilla::chunk_14",
      "text": "This allows us to balance several design trade-offs.\nFirst, it allows us to achieve a high port-level parallelism.\nLooking at AVX-512 capable CPUs and their ZMM instructions, on each cycle, we'll have at least 2 ports busy when dealing with long strings:\n\n- `VAESENC`: 5 cycles on port 0 on Intel Ice Lake, 4 cycles on ports 0/1 on AMD Zen4.\n- `VPSHUFB_Z`: 3 cycles on port 5 on Intel Ice Lake, 2 cycles on ports 1/2 on AMD Zen4.\n- `VPADDQ`: 1 cycle on ports 0/5 on Intel Ice Lake, 1 cycle on ports 0/1/2/3 on AMD Zen4.\n\nWhen dealing with smaller strings, we design our approach to avoid large registers and maintain the CPU at the same energy state, thereby avoiding downclocking and expensive power-state transitions.\n\nUnlike some AES-accelerated alternatives, the length of the input is not mixed into the AES block at the start to allow incremental construction, when the final length is not known in advance.\nAlso, unlike some alternatives, with \"masked\" AVX-512 and \"predicated\" SVE loads, we avoid expensive block-shuffling procedures on non-divisible-by-16 lengths.\n\n> § Reading materials.\n> [Stress-testing hash functions for avalance behaviour, collision bias, and distribution](https://github.com/ashvardanian/HashEvals).\n\nSHA-256 Checksums\n\nIn addition to the fast AES-based hash, StringZilla implements hardware-accelerated SHA-256 cryptographic checksums.\nThe implementation follows the [FIPS 180-4 specification](https://nvlpubs.nist.gov/nistpubs/FIPS/NIST.FIPS.180-4.pdf) and provides multiple backends.\n\nRandom Generation\n\nStringZilla implements a fast [Pseudorandom Number Generator][faq-prng] inspired by the \"AES-CTR-128\" algorithm, reusing the same AES primitives as the hash function.\nUnlike \"NIST SP 800-90A\" which uses multiple AES rounds, StringZilla uses only one round of AES mixing for performance while maintaining reproducible output across platforms.\nThe generator operates in counter mode with `AESENC(nonce + lane_index, nonce ⊕ pi_constants)`, rotating through the first 512 bits of π for each 16-byte block.\nThe only state required to reproduce an output is a 64-bit `nonce`, which is much cheaper than a Mersenne Twister.\n\nSorting\n\nFor lexicographic sorting of string collections, StringZilla exports pointer-sized n‑grams (\"pgrams\") into a contiguous buffer to improve locality, then recursively QuickSorts those pgrams with a 3‑way partition and dives into equal pgrams to compare deeper characters.\nVery small inputs fall back to insertion sort.\n\n- Average time complexity: O(n log n)\n- Worst-case time complexity: quadratic (due to QuickSort), mitigated in practice by 3‑way partitioning and the n‑gram staging\n\nUnicode 17, UTF-8, and Wide Characters\n\nMost StringZilla operations are byte-level, so they work well with ASCII and UTF-8 content out of the box.\nIn some cases, like edit-distance computation, the result of byte-level evaluation and character-level evaluation may differ.\n\n- `szs_levenshtein_distances_utf8(\"αβγδ\", \"αγδ\") == 1` — one unicode symbol.\n- `szs_levenshtein_distances(\"αβγδ\", \"αγδ\") == 2` — one unicode symbol is two bytes long.",
      "word_count": 445
    },
    {
      "chunk_id": "stringzilla::chunk_15",
      "text": "Java, JavaScript, Python 2, C#, and Objective-C, however, use wide characters (`wchar`) - two byte long codes, instead of the more reasonable fixed-length UTF-32 or variable-length UTF-8.\nThis leads [to all kinds of offset-counting issues][wide-char-offsets] when facing four-byte long Unicode characters.\nStringZilla uses proper 32-bit \"runes\" to represent unpacked Unicode codepoints, ensuring correct results in all operations.\nMoreover, it implements the Unicode 17.0 standard, being practically the only library besides ICU and PCRE2 to do so, but with order(s) of magnitude better performance.\n\nCase-Folding and Case-Insensitive Search\n\nStringZilla provides Unicode-aware case-insensitive substring search that handles the full complexity of Unicode case folding.\nThis includes multi-character expansions:\n\nCharacter\n---------\n`ß`\n`ﬃ`\n`İ`\n\nThe search returns byte offsets and lengths in the original haystack, correctly handling length differences.\nFor example, searching for `\"STRASSE\"` (7 bytes) in `\"Straße\"` (7 bytes: 53 74 72 61 C3 9F 65) succeeds because both case-fold to `\"strasse\"`.\n\nNote that Turkish `İ` and ASCII `I` are distinct: `İstanbul` case-folds to `i̇stanbul` (with combining dot), while `ISTANBUL` case-folds to `istanbul` (without).\nThey will not match each other — this is correct Unicode behavior for Turkish locale handling.\n\nFor wide-character environments (Java, JavaScript, Python 2, C#), consider transcoding with [simdutf](https://github.com/simdutf/simdutf).\n\nDynamic Dispatch\n\nDue to the high-level of fragmentation of SIMD support in different CPUs, StringZilla uses the names of select Intel and ARM CPU generations for its backends.\nYou can query supported backends and use them manually.\nUse it to guarantee constant performance, or to explore how different algorithms scale on your hardware.\n\nStringZilla automatically picks the most advanced backend for the given CPU.\nSimilarly, in Python, you can log the auto-detected capabilities:\n\nYou can also explicitly set the backend to use, or scope the backend to a specific function.\n\nContributing\n\nPlease check out the [contributing guide](https://github.com/ashvardanian/StringZilla/blob/main/CONTRIBUTING.md) for more details on how to set up the development environment and contribute to this project.\nIf you like this project, you may also enjoy [USearch][usearch], [UCall][ucall], [UForm][uform], and [SimSIMD][simsimd].\n\nIf you like strings and value efficiency, you may also enjoy the following projects:\n\nIf you are looking for more reading materials on this topic, consider the following:\n\nLicense\n\nFeel free to use the project under Apache 2.0 or the Three-clause BSD license at your preference.",
      "word_count": 373
    }
  ]
}