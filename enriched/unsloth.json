{
  "name": "unsloth",
  "summary": "2-5X faster training, reinforcement learning & finetuning",
  "language": "python",
  "tags": [
    "cli",
    "data",
    "math",
    "ml",
    "ui"
  ],
  "chunks": [
    {
      "chunk_id": "unsloth::chunk_0",
      "text": "Train gpt-oss, DeepSeek, Gemma, Qwen & Llama 2x faster with 70% less VRAM!\n\nTrain for Free\n\nNotebooks are beginner friendly. Read our [guide](https://docs.unsloth.ai/get-started/fine-tuning-guide). Add dataset, run, then export your trained model to GGUF, llama.cpp, Ollama, vLLM, SGLang or Hugging Face.\n\nModel\n-----------\n**gpt-oss (20B)**\n**Mistral Ministral 3 (3B)**\n**gpt-oss (20B): GRPO**\n**Qwen3: Advanced GRPO**\n**Qwen3-VL (8B): GSPO**\n**Gemma 3 (270M)**\n**Gemma 3n (4B)**\n**DeepSeek-OCR (3B)**\n**Llama 3.1 (8B) Alpaca**\n**Llama 3.2 Conversational**\n**Orpheus-TTS (3B)**\n\n⚡ Quickstart\n\nLinux or WSL\n\nWindows\nFor Windows, `pip install unsloth` works only if you have Pytorch installed. Read our [Windows Guide](https://docs.unsloth.ai/get-started/installing-+-updating/windows-installation).\n\nDocker\nUse our official [Unsloth Docker image](https://hub.docker.com/r/unsloth/unsloth)  container. Read our [Docker Guide](https://docs.unsloth.ai/get-started/install-and-update/docker).\n\nBlackwell & DGX Spark\nFor RTX 50x, B200, 6000 GPUs: `pip install unsloth`. Read our [Blackwell Guide](https://docs.unsloth.ai/basics/training-llms-with-blackwell-rtx-50-series-and-unsloth) and [DGX Spark Guide](https://docs.unsloth.ai/new/fine-tuning-llms-with-nvidia-dgx-spark-and-unsloth) for more details.\n\nUnsloth News\n\nClick for more news\n\nLinks and Resources\nType\n-------------------------------\n&nbsp; **r/unsloth Reddit**\n **Documentation & Wiki**\n&nbsp; **Twitter (aka X)**\n **Installation**\n **Our Models**\n️ **Blog**\n\n⭐ Key Features\n- Supports **full-finetuning**, pretraining, 4b-bit, 16-bit and **FP8** training\n- Supports **all models** including [TTS](https://docs.unsloth.ai/basics/text-to-speech-tts-fine-tuning), multimodal, [BERT](https://docs.unsloth.ai/get-started/unsloth-notebooks#other-important-notebooks) and more! Any model that works in transformers, works in Unsloth.\n- The most efficient library for [Reinforcement Learning (RL)](https://docs.unsloth.ai/get-started/reinforcement-learning-rl-guide), using 80% less VRAM. Supports GRPO, GSPO, DrGRPO, DAPO etc.\n- **0% loss in accuracy** - no approximation methods - all exact.\n- Supports NVIDIA (since 2018), [AMD](https://docs.unsloth.ai/get-started/install-and-update/amd) and Intel GPUs. Minimum CUDA Capability 7.0 (V100, T4, Titan V, RTX 20, 30, 40x, A100, H100, L40 etc)\n- Works on **Linux**, WSL and **Windows**\n- All kernels written in [OpenAI's Triton](https://openai.com/index/triton/) language. Manual backprop engine.\n- If you trained a model with Unsloth, you can use this cool sticker! &nbsp;\n\nInstall Unsloth\nYou can also see our docs for more detailed installation and updating instructions [here](https://docs.unsloth.ai/get-started/installing-+-updating).\n\nUnsloth supports Python 3.13 or lower.\n\nPip Installation\n**Install with pip (recommended) for Linux devices:**\n\n**To update Unsloth:**\n\nSee [here](#advanced-pip-installation) for advanced pip install instructions.\n\nWindows Installation\n\n1. **Install NVIDIA Video Driver:**\n  You should install the latest driver for your GPU. Download drivers here: [NVIDIA GPU Driver](https://www.nvidia.com/Download/index.aspx).\n\n3. **Install Visual Studio C++:**\n   You will need Visual Studio, with C++ installed. By default, C++ is not installed with [Visual Studio](https://visualstudio.microsoft.com/vs/community/), so make sure you select all of the C++ options. Also select options for Windows 10/11 SDK. For detailed instructions with options, see [here](https://docs.unsloth.ai/get-started/installing-+-updating).\n\n5. **Install CUDA Toolkit:**\n   Follow the instructions to install [CUDA Toolkit](https://developer.nvidia.com/cuda-toolkit-archive).\n\n6. **Install PyTorch:**\n   You will need the correct version of PyTorch that is compatible with your CUDA drivers, so make sure to select them carefully.\n   [Install PyTorch](https://pytorch.org/get-started/locally/).\n\n7. **Install Unsloth:**\n\nNotes\nTo run Unsloth directly on Windows:\n- Install Triton from this Windows fork and follow the instructions [here](https://github.com/woct0rdho/triton-windows) (be aware that the Windows fork requires PyTorch >= 2.4 and CUDA 12)\n- In the `SFTConfig`, set `dataset_num_proc=1` to avoid a crashing issue:\n\nAdvanced/Troubleshooting\n\nFor **advanced installation instructions** or if you see weird errors during installations:\n\nFirst try using an isolated environment via then `pip install unsloth`",
      "word_count": 495
    },
    {
      "chunk_id": "unsloth::chunk_1",
      "text": "1. Install `torch` and `triton`. Go to https://pytorch.org to install it. For example `pip install torch torchvision torchaudio triton`\n2. Confirm if CUDA is installed correctly. Try `nvcc`. If that fails, you need to install `cudatoolkit` or CUDA drivers.\n3. Install `xformers` manually via:\n\n5. For GRPO runs, you can try installing `vllm` and seeing if `pip install vllm` succeeds.\n6. Double check that your versions of Python, CUDA, CUDNN, `torch`, `triton`, and `xformers` are compatible with one another. The [PyTorch Compatibility Matrix](https://github.com/pytorch/pytorch/blob/main/RELEASE.md#release-compatibility-matrix) may be useful. \n5. Finally, install `bitsandbytes` and check it with `python -m bitsandbytes`\n\nConda Installation (Optional)\n`⚠️Only use Conda if you have it. If not, use Pip`. Select either `pytorch-cuda=11.8,12.1` for CUDA 11.8 or CUDA 12.1. We support `python=3.10,3.11,3.12`.\n\nIf you're looking to install Conda in a Linux environment, read here, or run the below\n\nAdvanced Pip Installation\n`⚠️Do **NOT** use this if you have Conda.` Pip is a bit more complex since there are dependency issues. The pip command is different for `torch 2.2,2.3,2.4,2.5,2.6,2.7,2.8,2.9` and CUDA versions.\n\nFor other torch versions, we support `torch211`, `torch212`, `torch220`, `torch230`, `torch240`, `torch250`, `torch260`, `torch270`, `torch280`, `torch290` and for CUDA versions, we support `cu118` and `cu121` and `cu124`. For Ampere devices (A100, H100, RTX3090) and above, use `cu118-ampere` or `cu121-ampere` or `cu124-ampere`.\n\nFor example, if you have `torch 2.4` and `CUDA 12.1`, use:\n\nAnother example, if you have `torch 2.9` and `CUDA 13.0`, use:\n\nAnd other examples:\n\nOr, run the below in a terminal to get the **optimal** pip installation command:\n\nOr, run the below manually in a Python REPL:\n\nDocker Installation\nYou can use our pre-built Docker container with all dependencies to use Unsloth instantly with no setup required.\n[Read our guide](https://docs.unsloth.ai/get-started/install-and-update/docker).\n\nThis container requires installing [NVIDIA's Container Toolkit](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html).\n\nAccess Jupyter Lab at `http://localhost:8888` and start fine-tuning!\n\nDocumentation\n- Go to our official [Documentation](https://docs.unsloth.ai) for [running models](https://docs.unsloth.ai/basics/running-and-saving-models), [saving to GGUF](https://docs.unsloth.ai/basics/running-and-saving-models/saving-to-gguf), [checkpointing](https://docs.unsloth.ai/basics/finetuning-from-last-checkpoint), [evaluation](https://docs.unsloth.ai/get-started/fine-tuning-llms-guide#evaluation) and more!\n- Read our Guides for: [Fine-tuning](https://docs.unsloth.ai/get-started/fine-tuning-llms-guide), [Reinforcement Learning](https://docs.unsloth.ai/get-started/reinforcement-learning-rl-guide), [Text-to-Speech (TTS)](https://docs.unsloth.ai/basics/text-to-speech-tts-fine-tuning), [Vision](https://docs.unsloth.ai/basics/vision-fine-tuning) and [any model](https://docs.unsloth.ai/models/tutorials-how-to-fine-tune-and-run-llms).\n- We support Huggingface's transformers, TRL, Trainer, Seq2SeqTrainer and Pytorch code.\n\nUnsloth example code to fine-tune gpt-oss-20b:\n\nReinforcement Learning\n[RL](https://docs.unsloth.ai/get-started/reinforcement-learning-rl-guide) including [GRPO](https://docs.unsloth.ai/get-started/reinforcement-learning-rl-guide#training-with-grpo), [GSPO](https://docs.unsloth.ai/get-started/reinforcement-learning-rl-guide/gspo-reinforcement-learning), **FP8** traning, DrGRPO, DAPO, PPO, Reward Modelling, Online DPO all work with Unsloth.\nRead our [Reinforcement Learning Guide](https://docs.unsloth.ai/get-started/reinforcement-learning-rl-guide) or our [advanced RL docs](https://docs.unsloth.ai/get-started/reinforcement-learning-rl-guide/advanced-rl-documentation) for batching, generation & training parameters.\n\nList of RL notebooks:\n\nPerformance Benchmarking\n- For our most detailed benchmarks, read our [Llama 3.3 Blog](https://unsloth.ai/blog/llama3-3).\n- Benchmarking of Unsloth was also conducted by [Hugging Face](https://huggingface.co/blog/unsloth-trl).\n\nWe tested using the Alpaca  Dataset, a batch size of 2, gradient accumulation steps of 4, rank = 32, and applied QLoRA on all linear layers (q, k, v, o, gate, up, down):\n  \nModel\n----------------\nLlama 3.3 (70B)\nLlama 3.1 (8B)\n\nContext length benchmarks",
      "word_count": 453
    },
    {
      "chunk_id": "unsloth::chunk_2",
      "text": "Llama 3.1 (8B) max. context length\nWe tested Llama 3.1 (8B) Instruct and did 4bit QLoRA on all linear layers (Q, K, V, O, gate, up and down) with rank = 32 with a batch size of 1. We padded all sequences to a certain maximum sequence length to mimic long context finetuning workloads.\nGPU VRAM\n----------\n8 GB\n12 GB\n16 GB\n24 GB\n40 GB\n48 GB\n80 GB\n\nLlama 3.3 (70B) max. context length\nWe tested Llama 3.3 (70B) Instruct on a 80GB A100 and did 4bit QLoRA on all linear layers (Q, K, V, O, gate, up and down) with rank = 32 with a batch size of 1. We padded all sequences to a certain maximum sequence length to mimic long context finetuning workloads.\n\nGPU VRAM\n----------\n48 GB\n80 GB\n\nCitation\n\nYou can cite the Unsloth repo as follows:\n\nThank You to\n- And of course for every single person who has contributed or has used Unsloth!",
      "word_count": 163
    }
  ],
  "usage_description": "This library is used to significantly accelerate the training process for various large language models, enabling developers to train models 2-5X faster and with 70% less VRAM. Developers can leverage this library to efficiently fine-tune and deploy a range of pre-trained models across different applications and use cases."
}