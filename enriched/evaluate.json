{
  "name": "evaluate",
  "summary": "HuggingFace community-driven open-source library of evaluation",
  "language": "python",
  "tags": [
    "math",
    "ml"
  ],
  "chunks": [
    {
      "chunk_id": "evaluate::chunk_0",
      "text": "Installation\n\nWith pip\n\n Evaluate can be installed from PyPi and has to be installed in a virtual environment (venv or conda for instance)\n\nUsage\n\n Evaluate's main methods are:\n\n- `evaluate.list_evaluation_modules()` to list the available metrics, comparisons and measurements\n- `evaluate.load(module_name, **kwargs)` to instantiate an evaluation module\n- `results = module.compute(*kwargs)` to compute the result of an evaluation module\n\nAdding a new evaluation module\n\nFirst install the necessary dependencies to create a new metric with the following command:\n\nThen you can get started with the following command which will create a new folder for your metric and display the necessary steps:\n\nSee this [step-by-step guide](https://huggingface.co/docs/evaluate/creating_and_sharing) in the documentation for detailed instructions.\n\nCredits\n\nThanks to [@marella](https://github.com/marella) for letting us use the `evaluate` namespace on PyPi previously used by his [library](https://github.com/marella/evaluate).",
      "word_count": 128
    }
  ],
  "usage_description": "This library is used to provide a collection of evaluation metrics, comparisons, and measurements that can be easily integrated into machine learning workflows. With Evaluate, developers can seamlessly compute results for various evaluation tasks using its simple and intuitive API."
}