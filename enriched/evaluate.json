{
  "name": "evaluate",
  "summary": "HuggingFace community-driven open-source library of evaluation",
  "language": "python",
  "tags": [
    "math",
    "ml"
  ],
  "chunks": [
    {
      "chunk_id": "evaluate::chunk_0",
      "text": "Installation\n\nWith pip\n\n Evaluate can be installed from PyPi and has to be installed in a virtual environment (venv or conda for instance)\n\nUsage\n\n Evaluate's main methods are:\n\n- `evaluate.list_evaluation_modules()` to list the available metrics, comparisons and measurements\n- `evaluate.load(module_name, **kwargs)` to instantiate an evaluation module\n- `results = module.compute(*kwargs)` to compute the result of an evaluation module\n\nAdding a new evaluation module\n\nFirst install the necessary dependencies to create a new metric with the following command:\n\nThen you can get started with the following command which will create a new folder for your metric and display the necessary steps:\n\nSee this [step-by-step guide](https://huggingface.co/docs/evaluate/creating_and_sharing) in the documentation for detailed instructions.\n\nCredits\n\nThanks to [@marella](https://github.com/marella) for letting us use the `evaluate` namespace on PyPi previously used by his [library](https://github.com/marella/evaluate).",
      "word_count": 128
    }
  ],
  "usage_description": "Here is a 2-sentence summary of what a developer can achieve with this library:\n\nThis library is used to provide a suite of evaluation tools for various machine learning and natural language processing tasks, allowing developers to easily compute and compare metrics, measurements, and comparisons. With Evaluate, developers can quickly integrate a wide range of evaluation modules into their projects, streamlining the process of model evaluation and comparison."
}