{
  "name": "x-transformers",
  "summary": "X-Transformers",
  "language": "python",
  "tags": [
    "math",
    "ml"
  ],
  "chunks": [
    {
      "chunk_id": "x-transformers::chunk_0",
      "text": "x-transformers\n\nA concise but fully-featured transformer, complete with a set of promising e**x**perimental features from various papers.\n\nInstall\n\nUsage\n\nFull encoder / decoder\n\nDecoder-only (GPT-like)\n\nGPT3 would be approximately the following (but you wouldn't be able to run it anyways)\n\nEncoder-only (BERT-like)\n\nState of the art image classification (SimpleViT)\n\nImage -> caption\n\nPaLI, state of the art language-vision model\n\nDropouts\n\nFeatures\n\nFlash Attention\n\nWhat originally started off as a short paper from Markus Rabe culminated as a practical fused attention CUDA kernel, named Flash Attention by Tri Dao.\n\nThe technique processes the attention matrix in tiles, only keeping track of the running softmax and exponentiated weighted sums. By recomputing on the backwards pass in a tiled fashion, one is able to keep the memory linear with respect to sequence length. This allows a lot of recent models  to be able to reach for longer context lengths without worrying about the memory bottleneck.\n\nOther engineering decisions made by Tri Dao led to its enormous success, namely minimizing HBM accesses so that both the forwards and backwards outperform naive attention. In other words, flash attention is not only more memory efficient, but faster as well, making it a necessity for training transformers.\n\nMetaAI has recently added the ability to use Tri Dao's CUDA kernel through the scaled_dot_product_attention function in Pytorch 2.0. (They also have a `mem_efficient` attention, which is identical to flash attention design, just that the tiles are traversed differently)\n\nLlama was trained using Flash Attention. The only reason to avoid it is if you require operating on the attention matrix (dynamic positional bias, talking heads, residual attention).\n\nYou can use it in this repository by setting `attn_flash` to `True` and enjoy the immediate memory savings and increase in speed.\n\nex.\n\nAugmenting Self-attention with Persistent Memory\n\nProposes adding learned memory key / values prior to attention. They were able to remove feedforwards altogether and attain similar performance to the original transformers. I have found that keeping the feedforwards and adding the memory key / values leads to even better performance.\n\nMemory Transformers\n\nProposes adding learned tokens, akin to CLS tokens, named memory tokens, that is passed through the attention layers alongside the input tokens. This setting is compatible with both encoder and decoder training.\n\nUpdate: MetaAI researchers have found that adding memory tokens (they call them register tokens), alleviates outliers (which is suspected now to be a pathology of attention networks unable to attend to nothing).\n\nUpdate 2: a hybrid architecture out of Nvidia named Hymba used memory tokens successfully in the autoregressive case, termed meta tokens in their paper.\n\nUpdate 3: further corroborated by a paper trying to extend memory in attention networks, termed persistent memory\n\nTransformers Without Tears\n\nThey experiment with alternatives to Layer normalization and found one that is both effective and simpler. Researchers have shared with me this leads to faster convergence.",
      "word_count": 475
    },
    {
      "chunk_id": "x-transformers::chunk_1",
      "text": "You can also use the l2 normalized embeddings proposed as part of `fixnorm`. I have found it leads to improved convergence, when paired with small initialization (proposed by BlinkDL). The small initialization will be taken care of as long as `l2norm_embed` is set to `True`\n\nAlong the same lines of l2 normalized embeddings, Huggingface's 175B parameter BLOOM also places a layernorm right after the embeddings and just before the tokens enter the attention layers. This was corroborated by Yandex's 100B parameter YaLM to stabilize training.\n\nIt is recommended you either have either `l2norm_embed` or `post_emb_norm` set to `True` but not both, as they probably serve the same purpose.\n\nRoot Mean Square Layer Normalization\n\nThe authors propose to replace layer normalization with a simpler alternative, without mean centering and the learned bias. An investigative paper found this to be the best performing normalization variant. It was also used in Deepmind's latest large language models, Retro and Gopher.\n\n*July 2023* A linear attention paper has experiments to show that removing the learned multiplicative gamma led to no performance degradation. This simplifies the RMS normalization to a satisfying `l2norm(x) * sqrt(dim)`.\n\nGLU Variants Improve Transformer\n\nNoam Shazeer paper that explores gating in the feedforward, finding that simple gating with GELU leads to significant improvements. This variant also showed up in the latest mT5 architecture. You should always turn this on (I may eventually turn it on by default).\n\nThe PaLM language model also chose to use the Swish GLU variant. You can turn this on by setting two flags\n\npython\nimport torch\nfrom x_transformers import TransformerWrapper, Decoder, Encoder\n\nmodel = TransformerWrapper(\n)\npython\nimport torch\nfrom x_transformers import TransformerWrapper, Decoder, Encoder\n\nmodel = TransformerWrapper(\n)\npython\nimport torch\nfrom x_transformers import TransformerWrapper, Decoder\n\nmodel = TransformerWrapper(\n)\npython\nmodel = TransformerWrapper(\n)\npython\nimport torch\nfrom x_transformers import TransformerWrapper, Decoder\n\nmodel = TransformerWrapper(\n)\npython\nimport torch\nfrom x_transformers import TransformerWrapper, Decoder\n\nmodel = TransformerWrapper(\n)\npython\nimport torch\nfrom x_transformers import TransformerWrapper, Decoder\n\nmodel = TransformerWrapper(\n)\npython\nimport torch\nfrom x_transformers import TransformerWrapper, Encoder\n\nmodel = TransformerWrapper(\n)\npython\nimport torch\nfrom x_transformers import TransformerWrapper, Encoder\n\nmodel = TransformerWrapper(\n)\npython\nimport torch\nfrom x_transformers import TransformerWrapper, Encoder\n\nmodel = TransformerWrapper(\n)\npython\nimport torch\nfrom x_transformers import TransformerWrapper, Encoder\n\nmodel = TransformerWrapper(\n)\npython\nimport torch\nfrom x_transformers import TransformerWrapper, Decoder\n\nmodel = TransformerWrapper(\n)\npython\nimport torch\nfrom x_transformers import TransformerWrapper, Encoder\n\nmodel = TransformerWrapper(\n)\npython\nimport torch\nfrom x_transformers import TransformerWrapper, Decoder\n\nmodel = TransformerWrapper(\n)\npython\nimport torch\nfrom x_transformers import TransformerWrapper, Encoder\n\nmodel = TransformerWrapper(\n)\npython\nimport torch\nfrom x_transformers import XTransformer\n\nmodel = XTransformer(\n)\npython\nimport torch\nfrom x_transformers import TransformerWrapper, Decoder\n\nmodel_xl = TransformerWrapper(\n)\n\nseg1 = torch.randint(0, 20000, (1, 512))\nseg2 = torch.randint(0, 20000, (1, 512))\nseg3 = torch.randint(0, 20000, (1, 512))\n\nlogits1, mems1  = model_xl(seg1, return_mems = True)\nlogits2, mems2  = model_xl(seg2, mems = mems1, return_mems = True)\nlogits3, mems3  = model_xl(seg3, mems = mems2, return_mems = True)\npython",
      "word_count": 500
    },
    {
      "chunk_id": "x-transformers::chunk_2",
      "text": "pass in the above model_xl\n\nxl_wrapper = XLAutoregressiveWrapper(model_xl)\n\nseg = torch.randint(0, 20000, (1, 4096)).cuda()  # sequence exceeding max length, automatically segmented and memory managed\n\nloss = xl_wrapper(seg)\nloss.backward()\n\nthen, after much training\n\nprime = seg[:, :1024]   # if prime exceeds max length, memory will be caught up before generating\n\ngenerated = xl_wrapper.generate(prime, 4096)  # (1, 4096)\npython\nimport torch\nfrom x_transformers import TransformerWrapper, Decoder\n\nmodel_xl = TransformerWrapper(\n)\n\nseg1 = torch.randint(0, 20000, (1, 512))\nseg2 = torch.randint(0, 20000, (1, 512))\nseg3 = torch.randint(0, 20000, (1, 512))\n\nlogits1, mems1  = model_xl(seg1, return_mems = True)\nlogits2, mems2  = model_xl(seg2, mems = mems1, return_mems = True) # mems1 of layer N are automatically routed to the layer N-1\npython\nimport torch\nfrom x_transformers import TransformerWrapper, Decoder\n\nmodel = TransformerWrapper(\n)\npython\nimport torch\nfrom x_transformers import TransformerWrapper, Decoder\n\nmodel = TransformerWrapper(\n)\npython\nimport torch\nfrom x_transformers import TransformerWrapper, Decoder\n\nmodel = TransformerWrapper(\n)\npython\nimport torch\nfrom x_transformers import TransformerWrapper, Decoder\n\nmodel = TransformerWrapper(\n)\npython\nimport torch\nfrom x_transformers import TransformerWrapper, Decoder\n\nmodel = TransformerWrapper(\n)\npython\nimport torch\nfrom x_transformers import TransformerWrapper, Decoder\n\nmodel = TransformerWrapper(\n)\npython\nimport torch\nfrom x_transformers import TransformerWrapper, Decoder\n\nmodel = TransformerWrapper(\n)\npython\nimport torch\nfrom x_transformers import TransformerWrapper, Decoder\n\nmodel = TransformerWrapper(\n)\n\nx = torch.randint(0, 20000, (1, 1024))\nmodel(x)\npython\nimport torch\nfrom x_transformers import TransformerWrapper, Decoder\n\nmodel = TransformerWrapper(\n)\n\nx = torch.randint(0, 20000, (1, 1024))\nmodel(x)\npython\nimport torch\nfrom x_transformers import TransformerWrapper, Decoder\n\nmodel = TransformerWrapper(\n)\n\nx = torch.randint(0, 20000, (1, 1024))\nmodel(x)\npython\nimport torch\nfrom x_transformers import TransformerWrapper, Decoder\n\nmodel = TransformerWrapper(\n)\n\nx = torch.randint(0, 20000, (1, 1024))\nmodel(x)\npython\nimport torch\nfrom x_transformers import TransformerWrapper, Decoder\n\nmodel = TransformerWrapper(\n)\n\nx = torch.randint(0, 20000, (1, 1024))\nmodel(x)\npython\nimport torch\nfrom x_transformers import TransformerWrapper, Decoder\n\nmodel = TransformerWrapper(\n)\n\nx = torch.randint(0, 20000, (1, 1024))\nmodel(x)\npython\nimport torch\nfrom x_transformers import TransformerWrapper, Decoder\n\nmodel = TransformerWrapper(\n)\n\nx = torch.randint(0, 20000, (1, 1024))\nmodel(x)\npython\nimport torch\nfrom x_transformers import TransformerWrapper, Decoder\n\nmodel = TransformerWrapper(\n)\n\nx = torch.randint(0, 256, (1, 1024))\nmodel(x)\npython\nimport torch\nfrom x_transformers import TransformerWrapper, Decoder\n\nmodel = TransformerWrapper(\n)\n\nx = torch.randint(0, 20000, (1, 1024))\nmodel(x)\npython\nimport torch\nfrom x_transformers import TransformerWrapper, Decoder, AutoregressiveWrapper\n\nmodel = TransformerWrapper(\n)\n\nmodel = AutoregressiveWrapper(\n).cuda()\n\nmock data\n\nx = torch.randint(0, 20000, (1, 1024)).cuda()\n\nderive cross entropy loss, masking all taken care of\n\nloss = model(x)\nloss.backward()\npython\nimport torch\nfrom x_transformers import Encoder, CrossAttender\n\nenc = Encoder(dim = 512, depth = 6)\nmodel = CrossAttender(dim = 512, depth = 6)\n\nnodes = torch.randn(1, 1, 512)\nnode_masks = torch.ones(1, 1).bool()\n\nneighbors = torch.randn(1, 5, 512)\nneighbor_masks = torch.ones(1, 5).bool()\n\nencoded_neighbors = enc(neighbors, mask = neighbor_masks)\nmodel(nodes, context = encoded_neighbors, mask = node_masks, context_mask = neighbor_masks) # (1, 1, 512)\n\npython\nimport torch\nfrom x_transformers import ContinuousTransformerWrapper, Decoder\n\nmodel = ContinuousTransformerWrapper(\n)\n\nx = torch.randn((1, 1024, 32))\nmask = torch.ones(1, 1024).bool()",
      "word_count": 491
    },
    {
      "chunk_id": "x-transformers::chunk_3",
      "text": "model(x, mask = mask) # (1, 1024, 100)\npython\nimport torch\nfrom x_transformers import ContinuousTransformerWrapper, Decoder\nfrom x_transformers import ContinuousAutoregressiveWrapper\n\nmodel = ContinuousTransformerWrapper(\n)\n\nwrap it with the continuous autoregressive wrapper\n\nmodel = ContinuousAutoregressiveWrapper(model)\n\nmock data\n\nx = torch.randn((1, 1024, 777))\nmask = torch.ones(1, 1024).bool()\n\ntrain on a lot of data above\n\nloss = model(x, mask = mask)\nloss.backward\n\nthen generate\n\nstart_emb = torch.randn(1, 777)\ngenerated = model.generate(start_emb, 17) # (17, 777)\npython\nimport torch\n\nfrom x_transformers import (\n)\n\nmodel = XValTransformerWrapper(\n)\n\nwrap it with the xval autoregressive wrapper\n\nmodel = XValAutoregressiveWrapper(model)\n\nmock data\n\nids = torch.randint(0, 4, (1, 777))\nnums = torch.randn(1, 777)\n\ntrain on a lot of data above\n\nloss = model(ids, nums)\nloss.backward()\n\nthen generate\n\nstart_ids = torch.randint(0, 4, (1, 1))\nstart_nums = torch.randn(1, 1)\n\nids_out, num_out, is_number_mask = model.generate(start_ids, start_nums, 17)\n\n(1, 17), (1, 17), (1, 17)\n\ndiscrete, continuous, mask for discrete / continuous\nbibtex\n@misc{vaswani2017attention,\n}\nbibtex\n@article{DBLP:journals/corr/abs-1907-01470,\n}\nbibtex\n@article{1910.05895,\n}\nbibtex\n@misc{shazeer2020glu,\n}\nbibtex\n@inproceedings{Zoph2022STMoEDS,\n}\nbibtex\n@misc{bhojanapalli2020lowrank,\n}\nbibtex\n@misc{burtsev2020memory,\n}\nbibtex\n@misc{zhao2019explicit,\n}\nbibtex\n@misc{correia2019adaptively,\n}\nbibtex\n@misc{shazeer2020talkingheads,\n}\nbibtex\n@misc{press2020improving,\n}\nbibtex\n@misc{lu2019understanding,\n}\nbibtex\n@misc{ke2020rethinking,\n}\nbibtex\n@misc{dosovitskiy2020image,\n}\nbibtex\n@misc{huang2019attention,\n}\nbibtex\n@misc{raffel2020exploring,\n}\nbibtex\n@inproceedings{martins-etal-2020-sparse,\n}\nbibtex\n@misc{he2020realformer,\n}\nbibtex\n@misc{carion2020endtoend,\n}\nbibtex\n@misc{press2021ALiBi,\n}\nbibtex\n@misc{parisotto2019stabilizing,\n}\nbibtex\n@misc{narang2021transformer,\n}\nbibtex\n@misc{zhang2019root,\n}\nbibtex\n@inproceedings{Qin2023ScalingTT,\n}\nbibtex\n@misc{su2021roformer,\n}\nbibtex\n@inproceedings{Yang2025RopeTN,\n}\nbibtex\n@inproceedings{Chen2023ExtendingCW,\n}\nbibtex\n@inproceedings{Sun2022ALT,\n  title     = {A Length-Extrapolatable Transformer},\n  author    = {Yutao Sun and Li Dong and Barun Patra and Shuming Ma and Shaohan Huang and Alon Benhaim and Vishrav Chaudhary and Xia Song and Furu Wei},\n  year      = {2022}\n}\nbibtex\n@Article{AlphaFold2021,\n}\nbibtex\n@software{peng_bo_2021_5196578,\n}\nbibtex\n@misc{csord√°s2021devil,\n}\nbibtex\n@misc{so2021primer,\n}\nbibtex\n@misc{ding2021erniedoc,\n}\nbibtex\n@misc{ding2021cogview,\n}\nbibtex\n@inproceedings{anonymous2022normformer,\n}\nbibtex\n@misc{henry2020querykey,\n}\nbibtex\n@misc{liu2021swin,\n}\nbibtex\n@article{Haviv2022TransformerLM,\n}\nbibtex\n@article{chowdhery2022PaLM,\n}\nbibtex\n@article{Shazeer2019FastTD,\n}\nbibtex\n@article{Ainslie2023GQATG,\n}\nbibtex\n@article{Liu2022FCMFC,\n}\nbibtex\n@inproceedings{Huang2016DeepNW,\n}\nbibtex\n@inproceedings{Hua2022TransformerQI,\n}\nbibtex\n@article{Chang2022MaskGITMG,\n}\nbibtex\n@article{Lezama2022ImprovedMI,\n}\nbibtex\n@misc{https://doi.org/10.48550/arxiv.2302.01327,\n}\nbibtex\n@inproceedings{dao2022flashattention,\n}\nbibtex\n@inproceedings{Dehghani2023ScalingVT,\n}\nbibtex\n@article{Beyer2022BetterPV,\n}\nbibtex\n@article{Kazemnejad2023TheIO,\n}\nbibtex\n@misc{bloc97-2023\n}\nbibtex\n@inproceedings{Zoph2022STMoEDS,\n}\nbibtex\n@article{Lan2019ALBERTAL,\n}\nbibtex\n@inproceedings{Li2022ContrastiveDO,\n}\nbibtex\n@inproceedings{OBrien2023ContrastiveDI,\n}\nbibtex\n@inproceedings{Darcet2023VisionTN,\n}\nbibtex\n@article{Bondarenko2023QuantizableTR,\n}\nbibtex\n@inproceedings{Golkar2023xValAC,\n}\nbibtex\n@article{Wang2022DeepNetST,\n}\nbibtex\n@article{Rafailov2023DirectPO,\n}\nbibtex\n@misc{xAI2024Grok,\n}\nbibtex\n@inproceedings{Golovneva2024ContextualPE,\n}\nbibtex\n@article{Peebles2022ScalableDM,\n}\nbibtex\n@misc{Rubin2024,\n}\nbibtex\n@article{Mesnard2024GemmaOM,\n}\nbibtex\n@article{Nguyen2024MinPS,\n}\nbibtex\n@article{Bao2022AllAW,\n}\nbibtex\n@article{Jumper2021HighlyAP,\n}\nbibtex\n@article{Yang2017BreakingTS,\n}\nbibtex\n@inproceedings{Kanai2018SigsoftmaxRO,\nbibtex\n@article{Kim2020TheLC,\n}\nbibtex\n@inproceedings{Ramapuram2024TheoryAA,\n}\nbibtex\n@inproceedings{Leviathan2024SelectiveAI,\n}\nbibtex\n@article{Bai2019DeepEM,\n}\nbibtex\n@article{Wu2021MuseMorphoseFA,\n}\nbibtex\n@inproceedings{Zhou2024ValueRL,\n}\nbibtex\n@inproceedings{anonymous2024forgetting,\n}\nbibtex\n@inproceedings{anonymous2024from,\n}\nbibtex\n@inproceedings{Duvvuri2024LASERAW,\n}\nbibtex\n@article{Zhu2024HyperConnections,\n}\nbibtex\n@inproceedings{anonymous2024hymba,\n}\nbibtex\n@article{Shao2024DeepSeekV2AS,\n}\nbibtex\n@inproceedings{Gerasimov2025YouDN,\n}\nbibtex\n@inproceedings{Hu2024TheBS,\n}\nbibtex\n@article{Charpentier2024GPTOB,\n}\nbibtex\n@inproceedings{Zhu2025TransformersWN,\n}\nbibtex\n@article{Pagnoni2024ByteLT,\n}\nbibtex\n@misc{Jordan2024,\n}\nbibtex\n@inproceedings{Assran2025VJEPA2S,\n}\nbibtex\n@misc{bloem2025universalpretrainingiteratedrandom,\n}\nbibtex\n@misc{openai_gpt_oss,\n}\nbibtex\n@article{Sahoo2024SimpleAE,\n}\nbibtex\n@misc{kimiteam2025kimik2openagentic,\n}\nbibtex\n@misc{zhao2023learningfinegrainedbimanualmanipulation,\n}\nbibtex\n@misc{jordan2024muon,\n  author    = {Keller Jordan and Yuchen Jin and Vlado Boza and Jiacheng You and Franz Cesista and Laker Newhouse and Jeremy Bernstein},\n  title     = {Muon: An optimizer for hidden layers in neural networks},\n  year      = {2024},\n  url       = {https://kellerjordan.github.io/posts/muon/}\n}\nbibtex\n@misc{wang2025muonoutperformsadamtailend,\n}\nbibtex\n@misc{yan2017hierarchicalmultiscaleattentionnetworks,\n}\nbibtex\n@misc{lv2025expressiveattentionnegativeweights,\n}\nbibtex\n@inproceedings{Fleuret2025TheFT,\n}\nbibtex\n@inproceedings{anonymous2025beliefformer,\n}\nbibtex\n@misc{chen2025strongernormalizationfreetransformers,\n}\nbibtex\n@misc{gopalakrishnan2025decouplingwhatwherepolar,\n}\n```",
      "word_count": 534
    },
    {
      "chunk_id": "x-transformers::chunk_4",
      "text": "*solve intelligence... then use that to solve everything else.* - Demis Hassabis",
      "word_count": 12
    }
  ]
}