{
  "name": "semchunk",
  "summary": "A fast, lightweight and easy-to-use Python library for splitting text into semantically meaningful chunks.",
  "language": "python",
  "tags": [
    "math",
    "ml",
    "web"
  ],
  "chunks": [
    {
      "chunk_id": "semchunk::chunk_0",
      "text": "semchunk\n\n**`semchunk`** by [**Isaacus**](https://isaacus.com/) is a fast, lightweight and easy-to-use Python library for splitting text into semantically meaningful chunks.\n\nIt has built-in support for tokenizers from OpenAI's `tiktoken` and Hugging Face's `transformers` and `tokenizers` libraries, in addition to supporting custom tokenizers and token counters. It can also overlap chunks as well as return their offsets.\n\nPowered by an efficient yet highly accurate chunking algorithm ([How It Works ](https://github.com/isaacus-dev/semchunk#how-it-works-)), `semchunk` produces chunks that are more semantically meaningful than regular token and recursive character chunkers like `langchain`'s `RecursiveCharacterTextSplitter`, while also being 85% faster than its closest alternative, `semantic-text-splitter` ([Benchmarks ](https://github.com/isaacus-dev/semchunk#benchmarks-)).\n\n`semchunk` is production ready, being used every day in the [Isaacus API](https://docs.isaacus.com) to split extremely long legal documents into more manageable chunks for our [Kanon legal AI models](https://docs.isaacus.com/models).\n\nInstallation \n`semchunk` can be installed with `pip`:\n\n`semchunk` is also available on `conda-forge`:\n\nIn addition, [@dominictarro](https://github.com/dominictarro) maintains a Rust port of `semchunk` named [`semchunk-rs`](https://crates.io/crates/semchunk-rs).\n\nQuickstart ‍\nThe code snippet below demonstrates how to chunk text with `semchunk`:\n\nUsage ️\n\n`chunkerify()`\n\n`chunkerify()` constructs a chunker that splits one or more texts into semantically meaningful chunks of a specified size as determined by the provided tokenizer or token counter.\n\n`tokenizer_or_token_counter` is either: the name of a `tiktoken` or `transformers` tokenizer (with priority given to the former); a tokenizer that possesses an `encode` attribute (e.g., a `tiktoken`, `transformers` or `tokenizers` tokenizer); or a token counter that returns the number of tokens in an input.\n\n`chunk_size` is the maximum number of tokens a chunk may contain. It defaults to `None` in which case it will be set to the same value as the tokenizer's `model_max_length` attribute (deducted by the number of tokens returned by attempting to tokenize an empty string) if possible, otherwise a `ValueError` will be raised.\n\n`max_token_chars` is the maximum number of characters a token may contain. It is used to significantly speed up the token counting of long inputs. It defaults to `None` in which case it will either not be used or will, if possible, be set to the number of characters in the longest token in the tokenizer's vocabulary as determined by the `token_byte_values` or `get_vocab` methods.\n\n`memoize` flags whether to memoize the token counter. It defaults to `True`.\n\n`cache_maxsize` is the maximum number of text-token count pairs that can be stored in the token counter's cache. It defaults to `None`, which makes the cache unbounded. This argument is only used if `memoize` is `True`.",
      "word_count": 402
    },
    {
      "chunk_id": "semchunk::chunk_1",
      "text": "This function returns a chunker that takes either a single text or a sequence of texts and returns, depending on whether multiple texts have been provided, a list or list of lists of chunks up to `chunk_size`-tokens-long with any whitespace used to split the text removed, and, if the optional `offsets` argument to the chunker is `True`, a list or lists of tuples of the form `(start, end)` where `start` is the index of the first character of a chunk in a text and `end` is the index of the character succeeding the last character of the chunk such that `chunks[i] == text[offsets[i][0]:offsets[i][1]]`.\n\nThe resulting chunker can be passed a `processes` argument that specifies the number of processes to be used when chunking multiple texts.\n\nIt is also possible to pass a `progress` argument which, if set to `True` and multiple texts are passed, will display a progress bar.\n\nAs described above, the `offsets` argument, if set to `True`, will cause the chunker to return the start and end offsets of each chunk.\n\nThe chunker accepts an `overlap` argument that specifies the proportion of the chunk size, or, if >=1, the number of tokens, by which chunks should overlap. It defaults to `None`, in which case no overlapping occurs.\n\n`chunk()`\n\n`chunk()` splits a text into semantically meaningful chunks of a specified size as determined by the provided token counter.\n\n`text` is the text to be chunked.\n\n`chunk_size` is the maximum number of tokens a chunk may contain.\n\n`token_counter` is a callable that takes a string and returns the number of tokens in it.\n\n`memoize` flags whether to memoize the token counter. It defaults to `True`.\n\n`offsets` flags whether to return the start and end offsets of each chunk. It defaults to `False`.\n\n`overlap` specifies the proportion of the chunk size, or, if >=1, the number of tokens, by which chunks should overlap. It defaults to `None`, in which case no overlapping occurs.\n\n`cache_maxsize` is the maximum number of text-token count pairs that can be stored in the token counter's cache. It defaults to `None`, which makes the cache unbounded. This argument is only used if `memoize` is `True`.\n\nThis function returns a list of chunks up to `chunk_size`-tokens-long, with any whitespace used to split the text removed, and, if `offsets` is `True`, a list of tuples of the form `(start, end)` where `start` is the index of the first character of the chunk in the original text and `end` is the index of the character after the last character of the chunk such that `chunks[i] == text[offsets[i][0]:offsets[i][1]]`.",
      "word_count": 427
    },
    {
      "chunk_id": "semchunk::chunk_2",
      "text": "How It Works \n`semchunk` works by recursively splitting texts until all resulting chunks are equal to or less than a specified chunk size. In particular, it:\n1. Splits text using the most semantically meaningful splitter possible;\n1. Recursively splits the resulting chunks until a set of chunks equal to or less than the specified chunk size is produced;\n1. Merges any chunks that are under the chunk size back together until the chunk size is reached;\n1. Reattaches any non-whitespace splitters back to the ends of chunks barring the final chunk if doing so does not bring chunks over the chunk size, otherwise adds non-whitespace splitters as their own chunks; and\n1. Since version 3.0.0, excludes chunks consisting entirely of whitespace characters.\n\nTo ensure that chunks are as semantically meaningful as possible, `semchunk` uses the following splitters, in order of precedence:\n1. The largest sequence of newlines (`\\n`) and/or carriage returns (`\\r`);\n1. The largest sequence of tabs;\n1. The largest sequence of whitespace characters (as defined by regex's `\\s` character class) or, since version 3.2.0, if the largest sequence of whitespace characters is only a single character and there exist whitespace characters preceded by any of the semantically meaningful non-whitespace characters listed below (in the same order of precedence), then only those specific whitespace characters;\n1. Sentence terminators (`.`, `?`, `!` and `*`);\n1. Clause separators (`;`, `,`, `(`, `)`, `[`, `]`, `“`, `”`, `‘`, `’`, `'`, `\"` and `` ` ``);\n1. Sentence interrupters (`:`, `—` and `…`);\n1. Word joiners (`/`, `\\`, `–`, `&` and `-`); and\n1. All other characters.\n\nIf overlapping chunks have been requested, `semchunk` also:\n1. Internally reduces the chunk size to `min(overlap, chunk_size - overlap)` (`overlap` being computed as `floor(chunk_size * overlap)` for relative overlaps and `min(overlap, chunk_size - 1)` for absolute overlaps); and\n1. Merges every `floor(original_chunk_size / reduced_chunk_size)` chunks starting from the first chunk and then jumping by `floor((original_chunk_size - overlap) / reduced_chunk_size)` chunks until the last chunk is reached.\n\nBenchmarks \nOn a desktop with a Ryzen 9 7900X, 96 GB of DDR5 5600MHz CL40 RAM, Windows 11 and Python 3.12.4, it takes `semchunk` 3.04 seconds to split every sample in [NLTK's Gutenberg Corpus](https://www.nltk.org/howto/corpus.html#plaintext-corpora) into 512-token-long chunks with GPT-4's tokenizer (for context, the Corpus contains 18 texts and 3,001,260 tokens). By comparison, it takes [`semantic-text-splitter`](https://pypi.org/project/semantic-text-splitter/) (with multiprocessing) 24.84 seconds to chunk the same texts into 512-token-long chunks — a difference of 87.76%.\n\nThe code used to benchmark `semchunk` and `semantic-text-splitter` is available [here](https://github.com/isaacus-dev/semchunk/blob/main/tests/bench.py).\n\nLicence \nThis library is licensed under the [MIT License](https://github.com/isaacus-dev/semchunk/blob/main/LICENCE).",
      "word_count": 422
    }
  ]
}