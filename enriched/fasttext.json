{
  "name": "fasttext",
  "summary": "fasttext Python bindings",
  "language": "python",
  "tags": [
    "data",
    "math",
    "web"
  ],
  "chunks": [
    {
      "chunk_id": "fasttext::chunk_0",
      "text": "fastText |CircleCI|\n===================\n\n`fastText `__ is a library for efficient learning\nof word representations and sentence classification.\n\nIn this document we present how to use fastText in python.\n\nTable of contents\n-----------------\n\n-  `Requirements `__\n-  `Installation `__\n-  `Usage overview `__\n-  `Word representation model `__\n-  `Text classification model `__\n-  `IMPORTANT: Preprocessing data / encoding\n   conventions `__\n-  `More examples `__\n-  `API `__\n-  `train_unsupervised parameters `__\n-  `train_supervised parameters `__\n-  `model object `__\n\nRequirements\n============\n\n`fastText `__ builds on modern Mac OS and Linux\ndistributions. Since it uses C++11 features, it requires a compiler with\ngood C++11 support. You will need `Python `__\n(version 2.7 or â‰¥ 3.4), `NumPy `__ &\n`SciPy `__ and\n`pybind11 `__.\n\nInstallation\n============\n\nTo install the latest release, you can do :\n\n.. code:: bash\n\nor, to get the latest development version of fasttext, you can install\nfrom our github repository :\n\n.. code:: bash\n\nUsage overview\n==============\n\nWord representation model\n-------------------------\n\nIn order to learn word vectors, as `described\nhere `__,\nwe can use ``fasttext.train_unsupervised`` function like this:\n\n.. code:: py\n\nwhere ``data.txt`` is a training file containing utf-8 encoded text.\n\nThe returned ``model`` object represents your learned model, and you can\nuse it to retrieve information.\n\n.. code:: py\n\nSaving and loading a model object\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nYou can save your trained model object by calling the function\n``save_model``.\n\n.. code:: py\n\nand retrieve it later thanks to the function ``load_model`` :\n\n.. code:: py\n\nFor more information about word representation usage of fasttext, you\ncan refer to our `word representations\ntutorial `__.\n\nText classification model\n-------------------------\n\nIn order to train a text classifier using the method `described\nhere `__,\nwe can use ``fasttext.train_supervised`` function like this:\n\n.. code:: py\n\nwhere ``data.train.txt`` is a text file containing a training sentence\nper line along with the labels. By default, we assume that labels are\nwords that are prefixed by the string ``__label__``\n\nOnce the model is trained, we can retrieve the list of words and labels:\n\n.. code:: py\n\nTo evaluate our model by computing the precision at 1 (P@1) and the\nrecall on a test set, we use the ``test`` function:\n\n.. code:: py\n\nWe can also predict labels for a specific text :\n\n.. code:: py\n\nBy default, ``predict`` returns only one label : the one with the\nhighest probability. You can also predict more than one label by\nspecifying the parameter ``k``:\n\n.. code:: py\n\nIf you want to predict more than one sentence you can pass an array of\nstrings :\n\n.. code:: py\n\nOf course, you can also save and load a model to/from a file as `in the\nword representation usage `__.\n\nFor more information about text classification usage of fasttext, you\ncan refer to our `text classification\ntutorial `__.\n\nCompress model files with quantization\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~",
      "word_count": 473
    },
    {
      "chunk_id": "fasttext::chunk_1",
      "text": "When you want to save a supervised model file, fastText can compress it\nin order to have a much smaller model file by sacrificing only a little\nbit performance.\n\n.. code:: py\n\n``model_filename.ftz`` will have a much smaller size than\n``model_filename.bin``.\n\nFor further reading on quantization, you can refer to `this paragraph\nfrom our blog\npost `__.\n\nIMPORTANT: Preprocessing data / encoding conventions\n----------------------------------------------------\n\nIn general it is important to properly preprocess your data. In\nparticular our example scripts in the `root\nfolder `__ do this.\n\nfastText assumes UTF-8 encoded text. All text must be `unicode for\nPython2 `__\nand `str for\nPython3 `__.\nThe passed text will be `encoded as UTF-8 by\npybind11 `__\nbefore passed to the fastText C++ library. This means it is important to\nuse UTF-8 encoded text when building a model. On Unix-like systems you\ncan convert text using `iconv `__.\n\nfastText will tokenize (split text into pieces) based on the following\nASCII characters (bytes). In particular, it is not aware of UTF-8\nwhitespace. We advice the user to convert UTF-8 whitespace / word\nboundaries into one of the following symbols as appropiate.\n\n-  space\n-  tab\n-  vertical tab\n-  carriage return\n-  formfeed\n-  the null character\n\nThe newline character is used to delimit lines of text. In particular,\nthe EOS token is appended to a line of text if a newline character is\nencountered. The only exception is if the number of tokens exceeds the\nMAX\\_LINE\\_SIZE constant as defined in the `Dictionary\nheader `__.\nThis means if you have text that is not separate by newlines, such as\nthe `fil9 dataset `__, it will be\nbroken into chunks with MAX\\_LINE\\_SIZE of tokens and the EOS token is\nnot appended.\n\nThe length of a token is the number of UTF-8 characters by considering\nthe `leading two bits of a\nbyte `__ to identify\n`subsequent bytes of a multi-byte\nsequence `__.\nKnowing this is especially important when choosing the minimum and\nmaximum length of subwords. Further, the EOS token (as specified in the\n`Dictionary\nheader `__)\nis considered a character and will not be broken into subwords.\n\nMore examples\n-------------\n\nIn order to have a better knowledge of fastText models, please consider\nthe main\n`README `__\nand in particular `the tutorials on our\nwebsite `__.\n\nYou can find further python examples in `the doc\nfolder `__.\n\nAs with any package you can get help on any Python function using the\nhelp function.\n\nFor example\n\n::\n\nAPI\n===\n\n``train_unsupervised`` parameters\n---------------------------------\n\n.. code:: python\n\n``train_supervised`` parameters\n-------------------------------\n\n.. code:: python\n\n``model`` object\n----------------\n\n``train_supervised``, ``train_unsupervised`` and ``load_model``\nfunctions return an instance of ``_FastText`` class, that we generaly\nname ``model`` object.\n\nThis object exposes those training arguments as properties : ``lr``,\n``dim``, ``ws``, ``epoch``, ``minCount``, ``minCountLabel``, ``minn``,\n``maxn``, ``neg``, ``wordNgrams``, ``loss``, ``bucket``, ``thread``,\n``lrUpdateRate``, ``t``, ``label``, ``verbose``, ``pretrainedVectors``.\nSo ``model.wordNgrams`` will give you the max length of word ngram used\nfor training this model.\n\nIn addition, the object exposes several functions :\n\n.. code:: python",
      "word_count": 499
    },
    {
      "chunk_id": "fasttext::chunk_2",
      "text": "The properties ``words``, ``labels`` return the words and labels from\nthe dictionary :\n\n.. code:: py\n\nThe object overrides ``__getitem__`` and ``__contains__`` functions in\norder to return the representation of a word and to check if a word is\nin the vocabulary.\n\n.. code:: py\n\nJoin the fastText community\n---------------------------\n\n-  `Facebook page `__\n-  `Stack\n   overflow `__\n-  `Google\n   group `__\n-  `GitHub `__\n\n.. |CircleCI| image:: https://circleci.com/gh/facebookresearch/fastText/tree/master.svg?style=svg\n   :target: https://circleci.com/gh/facebookresearch/fastText/tree/master",
      "word_count": 71
    }
  ],
  "usage_description": "This library is used to efficiently learn word representations and classify text using machine learning algorithms. It provides a Python interface for training models on word representation and text classification tasks."
}