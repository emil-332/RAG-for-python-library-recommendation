{
  "name": "pyiqa",
  "summary": "PyTorch Toolbox for Image Quality Assessment",
  "language": "python",
  "tags": [
    "data",
    "dev",
    "math",
    "ml",
    "ui"
  ],
  "chunks": [
    {
      "chunk_id": "pyiqa::chunk_0",
      "text": "PyTorch Toolbox for Image Quality Assessment\n\nAn IQA toolbox with pure python and pytorch. Please refer to [Awesome-Image-Quality-Assessment](https://github.com/chaofengc/Awesome-Image-Quality-Assessment) for a comprehensive survey of IQA methods and download links for IQA datasets.\n\n(https://pypi.org/project/pyiqa/)\n(https://pepy.tech/project/pyiqa)\n\n(https://github.com/chaofengc/Awesome-Image-Quality-Assessment)\n(https://github.com/chaofengc/IQA-PyTorch/blob/main/README.md#bookmark_tabs-citation)\n(https://www.zhihu.com/column/c_1565424954811846656)\n\n:open_book: Introduction\n\nThis is a comprehensive image quality assessment (IQA) toolbox built with **pure Python and PyTorch**. We provide reimplementation of many widely used full reference (FR) and no reference (NR) metrics, **with results calibrated against official MATLAB scripts when available**. With GPU acceleration, **our implementations are much faster than their Matlab counterparts.** Please refer to the following documents for details:\n\n[Model Cards](docs/ModelCard.md)  |  ️ [Dataset Cards](docs/Dataset_Preparation.md) |  [Datasets Download](https://huggingface.co/datasets/chaofengc/IQA-Toolbox-Datasets/tree/main) |  [Documentation](https://iqa-pytorch.readthedocs.io/en/latest/) | [Benchmark](https://github.com/chaofengc/IQA-PyTorch/tree/main?tab=readme-ov-file#performance-evaluation-protocol)\n\n---\n\n:triangular_flag_on_post: Updates/Changelog\n- **Jun, 2025**. Add `sfid`, a commonly used metric in generative models.\n- **Nov, 2024**. Add `pyiqa.load_dataset` for easy loading of several common datasets. \n- **Nov, 2024**. Add `compare2score` and `deepdc`. Thanks to [hanwei](https://github.com/h4nwei) for their great work , and please refer to their official papers for more details! \n- [**More**](docs/history_changelog.md)\n\n---\n\n:zap: Quick Start\n\nInstallation\n\nBasic Usage\n\nYou can simply use the package with commandline interface.\n\nAdvanced Usage with Codes\n\nTest metrics\n\nUse as loss functions\n\nNote that gradient propagation is disabled by default. Set `as_loss=True` to enable it as a loss function. **Not all metrics support backpropagation, please refer to [Model Cards](docs/ModelCard.md) and be sure that you are using it in a `lower_better` way.**\n\nUse custom settings and weights\n\nWe also provide a flexible way to use custom settings and weights in case you want to retrain or fine-tune the models.\n\nExample test script\n\nExample test script with input directory/images and reference directory/images.\n\nEasy load of popular datasets\n\nWe offer an easy way to load popular IQA datasets through the configuration file `pyiqa/default_dataset_configs.yml`. The specified datasets will automatically download from the [huggingface IQA-PyTorch-Dataset](https://huggingface.co/datasets/chaofengc/IQA-PyTorch-Datasets). See example code below:\n\n**Please refer to [Dataset Cards](docs/Dataset_Preparation.md) for more details about the `dataset_opts`.**\n\n:1st_place_medal: Benchmark Performances and Model Zoo\n\nResults Calibration\n\nPlease refer to the [results calibration](./ResultsCalibra/ResultsCalibra.md) to verify the correctness of the python implementations compared with official scripts in matlab or python.\n\n⏬ Download Benchmark Datasets\n\nFor convenience, we upload all related datasets to [huggingface IQA-Toolbox-Dataset](https://huggingface.co/datasets/chaofengc/IQA-Toolbox-Datasets), and corresponding meta information files to [huggingface IQA-Toolbox-Dataset-metainfo](https://huggingface.co/datasets/chaofengc/IQA-Toolbox-Datasets-metainfo). \nHere are example codes to download them from huggingface:\n\n>[!CAUTION]\n> we only collect the datasets for academic, research, and educational purposes. It is important for the users to adhere to the usage guidelines, licensing terms, and conditions set forth by the original creators or owners of each dataset.\n\nDownload meta information from Huggingface with `git clone` or update with `git pull`:\n\nExamples to specific dataset options can be found in `./pyiqa/default_dataset_configs.yml`. Details of the dataloader interface and meta information files can be found in [Dataset Preparation](docs/Dataset_Preparation.md)\n\nPerformance Evaluation Protocol\n\n**We use official models for evaluation if available.** Otherwise, we use the following settings to train and evaluate different models for simplicity and consistency:\n\nMetric Type\n-------------\nFR\nNR\nAesthetic IQA\nFace IQA\nEfficiency",
      "word_count": 489
    },
    {
      "chunk_id": "pyiqa::chunk_1",
      "text": "Results are calculated with:\n- **PLCC without any correction**. Although test time value correction is common in IQA papers, we want to use the original value in our benchmark.\n- **Full image single input.** We **do not** use multi-patch testing unless necessary.\n\nBasically, we use the largest existing datasets for training, and cross dataset evaluation performance for fair comparison. The following models do not provide official weights, and are retrained by our scripts:\n\nMetric Type\n-------------\nFR\nNR\nAesthetic IQA\n\n>[!NOTE]\n>- Due to optimized training process, performance of some retrained approaches may be different with original paper.\n>- Results of all **retrained models by ours** are normalized to [0, 1] and change to higher better for convenience.\n>- Results of KonIQ-10k, AVA are both tested with official split.\n>- NIMA is only applicable to AVA dataset now. We use `inception_resnet_v2` for default `nima`.\n>- MUSIQ is not included in the IAA benchmark because we do not have train/split information of the official model.\n\nBenchmark Performance with Provided Script\n\nHere is an example script to get performance benchmark on different datasets:\n\n:hammer_and_wrench: Train\n\nExample Train Script\n\nExample to train DBCNN on LIVEChallenge dataset\n\nExample for distributed training\n\n:beers: Contribution\n\nAny contributions to this repository are greatly appreciated. Please follow the [contribution instructions](docs/Instruction.md) for contribution guidance.\n\n:scroll: License\n\nThis work is licensed under a [NTU S-Lab License](https://github.com/chaofengc/IQA-PyTorch/blob/main/LICENSE_NTU-S-Lab) and Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License.\n\n:bookmark_tabs: Citation\n\nIf you find our codes helpful to your research, please consider to use the following citation:\n\nPlease also consider to cite our works on image quality assessment if it is useful to you:\n\n:heart: Acknowledgement\n\nThe code architecture is borrowed from [BasicSR](https://github.com/xinntao/BasicSR). Several implementations are taken from: [IQA-optimization](https://github.com/dingkeyan93/IQA-optimization), [Image-Quality-Assessment-Toolbox](https://github.com/RyanXingQL/Image-Quality-Assessment-Toolbox), [piq](https://github.com/photosynthesis-team/piq), [piqa](https://github.com/francois-rozet/piqa), [clean-fid](https://github.com/GaParmar/clean-fid)\n\nWe also thanks the following public repositories: [MUSIQ](https://github.com/google-research/google-research/tree/master/musiq), [DBCNN](https://github.com/zwx8981/DBCNN-PyTorch), [NIMA](https://github.com/kentsyx/Neural-IMage-Assessment), [HyperIQA](https://github.com/SSL92/hyperIQA), [CNNIQA](https://github.com/lidq92/CNNIQA), [WaDIQaM](https://github.com/lidq92/WaDIQaM), [PieAPP](https://github.com/prashnani/PerceptualImageError), [paq2piq](https://github.com/baidut/paq2piq), [MANIQA](https://github.com/IIGROUP/MANIQA)\n\n:e-mail: Contact\n\nIf you have any questions, please email `chaofenghust@gmail.com`",
      "word_count": 313
    }
  ],
  "usage_description": "Here is a 2-sentence summary of what a developer can achieve with this library:\n\nThis library is used to implement various image quality assessment (IQA) metrics for both full-reference and no-reference scenarios using PyTorch. Developers can use pyiqa to evaluate the quality of images, enable image processing pipelines to automatically assess image degradation, or integrate IQA capabilities into their applications."
}