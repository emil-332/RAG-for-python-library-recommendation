{
  "name": "tensorflow-io",
  "summary": "TensorFlow IO",
  "language": "python",
  "tags": [
    "data",
    "math",
    "ml",
    "web"
  ],
  "chunks": [
    {
      "chunk_id": "tensorflow-io::chunk_0",
      "text": "TensorFlow I/O\n\n(https://github.com/tensorflow/io/actions?query=branch%3Amaster)\n(https://pypi.org/project/tensorflow-io/)\n(https://github.com/tensorflow/io/blob/master/LICENSE)\n(https://www.tensorflow.org/io)\n\nTensorFlow I/O is a collection of file systems and file formats that are not\navailable in TensorFlow's built-in support. A full list of supported file systems\nand file formats by TensorFlow I/O can be found [here](https://www.tensorflow.org/io/api_docs/python/tfio).\n\nThe use of tensorflow-io is straightforward with keras. Below is an example\nto [Get Started with TensorFlow](https://www.tensorflow.org/tutorials/quickstart/beginner) with\nthe data processing aspect replaced by tensorflow-io:\n\nIn the above [MNIST](http://yann.lecun.com/exdb/mnist/) example, the URL's\nto access the dataset files are passed directly to the `tfio.IODataset.from_mnist` API call.\nThis is due to the inherent support that `tensorflow-io` provides for `HTTP`/`HTTPS` file system,\nthus eliminating the need for downloading and saving datasets on a local directory.\n\nNOTE: Since `tensorflow-io` is able to detect and uncompress the MNIST dataset automatically if needed,\nwe can pass the URL's for the compressed files (gzip) to the API call as is.\n\nPlease check the official [documentation](https://www.tensorflow.org/io) for more\ndetailed and interesting usages of the package.\n\nInstallation\n\nPython Package\n\nThe `tensorflow-io` Python package can be installed with pip directly using:\n\nPeople who are a little more adventurous can also try our nightly binaries:\n\nTo ensure you have a version of TensorFlow that is compatible with TensorFlow-IO,\nyou can specify the `tensorflow` extra requirement during install:\n\nSimilar extras exist for the `tensorflow-gpu`, `tensorflow-cpu` and `tensorflow-rocm`\npackages.\n\nDocker Images\n\nIn addition to the pip packages, the docker images can be used to quickly get started.\n\nFor stable builds:\n\nFor nightly builds:\n\nR Package\n\nOnce the `tensorflow-io` Python package has been successfully installed, you\ncan install the development version of the R package from GitHub via the following:\n\nTensorFlow Version Compatibility\n\nTo ensure compatibility with TensorFlow, it is recommended to install a matching\nversion of TensorFlow I/O according to the table below. You can find the list\nof releases [here](https://github.com/tensorflow/io/releases).\n\nTensorFlow I/O Version\n---\n0.37.1\n0.37.0\n0.36.0\n0.35.0\n0.34.0\n0.33.0\n0.32.0\n0.31.0\n0.30.0\n0.29.0\n0.28.0\n0.27.0\n0.26.0\n0.25.0\n0.24.0\n0.23.1\n0.23.0\n0.22.0\n0.21.0\n0.20.0\n0.19.1\n0.19.0\n0.18.0\n0.17.1\n0.17.0\n0.16.0\n0.15.0\n0.14.0\n0.13.0\n0.12.0\n0.11.0\n0.10.0\n0.9.1\n0.9.0\n0.8.1\n0.8.0\n0.7.2\n0.7.1\n0.7.0\n0.6.0\n0.5.0\n0.4.0\n0.3.0\n0.2.0\n0.1.0\n\nPerformance Benchmarking\n\nWe use [github-pages](https://tensorflow.github.io/io/dev/bench/) to document the results of API performance benchmarks. The benchmark job is triggered on every commit to `master` branch and\nfacilitates tracking performance w.r.t commits.\n\nContributing\n\nTensorflow I/O is a community led open source project. As such, the project\ndepends on public contributions, bug-fixes, and documentation. Please see:\n\n- [contribution guidelines](CONTRIBUTING.md) for a guide on how to contribute.\n- [development doc](docs/development.md) for instructions on the development environment setup.\n- [tutorials](docs/tutorials) for a list of tutorial notebooks and instructions on how to write one.\n\nBuild Status and CI\n\nBuild\n---\n\nBecause of manylinux2010 requirement, TensorFlow I/O is built with\nUbuntu:16.04 + Developer Toolset 7 (GCC 7.3) on Linux. Configuration\nwith Ubuntu 16.04 with Developer Toolset 7 is not exactly straightforward.\nIf the system have docker installed, then the following command\nwill automatically build manylinux2010 compatible whl package:",
      "word_count": 490
    },
    {
      "chunk_id": "tensorflow-io::chunk_1",
      "text": "It takes some time to build, but once complete, there will be python\n`3.5`, `3.6`, `3.7` compatible whl packages available in `wheelhouse`\ndirectory.\n\nOn macOS, the same command could be used. However, the script expects `python` in shell\nand will only generate a whl package that matches the version of `python` in shell. If\nyou want to build a whl package for a specific python then you have to alias this version\nof python to `python` in shell. See [.github/workflows/build.yml](.github/workflows/build.yml)\nAuditwheel step for instructions how to do that.\n\nNote the above command is also the command we use when releasing packages for Linux and macOS.\n\nTensorFlow I/O uses both GitHub Workflows and Google CI (Kokoro) for continuous integration.\nGitHub Workflows is used for macOS build and test. Kokoro is used for Linux build and test.\nAgain, because of the manylinux2010 requirement, on Linux whl packages are always\nbuilt with Ubuntu 16.04 + Developer Toolset 7. Tests are done on a variatiy of systems\nwith different python3 versions to ensure a good coverage:\n\nPython\n-------\n2.7\n3.7\n3.8\n\nTensorFlow I/O has integrations with many systems and cloud vendors such as\nPrometheus, Apache Kafka, Apache Ignite, Google Cloud PubSub, AWS Kinesis,\nMicrosoft Azure Storage, Alibaba Cloud OSS etc.\n\nWe tried our best to test against those systems in our continuous integration\nwhenever possible. Some tests such as Prometheus, Kafka, and Ignite\nare done with live systems, meaning we install Prometheus/Kafka/Ignite on CI machine before\nthe test is run. Some tests such as Kinesis, PubSub, and Azure Storage are done\nthrough official or non-official emulators. Offline tests are also performed whenever\npossible, though systems covered through offine tests may not have the same\nlevel of coverage as live systems or emulators.\n\nLive System\n-------\nApache Kafka\nApache Ignite\nPrometheus\nGoogle PubSub\nAzure Storage\nAWS Kinesis\nAlibaba Cloud OSS\nGoogle BigTable/BigQuery\nElasticsearch (experimental)\nMongoDB (experimental)\n\nReferences for emulators:\n\nCommunity\n\n* SIG IO [Google Group](https://groups.google.com/a/tensorflow.org/forum/#!forum/io) and mailing list: [io@tensorflow.org](io@tensorflow.org)\n* SIG IO [Monthly Meeting Notes](https://docs.google.com/document/d/1CB51yJxns5WA4Ylv89D-a5qReiGTC0GYum6DU-9nKGo/edit)\n* Gitter room: [tensorflow/sig-io](https://gitter.im/tensorflow/sig-io)\n\nAdditional Information\n\n* [Streaming Machine Learning with Tiered Storage and Without a Data Lake](https://www.confluent.io/blog/streaming-machine-learning-with-tiered-storage/) - [Kai Waehner](https://github.com/kaiwaehner)\n* [TensorFlow with Apache Arrow Datasets](https://medium.com/tensorflow/tensorflow-with-apache-arrow-datasets-cdbcfe80a59f) - [Bryan Cutler](https://github.com/BryanCutler)\n* [How to build a custom Dataset for Tensorflow](https://towardsdatascience.com/how-to-build-a-custom-dataset-for-tensorflow-1fe3967544d8) - [Ivelin Ivanov](https://github.com/ivelin)\n* [TensorFlow on Apache Ignite](https://medium.com/tensorflow/tensorflow-on-apache-ignite-99f1fc60efeb) - [Anton Dmitriev](https://github.com/dmitrievanthony)\n\nLicense\n\n[Apache License 2.0](LICENSE)",
      "word_count": 384
    }
  ],
  "usage_description": "Here is a 2-sentence summary of what a developer can achieve with this library:\n\nThis library is used to enable support for various file systems and formats not natively supported by TensorFlow, allowing developers to easily read and write data from these sources. By leveraging tensorflow-io, developers can seamlessly integrate data processing into their TensorFlow workflows, streamlining tasks such as data loading and management."
}