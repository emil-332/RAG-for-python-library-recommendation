{
  "name": "bitsandbytes",
  "summary": "k-bit optimizers and matrix multiplication routines.",
  "language": "python",
  "tags": [
    "math",
    "ml"
  ],
  "chunks": [
    {
      "chunk_id": "bitsandbytes::chunk_0",
      "text": "System Requirements\nbitsandbytes has the following minimum requirements for all platforms:\n\n* Python 3.10+\n* [PyTorch](https://pytorch.org/get-started/locally/) 2.3+\n  * _Note: While we aim to provide wide backwards compatibility, we recommend using the latest version of PyTorch for the best experience._\n\nAccelerator support:\n\nNote: this table reflects the status of the current development branch. For the latest stable release, see the\n[document in the 0.49.0 tag](https://github.com/bitsandbytes-foundation/bitsandbytes/blob/0.49.0/README.md#accelerator-support).\n\nLegend:\n = In Development,\n〰️ = Partially Supported,\n = Supported,\n = Slow Implementation Supported,\n = Not Supported\n\n:book: Documentation\n* [Official Documentation](https://huggingface.co/docs/bitsandbytes/main)\n*  [Transformers](https://huggingface.co/docs/transformers/quantization/bitsandbytes)\n*  [Diffusers](https://huggingface.co/docs/diffusers/quantization/bitsandbytes)\n*  [PEFT](https://huggingface.co/docs/peft/developer_guides/quantization#quantize-a-model)\n\n:heart: Sponsors\nThe continued maintenance and development of `bitsandbytes` is made possible thanks to the generous support of our sponsors. Their contributions help ensure that we can keep improving the project and delivering valuable updates to the community.\n\n&nbsp;\n\nLicense\n`bitsandbytes` is MIT licensed.\n\nHow to cite us\nIf you found this library useful, please consider citing our work:\n\nQLoRA\n\nLLM.int8()\n\n8-bit Optimizers",
      "word_count": 155
    }
  ]
}