{
  "name": "assemblyai",
  "summary": "AssemblyAI Python SDK",
  "language": "python",
  "tags": [
    "math",
    "ml",
    "web"
  ],
  "chunks": [
    {
      "chunk_id": "assemblyai::chunk_0",
      "text": "AssemblyAI's Python SDK\n\n> _Build with AI models that can transcribe and understand audio_\n\nWith a single API call, get access to AI models built on the latest AI breakthroughs to transcribe and understand audio and speech data securely at large scale.\n\nOverview\n\n- [AssemblyAI's Python SDK](#assemblyais-python-sdk)\n- [Overview](#overview)\n- [Documentation](#documentation)\n- [Quick Start](#quick-start)\n  - [Installation](#installation)\n  - [Examples](#examples)\n  - [Playgrounds](#playgrounds)\n- [Advanced](#advanced)\n  - [How the SDK handles Default Configurations](#how-the-sdk-handles-default-configurations)\n  - [Synchronous vs Asynchronous](#synchronous-vs-asynchronous)\n  - [Polling Intervals](#polling-intervals)\n  - [Retrieving Existing Transcripts](#retrieving-existing-transcripts)\n\nDocumentation\n\nVisit our [AssemblyAI API Documentation](https://www.assemblyai.com/docs) to get an overview of our models!\n\nQuick Start\n\nInstallation\n\nExamples\n\nBefore starting, you need to set the API key. If you don't have one yet, [**sign up for one**](https://www.assemblyai.com/dashboard/signup)!\n\n---\n\n**Core Examples**\n\nTranscribe a local audio file\n\nTranscribe an URL\n\nTranscribe binary data\n\nExport subtitles of an audio file\n\nList all sentences and paragraphs\n\nSearch for words in a transcript\n\nAdd custom spellings on a transcript\n\nUpload a file\n\nDelete a transcript\n\nList transcripts\n\nThis returns a page of transcripts you created.\n\nYou can apply filter parameters:\n\nYou can also paginate over all pages by using the helper property `before_id_of_prev_url`.\n\nThe `prev_url` always points to a page with older transcripts. If you extract the `before_id`\nof the `prev_url` query parameters, you can paginate over all pages from newest to oldest.\n\n---\n\n**LeMUR Examples**\n\nUse LeMUR to summarize an audio file\n\nOr use the specialized Summarization endpoint that requires no prompt engineering and facilitates more deterministic and structured outputs:\n\nUse LeMUR to ask questions about your audio data\n\nOr use the specialized Q&A endpoint that requires no prompt engineering and facilitates more deterministic and structured outputs:\n\nUse LeMUR with customized input text\n\nApply LeMUR to multiple transcripts\n\nDelete data previously sent to LeMUR\n\n---\n\n**Audio Intelligence Examples**\n\nPII Redact a transcript\n\nTo request a copy of the original audio file with the redacted information \"beeped\" out, set `redact_pii_audio=True` in the config.\nOnce the `Transcript` object is returned, you can access the URL of the redacted audio file with `get_redacted_audio_url`, or save the redacted audio directly to disk with `save_redacted_audio`.\n\n[Read more about PII redaction here.](https://www.assemblyai.com/docs/Models/pii_redaction)\n\nSummarize the content of a transcript over time\n\n[Read more about auto chapters here.](https://www.assemblyai.com/docs/Models/auto_chapters)\n\nSummarize the content of a transcript\n\nBy default, the summarization model will be `informative` and the summarization type will be `bullets`. [Read more about summarization models and types here](https://www.assemblyai.com/docs/Models/summarization#types-and-models).\n\nTo change the model and/or type, pass additional parameters to the `TranscriptionConfig`:\n\nDetect sensitive content in a transcript\n\n[Read more about the content safety categories.](https://www.assemblyai.com/docs/Models/content_moderation#all-labels-supported-by-the-model)\n\nBy default, the content safety model will only include labels with a confidence greater than 0.5 (50%). To change this, pass `content_safety_confidence` (as an integer percentage between 25 and 100, inclusive) to the `TranscriptionConfig`:\n\nAnalyze the sentiment of sentences in a transcript\n\nIf `speaker_labels` is also enabled, then each sentiment analysis result will also include a `speaker` field.\n\n[Read more about sentiment analysis here.](https://www.assemblyai.com/docs/Models/sentiment_analysis)\n\nIdentify entities in a transcript\n\n[Read more about entity detection here.](https://www.assemblyai.com/docs/Models/entity_detection)",
      "word_count": 494
    },
    {
      "chunk_id": "assemblyai::chunk_1",
      "text": "Detect topics in a transcript (IAB Classification)\n\n[Read more about IAB classification here.](https://www.assemblyai.com/docs/Models/iab_classification)\n\nIdentify important words and phrases in a transcript\n\n[Read more about auto highlights here.](https://www.assemblyai.com/docs/Models/key_phrases)\n\n---\n\n**Streaming Examples**\n\n[Read more about our streaming service.](https://www.assemblyai.com/docs/getting-started/transcribe-streaming-audio)\n\nStream your microphone in real-time\n\nTranscribe a local audio file in real-time\n\n---\n\n**Change the default settings**\n\nYou'll find the `Settings` class with all default values in [types.py](./assemblyai/types.py).\n\nChange the default timeout and polling interval\n\n---\n\nPlayground\n\nVisit our Playground to try our all of our Speech AI models and LeMUR for free:\n\n- [Playground](https://www.assemblyai.com/playground)\n\nAdvanced\n\nHow the SDK handles Default Configurations\n\nDefining Defaults\n\nWhen no `TranscriptionConfig` is being passed to the `Transcriber` or its methods, it will use a default instance of a `TranscriptionConfig`.\n\nIf you would like to re-use the same `TranscriptionConfig` for all your transcriptions,\nyou can set it on the `Transcriber` directly:\n\nOverriding Defaults\n\nYou can override the default configuration later via the `.config` property of the `Transcriber`:\n\nIn case you want to override the `Transcriber`'s configuration for a specific operation with a different one, you can do so via the `config` parameter of a `.transcribe*(...)` method:\n\nSynchronous vs Asynchronous\n\nCurrently, the SDK provides two ways to transcribe audio files.\n\nThe synchronous approach halts the application's flow until the transcription has been completed.\n\nThe asynchronous approach allows the application to continue running while the transcription is being processed. The caller receives a [`concurrent.futures.Future`](https://docs.python.org/3/library/concurrent.futures.html) object which can be used to check the status of the transcription at a later time.\n\nYou can identify those two approaches by the `_async` suffix in the `Transcriber`'s method name (e.g. `transcribe` vs `transcribe_async`).\n\nGetting the HTTP status code\n\nThere are two ways of accessing the HTTP status code:\n\n- All custom AssemblyAI Error classes have a `status_code` attribute.\n- The latest HTTP response is stored in `aai.Client.get_default().latest_response` after every API call. This approach works also if no Exception is thrown.\n\nPolling Intervals\n\nBy default we poll the `Transcript`'s status each `3s`. In case you would like to adjust that interval:\n\nRetrieving Existing Transcripts\n\nRetrieving a Single Transcript\n\nIf you previously created a transcript, you can use its ID to retrieve it later.\n\nRetrieving Multiple Transcripts as a Group\n\nYou can also retrieve multiple existing transcripts and combine them into a single `TranscriptGroup` object. This allows you to perform operations on the transcript group as a single unit, such as querying the combined transcripts with LeMUR.\n\nRetrieving Transcripts Asynchronously\n\nBoth `Transcript.get_by_id` and `TranscriptGroup.get_by_ids` have asynchronous counterparts, `Transcript.get_by_id_async` and `TranscriptGroup.get_by_ids_async`, respectively. These functions immediately return a `Future` object, rather than blocking until the transcript(s) are retrieved.\n\nSee the above section on [Synchronous vs Asynchronous](#synchronous-vs-asynchronous) for more information.",
      "word_count": 440
    }
  ]
}