{
  "name": "nemo-toolkit",
  "summary": "NeMo - a toolkit for Conversational AI",
  "language": "python",
  "tags": [
    "data",
    "math",
    "ml",
    "ui"
  ],
  "chunks": [
    {
      "chunk_id": "nemo-toolkit::chunk_0",
      "text": "**NVIDIA NeMo Framework**\n\nLatest News\n\nPivot notice: This repo will pivot to focus on speech models only\n\nPretrain and finetune :hugs:Hugging Face models via AutoModel\n\n- AutoModelForCausalLM in the Text Generation category\n- AutoModelForImageTextToText in the Image-Text-to-Text category\n\nMore Details in Blog: Run Hugging Face Models Instantly with Day-0 Support from NVIDIA NeMo Framework. Future releases will enable support for more model families such as Video Generation models.(2025-05-19)\n\nTraining on Blackwell using Nemo\n\nTraining Performance on GPU Tuning Guide\n\nNew Models Support\n\nNeMo Framework 2.0\n\nNew Cosmos World Foundation Models Support\n\nLarge Language Models and Multimodal Models\n\nSpeech Recognition\n\nIntroduction\n\nNVIDIA NeMo Framework is a scalable and cloud-native generative AI\nframework built for researchers and PyTorch developers working on Large\nLanguage Models (LLMs), Multimodal Models (MMs), Automatic Speech\nRecognition (ASR), Text to Speech (TTS), and Computer Vision (CV)\ndomains. It is designed to help you efficiently create, customize, and\ndeploy new generative AI models by leveraging existing code and\npre-trained model checkpoints.\n\nFor technical documentation, please see the [NeMo Framework User\nGuide](https://docs.nvidia.com/nemo-framework/user-guide/latest/playbooks/index.html).\n\nWhat's New in NeMo 2.0\n\nNVIDIA NeMo 2.0 introduces several significant improvements over its predecessor, NeMo 1.0, enhancing flexibility, performance, and scalability.\n\n- **Python-Based Configuration** - NeMo 2.0 transitions from YAML files to a Python-based configuration, providing more flexibility and control. This shift makes it easier to extend and customize configurations programmatically.\n\n- **Modular Abstractions** - By adopting PyTorch Lightningâ€™s modular abstractions, NeMo 2.0 simplifies adaptation and experimentation. This modular approach allows developers to more easily modify and experiment with different components of their models.\n\n- **Scalability** - NeMo 2.0 seamlessly scaling large-scale experiments across thousands of GPUs using [NeMo-Run](https://github.com/NVIDIA/NeMo-Run), a powerful tool designed to streamline the configuration, execution, and management of machine learning experiments across computing environments.\n\nOverall, these enhancements make NeMo 2.0 a powerful, scalable, and user-friendly framework for AI model development.\n\n> [!IMPORTANT]  \n> NeMo 2.0 is currently supported by the LLM (large language model) and VLM (vision language model) collections.\n\nGet Started with NeMo 2.0\n\nGet Started with Cosmos\n\nNeMo Curator and NeMo Framework support video curation and post-training of the Cosmos World Foundation Models, which are open and available on [NGC](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/cosmos/collections/cosmos) and [Hugging Face](https://huggingface.co/collections/nvidia/cosmos-6751e884dc10e013a0a0d8e6). For more information on video datasets, refer to [NeMo Curator](https://developer.nvidia.com/nemo-curator). To post-train World Foundation Models using the NeMo Framework for your custom physical AI tasks, see the [Cosmos Diffusion models](https://github.com/NVIDIA/Cosmos/blob/main/cosmos1/models/diffusion/nemo/post_training/README.md) and the [Cosmos Autoregressive models](https://github.com/NVIDIA/Cosmos/blob/main/cosmos1/models/autoregressive/nemo/post_training/README.md).\n\nLLMs and MMs Training, Alignment, and Customization\n\nAll NeMo models are trained with\n[Lightning](https://github.com/Lightning-AI/lightning). Training is\nautomatically scalable to 1000s of GPUs. You can check the performance benchmarks using the\nlatest NeMo Framework container [here](https://docs.nvidia.com/nemo-framework/user-guide/latest/performance/performance_summary.html).\n\nWhen applicable, NeMo models leverage cutting-edge distributed training\ntechniques, incorporating [parallelism\nstrategies](https://docs.nvidia.com/nemo-framework/user-guide/latest/modeloverview.html)\nto enable efficient training of very large models. These techniques\ninclude Tensor Parallelism (TP), Pipeline Parallelism (PP), Fully\nSharded Data Parallelism (FSDP), Mixture-of-Experts (MoE), and Mixed\nPrecision Training with BFloat16 and FP8, as well as others.",
      "word_count": 479
    },
    {
      "chunk_id": "nemo-toolkit::chunk_1",
      "text": "NeMo Transformer-based LLMs and MMs utilize [NVIDIA Transformer\nEngine](https://github.com/NVIDIA/TransformerEngine) for FP8 training on\nNVIDIA Hopper GPUs, while leveraging [NVIDIA Megatron\nCore](https://github.com/NVIDIA/Megatron-LM/tree/main/megatron/core) for\nscaling Transformer model training.\n\nNeMo LLMs can be aligned with state-of-the-art methods such as SteerLM,\nDirect Preference Optimization (DPO), and Reinforcement Learning from\nHuman Feedback (RLHF). See [NVIDIA NeMo\nAligner](https://github.com/NVIDIA/NeMo-Aligner) for more information.\n\nIn addition to supervised fine-tuning (SFT), NeMo also supports the\nlatest parameter efficient fine-tuning (PEFT) techniques such as LoRA,\nP-Tuning, Adapters, and IA3. Refer to the [NeMo Framework User\nGuide](https://docs.nvidia.com/nemo-framework/user-guide/latest/sft_peft/index.html)\nfor the full list of supported models and techniques.\n\nLLMs and MMs Deployment and Optimization\n\nNeMo LLMs and MMs can be deployed and optimized with [NVIDIA NeMo\nMicroservices](https://developer.nvidia.com/nemo-microservices-early-access).\n\nSpeech AI\n\nNeMo ASR and TTS models can be optimized for inference and deployed for\nproduction use cases with [NVIDIA Riva](https://developer.nvidia.com/riva).\n\nNeMo Framework Launcher\n\n> [!IMPORTANT]  \n> NeMo Framework Launcher is compatible with NeMo version 1.0 only. [NeMo-Run](https://github.com/NVIDIA/NeMo-Run) is recommended for launching experiments using NeMo 2.0.\n\n[NeMo Framework\nLauncher](https://github.com/NVIDIA/NeMo-Megatron-Launcher) is a\ncloud-native tool that streamlines the NeMo Framework experience. It is\nused for launching end-to-end NeMo Framework training jobs on CSPs and\nSlurm clusters.\n\nThe NeMo Framework Launcher includes extensive recipes, scripts,\nutilities, and documentation for training NeMo LLMs. It also includes\nthe NeMo Framework [Autoconfigurator](https://github.com/NVIDIA/NeMo-Megatron-Launcher#53-using-autoconfigurator-to-find-the-optimal-configuration),\nwhich is designed to find the optimal model parallel configuration for\ntraining on a specific cluster.\n\nTo get started quickly with the NeMo Framework Launcher, please see the\n[NeMo Framework\nPlaybooks](https://docs.nvidia.com/nemo-framework/user-guide/latest/playbooks/index.html).\nThe NeMo Framework Launcher does not currently support ASR and TTS\ntraining, but it will soon.\n\nGet Started with NeMo Framework\n\nGetting started with NeMo Framework is easy. State-of-the-art pretrained\nNeMo models are freely available on [Hugging Face\nHub](https://huggingface.co/models?library=nemo&sort=downloads&search=nvidia)\nand [NVIDIA\nNGC](https://catalog.ngc.nvidia.com/models?query=nemo&orderBy=weightPopularDESC).\nThese models can be used to generate text or images, transcribe audio,\nand synthesize speech in just a few lines of code.\n\nWe have extensive\n[tutorials](https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/stable/starthere/tutorials.html)\nthat can be run on [Google Colab](https://colab.research.google.com) or\nwith our [NGC NeMo Framework\nContainer](https://catalog.ngc.nvidia.com/orgs/nvidia/containers/nemo).\nWe also have\n[playbooks](https://docs.nvidia.com/nemo-framework/user-guide/latest/playbooks/index.html)\nfor users who want to train NeMo models with the NeMo Framework\nLauncher.\n\nFor advanced users who want to train NeMo models from scratch or\nfine-tune existing NeMo models, we have a full suite of [example\nscripts](https://github.com/NVIDIA/NeMo/tree/main/examples) that support\nmulti-GPU/multi-node training.\n\nKey Features\n\n- [Large Language Models](nemo/collections/nlp/README.md)\n- [Multimodal](nemo/collections/multimodal/README.md)\n- [Automatic Speech Recognition](nemo/collections/asr/README.md)\n- [Text to Speech](nemo/collections/tts/README.md)\n- [Computer Vision](nemo/collections/vision/README.md)\n\nRequirements\n\n- Python 3.10 or above\n- Pytorch 2.5 or above\n- NVIDIA GPU (if you intend to do model training)\n\nDeveloper Documentation\n\nVersion\n-------\nLatest\nStable\n\nInstall NeMo Framework\n\nThe NeMo Framework can be installed in a variety of ways, depending on\nyour needs. Depending on the domain, you may find one of the following\ninstallation methods more suitable.",
      "word_count": 446
    },
    {
      "chunk_id": "nemo-toolkit::chunk_2",
      "text": "- [Conda / Pip](#conda--pip): Install NeMo-Framework with native Pip into a virtual environment.\n  - Used to explore NeMo on any supported platform.\n  - This is the recommended method for ASR and TTS domains.\n  - Limited feature-completeness for other domains.\n- [NGC PyTorch container](#ngc-pytorch-container): Install NeMo-Framework from source with feature-completeness into a highly optimized container.\n  - For users that want to install from source in a highly optimized container.\n- [NGC NeMo container](#ngc-nemo-container): Ready-to-go solution of NeMo-Framework\n  - For users that seek highest performance.\n  - Contains all dependencies installed and tested for performance and convergence.\n\nSupport matrix\n\nNeMo-Framework provides tiers of support based on OS / Platform and mode of installation. Please refer the following overview of support levels:\n\n- Fully supported: Max performance and feature-completeness.\n- Limited supported: Used to explore NeMo.\n- No support yet: In development.\n- Deprecated: Support has reached end of life.\n\nPlease refer to the following table for current support levels:\n\nOS / Platform\n----------------------------\n`linux` - `amd64/x84_64`\n`linux` - `arm64`\n`darwin` - `amd64/x64_64`\n`darwin` - `arm64`\n`windows` - `amd64/x64_64`\n`windows` - `arm64`\n\nConda / Pip\n\nInstall NeMo in a fresh Conda environment:\n\nPick the right version\n\nNeMo-Framework publishes pre-built wheels with each release.\nTo install nemo_toolkit from such a wheel, use the following installation method:\n\nIf a more specific version is desired, we recommend a Pip-VCS install. From [NVIDIA/NeMo](github.com/NVIDIA/NeMo), fetch the commit, branch, or tag that you would like to install.  \nTo install nemo_toolkit from this Git reference `$REF`, use the following installation method:\n\nInstall a specific Domain\n\nTo install a specific domain of NeMo, you must first install the\nnemo_toolkit using the instructions listed above. Then, you run the\nfollowing domain-specific commands:\n\nNGC PyTorch container\n\n**NOTE: The following steps are supported beginning with 24.04 (NeMo-Toolkit 2.3.0)**\n\nWe recommended that you start with a base NVIDIA PyTorch container:\nnvcr.io/nvidia/pytorch:25.01-py3.\n\nIf starting with a base NVIDIA PyTorch container, you must first launch\nthe container:\n\nFrom [NVIDIA/NeMo](github.com/NVIDIA/NeMo), fetch the commit/branch/tag that you want to install.  \nTo install nemo_toolkit including all of its dependencies from this Git reference `$REF`, use the following installation method:\n\nNGC NeMo container\n\nNeMo containers are launched concurrently with NeMo version updates.\nNeMo Framework now supports LLMs, MMs, ASR, and TTS in a single\nconsolidated Docker container. You can find additional information about\nreleased containers on the [NeMo releases\npage](https://github.com/NVIDIA/NeMo/releases).\n\nTo use a pre-built container, run the following code:\n\nFuture Work\n\nThe NeMo Framework Launcher does not currently support ASR and TTS\ntraining, but it will soon.\n\nDiscussions Board\n\nFAQ can be found on the NeMo [Discussions\nboard](https://github.com/NVIDIA/NeMo/discussions). You are welcome to\nask questions or start discussions on the board.\n\nContribute to NeMo\n\nWe welcome community contributions! Please refer to\n[CONTRIBUTING.md](https://github.com/NVIDIA/NeMo/blob/stable/CONTRIBUTING.md)\nfor the process.\n\nPublications\n\nWe provide an ever-growing list of\n[publications](https://nvidia.github.io/NeMo/publications/) that utilize\nthe NeMo Framework.\n\nTo contribute an article to the collection, please submit a pull request\nto the `gh-pages-src` branch of this repository. For detailed\ninformation, please consult the README located at the [gh-pages-src\nbranch](https://github.com/NVIDIA/NeMo/tree/gh-pages-src#readme).\n\nBlogs",
      "word_count": 496
    },
    {
      "chunk_id": "nemo-toolkit::chunk_3",
      "text": "Large Language Models and Multimodal Models\n\nLicenses\n\nNeMo is licensed under the [Apache License 2.0](https://github.com/NVIDIA/NeMo?tab=Apache-2.0-1-ov-file).",
      "word_count": 15
    }
  ]
}