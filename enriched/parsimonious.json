{
  "name": "parsimonious",
  "summary": "(Soon to be) the fastest pure-Python PEG parser I could muster",
  "language": "python",
  "tags": [
    "dev",
    "math",
    "web"
  ],
  "chunks": [
    {
      "chunk_id": "parsimonious::chunk_0",
      "text": "============\nParsimonious\n============\n\nParsimonious aims to be the fastest arbitrary-lookahead parser written in pure\nPython—and the most usable. It's based on parsing expression grammars (PEGs),\nwhich means you feed it a simplified sort of EBNF notation. Parsimonious was\ndesigned to undergird a MediaWiki parser that wouldn't take 5 seconds or a GB\nof RAM to do one page, but it's applicable to all sorts of languages.\n\n:Code:    https://github.com/erikrose/parsimonious/\n:Issues:  https://github.com/erikrose/parsimonious/issues\n:License: MIT License (MIT)\n:Package: https://pypi.org/project/parsimonious/\n\nGoals\n=====\n\n* Speed\n* Frugal RAM use\n* Minimalistic, understandable, idiomatic Python code\n* Readable grammars\n* Extensible grammars\n* Complete test coverage\n* Separation of concerns. Some Python parsing kits mix recognition with\n  instructions about how to turn the resulting tree into some kind of other\n  representation. This is limiting when you want to do several different things\n  with a tree: for example, render wiki markup to HTML *or* to text.\n* Good error reporting. I want the parser to work *with* me as I develop a\n  grammar.\n\nInstall\n=======\n\nTo install Parsimonious, run::\n\nExample Usage\n=============\n\nHere's how to build a simple grammar:\n\n.. code:: python\n\nYou can have forward references and even right recursion; it's all taken care\nof by the grammar compiler. The first rule is taken to be the default start\nsymbol, but you can override that.\n\nNext, let's parse something and get an abstract syntax tree:\n\n.. code:: python\n\nYou'd typically then use a ``nodes.NodeVisitor`` subclass (see below) to walk\nthe tree and do something useful with it.\n\nAnother example would be to implement a parser for ``.ini``-files. Consider the following:\n\n.. code:: python\n\nWe could now implement a subclass of ``NodeVisitor`` like so:\n\n.. code:: python\n\nAnd call it like that:\n\n.. code:: python\n\nThis would yield\n\n.. code:: python\n\nStatus\n======\n\n* Everything that exists works. Test coverage is good.\n* I don't plan on making any backward-incompatible changes to the rule syntax\n  in the future, so you can write grammars with confidence.\n* It may be slow and use a lot of RAM; I haven't measured either yet. However,\n  I have yet to begin optimizing in earnest.\n* Error reporting is now in place. ``repr`` methods of expressions, grammars,\n  and nodes are clear and helpful as well. The ``Grammar`` ones are\n  even round-trippable!\n* The grammar extensibility story is underdeveloped at the moment. You should\n  be able to extend a grammar by simply concatenating more rules onto the\n  existing ones; later rules of the same name should override previous ones.\n  However, this is untested and may not be the final story.\n* Sphinx docs are coming, but the docstrings are quite useful now.\n* Note that there may be API changes until we get to 1.0, so be sure to pin to\n  the version you're using.\n\nComing Soon\n-----------\n\n* Optimizations to make Parsimonious worthy of its name\n* Tighter RAM use\n* Better-thought-out grammar extensibility story\n* Amazing grammar debugging\n\nA Little About PEG Parsers\n==========================",
      "word_count": 493
    },
    {
      "chunk_id": "parsimonious::chunk_1",
      "text": "PEG parsers don't draw a distinction between lexing and parsing; everything is\ndone at once. As a result, there is no lookahead limit, as there is with, for\ninstance, Yacc. And, due to both of these properties, PEG grammars are easier\nto write: they're basically just a more practical dialect of EBNF. With\ncaching, they take O(grammar size * text length) memory (though I plan to do\nbetter), but they run in O(text length) time.\n\nMore Technically\n----------------\n\nPEGs can describe a superset of *LL(k)* languages, any deterministic *LR(k)*\nlanguage, and many others—including some that aren't context-free\n(http://www.brynosaurus.com/pub/lang/peg.pdf). They can also deal with what\nwould be ambiguous languages if described in canonical EBNF. They do this by\ntrading the ``|`` alternation operator for the ``/`` operator, which works the\nsame except that it makes priority explicit: ``a / b / c`` first tries matching\n``a``. If that fails, it tries ``b``, and, failing that, moves on to ``c``.\nThus, ambiguity is resolved by always yielding the first successful recognition.\n\nWriting Grammars\n================\n\nGrammars are defined by a series of rules. The syntax should be familiar to\nanyone who uses regexes or reads programming language manuals. An example will\nserve best:\n\n.. code:: python\n\nYou can wrap a rule across multiple lines if you like; the syntax is very\nforgiving.\n\nIf you want to save your grammar into a separate file, you should name it using\n``.ppeg`` extension.\n\nSyntax Reference\n----------------\n\n====================    ========================================================\n``\"some literal\"``      Used to quote literals. Backslash escaping and Python\n\n``b\"some literal\"``     A bytes literal. Using bytes literals and regular\n\n[space]                 Sequences are made out of space- or tab-delimited\n\n``a / b / c``           Alternatives. The first to succeed of ``a / b / c``\n\n``thing?``              An optional expression. This is greedy, always consuming\n\n``&thing``              A lookahead assertion. Ensures ``thing`` matches at the\n\n``!thing``              A negative lookahead assertion. Matches if ``thing``\n\n``things*``             Zero or more things. This is greedy, always consuming as\n\n``things+``             One or more things. This is greedy, always consuming as\n\n``~r\"regex\"ilmsuxa``    Regexes have ``~`` in front and are quoted like\n\n``~br\"regex\"``          A bytes regex; required if your grammar parses\n\n``(things)``            Parentheses are used for grouping, like in every other\n\n``thing{n}``            Exactly ``n`` repetitions of ``thing``.\n\n``thing{n,m}``          Between ``n`` and ``m`` repititions (inclusive.)\n\n``thing{,m}``           At most ``m`` repetitions of ``thing``.\n\n``thing{n,}``           At least ``n`` repetitions of ``thing``.\n\n====================    ========================================================\n\n.. _flags: https://docs.python.org/3/howto/regex.html#compilation\n.. _regex: https://github.com/mrabarnett/mrab-regex\n\nOptimizing Grammars\n===================\n\nDon't Repeat Expressions\n------------------------\n\nIf you need a ``~\"[a-z0-9]\"i`` at two points in your grammar, don't type it\ntwice. Make it a rule of its own, and reference it from wherever you need it.\nYou'll get the most out of the caching this way, since cache lookups are by\nexpression object identity (for speed).",
      "word_count": 456
    },
    {
      "chunk_id": "parsimonious::chunk_2",
      "text": "Even if you have an expression that's very simple, not repeating it will\nsave RAM, as there can, at worst, be a cached int for every char in the text\nyou're parsing. In the future, we may identify repeated subexpressions\nautomatically and factor them up while building the grammar.\n\nHow much should you shove into one regex, versus how much should you break them\nup to not repeat yourself? That's a fine balance and worthy of benchmarking.\nMore stuff jammed into a regex will execute faster, because it doesn't have to\nrun any Python between pieces, but a broken-up one will give better cache\nperformance if the individual pieces are re-used elsewhere. If the pieces of a\nregex aren't used anywhere else, by all means keep the whole thing together.\n\nQuantifiers\n-----------\n\nBring your ``?`` and ``*`` quantifiers up to the highest level you\ncan. Otherwise, lower-level patterns could succeed but be empty and put a bunch\nof useless nodes in your tree that didn't really match anything.\n\nProcessing Parse Trees\n======================\n\nA parse tree has a node for each expression matched, even if it matched a\nzero-length string, like ``\"thing\"?`` might.\n\nThe ``NodeVisitor`` class provides an inversion-of-control framework for\nwalking a tree and returning a new construct (tree, string, or whatever) based\non it. For now, have a look at its docstrings for more detail. There's also a\ngood example in ``grammar.RuleVisitor``. Notice how we take advantage of nodes'\niterability by using tuple unpacks in the formal parameter lists:\n\n.. code:: python\n\nFor reference, here is the production the above unpacks::\n\nWhen something goes wrong in your visitor, you get a nice error like this::\n\nThe parse tree is tacked onto the exception, and the node whose visitor method\nraised the error is pointed out.\n\nWhy No Streaming Tree Processing?\n---------------------------------\n\nSome have asked why we don't process the tree as we go, SAX-style. There are\ntwo main reasons:\n\n1. It wouldn't work. With a PEG parser, no parsing decision is final until the\n   whole text is parsed. If we had to change a decision, we'd have to backtrack\n   and redo the SAX-style interpretation as well, which would involve\n   reconstituting part of the AST and quite possibly scuttling whatever you\n   were doing with the streaming output. (Note that some bursty SAX-style\n   processing may be possible in the future if we use cuts.)\n\n2. It interferes with the ability to derive multiple representations from the\n   AST: for example, turning wiki markup into first HTML and then text.\n\nFuture Directions\n=================\n\nRule Syntax Changes\n-------------------\n\n* Maybe support left-recursive rules like PyMeta, if anybody cares.\n* Ultimately, I'd like to get rid of explicit regexes and break them into more\n  atomic things like character classes. Then we can dynamically compile bits\n  of the grammar into regexes as necessary to boost speed.\n\nOptimizations\n-------------",
      "word_count": 472
    },
    {
      "chunk_id": "parsimonious::chunk_3",
      "text": "* Make RAM use almost constant by automatically inserting \"cuts\", as described\n  in\n  This would also improve error reporting, as we wouldn't backtrack out of\n  everything informative before finally failing.\n* Find all the distinct subexpressions, and unify duplicates for a better cache\n  hit ratio.\n* Think about having the user (optionally) provide some representative input\n  along with a grammar. We can then profile against it, see which expressions\n  are worth caching, and annotate the grammar. Perhaps there will even be\n  positions at which a given expression is more worth caching. Or we could keep\n  a count of how many times each cache entry has been used and evict the most\n  useless ones as RAM use grows.\n* We could possibly compile the grammar into VM instructions, like in \"A\n  parsing machine for PEGs\" by Medeiros.\n* If the recursion gets too deep in practice, use trampolining to dodge it.\n\nNiceties\n--------\n\n* Pijnu has a raft of tree manipulators. I don't think I want all of them, but\n  a judicious subset might be nice. Don't get into mixing formatting with tree\n  manipulation.\n  parsing lib exposes a sane subset:\n\nVersion History\n===============\n\n0.11.0\n  * Correctly handle `/` expressions with multiple terms in a row. (lucaswiman)\n  * Start using pyproject.toml. (Kolanich)\n  * Add a ``ParsimoniousError`` exception base class. (Kevin Kirsche)\n  * Fall back to ``re`` when the ``regex`` lib is not available. (Pavel Kirienko)\n\n0.10.0\n  * Fix infinite recursion in __eq__ in some cases. (FelisNivalis)\n  * Improve error message in left-recursive rules. (lucaswiman)\n  * Add support for range ``{min,max}`` repetition expressions (righthandabacus)\n  * Fix bug in ``*`` and ``+`` for token grammars (lucaswiman)\n  * Add support for grammars on bytestrings (lucaswiman)\n  * Fix LazyReference resolution bug #134 (righthandabacus)\n  * ~15% speedup on benchmarks with a faster node cache (ethframe)\n\n.. warning::\n\n0.9.0\n  * Add support for Python 3.7, 3.8, 3.9, 3.10 (righthandabacus, Lonnen)\n  * Drop support for Python 2.x, 3.3, 3.4 (righthandabacus, Lonnen)\n  * Remove six and go all in on Python 3 idioms (Lonnen)\n  * Replace re with regex for improved handling of unicode characters\n  * Dropped nose for unittest (swayson)\n  * `Grammar.__repr__()` now correctly escapes backslashes (ingolemo)\n  * Custom rules can now be class methods in addition to\n  * Make the ascii flag available in the regex syntax (Roman Inflianskas)\n\n0.8.1\n  * Switch to a function-style ``print`` in the benchmark tests so we work\n\n0.8.0\n  * Make Grammar iteration ordered, making the ``__repr__`` more like the\n  * Improve text representation and error messages for anonymous\n  * Expose BadGrammar and VisitationError as top-level imports.\n  * No longer crash when you try to compare a Node to an instance of a\n  * Pin ``six`` at 1.9.0 to ensure we have ``python_2_unicode_compatible``.\n  * Drop Python 2.6 support.",
      "word_count": 455
    },
    {
      "chunk_id": "parsimonious::chunk_4",
      "text": "0.7.0\n  * Add experimental token-based parsing, via TokenGrammar class, for those\n  * Common codebase for Python 2 and 3: no more 2to3 translation step (Mattias\n  * Drop Python 3.1 and 3.2 support.\n  * Fix a bug in ``Grammar.__repr__`` which fails to work on Python 3 since the\n  * Don't lose parentheses when printing representations of expressions.\n  * Make Grammar an immutable mapping (until we add automatic recompilation).\n\n0.6.2\n  * Make grammar compilation 100x faster. Thanks to dmoisset for the initial\n\n0.6.1\n  * Fix bug which made the default rule of a grammar invalid when it\n\n0.6\n  .. warning::\n\n* Add support for \"custom rules\" in Grammars. These provide a hook for simple\n  * Allow grammars without a default rule (in cases where there are no string\n  * Add ``@rule`` decorator, allowing grammars to be constructed out of\n  * Add ``parse()`` and ``match()`` convenience methods to ``NodeVisitor``.\n  * Improve exception message when you forget to declare a visitor method.\n  * Add ``unwrapped_exceptions`` attribute to ``NodeVisitor``, letting you\n  * Expose much more of the library in ``__init__``, making your imports\n  * Drastically simplify reference resolution machinery. (Vladimir Keleshev)\n\n0.5\n  .. warning::\n\n* Add alpha-quality error reporting. Now, rather than returning ``None``,\n  * Grammar construction now raises ``ParseError`` rather than ``BadGrammar``\n  * ``parse()`` now takes an optional ``pos`` argument, like ``match()``.\n  * Make the ``_str__()`` method of ``UndefinedLabel`` return the right type.\n  * Support splitting rules across multiple lines, interleaving comments,\n  * Tolerate whitespace after opening parens.\n  * Add support for single-quoted literals.\n\n0.4\n  * Support Python 3.\n  * Fix ``import *`` for ``parsimonious.expressions``.\n  * Rewrite grammar compiler so right-recursive rules can be compiled and\n\n0.3\n  * Support comments, the ``!`` (\"not\") operator, and parentheses in grammar\n  * Change the ``&`` operator to a prefix operator to conform to the original\n  * Take the ``print`` statements out of the benchmark tests.\n  * Give Node an evaluate-able ``__repr__``.\n\n0.2\n  * Support matching of prefixes and other not-to-the-end slices of strings by\n  * Report a ``BadGrammar`` exception (rather than crashing) when there are\n  * Simplify grammar compilation internals: get rid of superfluous visitor\n  * Add ``NodeVisitor.lift_child`` convenience method.\n  * Rename ``VisitationException`` to ``VisitationError`` for consistency with\n  * Rework ``repr`` and ``str`` values for grammars and expressions. Now they\n  * Add tox for testing. Stop advertising Python 2.5 support, which never\n  * Settle (hopefully) on the term \"rule\" to mean \"the string representation of\n\n0.1\n  * A rough but useable preview release\n\nThanks to Wiki Loves Monuments Panama for showing their support with a generous\ngift.",
      "word_count": 420
    }
  ],
  "usage_description": "This library is used to parse languages efficiently using parsing expression grammars (PEGs), enabling developers to create high-performance parsers for various languages with minimal memory usage. With Parsimonious, developers can build fast and lightweight parsers that meet their specific language parsing needs."
}