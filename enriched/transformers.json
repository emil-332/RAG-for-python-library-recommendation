{
  "name": "transformers",
  "summary": "State-of-the-art Machine Learning for JAX, PyTorch and TensorFlow",
  "language": "python",
  "tags": [
    "math",
    "ml",
    "web"
  ],
  "chunks": [
    {
      "chunk_id": "transformers::chunk_0",
      "text": "Installation\n\nTransformers works with Python 3.9+ [PyTorch](https://pytorch.org/get-started/locally/) 2.1+, [TensorFlow](https://www.tensorflow.org/install/pip) 2.6+, and [Flax](https://flax.readthedocs.io/en/latest/) 0.4.1+.\n\nCreate and activate a virtual environment with [venv](https://docs.python.org/3/library/venv.html) or [uv](https://docs.astral.sh/uv/), a fast Rust-based Python package and project manager.\n\nInstall Transformers in your virtual environment.\n\nInstall Transformers from source if you want the latest changes in the library or are interested in contributing. However, the *latest* version may not be stable. Feel free to open an [issue](https://github.com/huggingface/transformers/issues) if you encounter an error.\n\nQuickstart\n\nGet started with Transformers right away with the [Pipeline](https://huggingface.co/docs/transformers/pipeline_tutorial) API. The `Pipeline` is a high-level inference class that supports text, audio, vision, and multimodal tasks. It handles preprocessing the input and returns the appropriate output.\n\nInstantiate a pipeline and specify model to use for text generation. The model is downloaded and cached so you can easily reuse it again. Finally, pass some text to prompt the model.\n\nTo chat with a model, the usage pattern is the same. The only difference is you need to construct a chat history (the input to `Pipeline`) between you and the system.\n\n> [!TIP]\n> You can also chat with a model directly from the command line.\n> \n\nExpand the examples below to see how `Pipeline` works for different modalities and tasks.\n\nAutomatic speech recognition\n\nImage classification\n\nVisual question answering\n\nWhy should I use Transformers?\n\n1. Easy-to-use state-of-the-art models:\n\n1. Lower compute costs, smaller carbon footprint:\n\n1. Choose the right framework for every part of a models lifetime:\n\n1. Easily customize a model or an example to your needs:\n\nWhy shouldn't I use Transformers?\n\n- This library is not a modular toolbox of building blocks for neural nets. The code in the model files is not refactored with additional abstractions on purpose, so that researchers can quickly iterate on each of the models without diving into additional abstractions/files.\n- The training API is optimized to work with PyTorch models provided by Transformers. For generic machine learning loops, you should use another library like [Accelerate](https://huggingface.co/docs/accelerate).\n- The [example scripts](https://github.com/huggingface/transformers/tree/main/examples) are only *examples*. They may not necessarily work out-of-the-box on your specific use case and you'll need to adapt the code for it to work.\n\n100 projects using Transformers\n\nTransformers is more than a toolkit to use pretrained models, it's a community of projects built around it and the\nHugging Face Hub. We want Transformers to enable developers, researchers, students, professors, engineers, and anyone\nelse to build their dream projects.\n\nIn order to celebrate Transformers 100,000 stars, we wanted to put the spotlight on the\ncommunity with the [awesome-transformers](./awesome-transformers.md) page which lists 100\nincredible projects built with Transformers.\n\nIf you own or use a project that you believe should be part of the list, please open a PR to add it!\n\nExample models\n\nYou can test most of our models directly on their [Hub model pages](https://huggingface.co/models).\n\nExpand each modality below to see a few example models for various use cases.\n\nAudio\n\nComputer vision\n\nMultimodal\n\nNLP\n\nCitation\n\nWe now have a [paper](https://www.aclweb.org/anthology/2020.emnlp-demos.6/) you can cite for the  Transformers library:",
      "word_count": 497
    }
  ],
  "usage_description": "Here is a 2-sentence summary of what a developer can achieve with this library:\n\nThis library is used to leverage state-of-the-art machine learning capabilities in various frameworks, including JAX, PyTorch, and TensorFlow, enabling developers to build and deploy advanced models. With Transformers, developers can tap into a wide range of pre-trained models and fine-tune them for their specific tasks, simplifying the process of building high-performance AI applications."
}