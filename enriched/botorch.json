{
  "name": "botorch",
  "summary": "Bayesian Optimization in PyTorch",
  "language": "python",
  "tags": [
    "dev",
    "math",
    "ml",
    "web"
  ],
  "chunks": [
    {
      "chunk_id": "botorch::chunk_0",
      "text": "Why BoTorch ?\nBoTorch\n* Provides a modular and easily extensible interface for composing Bayesian\n  optimization primitives, including probabilistic models, acquisition functions,\n  and optimizers.\n* Harnesses the power of PyTorch, including auto-differentiation, native support\n  for highly parallelized modern hardware (e.g. GPUs) using device-agnostic code,\n  and a dynamic computation graph.\n* Supports Monte Carlo-based acquisition functions via the\n  [reparameterization trick](https://arxiv.org/abs/1312.6114), which makes it\n  straightforward to implement new ideas without having to impose restrictive\n  assumptions about the underlying model.\n* Enables seamless integration with deep and/or convolutional architectures in PyTorch.\n* Has first-class support for state-of-the art probabilistic models in\n  [GPyTorch](http://www.gpytorch.ai/), including support for multi-task Gaussian\n  Processes (GPs) deep kernel learning, deep GPs, and approximate inference.\n\nTarget Audience\n\nThe primary audience for hands-on use of BoTorch are researchers and\nsophisticated practitioners in Bayesian Optimization and AI.\nWe recommend using BoTorch as a low-level API for implementing new algorithms\nfor [Ax](https://ax.dev). Ax has been designed to be an easy-to-use platform\nfor end-users, which at the same time is flexible enough for Bayesian\nOptimization researchers to plug into for handling of feature transformations,\n(meta-)data management, storage, etc.\nWe recommend that end-users who are not actively doing research on Bayesian\nOptimization simply use Ax.\n\nInstallation\n\n**Installation Requirements**\n- Python >= 3.10\n- PyTorch >= 2.0.1\n- gpytorch >= 1.14\n- linear_operator >= 0.6\n- pyro-ppl >= 1.8.4\n- scipy\n- multiple-dispatch\n\nOption 1: Installing the latest release\n\nThe latest release of BoTorch is easily installed via `pip`:\n\n_Note_: Make sure the `pip` being used is actually the one from the newly created\nConda environment. If you're using a Unix-based OS, you can use `which pip` to check.\n\nBoTorch [stopped publishing](https://github.com/meta-pytorch/botorch/discussions/2613#discussion-7431533)\nan official Anaconda package to the `pytorch` channel after the 0.12 release. However,\nusers can still use the package published to the `conda-forge` channel and install botorch via\n\nOption 2: Installing from latest main branch\n\nIf you would like to try our bleeding edge features (and don't mind potentially\nrunning into the occasional bug here or there), you can install the latest\ndevelopment version directly from GitHub. You may also want to install the\ncurrent `gpytorch` and `linear_operator` development versions:\n\nOption 3: Editable/dev install\n\nIf you want to [contribute](CONTRIBUTING.md) to BoTorch, you will want to install editably so that you can change files and have the\nchanges reflected in your local install.\n\nIf you want to install the current `gpytorch` and `linear_operator` development versions, as in Option 2, do that\nbefore proceeding.\n\nOption 3a: Bare-bones editable install\n\nOption 3b: Editable install with development and tutorials dependencies\n\n* `dev`: Specifies tools necessary for development\n  (testing, linting, docs building; see [Contributing](#contributing) below).\n* `tutorials`: Also installs all packages necessary for running the tutorial notebooks.\n* You can also install either the dev or tutorials dependencies without installing both, e.g. by changing the last command to `pip install -e \".[dev]\"`.\n\nGetting Started\n\nHere's a quick run down of the main components of a Bayesian optimization loop.\nFor more details see our [Documentation](https://botorch.org/docs/introduction) and the\n[Tutorials](https://botorch.org/docs/tutorials).\n\n1. Fit a Gaussian Process model to data\n  \n\n2. Construct an acquisition function\n  \n\n3. Optimize the acquisition function\n\nCiting BoTorch\n\nIf you use BoTorch, please cite the following paper:\n> [M. Balandat, B. Karrer, D. R. Jiang, S. Daulton, B. Letham, A. G. Wilson, and E. Bakshy. BoTorch: A Framework for Efficient Monte-Carlo Bayesian Optimization. Advances in Neural Information Processing Systems 33, 2020.](https://arxiv.org/abs/1910.06403)\n\nSee [here](https://botorch.org/docs/papers) for an incomplete selection of peer-reviewed papers that build off of BoTorch.\n\nContributing\nSee the [CONTRIBUTING](CONTRIBUTING.md) file for how to help out.\n\nLicense\nBoTorch is MIT licensed, as found in the [LICENSE](LICENSE) file.",
      "word_count": 595
    }
  ]
}