{
  "name": "courlan",
  "summary": "Clean, filter and sample URLs to optimize data collection â€“ includes spam, content type and language filters.",
  "language": "python",
  "tags": [
    "math",
    "web"
  ],
  "chunks": [
    {
      "chunk_id": "courlan::chunk_0",
      "text": "coURLan: Clean, filter, normalize, and sample URLs\n\n(https://pypi.python.org/pypi/courlan)\n(https://pypi.python.org/pypi/courlan)\n(https://codecov.io/gh/adbar/courlan)\n(https://github.com/psf/black)\n\nWhy coURLan?\n\n> \"It is important for the crawler to visit 'important' pages first,\n> so that the fraction of the Web that is visited (and kept up to date)\n> is more meaningful.\" (Cho et al. 1998)\n>\n> \"Given that the bandwidth for conducting crawls is neither infinite\n> nor free, it is becoming essential to crawl the Web in not only a\n> scalable, but efficient way, if some reasonable measure of quality or\n> freshness is to be maintained.\" (Edwards et al. 2001)\n\nThis library provides an additional \"brain\" for web crawling, scraping\nand document management. It facilitates web navigation through a set of\nfilters, enhancing the quality of resulting document collections:\n\n- Save bandwidth and processing time by steering clear of pages deemed\n  low-value\n- Identify specific pages based on language or text content\n- Pinpoint pages relevant for efficient link gathering\n\nAdditional utilities needed include URL storage, filtering, and\ndeduplication.\n\nFeatures\n\nSeparate the wheat from the chaff and optimize document discovery and\nretrieval:\n\n- URL handling\n   - Validation\n   - Normalization\n   - Sampling\n- Heuristics for link filtering\n   - Spam, trackers, and content-types\n   - Locales and internationalization\n   - Web crawling (frontier, scheduling)\n- Data store specifically designed for URLs\n- Usable with Python or on the command-line\n\n**Let the coURLan fish up juicy bits for you!**\n\nHere is a [courlan](https://en.wiktionary.org/wiki/courlan) (source:\n[Limpkin at Harn's Marsh by\nRuss](https://commons.wikimedia.org/wiki/File:Limpkin,_harns_marsh_(33723700146).jpg),\nCC BY 2.0).\n\nInstallation\n\nThis package is compatible with with all common versions of Python, it\nis tested on Linux, macOS and Windows systems.\n\nCourlan is available on the package repository [PyPI](https://pypi.org/)\nand can notably be installed with the Python package manager `pip`:\n\nThe last version to support Python 3.6 and 3.7 is `courlan==1.2.0`.\n\nPython\n\nMost filters revolve around the `strict` and `language` arguments.\n\ncheck_url()\n\nAll useful operations chained in `check_url(url)`:\n\nLanguage-aware heuristics, notably internationalization in URLs, are\navailable in `lang_filter(url, language)`:\n\nDefine stricter restrictions on the expected content type with\n`strict=True`. This also blocks certain platforms and page types\nwhere machines get lost.\n\nSampling by domain name\n\nWeb crawling and URL handling\n\nLink extraction and preprocessing:\n\nThe `filter_links()` function provides additional filters for crawling purposes:\nuse of robots.txt rules and link priorization. See `courlan.core` for details.\n\nDetermine if a link leads to another host:\n\nOther useful functions dedicated to URL handling:\n\n-   `extract_domain(url, fast=True)`: find domain and subdomain or just\n-   `get_base_url(url)`: strip the URL of some of its parts\n-   `get_host_and_path(url)`: decompose URLs in two parts: protocol +\n-   `get_hostinfo(url)`: extract domain and host info (protocol +\n-   `fix_relative_urls(baseurl, url)`: prepend necessary information to\n\nOther filters dedicated to crawl frontier management:\n\n-   `is_not_crawlable(url)`: check for deep web or pages generally not\n-   `is_navigation_page(url)`: check for navigation and overview pages\n\nSee also [URL management page](https://trafilatura.readthedocs.io/en/latest/url-management.html)\nof the Trafilatura documentation.\n\nPython helpers\n\nHelper function, scrub and normalize:\n\nBasic scrubbing only:\n\nBasic canonicalization/normalization only, i.e. modifying and\nstandardizing URLs in a consistent manner:\n\nBasic URL validation only:\n\nTroubleshooting",
      "word_count": 500
    },
    {
      "chunk_id": "courlan::chunk_1",
      "text": "Courlan uses an internal cache to speed up URL parsing. It can be reset\nas follows:\n\nUrlStore class\n\nThe `UrlStore` class allow for storing and retrieving domain-classified\nURLs, where a URL like `https://example.org/path/testpage` is stored as\nthe path `/path/testpage` within the domain `https://example.org`. It\nfeatures the following methods:\n\n- URL management\n   - `add_urls(urls=[], appendleft=None, visited=False)`: Add a\n   - `add_from_html(htmlstring, url, external=False, lang=None, with_nav=True)`:\n   - `discard(domains)`: Declare domains void and prune the store.\n   - `dump_urls()`: Return a list of all known URLs.\n   - `print_urls()`: Print all URLs in store (URL + TAB + visited or not).\n   - `print_unvisited_urls()`: Print all unvisited URLs in store.\n   - `get_all_counts()`: Return all download counts for the hosts in store.\n   - `get_known_domains()`: Return all known domains as a list.\n   - `get_unvisited_domains()`: Find all domains for which there are unvisited URLs.\n   - `total_url_number()`: Find number of all URLs in store.\n   - `is_known(url)`: Check if the given URL has already been stored.\n   - `has_been_visited(url)`: Check if the given URL has already been visited.\n   - `filter_unknown_urls(urls)`: Take a list of URLs and return the currently unknown ones.\n   - `filter_unvisited_urls(urls)`: Take a list of URLs and return the currently unvisited ones.\n   - `find_known_urls(domain)`: Get all already known URLs for the\n   - `find_unvisited_urls(domain)`: Get all unvisited URLs for the given domain.\n   - `get_unvisited_domains()`: Return all domains which have not been all visited.\n   - `reset()`: Re-initialize the URL store.\n\n- Crawling and downloads\n   - `get_url(domain)`: Retrieve a single URL and consider it to\n   - `get_rules(domain)`: Return the stored crawling rules for the given website.\n   - `store_rules(website, rules=None)`: Store crawling rules for a given website.\n   - `get_crawl_delay()`: Return the delay as extracted from robots.txt, or a given default.\n   - `get_download_urls(max_urls=100, time_limit=10)`: Get a list of immediately\n   - `establish_download_schedule(max_urls=100, time_limit=10)`:\n   - `download_threshold_reached(threshold)`: Find out if the\n   - `unvisited_websites_number()`: Return the number of websites\n   - `is_exhausted_domain(domain)`: Tell if all known URLs for\n\n- Persistance\n   - `write(filename)`: Save the store to disk.\n   - `load_store(filename)`: Read a UrlStore from disk (separate function, not class method).\n\n- Optional settings:\n   - `compressed=True`: activate compression of URLs and rules\n   - `language=XX`: focus on a particular target language (two-letter code)\n   - `strict=True`: stricter URL filtering\n   - `verbose=True`: dump URLs if interrupted (requires use of `signal`)\n\nCommand-line\n\nThe main fonctions are also available through a command-line utility:\n\nLicense\n\n*coURLan* is distributed under the [Apache 2.0\nlicense](https://www.apache.org/licenses/LICENSE-2.0.html).\n\nVersions prior to v1 were under GPLv3+ license.\n\nSettings\n\n`courlan` is optimized for English and German but its generic approach\nis also usable in other contexts.\n\nDetails of strict URL filtering can be reviewed and changed in the file\n`settings.py`. To override the default settings, clone the repository and\n[re-install the package\nlocally](https://packaging.python.org/tutorials/installing-packages/#installing-from-a-local-src-tree).\n\nContributing\n\n[Contributions](https://github.com/adbar/courlan/blob/master/CONTRIBUTING.md)\nare welcome!\n\nFeel free to file issues on the [dedicated\npage](https://github.com/adbar/courlan/issues).\n\nAuthor",
      "word_count": 451
    },
    {
      "chunk_id": "courlan::chunk_2",
      "text": "Developed with practical applications of academic research in mind, this software\nis part of a broader effort to derive information from web documents.\nExtracting and pre-processing web texts to the exacting standards of\nscientific research presents a substantial challenge.\nThis software package simplifies text data collection and enhances corpus quality,\nit is currently used to build [text databases for research](https://www.dwds.de/d/k-web).\n\n- Barbaresi, A. \"[Trafilatura: A Web Scraping Library and\n  Command-Line Tool for Text Discovery and\n  Extraction](https://aclanthology.org/2021.acl-demo.15/).\"\n  *Proceedings of ACL/IJCNLP 2021: System Demonstrations*, 2021, pp. 122-131.\n\nContact: see [homepage](https://adrien.barbaresi.eu/).\n\nSoftware ecosystem: see [this\ngraphic](https://github.com/adbar/trafilatura/blob/master/docs/software-ecosystem.png).\n\nSimilar work\n\nThese Python libraries perform similar handling and normalization tasks\nbut do not entail language or content filters. They also do not\nprimarily focus on crawl optimization:\n\nReferences\n\n-   Cho, J., Garcia-Molina, H., & Page, L. (1998). Efficient crawling\n-   Edwards, J., McCurley, K. S., and Tomlin, J. A. (2001). \"An",
      "word_count": 145
    }
  ],
  "usage_description": "Here is a 2-sentence summary of what a developer can achieve with this library:\n\nThis library is used to efficiently optimize data collection by cleaning, filtering, and sampling URLs to prioritize important pages and reduce unnecessary requests. By leveraging courlan's URL normalization and quality control features, developers can improve the scalability and efficiency of their web crawling and scraping applications."
}