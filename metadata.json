[{"name": "accelerate", "tags": ["cli", "math", "ml", "ui", "web"], "summary": "Accelerate", "text": "Easy to integrate\n\nAccelerate was created for PyTorch users who like to write the training loop of PyTorch models but are reluctant to write and maintain the boilerplate code needed to use multi-GPUs/TPU/fp16.\n\nAccelerate abstracts exactly and only the boilerplate code related to multi-GPUs/TPU/fp16 and leaves the rest of your code unchanged.\n\nHere is an example:\n\nAs you can see in this example, by adding 5-lines to any standard PyTorch training script you can now run on any kind of single or distributed node setting (single CPU, single GPU, multi-GPUs and TPUs) as well as with or without mixed precision (fp8, fp16, bf16).\n\nIn particular, the same code can then be run without modification on your local machine for debugging or your training environment.\n\nAccelerate even handles the device placement for you (which requires a few more changes to your code, but is safer in general), so you can even simplify your training loop further:\n\nWant to learn more? Check out the [documentation](https://huggingface.co/docs/accelerate) or have a look at our [examples](https://github.com/huggingface/accelerate/tree/main/examples).\n\nLaunching script\n\nAccelerate also provides an optional CLI tool that allows you to quickly configure and test your training environment before launching the scripts. No need to remember how to use `torch.distributed.run` or to write a specific launcher for TPU training!\nOn your machine(s) just run:\n\nand answer the questions asked. This will generate a config file that will be used automatically to properly set the default options when doing\n\nFor instance, here is how you would run the GLUE example on the MRPC task (from the root of the repo):\n\nThis CLI tool is **optional**, and you can still use `python my_script.py` or `python -m torchrun my_script.py` at your convenience.\n\nYou can also directly pass in the arguments you would to `torchrun` as arguments to `accelerate launch` if you wish to not run` accelerate config`.\n\nFor example, here is how to launch on two GPUs:\n\nTo learn more, check the CLI documentation available [here](https://huggingface.co/docs/accelerate/package_reference/cli).\n\nOr view the configuration zoo [here](https://github.com/huggingface/accelerate/blob/main/examples/config_yaml_templates/)\n\nLaunching multi-CPU run using MPI\n\nHere is another way to launch multi-CPU run using MPI. You can learn how to install Open MPI on [this page](https://www.open-mpi.org/faq/?category=building#easy-build). You can use Intel MPI or MVAPICH as well.\nOnce you have MPI setup on your cluster, just run:\n\nAnswer the questions that are asked, selecting to run using multi-CPU, and answer \"yes\" when asked if you want accelerate to launch mpirun.\nThen, use `accelerate launch` with your script like:\n\nAlternatively, you can use mpirun directly, without using the CLI like:\n\nLaunching training using DeepSpeed\n\nAccelerate supports training on single/multiple GPUs using DeepSpeed. To use it, you don't need to change anything in your training code; you can set everything using just `accelerate config`. However, if you desire to tweak your DeepSpeed related args from your Python script, we provide you the `DeepSpeedPlugin`.\n\nNote: DeepSpeed support is experimental for now. In case you get into some problem, please open an issue.\n\nLaunching your training from a notebook"}, {"name": "accelerate", "tags": ["cli", "math", "ml", "ui", "web"], "summary": "Accelerate", "text": "Accelerate also provides a `notebook_launcher` function you can use in a notebook to launch a distributed training. This is especially useful for Colab or Kaggle notebooks with a TPU backend. Just define your training loop in a `training_function` then in your last cell, add:\n\nAn example can be found in [this notebook](https://github.com/huggingface/notebooks/blob/main/examples/accelerate_examples/simple_nlp_example.ipynb). (https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/accelerate_examples/simple_nlp_example.ipynb)\n\nWhy should I use  Accelerate?\n\nYou should use  Accelerate when you want to easily run your training scripts in a distributed environment without having to renounce full control over your training loop. This is not a high-level framework above PyTorch, just a thin wrapper so you don't have to learn a new library. In fact, the whole API of  Accelerate is in one class, the `Accelerator` object.\n\nWhy shouldn't I use  Accelerate?\n\nYou shouldn't use  Accelerate if you don't want to write a training loop yourself. There are plenty of high-level libraries above PyTorch that will offer you that,  Accelerate is not one of them.\n\nFrameworks using  Accelerate\n\nIf you like the simplicity of  Accelerate but would prefer a higher-level abstraction around its capabilities, some frameworks and libraries that are built on top of  Accelerate are listed below:\n\n* [Amphion](https://github.com/open-mmlab/Amphion) is a toolkit for Audio, Music, and Speech Generation. Its purpose is to support reproducible research and help junior researchers and engineers get started in the field of audio, music, and speech generation research and development.\n* [Animus](https://github.com/Scitator/animus) is a minimalistic framework to run machine learning experiments. Animus highlights common \"breakpoints\" in ML experiments and provides a unified interface for them within [IExperiment](https://github.com/Scitator/animus/blob/main/animus/core.py#L76).\n* [Catalyst](https://github.com/catalyst-team/catalyst#getting-started) is a PyTorch framework for Deep Learning Research and Development. It focuses on reproducibility, rapid experimentation, and codebase reuse so you can create something new rather than write yet another train loop. Catalyst provides a [Runner](https://catalyst-team.github.io/catalyst/api/core.html#runner) to connect all parts of the experiment: hardware backend, data transformations, model training, and inference logic.\n* [fastai](https://github.com/fastai/fastai#installing) is a PyTorch framework for Deep Learning that simplifies training fast and accurate neural nets using modern best practices. fastai provides a [Learner](https://docs.fast.ai/learner.html#Learner) to handle the training, fine-tuning, and inference of deep learning algorithms.\n* [Finetuner](https://github.com/jina-ai/finetuner) is a service that enables models to create higher-quality embeddings for semantic search, visual similarity search, cross-modal textimage search, recommendation systems, clustering, duplication detection, anomaly detection, or other uses.\n* [InvokeAI](https://github.com/invoke-ai/InvokeAI) is a creative engine for Stable Diffusion models, offering industry-leading WebUI, terminal usage support, and serves as the foundation for many commercial products.\n* [Kornia](https://kornia.readthedocs.io/en/latest/get-started/introduction.html) is a differentiable library that allows classical computer vision to be integrated into deep learning models. Kornia provides a [Trainer](https://kornia.readthedocs.io/en/latest/x.html#kornia.x.Trainer) with the specific purpose to train and fine-tune the supported deep learning algorithms within the library.\n* [Open Assistant](https://projects.laion.ai/Open-Assistant/) is a chat-based assistant that understands tasks, can interact with their party systems, and retrieve information dynamically to do so. \n* [pytorch-accelerated](https://github.com/Chris-hughes10/pytorch-accelerated) is a lightweight training library, with a streamlined feature set centered around a general-purpose [Trainer](https://pytorch-accelerated.readthedocs.io/en/latest/trainer.html), that places a huge emphasis on simplicity and transparency; enabling users to understand exactly what is going on under the hood, but without having to write and maintain the boilerplate themselves!\n* [Stable Diffusion web UI](https://github.com/AUTOMATIC1111/stable-diffusion-webui) is an open-source browser-based easy-to-use interface based on the Gradio library for Stable Diffusion.\n* [torchkeras](https://github.com/lyhue1991/torchkeras) is a simple tool for training pytorch model just in a keras style, a dynamic and beautiful plot is provided in notebook to monitor your loss or metric.\n* [transformers](https://github.com/huggingface/transformers) as a tool for helping train state-of-the-art machine learning models in PyTorch, Tensorflow, and JAX. (Accelerate is the backend for the PyTorch side)."}, {"name": "accelerate", "tags": ["cli", "math", "ml", "ui", "web"], "summary": "Accelerate", "text": "Installation\n\nThis repository is tested on Python 3.8+ and PyTorch 1.10.0+\n\nYou should install  Accelerate in a [virtual environment](https://docs.python.org/3/library/venv.html). If you're unfamiliar with Python virtual environments, check out the [user guide](https://packaging.python.org/guides/installing-using-pip-and-virtual-environments/).\n\nFirst, create a virtual environment with the version of Python you're going to use and activate it.\n\nThen, you will need to install PyTorch: refer to the [official installation page](https://pytorch.org/get-started/locally/#start-locally) regarding the specific install command for your platform. Then  Accelerate can be installed using pip as follows:\n\nSupported integrations\n\n- CPU only\n- multi-CPU on one node (machine)\n- multi-CPU on several nodes (machines)\n- single GPU\n- multi-GPU on one node (machine)\n- multi-GPU on several nodes (machines)\n- TPU\n- FP16/BFloat16 mixed precision\n- FP8 mixed precision with [Transformer Engine](https://github.com/NVIDIA/TransformerEngine) or [MS-AMP](https://github.com/Azure/MS-AMP/)\n- DeepSpeed support (Experimental)\n- PyTorch Fully Sharded Data Parallel (FSDP) support (Experimental)\n- Megatron-LM support (Experimental)\n\nCiting  Accelerate\n\nIf you use  Accelerate in your publication, please cite it by using the following BibTeX entry."}, {"name": "accelerate", "tags": ["cli", "math", "ml", "ui", "web"], "summary": "Accelerate", "text": "This library is used to simplify multi-GPU/TPU and mixed precision training in PyTorch models by abstracting away boilerplate code. By integrating Accelerate, developers can easily scale their model training across various hardware configurations with minimal modifications to their existing PyTorch code."}, {"name": "adjusttext", "tags": ["math", "visualization"], "summary": "Iteratively adjust text position in matplotlib plots to minimize overlaps", "text": "adjustText - automatic label placement for `matplotlib`\n\nInspired by **ggrepel** package for R/ggplot2 (https://github.com/slowkow/ggrepel) \n\nAlternative: **textalloc** https://github.com/ckjellson/textalloc\n\nBrief description\n\nThe idea is that often when we want to label multiple points on a graph the text will start heavily overlapping with both other labels and data points. This can be a major problem requiring manual solution. However this can be largely automatized by smart placing of the labels (difficult) or iterative adjustment of their positions to minimize overlaps (relatively easy). This library (well... script) implements the latter option to help with matplotlib graphs. Usage is very straightforward with usually pretty good results with no tweaking (most important is to just make text slightly smaller than default and maybe the figure a little larger). However the algorithm itself is highly configurable for complicated plots.\n\nGetting started\n\nInstallation\n\nShould be installable from pypi:\n\nOr with `conda`:\n\nFor the latest version from github:\n\nDocumentation\n\n[Wiki] has some basic introduction, and more advanced usage examples can be found [here].\n\nThanks to Christophe Van Neste @beukueb, **adjustText** has a simple documentation:\n\nCiting **adjustText**\n\nTo cite the library if you use it in scientific publications (or anywhere else, if you wish), please use the link to the GitHub repository (https://github.com/Phlya/adjustText) and a zenodo doi (see top of this page). Thank you!"}, {"name": "adjusttext", "tags": ["math", "visualization"], "summary": "Iteratively adjust text position in matplotlib plots to minimize overlaps", "text": "This library is used to automatically adjust text positions in matplotlib plots to minimize label overlaps, making it easier to effectively annotate multiple points on a graph without manual intervention. By iteratively adjusting label positions, this library helps ensure that labels are readable and do not obstruct data points or each other."}, {"name": "affine", "tags": ["data", "math"], "summary": "Matrices describing affine transformation of the plane", "text": "Affine\n======\n\nMatrices describing 2D affine transformation of the plane.\n\nThe Affine package is derived from Casey Duncan's Planar package. Please see\nthe copyright statement in `affine/__init__.py `__.\n\nUsage\n-----\n\nThe 3x3 augmented affine transformation matrix for transformations in two\ndimensions is illustrated below.\n\n::\n\nx'\ny'\n1\n\nMatrices can be created by passing the values ``a, b, c, d, e, f`` to the\n``affine.Affine`` constructor or by using its ``identity()``,\n``translation()``, ``scale()``, ``shear()``, and ``rotation()`` class methods.\n\n.. code-block:: pycon\n\n  >>> from affine import Affine\n  >>> Affine.identity()\n  Affine(1.0, 0.0, 0.0,\n  >>> Affine.translation(1.0, 5.0)\n  Affine(1.0, 0.0, 1.0,\n  >>> Affine.scale(2.0)\n  Affine(2.0, 0.0, 0.0,\n  >>> Affine.shear(45.0, 45.0)  # decimal degrees\n  Affine(1.0, 0.9999999999999999, 0.0,\n  >>> Affine.rotation(45.0)     # decimal degrees\n  Affine(0.7071067811865476, -0.7071067811865475, 0.0,\n\nThese matrices can be applied to ``(x, y)`` tuples to obtain transformed\ncoordinates ``(x', y')``.\n\n.. code-block:: pycon\n\n  >>> Affine.translation(1.0, 5.0) * (1.0, 1.0)\n  (2.0, 6.0)\n  >>> Affine.rotation(45.0) * (1.0, 1.0)\n  (1.1102230246251565e-16, 1.414213562373095)\n\nThey may also be multiplied together to combine transformations.\n\n.. code-block:: pycon\n\n  >>> Affine.translation(1.0, 5.0) * Affine.rotation(45.0)\n  Affine(0.7071067811865476, -0.7071067811865475, 1.0,\n\nUsage with GIS data packages\n----------------------------\n\nGeoreferenced raster datasets use affine transformations to map from image\ncoordinates to world coordinates. The ``affine.Affine.from_gdal()`` class\nmethod helps convert `GDAL GeoTransform\n`__,\nsequences of 6 numbers in which the first and fourth are the x and y offsets\nand the second and sixth are the x and y pixel sizes.\n\nUsing a GDAL dataset transformation matrix, the world coordinates ``(x, y)``\ncorresponding to the top left corner of the pixel 100 rows down from the\norigin can be easily computed.\n\n.. code-block:: pycon\n\n  >>> geotransform = (-237481.5, 425.0, 0.0, 237536.4, 0.0, -425.0)\n  >>> fwd = Affine.from_gdal(*geotransform)\n  >>> col, row = 0, 100\n  >>> fwd * (col, row)\n  (-237481.5, 195036.4)\n\nThe reverse transformation is obtained using the ``~`` operator.\n\n.. code-block:: pycon\n\n  >>> rev = ~fwd\n  >>> rev * fwd * (col, row)\n  (0.0, 99.99999999999999)"}, {"name": "affine", "tags": ["data", "math"], "summary": "Matrices describing affine transformation of the plane", "text": "This library is used to create and manipulate matrices that describe affine transformations of the plane in two dimensions, enabling developers to easily apply common geometric operations such as translation, scaling, shearing, and rotation. With this library, developers can efficiently build complex transformations by combining these basic operations."}, {"name": "agate", "tags": ["math"], "summary": "A data analysis library that is optimized for humans instead of machines.", "text": ".. image:: https://img.shields.io/pypi/dm/agate.svg\n\n.. image:: https://img.shields.io/pypi/v/agate.svg\n\n.. image:: https://img.shields.io/pypi/l/agate.svg\n\n.. image:: https://img.shields.io/pypi/pyversions/agate.svg\n\nagate is a Python data analysis library that is optimized for humans instead of machines. It is an alternative to numpy and pandas that solves real-world problems with readable code.\n\nagate was previously known as journalism.\n\nImportant links:\n\n* Documentation:    https://agate.rtfd.org\n* Repository:       https://github.com/wireservice/agate\n* Issues:           https://github.com/wireservice/agate/issues"}, {"name": "agate", "tags": ["math"], "summary": "A data analysis library that is optimized for humans instead of machines.", "text": "This library is used to simplify data analysis tasks with readable and human-friendly code, providing an alternative to traditional libraries like numpy and pandas. With agate, developers can focus on solving real-world problems without getting bogged down in complex, machine-optimized code."}, {"name": "agno", "tags": ["math", "ml", "web"], "summary": "Agno: a lightweight library for building Multi-Agent Systems", "text": "What is Agno?\n\nAgno is a multi-agent framework, runtime, and control plane. Use it to build private and secure AI products that run in your cloud.\n\n- **Build** agents, teams, and workflows with memory, knowledge, guardrails, and 100+ toolkits.\n- **Run** in production with a stateless FastAPI runtime. Horizontally scalable.\n- **Manage** with a control plane that connects directly to your runtime \u2014 no data leaves your environment.\n\nWhy Agno?\n\n- **Your cloud, your data:** AgentOS runs entirely in your infrastructure. Zero data leaves your environment.\n- **Production-ready from day one:** Pre-built FastAPI runtime with SSE endpoints, ready to deploy.\n- **Actually fast:** 529\u00d7 faster than LangGraph, 24\u00d7 lower memory. Matters at scale.\n\nGetting Started\n\nNew to Agno? Start with the [getting started guide](https://github.com/agno-agi/agno/tree/main/cookbook/00_getting_started).\n\nThen:\n- Browse the [cookbooks](https://github.com/agno-agi/agno/tree/main/cookbook) for real-world examples\n- Read the [docs](https://docs.agno.com) to learn more.\n\nResources\n\n- Docs: docs.agno.com\n- Cookbook: Cookbook\n- Community forum: community.agno.com\n- Discord: discord\n\nExample\n\nHere's an example of an Agent that connects to an MCP server, manages conversation state in a database, is served using a FastAPI application that you can chat with using the [AgentOS UI](https://os.agno.com).\n\nAgentOS - Production Runtime for Multi-Agent Systems\n\nBuilding Agents is easy, running them as a secure, scalable service is hard. AgentOS solves this by providing a high performance runtime for serving multi-agent systems in production. Key features include:\n\n1. **Pre-built FastAPI app**: AgentOS includes a ready-to-use FastAPI app for running your agents, teams and workflows. This gives you a significant head start when building an AI product.\n\n2. **Integrated Control Plane**: The [AgentOS UI](https://os.agno.com) connects directly to your runtime, so you can test, monitor and manage your system in real time with full operational visibility.\n\n3. **Private by Design**: AgentOS runs entirely in your cloud, ensuring complete data privacy. No data leaves your environment, making it ideal for security conscious enterprises..\n\nWhen you run the example script shared above, you get a FastAPI app that you can connect to the [AgentOS UI](https://os.agno.com). Here's what it looks like in action:\n\nPrivate by Design\n\nThis is the part we care most about.\n\nAgentOS runs in **your** cloud. The control plane UI connects directly to your runtime from your browser. Your data never touches our servers. No retention costs, no vendor lock-in, no compliance headaches.\n\nThis isn't a privacy mode or enterprise add-on. It's how Agno works.\n\nFeatures\n\n**Core**\n- Model agnostic \u2014 works with OpenAI, Anthropic, Google, local models, whatever\n- Type-safe I/O with `input_schema` and `output_schema`\n- Async-first, built for long-running tasks\n- Natively multimodal (text, images, audio, video, files)\n\n**Memory & Knowledge**\n- Persistent storage for session history and state\n- User memory that persists across sessions\n- Agentic RAG with 20+ vector stores, hybrid search, reranking\n- Culture \u2014 shared long-term memory across agents\n\n**Execution**\n- Human-in-the-loop (confirmations, approvals, overrides)\n- Guardrails for validation and security\n- Pre/post hooks for the agent lifecycle\n- First-class MCP and A2A support\n- 100+ built-in toolkits\n\n**Production**\n- Ready-to-use FastAPI runtime\n- Integrated control plane UI\n- Evals for accuracy, performance, latency\n- Durable execution for resumable workflows\n- RBAC and per-agent permissions\n\nPerformance\n\nWe're obsessive about performance because agent workloads spawn hundreds of instances and run long tasks. Stateless, horizontal scalability isn't optional.\n\n**Benchmarks** (Apple M4 MacBook Pro, Oct 2025):\n\nMetric\n--------\nInstantiation\nMemory\n\nRun the benchmarks yourself: [`cookbook/evals/performance`](https://github.com/agno-agi/agno/tree/main/cookbook/evals/performance)\n\nIDE Integration\n\nFor AI-assisted development, add our docs to your IDE:\n\n**Cursor:** Settings \u2192 Indexing & Docs \u2192 Add `https://docs.agno.com/llms-full.txt`\n\nWorks with VSCode, Windsurf, and other AI-enabled editors too.\n\nContributing\n\nWe welcome contributions. See the [contributing guide](https://github.com/agno-agi/agno/blob/v2.0/CONTRIBUTING.md).\n\nTelemetry\n\nAgno logs which model providers are used so we can prioritize updates. Disable with `AGNO_TELEMETRY=false`.\n\n  \u2b06\ufe0f Back to Top"}, {"name": "agno", "tags": ["math", "ml", "web"], "summary": "Agno: a lightweight library for building Multi-Agent Systems", "text": "This library is used to build private and secure Multi-Agent Systems (MAS) that run entirely within a developer's own cloud infrastructure. With Agno, developers can create scalable AI products with pre-built FastAPI runtime, stateless architecture, and integrated toolkits for building complex workflows."}, {"name": "albucore", "tags": ["math", "ml"], "summary": "High-performance image processing functions for deep learning and computer vision.", "text": "Albucore: High-Performance Image Processing Functions\n\nAlbucore is a library of optimized atomic functions designed for efficient image processing. These functions serve as the foundation for [AlbumentationsX](https://github.com/albumentations-team/AlbumentationsX), a popular image augmentation library.\n\nOverview\n\nImage processing operations can be implemented in various ways, each with its own performance characteristics depending on the image type, size, and number of channels. Albucore aims to provide the fastest implementation for each operation by leveraging different backends such as NumPy, OpenCV, and custom optimized code.\n\nKey features:\n\n- Optimized atomic image processing functions\n- Automatic selection of the fastest implementation based on input image characteristics\n- Seamless integration with Albumentations\n- Extensive benchmarking for performance validation\n\nGitAds Sponsored\n(https://gitads.dev/v1/ad-track?source=albumentations-team/albucore@github)\n\nInstallation\n\nUsage\n\nAlbucore automatically selects the most efficient implementation based on the input image type and characteristics.\n\nShape Conventions\n\nAlbucore expects images to follow specific shape conventions, with the channel dimension always present:\n\n- **Single image**: `(H, W, C)` - Height, Width, Channels\n- **Grayscale image**: `(H, W, 1)` - Height, Width, 1 channel\n- **Batch of images**: `(N, H, W, C)` - Number of images, Height, Width, Channels\n- **3D volume**: `(D, H, W, C)` - Depth, Height, Width, Channels\n- **Batch of volumes**: `(N, D, H, W, C)` - Number of volumes, Depth, Height, Width, Channels\n\nImportant Notes:\n\n1. **Channel dimension is always required**, even for grayscale images (use shape `(H, W, 1)`)\n2. Single-channel images should have shape `(H, W, 1)` not `(H, W)`\n3. Batches and volumes are treated uniformly - a 4D array `(N, H, W, C)` can represent either a batch of images or a 3D volume\n\nExamples:\n\nFunctions\n\nAlbucore includes optimized implementations for various image processing operations, including:\n\n- Arithmetic operations (add, multiply, power)\n- Normalization (per-channel, global)\n- Geometric transformations (vertical flip, horizontal flip)\n- Helper decorators (to_float, to_uint8)\n\nBatch Processing\n\nMany functions in Albucore support batch processing out of the box. The library automatically handles different input shapes:\n\n- Single images: `(H, W, C)`\n- Batches: `(N, H, W, C)`\n- Volumes: `(D, H, W, C)`\n- Batch of volumes: `(N, D, H, W, C)`\n\nFunctions will preserve the input shape structure, applying operations efficiently across all images/slices in the batch.\n\nDecorators\n\nAlbucore provides several useful decorators:\n\n- `@preserve_channel_dim`: Ensures single-channel images maintain their shape `(H, W, 1)` when OpenCV operations might drop the channel dimension\n- `@contiguous`: Ensures arrays are C-contiguous for optimal performance\n- `@uint8_io` and `@float32_io`: Handle automatic type conversions for functions that work best with specific data types\n\nPerformance\n\nAlbucore uses a combination of techniques to achieve high performance:\n\n1. **Multiple Implementations**: Each function may have several implementations using different backends (NumPy, OpenCV, custom code).\n2. **Automatic Selection**: The library automatically chooses the fastest implementation based on the input image type, size, and number of channels.\n3. **Optimized Algorithms**: Custom implementations are optimized for specific use cases, often outperforming general-purpose libraries.\n\nLicense\n\nMIT\n\nAcknowledgements\n\nAlbucore is part of the [AlbumentationsX](https://github.com/albumentations-team/AlbumentationsX) project. We'd like to thank all contributors to [AlbumentationsX](https://albumentations.ai/people) and the broader computer vision community for their inspiration and support."}, {"name": "albucore", "tags": ["math", "ml"], "summary": "High-performance image processing functions for deep learning and computer vision.", "text": "This library is used to accelerate high-performance image processing for deep learning and computer vision applications. It optimizes various operations like resizing, normalization, and conversion to speed up image processing workflows."}, {"name": "albumentations", "tags": ["data", "math", "ml", "ui", "visualization", "web"], "summary": "Fast, flexible, and advanced augmentation library for deep learning, computer vision, and medical imaging. Albumentations offers a wide range of transformations for both 2D (images, masks, bboxes, keypoints) and 3D (volumes, volumetric masks, keypoints) data, with optimized performance and seamless integration into ML workflows.", "text": "Albumentations\n\n(https://pypi.org/project/albumentations/)\n(https://anaconda.org/conda-forge/albumentations)\n\n>  **Stay updated!** [Subscribe to our newsletter](https://albumentations.ai/subscribe) for the latest releases, tutorials, and tips directly from the Albumentations team.\n\n(https://opensource.org/licenses/MIT)\n(https://gurubase.io/g/albumentations)\n\n[Docs](https://albumentations.ai/docs/) | [Discord](https://discord.gg/AKPrrDYNAt) | [Twitter](https://twitter.com/albumentations) | [LinkedIn](https://www.linkedin.com/company/100504475/)\n\nAlbumentations is a Python library for image augmentation. Image augmentation is used in deep learning and computer vision tasks to increase the quality of trained models. The purpose of image augmentation is to create new training samples from the existing data.\n\nHere is an example of how you can apply some [pixel-level](#pixel-level-transforms) augmentations from Albumentations to create new images from the original one:\n\nWhy Albumentations\n\n- **Complete Computer Vision Support**: Works with [all major CV tasks](#i-want-to-use-albumentations-for-the-specific-task-such-as-classification-or-segmentation) including classification, segmentation (semantic & instance), object detection, and pose estimation.\n- **Simple, Unified API**: [One consistent interface](#a-simple-example) for all data types - RGB/grayscale/multispectral images, masks, bounding boxes, and keypoints.\n- **Created by Experts**: Built by [developers with deep experience in computer vision and machine learning competitions](#authors).\n\nCommunity-Driven Project, Supported By\n\nAlbumentations thrives on developer contributions. We appreciate our sponsors who help sustain the project's infrastructure.\n\nExclusive Partner\n-------------------\nYour company could be here\n\nIntegration Partner\n-------------------\nYour company could be here\n\nCommunity Sponsor\n-----------------\n\n---\n\nBecome a Sponsor\n\nYour sponsorship is a way to say \"thank you\" to the maintainers and contributors who spend their free time building and maintaining Albumentations. Sponsors are featured on our website and README. View sponsorship tiers on [our support page](https://albumentations.ai/support/)\n\nTable of contents\n\n- [Albumentations](#albumentations)\n  - [Why Albumentations](#why-albumentations)\n  - [Community-Driven Project, Supported By](#community-driven-project-supported-by)\n  - [Table of contents](#table-of-contents)\n  - [Authors](#authors)\n  - [Installation](#installation)\n  - [Documentation](#documentation)\n  - [A simple example](#a-simple-example)\n  - [Getting started](#getting-started)\n  - [Who is using Albumentations](#who-is-using-albumentations)\n  - [List of augmentations](#list-of-augmentations)\n  - [A few more examples of **augmentations**](#a-few-more-examples-of-augmentations)\n  - [Benchmarking results](#benchmark-results)\n  - [Performance Comparison](#performance-comparison)\n  - [Contributing](#contributing)\n  - [Community](#community)\n  - [Citing](#citing)\n\nAuthors\n\nCurrent Maintainer\n\n[**Vladimir I. Iglovikov**](https://www.linkedin.com/in/iglovikov/) | [Kaggle Grandmaster](https://www.kaggle.com/iglovikov)\n\nEmeritus Core Team Members\n\n[**Mikhail Druzhinin**](https://www.linkedin.com/in/mikhail-druzhinin-548229100/) | [Kaggle Expert](https://www.kaggle.com/dipetm)\n\n[**Alex Parinov**](https://www.linkedin.com/in/alex-parinov/) | [Kaggle Master](https://www.kaggle.com/creafz)\n\n[**Alexander Buslaev**](https://www.linkedin.com/in/al-buslaev/) | [Kaggle Master](https://www.kaggle.com/albuslaev)\n\n[**Eugene Khvedchenya**](https://www.linkedin.com/in/cvtalks/) | [Kaggle Grandmaster](https://www.kaggle.com/bloodaxe)\n\nInstallation\n\nAlbumentations requires Python 3.9 or higher. To install the latest version from PyPI:\n\nOther installation options are described in the [documentation](https://albumentations.ai/docs/getting_started/installation/).\n\nDocumentation\n\nThe full documentation is available at **[https://albumentations.ai/docs/](https://albumentations.ai/docs/)**.\n\nA simple example\n\nGetting started\n\nI am new to image augmentation\n\nPlease start with the [introduction articles](https://albumentations.ai/docs/#learning-path) about why image augmentation is important and how it helps to build better models.\n\nI want to use Albumentations for the specific task such as classification or segmentation\n\nIf you want to use Albumentations for a specific task such as classification, segmentation, or object detection, refer to the [set of articles](https://albumentations.ai/docs/#quick-start-guide) that has an in-depth description of this task. We also have a [list of examples](https://albumentations.ai/docs/examples/) on applying Albumentations for different use cases.\n\nI want to know how to use Albumentations with deep learning frameworks\n\nWe have [examples of using Albumentations](https://albumentations.ai/docs/#examples-of-how-to-use-albumentations-with-different-deep-learning-frameworks) along with PyTorch and TensorFlow.\n\nI want to explore augmentations and see Albumentations in action"}, {"name": "albumentations", "tags": ["data", "math", "ml", "ui", "visualization", "web"], "summary": "Fast, flexible, and advanced augmentation library for deep learning, computer vision, and medical imaging. Albumentations offers a wide range of transformations for both 2D (images, masks, bboxes, keypoints) and 3D (volumes, volumetric masks, keypoints) data, with optimized performance and seamless integration into ML workflows.", "text": "Check the [online demo of the library](https://albumentations-demo.herokuapp.com/). With it, you can apply augmentations to different images and see the result. Also, we have a [list of all available augmentations and their targets](#list-of-augmentations).\n\nWho is using Albumentations\n\nSee also\n\n- [A list of papers that cite Albumentations](https://scholar.google.com/citations?view_op=view_citation&citation_for_view=vkjh9X0AAAAJ:r0BpntZqJG4C).\n- [Open source projects that use Albumentations](https://github.com/albumentations-team/albumentations/network/dependents?dependent_type=PACKAGE).\n\nList of augmentations\n\nPixel-level transforms\n\nPixel-level transforms will change just an input image and will leave any additional targets such as masks, bounding boxes, and keypoints unchanged. For volumetric data (volumes and 3D masks), these transforms are applied independently to each slice along the Z-axis (depth dimension), maintaining consistency across the volume. The list of pixel-level transforms:\n\nSpatial-level transforms\n\nSpatial-level transforms will simultaneously change both an input image as well as additional targets such as masks, bounding boxes, and keypoints. For volumetric data (volumes and 3D masks), these transforms are applied independently to each slice along the Z-axis (depth dimension), maintaining consistency across the volume. The following table shows which additional targets are supported by each transform:\n\n- Volume: 3D array of shape (D, H, W) or (D, H, W, C) where D is depth, H is height, W is width, and C is number of channels (optional)\n- Mask3D: Binary or multi-class 3D mask of shape (D, H, W) where each slice represents segmentation for the corresponding volume slice\n\nTransform\n------------------------------------------------------------------------------------------------\n[Affine](https://explore.albumentations.ai/transform/Affine)\n[AtLeastOneBBoxRandomCrop](https://explore.albumentations.ai/transform/AtLeastOneBBoxRandomCrop)\n[BBoxSafeRandomCrop](https://explore.albumentations.ai/transform/BBoxSafeRandomCrop)\n[CenterCrop](https://explore.albumentations.ai/transform/CenterCrop)\n[CoarseDropout](https://explore.albumentations.ai/transform/CoarseDropout)\n[ConstrainedCoarseDropout](https://explore.albumentations.ai/transform/ConstrainedCoarseDropout)\n[Crop](https://explore.albumentations.ai/transform/Crop)\n[CropAndPad](https://explore.albumentations.ai/transform/CropAndPad)\n[CropNonEmptyMaskIfExists](https://explore.albumentations.ai/transform/CropNonEmptyMaskIfExists)\n[D4](https://explore.albumentations.ai/transform/D4)\n[ElasticTransform](https://explore.albumentations.ai/transform/ElasticTransform)\n[Erasing](https://explore.albumentations.ai/transform/Erasing)\n[FrequencyMasking](https://explore.albumentations.ai/transform/FrequencyMasking)\n[GridDistortion](https://explore.albumentations.ai/transform/GridDistortion)\n[GridDropout](https://explore.albumentations.ai/transform/GridDropout)\n[GridElasticDeform](https://explore.albumentations.ai/transform/GridElasticDeform)\n[HorizontalFlip](https://explore.albumentations.ai/transform/HorizontalFlip)\n[Lambda](https://explore.albumentations.ai/transform/Lambda)\n[LongestMaxSize](https://explore.albumentations.ai/transform/LongestMaxSize)\n[MaskDropout](https://explore.albumentations.ai/transform/MaskDropout)\n[Morphological](https://explore.albumentations.ai/transform/Morphological)\n[Mosaic](https://explore.albumentations.ai/transform/Mosaic)\n[NoOp](https://explore.albumentations.ai/transform/NoOp)\n[OpticalDistortion](https://explore.albumentations.ai/transform/OpticalDistortion)\n[OverlayElements](https://explore.albumentations.ai/transform/OverlayElements)\n[Pad](https://explore.albumentations.ai/transform/Pad)\n[PadIfNeeded](https://explore.albumentations.ai/transform/PadIfNeeded)\n[Perspective](https://explore.albumentations.ai/transform/Perspective)\n[PiecewiseAffine](https://explore.albumentations.ai/transform/PiecewiseAffine)\n[PixelDropout](https://explore.albumentations.ai/transform/PixelDropout)\n[RandomCrop](https://explore.albumentations.ai/transform/RandomCrop)\n[RandomCropFromBorders](https://explore.albumentations.ai/transform/RandomCropFromBorders)\n[RandomCropNearBBox](https://explore.albumentations.ai/transform/RandomCropNearBBox)\n[RandomGridShuffle](https://explore.albumentations.ai/transform/RandomGridShuffle)\n[RandomResizedCrop](https://explore.albumentations.ai/transform/RandomResizedCrop)\n[RandomRotate90](https://explore.albumentations.ai/transform/RandomRotate90)\n[RandomScale](https://explore.albumentations.ai/transform/RandomScale)\n[RandomSizedBBoxSafeCrop](https://explore.albumentations.ai/transform/RandomSizedBBoxSafeCrop)\n[RandomSizedCrop](https://explore.albumentations.ai/transform/RandomSizedCrop)\n[Resize](https://explore.albumentations.ai/transform/Resize)\n[Rotate](https://explore.albumentations.ai/transform/Rotate)\n[SafeRotate](https://explore.albumentations.ai/transform/SafeRotate)\n[ShiftScaleRotate](https://explore.albumentations.ai/transform/ShiftScaleRotate)\n[SmallestMaxSize](https://explore.albumentations.ai/transform/SmallestMaxSize)\n[SquareSymmetry](https://explore.albumentations.ai/transform/SquareSymmetry)\n[ThinPlateSpline](https://explore.albumentations.ai/transform/ThinPlateSpline)\n[TimeMasking](https://explore.albumentations.ai/transform/TimeMasking)\n[TimeReverse](https://explore.albumentations.ai/transform/TimeReverse)\n[Transpose](https://explore.albumentations.ai/transform/Transpose)\n[VerticalFlip](https://explore.albumentations.ai/transform/VerticalFlip)\n[XYMasking](https://explore.albumentations.ai/transform/XYMasking)\n\n3D transforms\n\n3D transforms operate on volumetric data and can modify both the input volume and associated 3D mask.\n\nWhere:\n\n- Volume: 3D array of shape (D, H, W) or (D, H, W, C) where D is depth, H is height, W is width, and C is number of channels (optional)\n- Mask3D: Binary or multi-class 3D mask of shape (D, H, W) where each slice represents segmentation for the corresponding volume slice\n\nTransform\n------------------------------------------------------------------------------\n[CenterCrop3D](https://explore.albumentations.ai/transform/CenterCrop3D)\n[CoarseDropout3D](https://explore.albumentations.ai/transform/CoarseDropout3D)\n[CubicSymmetry](https://explore.albumentations.ai/transform/CubicSymmetry)\n[Pad3D](https://explore.albumentations.ai/transform/Pad3D)\n[PadIfNeeded3D](https://explore.albumentations.ai/transform/PadIfNeeded3D)\n[RandomCrop3D](https://explore.albumentations.ai/transform/RandomCrop3D)\n\nA few more examples of **augmentations**\n\nSemantic segmentation on the Inria dataset\n\nMedical imaging\n\nObject detection and semantic segmentation on the Mapillary Vistas dataset\n\nKeypoints augmentation\n\nBenchmark Results\n\nImage Benchmark Results\n\nSystem Information\n\n- Platform: macOS-15.1-arm64-arm-64bit\n- Processor: arm\n- CPU Count: 16\n- Python Version: 3.12.8\n\nBenchmark Parameters\n\n- Number of images: 2000\n- Runs per transform: 5\n- Max warmup iterations: 1000\n\nLibrary Versions\n\n- albumentations: 2.0.4\n- augly: 1.0.0\n- imgaug: 0.4.0\n- kornia: 0.8.0\n- torchvision: 0.20.1\n\nPerformance Comparison\n\nNumber shows how many uint8 images per second can be processed on one CPU thread. Larger is better.\nThe Speedup column shows how many times faster Albumentations is compared to the fastest other\nlibrary for each transform."}, {"name": "albumentations", "tags": ["data", "math", "ml", "ui", "visualization", "web"], "summary": "Fast, flexible, and advanced augmentation library for deep learning, computer vision, and medical imaging. Albumentations offers a wide range of transformations for both 2D (images, masks, bboxes, keypoints) and 3D (volumes, volumetric masks, keypoints) data, with optimized performance and seamless integration into ML workflows.", "text": "Transform\n:---------------------\nAffine\nAutoContrast\nBlur\nBrightness\nCLAHE\nCenterCrop128\nChannelDropout\nChannelShuffle\nCoarseDropout\nColorJitter\nContrast\nCornerIllumination\nElastic\nEqualize\nErasing\nGaussianBlur\nGaussianIllumination\nGaussianNoise\nGrayscale\nHSV\nHorizontalFlip\nHue\nInvert\nJpegCompression\nLinearIllumination\nMedianBlur\nMotionBlur\nNormalize\nOpticalDistortion\nPad\nPerspective\nPlankianJitter\nPlasmaBrightness\nPlasmaContrast\nPlasmaShadow\nPosterize\nRGBShift\nRain\nRandomCrop128\nRandomGamma\nRandomResizedCrop\nResize\nRotate\nSaltAndPepper\nSaturation\nSharpen\nShear\nSnow\nSolarize\nThinPlateSpline\nVerticalFlip\n\nContributing\n\nTo create a pull request to the repository, follow the documentation at [CONTRIBUTING.md](CONTRIBUTING.md)\n\nCommunity\n\nCiting\n\nIf you find this library useful for your research, please consider citing [Albumentations: Fast and Flexible Image Augmentations](https://www.mdpi.com/2078-2489/11/2/125):\n\n---\n\nStay Connected\n\nNever miss updates, tutorials, and tips from the Albumentations team! [Subscribe to our newsletter](https://albumentations.ai/subscribe)."}, {"name": "albumentations", "tags": ["data", "math", "ml", "ui", "visualization", "web"], "summary": "Fast, flexible, and advanced augmentation library for deep learning, computer vision, and medical imaging. Albumentations offers a wide range of transformations for both 2D (images, masks, bboxes, keypoints) and 3D (volumes, volumetric masks, keypoints) data, with optimized performance and seamless integration into ML workflows.", "text": "This library is used to accelerate model development in deep learning, computer vision, and medical imaging by providing a wide range of transformations for data augmentation, both 2D and 3D. This library enables developers to efficiently generate diverse training datasets, enhancing model robustness and accuracy."}, {"name": "anndata", "tags": ["math", "visualization", "web"], "summary": "Annotated data.", "text": "anndata - Annotated data\n\nanndata is a Python package for handling annotated data matrices in memory and on disk, positioned between pandas and xarray. anndata offers a broad range of computationally efficient features including, among others, sparse data support, lazy operations, and a PyTorch interface.\n\n- Install via `pip install anndata` or `conda install anndata -c conda-forge`.\n- See [Scanpy's documentation](https://scanpy.readthedocs.io/) for usage related to single cell data. anndata was initially built for Scanpy.\n\n[//]: # (numfocus-fiscal-sponsor-attribution)\n\nanndata is part of the scverse\u00ae project ([website](https://scverse.org), [governance](https://scverse.org/about/roles)) and is fiscally sponsored by [NumFOCUS](https://numfocus.org/).\nIf you like scverse\u00ae and want to support our mission, please consider making a tax-deductible [donation](https://numfocus.org/donate-to-scverse) to help the project pay for developer time, professional services, travel, workshops, and a variety of other needs.\n\nPublic API\n\nOur public API is documented in the [API section][] of these docs.\nWe cannot guarantee the stability of our internal APIs, whether it's the location of a function, its arguments, or something else.\nIn other words, we do not officially support (or encourage users to do) something like `from anndata._core import AnnData` as `_core` is both not documented and contains a [leading underscore][].\nHowever, we are aware that [many users do use these internal APIs][] and thus encourage them to [open an issue][] or migrate to the public API.\nThat is, if something is missing from our public API as documented, for example a feature you wish to be exported publicly, please open an issue.\n\nCitation\n\nIf you use `anndata` in your work, please cite the `anndata` publication as follows:\n\n> **anndata: Annotated data**\n>\n> Isaac Virshup, Sergei Rybakov, Fabian J. Theis, Philipp Angerer, F. Alexander Wolf\n>\n> _JOSS_ 2024 Sep 16. doi: [10.21105/joss.04371](https://doi.org/10.21105/joss.04371).\n\nYou can cite the scverse publication as follows:\n\n> **The scverse project provides a computational ecosystem for single-cell omics data analysis**\n>\n> Isaac Virshup, Danila Bredikhin, Lukas Heumos, Giovanni Palla, Gregor Sturm, Adam Gayoso, Ilia Kats, Mikaela Koutrouli, Scverse Community, Bonnie Berger, Dana Pe\u2019er, Aviv Regev, Sarah A. Teichmann, Francesca Finotello, F. Alexander Wolf, Nir Yosef, Oliver Stegle & Fabian J. Theis\n>\n> _Nat Biotechnol._ 2023 Apr 10. doi: [10.1038/s41587-023-01733-8](https://doi.org/10.1038/s41587-023-01733-8)."}, {"name": "anndata", "tags": ["math", "visualization", "web"], "summary": "Annotated data.", "text": "This library is used to efficiently handle annotated data matrices in memory and on disk, offering features such as sparse data support and a PyTorch interface. This enables developers to perform computationally efficient operations with large datasets, particularly in the context of single-cell data analysis."}, {"name": "arch", "tags": ["dev", "math"], "summary": "ARCH for Python", "text": "arch\n\n(https://github.com/bashtage/arch)\n\nAutoregressive Conditional Heteroskedasticity (ARCH) and other tools for\nfinancial econometrics, written in Python (with Cython and/or Numba used\nto improve performance)\n\nMetric\n:-------------------------\n\n(https://anaconda.org/conda-forge/arch-py)\n**Continuous Integration**\n**Coverage**\n\n**Citation**\n**Documentation**\n\nModule Contents\n\n- [Univariate ARCH Models](#volatility)\n- [Unit Root Tests](#unit-root)\n- [Cointegration Testing and Analysis](#cointegration)\n- [Bootstrapping](#bootstrap)\n- [Multiple Comparison Tests](#multiple-comparison)\n- [Long-run Covariance Estimation](#long-run-covariance)\n\nPython 3\n\n`arch` is Python 3 only. Version 4.8 is the final version that supported Python 2.7.\n\nDocumentation\n\nDocumentation from the main branch is hosted on\n[my github pages](https://bashtage.github.io/arch/).\n\nReleased documentation is hosted on\n[read the docs](https://arch.readthedocs.org/en/latest/).\n\nMore about ARCH\n\nMore information about ARCH and related models is available in the notes and\nresearch available at [Kevin Sheppard's site](https://www.kevinsheppard.com).\n\nContributing\n\nContributions are welcome. There are opportunities at many levels to contribute:\n\n- Implement new volatility process, e.g., FIGARCH\n- Improve docstrings where unclear or with typos\n- Provide examples, preferably in the form of IPython notebooks\n\nExamples\n\nVolatility Modeling\n\n- Mean models\n  - Constant mean\n  - Heterogeneous Autoregression (HAR)\n  - Autoregression (AR)\n  - Zero mean\n  - Models with and without exogenous regressors\n- Volatility models\n  - ARCH\n  - GARCH\n  - TARCH\n  - EGARCH\n  - EWMA/RiskMetrics\n- Distributions\n  - Normal\n  - Student's T\n  - Generalized Error Distribution\n\nSee the [univariate volatility example notebook](https://bashtage.github.io/arch/univariate/univariate_volatility_modeling.html) for a more complete overview.\n\nUnit Root Tests\n\n- Augmented Dickey-Fuller\n- Dickey-Fuller GLS\n- Phillips-Perron\n- KPSS\n- Zivot-Andrews\n- Variance Ratio tests\n\nSee the [unit root testing example notebook](https://bashtage.github.io/arch/unitroot/unitroot_examples.html)\nfor examples of testing series for unit roots.\n\nCointegration Testing and Analysis\n\n- Tests\n  - Engle-Granger Test\n  - Phillips-Ouliaris Test\n- Cointegration Vector Estimation\n  - Canonical Cointegrating Regression\n  - Dynamic OLS\n  - Fully Modified OLS\n\nSee the [cointegration testing example notebook](https://bashtage.github.io/arch/unitroot/unitroot_cointegration_examples.html)\nfor examples of testing series for cointegration.\n\nBootstrap\n\n- Bootstraps\n  - IID Bootstrap\n  - Stationary Bootstrap\n  - Circular Block Bootstrap\n  - Moving Block Bootstrap\n- Methods\n  - Confidence interval construction\n  - Covariance estimation\n  - Apply method to estimate model across bootstraps\n  - Generic Bootstrap iterator\n\nSee the [bootstrap example notebook](https://bashtage.github.io/arch/bootstrap/bootstrap_examples.html)\nfor examples of bootstrapping the Sharpe ratio and a Probit model from statsmodels.\n\nMultiple Comparison Procedures\n\n- Test of Superior Predictive Ability (SPA), also known as the Reality\n- Stepwise (StepM)\n- Model Confidence Set (MCS)\n\nSee the [multiple comparison example notebook](https://bashtage.github.io/arch/multiple-comparison/multiple-comparison_examples.html)\nfor examples of the multiple comparison procedures.\n\nLong-run Covariance Estimation\n\nKernel-based estimators of long-run covariance including the\nBartlett kernel which is known as Newey-West in econometrics.\nAutomatic bandwidth selection is available for all of the\ncovariance estimators.\n\nRequirements\n\nThese requirements reflect the testing environment. It is possible\nthat arch will work with older versions.\n\n- Python (3.9+)\n- NumPy (1.19+)\n- SciPy (1.5+)\n- Pandas (1.1+)\n- statsmodels (0.12+)\n- matplotlib (3+), optional\n\nOptional Requirements\n\n- Numba (0.49+) will be used if available **and** when installed without building the binary modules. In order to ensure that these are not built, you must set the environment variable `ARCH_NO_BINARY=1` and install without the wheel.\n\nor if using Powershell on windows\n\n- jupyter and notebook are required to run the notebooks\n\nInstalling\n\nStandard installation with a compiler requires Cython. If you do not\nhave a compiler installed, the `arch` should still install. You will\nsee a warning but this can be ignored. If you don't have a compiler,\n`numba` is strongly recommended.\n\npip\n\nReleases are available PyPI and can be installed with `pip`.\n\nYou can alternatively install the latest version from GitHub\n\nSetting the environment variable `ARCH_NO_BINARY=1` can be used to\ndisable compilation of the extensions.\n\nAnaconda\n\n`conda` users can install from conda-forge,\n\n**Note**: The conda-forge name is `arch-py`.\n\nWindows\n\nBuilding extension using the community edition of Visual Studio is\nsimple when using Python 3.8 or later. Building is not necessary when numba\nis installed since just-in-time compiled code (numba) runs as fast as\nahead-of-time compiled extensions.\n\nDeveloping\n\nThe development requirements are:\n\n- Cython (0.29+, if not using ARCH_NO_BINARY=1, supports 3.0.0b2+)\n- pytest (For tests)\n- sphinx (to build docs)\n- sphinx-immaterial (to build docs)\n- jupyter, notebook and nbsphinx (to build docs)\n\nInstallation Notes\n\n1. If Cython is not installed, the package will be installed\n2. Setup does not verify these requirements. Please ensure these are"}, {"name": "arch", "tags": ["dev", "math"], "summary": "ARCH for Python", "text": "This library, arch, is used to implement and analyze financial econometric models for volatility, unit roots, cointegration, and other related metrics, providing a suite of tools for researchers and analysts. Developers can leverage this library to build applications that perform statistical modeling and analysis of time series data in finance."}, {"name": "arviz", "tags": ["math", "visualization"], "summary": "Exploratory analysis of Bayesian models", "text": "ArviZ in other languages\nArviZ also has a Julia wrapper available [ArviZ.jl](https://julia.arviz.org/).\n\nDocumentation\n\nThe ArviZ documentation can be found in the [official docs](https://python.arviz.org/en/latest/index.html).\nFirst time users may find the [quickstart](https://python.arviz.org/en/latest/getting_started/Introduction.html)\nto be helpful. Additional guidance can be found in the\n[user guide](https://python.arviz.org/en/latest/user_guide/index.html).\n\nInstallation\n\nStable\nArviZ is available for installation from [PyPI](https://pypi.org/project/arviz/).\nThe latest stable version can be installed using pip:\n\nArviZ is also available through [conda-forge](https://anaconda.org/conda-forge/arviz).\n\nDevelopment\nThe latest development version can be installed from the main branch using pip:\n\nAnother option is to clone the repository and install using git and setuptools:\n\n-------------------------------------------------------------------------------\n\n[Gallery](https://python.arviz.org/en/latest/examples/index.html)\n\n  \n  \n  \n  \n  \n\n  \n  \n  \n  \n  \n\n  \n  \n  \n  \n  \n\n  \n  \n  \n  \n  \n\n  \n  \n  \n  \n  \n\n  \n  \n  \n  \n  \n\n  \n  \n  \n  \n  \n\n  \n  \n  \n  \n  \n\n  \n  \n  \n  \n  \n\n  \n  \n  \n  \n  \n\n  \n  \n  \n  \n  \n\n  \n  \n  \n  \n  \n\n  And more...\n\nDependencies\n\nArviZ is tested on Python 3.10, 3.11 and 3.12, and depends on NumPy, SciPy, xarray, and Matplotlib.\n\nCitation\n\nIf you use ArviZ and want to cite it please use (https://doi.org/10.21105/joss.01143)\n\nHere is the citation in BibTeX format\n\nContributions\nArviZ is a community project and welcomes contributions.\nAdditional information can be found in the [Contributing Readme](https://github.com/arviz-devs/arviz/blob/main/CONTRIBUTING.md)\n\nCode of Conduct\nArviZ wishes to maintain a positive community. Additional details\ncan be found in the [Code of Conduct](https://github.com/arviz-devs/arviz/blob/main/CODE_OF_CONDUCT.md)\n\nDonations\nArviZ is a non-profit project under NumFOCUS umbrella. If you want to support ArviZ financially, you can donate [here](https://numfocus.org/donate-to-arviz).\n\nSponsors\n(https://numfocus.org)"}, {"name": "arviz", "tags": ["math", "visualization"], "summary": "Exploratory analysis of Bayesian models", "text": "This library is used to perform exploratory analysis of Bayesian models, providing tools for visualizing and diagnosing their behavior. It enables developers to inspect and understand the properties of complex Bayesian models in a more intuitive and interactive way."}, {"name": "assemblyai", "tags": ["math", "ml", "web"], "summary": "AssemblyAI Python SDK", "text": "AssemblyAI's Python SDK\n\n> _Build with AI models that can transcribe and understand audio_\n\nWith a single API call, get access to AI models built on the latest AI breakthroughs to transcribe and understand audio and speech data securely at large scale.\n\nOverview\n\n- [AssemblyAI's Python SDK](#assemblyais-python-sdk)\n- [Overview](#overview)\n- [Documentation](#documentation)\n- [Quick Start](#quick-start)\n  - [Installation](#installation)\n  - [Examples](#examples)\n  - [Playgrounds](#playgrounds)\n- [Advanced](#advanced)\n  - [How the SDK handles Default Configurations](#how-the-sdk-handles-default-configurations)\n  - [Synchronous vs Asynchronous](#synchronous-vs-asynchronous)\n  - [Polling Intervals](#polling-intervals)\n  - [Retrieving Existing Transcripts](#retrieving-existing-transcripts)\n\nDocumentation\n\nVisit our [AssemblyAI API Documentation](https://www.assemblyai.com/docs) to get an overview of our models!\n\nQuick Start\n\nInstallation\n\nExamples\n\nBefore starting, you need to set the API key. If you don't have one yet, [**sign up for one**](https://www.assemblyai.com/dashboard/signup)!\n\n---\n\n**Core Examples**\n\nTranscribe a local audio file\n\nTranscribe an URL\n\nTranscribe binary data\n\nExport subtitles of an audio file\n\nList all sentences and paragraphs\n\nSearch for words in a transcript\n\nAdd custom spellings on a transcript\n\nUpload a file\n\nDelete a transcript\n\nList transcripts\n\nThis returns a page of transcripts you created.\n\nYou can apply filter parameters:\n\nYou can also paginate over all pages by using the helper property `before_id_of_prev_url`.\n\nThe `prev_url` always points to a page with older transcripts. If you extract the `before_id`\nof the `prev_url` query parameters, you can paginate over all pages from newest to oldest.\n\n---\n\n**LeMUR Examples**\n\nUse LeMUR to summarize an audio file\n\nOr use the specialized Summarization endpoint that requires no prompt engineering and facilitates more deterministic and structured outputs:\n\nUse LeMUR to ask questions about your audio data\n\nOr use the specialized Q&A endpoint that requires no prompt engineering and facilitates more deterministic and structured outputs:\n\nUse LeMUR with customized input text\n\nApply LeMUR to multiple transcripts\n\nDelete data previously sent to LeMUR\n\n---\n\n**Audio Intelligence Examples**\n\nPII Redact a transcript\n\nTo request a copy of the original audio file with the redacted information \"beeped\" out, set `redact_pii_audio=True` in the config.\nOnce the `Transcript` object is returned, you can access the URL of the redacted audio file with `get_redacted_audio_url`, or save the redacted audio directly to disk with `save_redacted_audio`.\n\n[Read more about PII redaction here.](https://www.assemblyai.com/docs/Models/pii_redaction)\n\nSummarize the content of a transcript over time\n\n[Read more about auto chapters here.](https://www.assemblyai.com/docs/Models/auto_chapters)\n\nSummarize the content of a transcript\n\nBy default, the summarization model will be `informative` and the summarization type will be `bullets`. [Read more about summarization models and types here](https://www.assemblyai.com/docs/Models/summarization#types-and-models).\n\nTo change the model and/or type, pass additional parameters to the `TranscriptionConfig`:\n\nDetect sensitive content in a transcript\n\n[Read more about the content safety categories.](https://www.assemblyai.com/docs/Models/content_moderation#all-labels-supported-by-the-model)\n\nBy default, the content safety model will only include labels with a confidence greater than 0.5 (50%). To change this, pass `content_safety_confidence` (as an integer percentage between 25 and 100, inclusive) to the `TranscriptionConfig`:\n\nAnalyze the sentiment of sentences in a transcript\n\nIf `speaker_labels` is also enabled, then each sentiment analysis result will also include a `speaker` field.\n\n[Read more about sentiment analysis here.](https://www.assemblyai.com/docs/Models/sentiment_analysis)\n\nIdentify entities in a transcript\n\n[Read more about entity detection here.](https://www.assemblyai.com/docs/Models/entity_detection)"}, {"name": "assemblyai", "tags": ["math", "ml", "web"], "summary": "AssemblyAI Python SDK", "text": "Detect topics in a transcript (IAB Classification)\n\n[Read more about IAB classification here.](https://www.assemblyai.com/docs/Models/iab_classification)\n\nIdentify important words and phrases in a transcript\n\n[Read more about auto highlights here.](https://www.assemblyai.com/docs/Models/key_phrases)\n\n---\n\n**Streaming Examples**\n\n[Read more about our streaming service.](https://www.assemblyai.com/docs/getting-started/transcribe-streaming-audio)\n\nStream your microphone in real-time\n\nTranscribe a local audio file in real-time\n\n---\n\n**Change the default settings**\n\nYou'll find the `Settings` class with all default values in [types.py](./assemblyai/types.py).\n\nChange the default timeout and polling interval\n\n---\n\nPlayground\n\nVisit our Playground to try our all of our Speech AI models and LeMUR for free:\n\n- [Playground](https://www.assemblyai.com/playground)\n\nAdvanced\n\nHow the SDK handles Default Configurations\n\nDefining Defaults\n\nWhen no `TranscriptionConfig` is being passed to the `Transcriber` or its methods, it will use a default instance of a `TranscriptionConfig`.\n\nIf you would like to re-use the same `TranscriptionConfig` for all your transcriptions,\nyou can set it on the `Transcriber` directly:\n\nOverriding Defaults\n\nYou can override the default configuration later via the `.config` property of the `Transcriber`:\n\nIn case you want to override the `Transcriber`'s configuration for a specific operation with a different one, you can do so via the `config` parameter of a `.transcribe*(...)` method:\n\nSynchronous vs Asynchronous\n\nCurrently, the SDK provides two ways to transcribe audio files.\n\nThe synchronous approach halts the application's flow until the transcription has been completed.\n\nThe asynchronous approach allows the application to continue running while the transcription is being processed. The caller receives a [`concurrent.futures.Future`](https://docs.python.org/3/library/concurrent.futures.html) object which can be used to check the status of the transcription at a later time.\n\nYou can identify those two approaches by the `_async` suffix in the `Transcriber`'s method name (e.g. `transcribe` vs `transcribe_async`).\n\nGetting the HTTP status code\n\nThere are two ways of accessing the HTTP status code:\n\n- All custom AssemblyAI Error classes have a `status_code` attribute.\n- The latest HTTP response is stored in `aai.Client.get_default().latest_response` after every API call. This approach works also if no Exception is thrown.\n\nPolling Intervals\n\nBy default we poll the `Transcript`'s status each `3s`. In case you would like to adjust that interval:\n\nRetrieving Existing Transcripts\n\nRetrieving a Single Transcript\n\nIf you previously created a transcript, you can use its ID to retrieve it later.\n\nRetrieving Multiple Transcripts as a Group\n\nYou can also retrieve multiple existing transcripts and combine them into a single `TranscriptGroup` object. This allows you to perform operations on the transcript group as a single unit, such as querying the combined transcripts with LeMUR.\n\nRetrieving Transcripts Asynchronously\n\nBoth `Transcript.get_by_id` and `TranscriptGroup.get_by_ids` have asynchronous counterparts, `Transcript.get_by_id_async` and `TranscriptGroup.get_by_ids_async`, respectively. These functions immediately return a `Future` object, rather than blocking until the transcript(s) are retrieved.\n\nSee the above section on [Synchronous vs Asynchronous](#synchronous-vs-asynchronous) for more information."}, {"name": "assemblyai", "tags": ["math", "ml", "web"], "summary": "AssemblyAI Python SDK", "text": "This library is used to securely transcribe and understand audio and speech data at large scale using AI models. With a single API call, developers can access cutting-edge AI-powered transcription capabilities."}, {"name": "astropy", "tags": ["math", "web"], "summary": "Astronomy and astrophysics core library", "text": "Astropy Logo\n\n----\n\nActions Status\n\n----\n\nThe Astropy Project is a community effort to develop a\nsingle core package for astronomy in Python and foster interoperability between\npackages used in the field. This repository contains the core library.\n\n* `Website `_\n* `Documentation `_\n* `Slack `_\n* `Open Astronomy Discourse `_\n* `Astropy users mailing list `_\n* `Astropy developers mailing list `_\n\nInstallation\n============\n\nTo install `astropy` from PyPI, use:\n\n.. code-block:: bash\n\nFor more detailed instructions, see the `install guide\n`_ in the docs.\n\nContributing\n============\n\nUser Stats\n\nThe Astropy Project is made both by and for its users, so we welcome and\nencourage contributions of many kinds. Our goal is to keep this a positive,\ninclusive, successful, and growing community that abides by the\n`Astropy Community Code of Conduct\n`_.\n\nFor guidance on contributing to or submitting feedback for the Astropy Project,\nsee the `contributions page `_.\nFor contributing code specifically, the developer docs have a\n`guide `_ with a quickstart.\nThere's also a `summary of contribution guidelines `_.\n\nDeveloping with Codespaces\n==========================\n\nGitHub Codespaces is a cloud development environment using Visual Studio Code\nin your browser. This is a convenient way to start developing Astropy, using\nour `dev container `_ configured\nwith the required packages. For help, see the `GitHub Codespaces\ndocs `_.\n\nCodespaces\n\nAcknowledging and Citing\n========================\nSee the `acknowledgement and citation guide\n`_ and the `CITATION\n`_ file.\n\nSupporting the Project\n======================\n\nNumFOCUS\n\nThe Astropy Project is sponsored by NumFOCUS, a 501(c)(3) nonprofit in the\nUnited States. You can donate to the project by using the link above, and this\ndonation will support our mission to promote sustainable, high-level code base\nfor the astronomy community, open code development, educational materials, and\nreproducible scientific research.\n\nLicense\n=======\n\nAstropy is licensed under a 3-clause BSD style license - see the\n`LICENSE.rst `_ file.\n\n.. |Astropy Logo| image:: https://github.com/astropy/repo_stats/blob/main/dashboard_template/astropy_banner_gray.svg\n\n.. |User Stats| image:: https://github.com/astropy/repo_stats/blob/cache/cache/astropy_user_stats_light.png\n\n.. |CircleCI Status| image::  https://img.shields.io/circleci/build/github/astropy/astropy/main?logo=circleci&label=CircleCI\n\n.. |PyPI Status| image:: https://img.shields.io/pypi/v/astropy.svg\n\n.. |Supported Python Versions| image:: https://img.shields.io/pypi/pyversions/astropy\n\n.. |Documentation Status| image:: https://img.shields.io/readthedocs/astropy/latest.svg?logo=read%20the%20docs&logoColor=white&label=Docs&version=stable"}, {"name": "astropy", "tags": ["math", "web"], "summary": "Astronomy and astrophysics core library", "text": "This library is used to provide a single core package for astronomy in Python, enabling developers to perform various tasks such as data analysis, modeling, and visualization for astronomy and astrophysics applications. With astropy, developers can write efficient and interoperable code that integrates with other packages used in the field of astronomy."}, {"name": "autograd", "tags": ["math", "web"], "summary": "Efficiently computes derivatives of NumPy code.", "text": "Autograd  [\n\nAutograd can automatically differentiate native Python and Numpy code. It can\nhandle a large subset of Python's features, including loops, ifs, recursion and\nclosures, and it can even take derivatives of derivatives of derivatives. It\nsupports reverse-mode differentiation (a.k.a. backpropagation), which means it\ncan efficiently take gradients of scalar-valued functions with respect to\narray-valued arguments, as well as forward-mode differentiation, and the two can\nbe composed arbitrarily. The main intended application of Autograd is\ngradient-based optimization. For more information, check out the\n[tutorial](docs/tutorial.md) and the [examples directory](examples/).\n\nExample use:\n\nWe can continue to differentiate as many times as we like, and use numpy's\nvectorization of scalar-valued functions across many different input values:\n\nSee the [tanh example file](examples/tanh.py) for the code.\n\nDocumentation\n\nYou can find a tutorial [here.](docs/tutorial.md)\n\nEnd-to-end examples\n\n* [Simple neural net](examples/neural_net.py)\n* [Convolutional neural net](examples/convnet.py)\n* [Recurrent neural net](examples/rnn.py)\n* [LSTM](examples/lstm.py)\n* [Neural Turing Machine](https://github.com/DoctorTeeth/diffmem/blob/512aadeefd6dbafc1bdd253a64b6be192a435dc3/ntm/ntm.py)\n* [Backpropagating through a fluid simulation](examples/fluidsim/fluidsim.py)\n\n* [Variational inference in Bayesian neural network](examples/bayesian_neural_net.py)\n* [Gaussian process regression](examples/gaussian_process.py)\n* [Sampyl, a pure Python MCMC package with HMC and NUTS](https://github.com/mcleonard/sampyl)\n\nHow to install\n\nInstall Autograd using Pip:\n\nSome features require SciPy, which you can install separately or as an\noptional dependency along with Autograd:\n\nAuthors and maintainers\n\nAutograd was written by [Dougal Maclaurin](https://dougalmaclaurin.com),\n[David Duvenaud](https://www.cs.toronto.edu/~duvenaud/),\n[Matt Johnson](http://people.csail.mit.edu/mattjj/),\n[Jamie Townsend](https://github.com/j-towns)\nand many other contributors. The package is currently being maintained by\n[Agriya Khetarpal](https://github.com/agriyakhetarpal),\n[Fabian Joswig](https://github.com/fjosw) and\n[Jamie Townsend](https://github.com/j-towns).\nPlease feel free to submit any bugs or\nfeature requests. We'd also love to hear about your experiences with Autograd\nin general. Drop us an email!\n\nWe want to thank Jasper Snoek and the rest of the HIPS group (led by Prof. Ryan\nP. Adams) for helpful contributions and advice; Barak Pearlmutter for\nfoundational work on automatic differentiation and for guidance on our\nimplementation; and Analog Devices Inc. (Lyric Labs) and Samsung Advanced Institute\nof Technology for their generous support."}, {"name": "autograd", "tags": ["math", "web"], "summary": "Efficiently computes derivatives of NumPy code.", "text": "This library is used to automatically compute gradients and higher-order derivatives of NumPy code with high efficiency. It supports both reverse-mode and forward-mode differentiation for gradient-based optimization tasks."}, {"name": "awkward", "tags": ["math", "web"], "summary": "Manipulate JSON-like data with NumPy-like idioms.", "text": "Motivating example\n\nGiven an array of lists of objects with `x`, `y` fields (with nested lists in the `y` field),\n\nthe following slices out the `y` values, drops the first element from each inner list, and runs NumPy's `np.square` function on everything that is left:\n\nThe result is\n\nThe equivalent using only Python is\n\nThe expression using Awkward Arrays is more concise, using idioms familiar from NumPy, and it also has NumPy-like performance. For a similar problem 10 million times larger than the one above (single-threaded on a 2.2 GHz processor),\n\n   * the Awkward Array one-liner takes **1.5 seconds** to run and uses **2.1 GB** of memory,\n   * the equivalent using Python lists and dicts takes **140 seconds** to run and uses **22 GB** of memory.\n\nAwkward Array is even faster when used in [Numba](https://numba.pydata.org/)'s JIT-compiled functions.\n\nSee the [Getting started](https://awkward-array.org/doc/main/getting-started/index.html) documentation on [awkward-array.org](https://awkward-array.org) for an introduction, including a [no-install demo](https://awkward-array.org/doc/main/_static/try-it.html) you can try in your web browser.\n\nGetting help\n\n   * View the documentation on [awkward-array.org](https://awkward-array.org/).\n   * Report bugs, request features, and ask for additional documentation on [GitHub Issues](https://github.com/scikit-hep/awkward/issues).\n   * If you have a \"How do I...?\" question, start a [GitHub Discussion](https://github.com/scikit-hep/awkward/discussions) with category \"Q&A\".\n   * Alternatively, ask about it on [StackOverflow with the [awkward-array] tag](https://stackoverflow.com/questions/tagged/awkward-array). Be sure to include tags for any other libraries that you use, such as Pandas or PyTorch.\n   * To ask questions in real time, try the Gitter [Scikit-HEP/awkward-array](https://gitter.im/Scikit-HEP/awkward-array) chat room.\n\nInstallation\n\nAwkward Array can be installed from [PyPI](https://pypi.org/project/awkward) using pip:\n\nThe `awkward` package is pure Python, and it will download the `awkward-cpp` compiled components as a dependency. If there is no `awkward-cpp` binary package (wheel) for your platform and Python version, pip will attempt to compile it from source (which has additional dependencies, such as a C++ compiler).\n\nAwkward Array is also available on [conda-forge](https://conda-forge.org/docs/user/introduction.html#how-can-i-install-packages-from-conda-forge):"}, {"name": "awkward", "tags": ["math", "web"], "summary": "Manipulate JSON-like data with NumPy-like idioms.", "text": "This library is used to manipulate JSON-like data with NumPy-like idioms, allowing for concise and performance-optimized code. With Awkward Arrays, developers can efficiently process large datasets with a syntax familiar from NumPy."}, {"name": "azure-cognitiveservices-speech", "tags": ["math", "web"], "summary": "Microsoft Cognitive Services Speech SDK for Python", "text": "For an introduction to this package, have a look at `the quickstart\narticle `_.\n\nFor information about the Speech Service, please refer to `its\nwebsite `_.\n\nDocumentation\n-------------\n\nAPI documentation for this package can be found `here `_.\n\nLicense information\n-------------------\n\n- `Microsoft Software License Terms for the Speech SDK `_\n- `Third party notices `_"}, {"name": "azure-cognitiveservices-speech", "tags": ["math", "web"], "summary": "Microsoft Cognitive Services Speech SDK for Python", "text": "This library is used to develop Python applications that utilize the Azure Cognitive Services Speech capabilities, enabling developers to build speech-to-text and text-to-speech functionality into their projects. With this library, developers can easily integrate speech recognition and synthesis into their applications."}, {"name": "azureml-dataprep-rslex", "tags": ["math"], "summary": "Azure ML Data Preparation RustLex", "text": "Azure Machine Learning Data Prep RsLex\n\nAzure Machine Learning Data Prep RsLex is a Rust implementation of Data Prep's capabilities to load, transform, and write data for machine learning workflows.\nYou can interact with RsLex via the Data Prep SDK, `azureml-dataprep`, by calling `use_rust_execution(True)` before using the SDK.\n\nInstall the SDK\n\nTo install Azure Machine Learning Data Prep RsLex, use the following command:\n\nNOTE: This package is not intended for direct installation or usage. `azureml-dataprep` depends on this package."}, {"name": "azureml-dataprep-rslex", "tags": ["math"], "summary": "Azure ML Data Preparation RustLex", "text": "This library is used to load and transform data in machine learning workflows using a Rust implementation of Data Prep's capabilities. Developers can leverage it via the Data Prep SDK, `azureml-dataprep`, by enabling Rust execution."}, {"name": "azureml-mlflow", "tags": ["math", "ml", "web"], "summary": "Contains the integration code of AzureML with Mlflow.", "text": "Microsoft Azure Machine Learning Tracking server plugin for Python\n==================================================================\n\nThe azureml-mlflow package contains the integration code of AzureML with MLflow.\nMLflow (https://mlflow.org/) is an open-source platform for tracking machine learning experiments and managing models.\nYou can use MLflow logging APIs with Azure Machine Learning so that metrics and artifacts are logged to your Azure\nmachine learning workspace.\n\nUsage\n-----\n\nWithin an `AzureML Workspace `_,\nadd the code below to use MLflow.\n\n.. code-block:: python\n\n   import mlflow\n   from azureml.core import Workspace\n\n   workspace = Workspace.from_config()\n\n   mlflow.set_tracking_uri(workspace.get_mlflow_tracking_uri())\n\nMore examples can be found at https://aka.ms/azureml-mlflow-examples."}, {"name": "azureml-mlflow", "tags": ["math", "ml", "web"], "summary": "Contains the integration code of AzureML with Mlflow.", "text": "This library is used to integrate Azure Machine Learning with MLflow, allowing developers to log metrics and artifacts directly to their Azure machine learning workspace. With this integration, developers can seamlessly track machine learning experiments and manage models using MLflow's APIs within the AzureML ecosystem."}, {"name": "azureml-pipeline", "tags": ["math", "ml"], "summary": "Used to build, optimize, and manage their machine learning workflows.", "text": "Machine learning (ML) pipelines are used by data scientists to build, optimize, and manage their machine learning workflows. A typical pipeline involves a sequence of steps that cover the following areas:\n\n * Data preparation, such as normalizations and transformations\n * Model training, such as hyper parameter tuning and validation\n * Model deployment and evaluation\n\nThe Azure Machine Learning SDK for Python can be used to create ML pipelines as well as to submit and track individual pipeline runs.\n\nModule and ModuleVersion classes are added to manage reusable compute units in pipelines."}, {"name": "azureml-pipeline", "tags": ["math", "ml"], "summary": "Used to build, optimize, and manage their machine learning workflows.", "text": "This library is used to build, optimize, and manage machine learning workflows through the creation of customizable pipelines. It enables data scientists to streamline their workflow by automating tasks such as data preparation, model training, deployment, and evaluation."}, {"name": "azureml-sdk", "tags": ["math", "ml", "web"], "summary": "Used to build and run machine learning workflows upon the     Azure Machine Learning service.", "text": "What is the Azure Machine Learning SDK for Python?\n==================================================\n\nThe Azure Machine Learning SDK for Python is used by data scientists and AI developers \nto build and run machine learning workflows upon the Azure Machine Learning service. \nYou can interact with the service in any Python environment, including Jupyter Notebooks \nor your favorite Python IDE.\n\nUse this SDK to quickly build, train, and deploy your machine learning and deep learning \nmodels for various domains.\n\nAzure Machine Learning service stores free form text that the customer provides (e.g. names\nfor workspaces, resource groups, experiments, files, and images) or experiment parameters \nin the United States.\n\nGetting Started: https://docs.microsoft.com/en-us/python/api/overview/azure/ml/intro\n\nExamples: https://github.com/Azure/MachineLearningNotebooks\n\nRelease updates: https://docs.microsoft.com/en-us/azure/machine-learning/service/azure-machine-learning-release-notes"}, {"name": "azureml-sdk", "tags": ["math", "ml", "web"], "summary": "Used to build and run machine learning workflows upon the     Azure Machine Learning service.", "text": "This library is used to build and run machine learning workflows upon the Azure Machine Learning service, allowing for quick model development and deployment across various domains. It enables data scientists and AI developers to interact with the service from any Python environment, including Jupyter Notebooks or their preferred IDE."}, {"name": "azureml-train", "tags": ["math", "ml"], "summary": null, "text": "##########################################################################################\nMicrosoft Azure Machine Learning - azureml-train package\n##########################################################################################\n\nThe azureml-train package has been deprecated and might not receive future updates and removed from the distribution altogether. Please use azureml-train-core instead."}, {"name": "azureml-train", "tags": ["math", "ml"], "summary": null, "text": "This library is used to support machine learning model training in Microsoft Azure Machine Learning, but its functionality is now maintained by a newer package called azureml-train-core. This library should no longer be relied upon for new projects due to deprecation and potential removal from distribution."}, {"name": "beautifulsoup4", "tags": ["dev"], "summary": "Screen-scraping library", "text": "Quick start\n\nTo go beyond the basics, [comprehensive documentation is available](https://www.crummy.com/software/BeautifulSoup/bs4/doc/).\n\nLinks\n\n* [Homepage](https://www.crummy.com/software/BeautifulSoup/bs4/)\n* [Documentation](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)\n* [Discussion group](https://groups.google.com/group/beautifulsoup/)\n* [Development](https://code.launchpad.net/beautifulsoup/)\n* [Bug tracker](https://bugs.launchpad.net/beautifulsoup/)\n* [Complete changelog](https://git.launchpad.net/beautifulsoup/tree/CHANGELOG)\n\nNote on Python 2 sunsetting\n\nBeautiful Soup's support for Python 2 was discontinued on December 31,\n2020: one year after the sunset date for Python 2 itself. From this\npoint onward, new Beautiful Soup development will exclusively target\nPython 3. The final release of Beautiful Soup 4 to support Python 2\nwas 4.9.3.\n\nSupporting the project\n\nIf you use Beautiful Soup as part of your professional work, please consider a\n[Tidelift subscription](https://tidelift.com/subscription/pkg/pypi-beautifulsoup4?utm_source=pypi-beautifulsoup4&utm_medium=referral&utm_campaign=readme).\nThis will support many of the free software projects your organization\ndepends on, not just Beautiful Soup.\n\nIf you use Beautiful Soup for personal projects, the best way to say\nthank you is to read\n[Tool Safety](https://www.crummy.com/software/BeautifulSoup/zine/), a zine I\nwrote about what Beautiful Soup has taught me about software\ndevelopment.\n\nBuilding the documentation\n\nThe bs4/doc/ directory contains full documentation in Sphinx\nformat. Run `make html` in that directory to create HTML\ndocumentation.\n\nRunning the unit tests\n\nBeautiful Soup supports unit test discovery using Pytest:"}, {"name": "beautifulsoup4", "tags": ["dev"], "summary": "Screen-scraping library", "text": "This library is used to parse HTML and XML documents with ease, allowing developers to extract data from web pages and manipulate their structure. With BeautifulSoup4, developers can quickly and efficiently scrape websites for specific information without requiring in-depth knowledge of HTML parsing."}, {"name": "bedrock-agentcore", "tags": ["math", "ml", "web"], "summary": "An SDK for using Bedrock AgentCore", "text": "Overview\nAmazon Bedrock AgentCore enables you to deploy and operate highly effective agents securely, at scale using any framework and model. With Amazon Bedrock AgentCore, developers can accelerate AI agents into production with the scale, reliability, and security, critical to real-world deployment. AgentCore provides tools and capabilities to make agents more effective and capable, purpose-built infrastructure to securely scale agents, and controls to operate trustworthy agents. Amazon Bedrock AgentCore services are composable and work with popular open-source frameworks and any model, so you don\u2019t have to choose between open-source flexibility and enterprise-grade security and reliability.\n\nFrom Local Development to Bedrock AgentCore\n\n**What you get with Bedrock AgentCore:**\n-  **Keep your agent logic** - Works with Strands, LangGraph, CrewAI, Autogen, custom frameworks\n-  **Zero infrastructure management** - No servers, containers, or scaling concerns\n-  **Enterprise-grade platform** - Built-in auth, memory, observability, security\n-  **Production-ready deployment** - Reliable, scalable, compliant hosting\n\nAmazon Bedrock AgentCore services\n\n\ufe0f Deployment\n\n**Quick Start:** Use the [Bedrock AgentCore Starter Toolkit](https://github.com/aws/bedrock-agentcore-starter-toolkit) for rapid prototyping.\n\n**Production:** [AWS CDK](https://docs.aws.amazon.com/cdk/api/v2/docs/aws-cdk-lib.aws_bedrockagentcore-readme.html).\n\nLicense & Contributing\n\n- **License:** Apache 2.0 - see [LICENSE.txt](LICENSE.txt)\n- **Contributing:** See [CONTRIBUTING.md](CONTRIBUTING.md)\n- **Security:** Report vulnerabilities via [SECURITY.md](SECURITY.md)"}, {"name": "bedrock-agentcore", "tags": ["math", "ml", "web"], "summary": "An SDK for using Bedrock AgentCore", "text": "This library is used to accelerate the deployment of AI agents into production with scalable, secure, and reliable infrastructure. With this library, developers can deploy highly effective agents at scale using any framework and model while ensuring enterprise-grade security and reliability."}, {"name": "biopython", "tags": ["cli", "dev", "math", "web"], "summary": "Freely available tools for computational molecular biology.", "text": ".. image:: https://img.shields.io/pypi/v/biopython.svg?logo=pypi\n   :alt: Biopython on the Python Package Index (PyPI)\n   :target: https://pypi.python.org/pypi/biopython\n.. image:: https://img.shields.io/conda/vn/conda-forge/biopython.svg?logo=conda-forge\n   :alt: Biopython on the Conda package conda-forge channel\n   :target: https://anaconda.org/conda-forge/biopython\n\n:target: https://results.pre-commit.ci/latest/github/biopython/biopython/master\n   :alt: pre-commit.ci status\n.. image:: https://img.shields.io/circleci/build/github/biopython/biopython.svg?logo=circleci\n   :alt: Linux testing with CircleCI\n   :target: https://app.circleci.com/pipelines/github/biopython/biopython\n.. image:: https://img.shields.io/appveyor/ci/biopython/biopython/master.svg?logo=appveyor\n   :alt: Windows testing with AppVeyor\n   :target: https://ci.appveyor.com/project/biopython/biopython/history\n.. image:: https://img.shields.io/github/actions/workflow/status/biopython/biopython/ci.yml?logo=github-actions\n   :alt: GitHub workflow status\n   :target: https://github.com/biopython/biopython/actions\n.. image:: https://img.shields.io/codecov/c/github/biopython/biopython/master.svg?logo=codecov\n   :alt: Test coverage on CodeCov\n   :target: https://codecov.io/github/biopython/biopython/\n\n:alt: Research software impact on Depsy\n   :target: http://depsy.org/package/python/biopython\n\n.. image:: https://github.com/biopython/biopython/raw/master/Doc/images/biopython_logo_m.png\n   :alt: The Biopython Project\n   :target: http://biopython.org\n\nBiopython README file\n=====================\n\nThe Biopython Project is an international association of developers of freely\navailable Python tools for computational molecular biology.\n\nThis README file is intended primarily for people interested in working\nwith the Biopython source code, either one of the releases from the\n\nOur user-centric documentation, `The Biopython Tutorial and Cookbook, and API\ndocumentation `_, is generated from our\nrepository using Sphinx.\n\nThe `NEWS `_\nfile summarises the changes in each release of Biopython, alongside the\n`DEPRECATED\n`_\nfile which notes API breakages.\n\nThe Biopython package is open source software made available under generous\nterms. Please see the `LICENSE\n`_ file for\nfurther details.\n\nIf you use Biopython in work contributing to a scientific publication, we ask\nthat you cite our application note (below) or one of the module specific\npublications (listed on our website):\n\nCock, P.J.A. et al. Biopython: freely available Python tools for computational\nmolecular biology and bioinformatics. Bioinformatics 2009 Jun 1; 25(11) 1422-3\n\nFor the impatient\n=================\n\nPython includes the package management system \"pip\" which should allow you to\ninstall Biopython (and its dependency NumPy if needed), upgrade or uninstall\nwith just one terminal command::\n\nSince Biopython 1.70 we have provided pre-compiled binary wheel packages on\nPyPI for Linux, macOS and Windows. This means pip install should be quick,\nand not require a compiler.\n\nAs a developer or potential contributor, you may wish to download, build and\ninstall Biopython yourself. This is described below.\n\nPython Requirements\n===================\n\nWe currently recommend using Python 3.13 from http://www.python.org\n\nBiopython is currently supported and tested on the following Python\nimplementations:\n\n- Python 3.10, 3.11, 3.12, 3.13 and 3.14 -- see http://www.python.org\n\n- PyPy3.10 v7.3.17 -- or later, see http://www.pypy.org\n\nOptional Dependencies\n=====================\n\nBiopython requires NumPy (see http://www.numpy.org) which will be installed\nautomatically if you install Biopython with pip (see below for compiling\nBiopython yourself).\n\nDepending on which parts of Biopython you plan to use, there are a number of\nother optional Python dependencies, which can be installed later if needed:\n\n- ReportLab, see http://www.reportlab.com/opensource/ (optional)\n  This package is only used in ``Bio.Graphics``, so if you do not need this\n  functionality, you will not need to install this package.\n\n- matplotlib, see http://matplotlib.org/ (optional)\n  ``Bio.Phylo`` uses this package to plot phylogenetic trees.\n\n- networkx, see https://networkx.github.io/ (optional) and\n  pygraphviz or pydot, see https://pygraphviz.github.io/ and\n  These packages are used for certain niche functions in ``Bio.Phylo``.\n\n- rdflib, see https://github.com/RDFLib/rdflib (optional)\n  This package is used in the CDAO parser under ``Bio.Phylo``."}, {"name": "biopython", "tags": ["cli", "dev", "math", "web"], "summary": "Freely available tools for computational molecular biology.", "text": "- psycopg2, see http://initd.org/psycopg/ (optional) or\n  PyGreSQL (pgdb), see http://www.pygresql.org/ (optional)\n  These packages are used by ``BioSQL`` to access a PostgreSQL database.\n\n- MySQL Connector/Python, see http://dev.mysql.com/downloads/connector/python/\n  This package is used by ``BioSQL`` to access a MySQL database, and is\n  supported on PyPy too.\n\n- mysqlclient, see https://github.com/PyMySQL/mysqlclient-python (optional)\n  This is a fork of the older MySQLdb and is used by ``BioSQL`` to access a\n  MySQL database. It is supported by PyPy.\n\nIn addition there are a number of useful third party tools you may wish to\ninstall such as standalone NCBI BLAST, EMBOSS or ClustalW.\n\nInstallation From Source\n========================\n\nWe recommend using the pre-compiled binary wheels available on PyPI using::\n\nHowever, if you need to compile Biopython yourself, the following are required\nat compile time:\n\n- Python including development header files like ``python.h``, which on Linux\n  are often not installed by default (trying looking for and installing a\n  package named ``python-dev`` or ``python-devel`` as well as the ``python``\n  package).\n\n- Appropriate C compiler for your version of Python, for example GCC on Linux,\n  MSVC on Windows. For Mac OS X, or as it is now branded, macOS, use Apple's\n  command line tools, which can be installed with the terminal command::\n\nThis will offer to install Apple's XCode development suite - you can, but it\n  is not needed and takes a lot of disk space.\n\nThen either download and decompress our source code, or fetch it using git.\nNow change directory to the Biopython source code folder and run::\n\nSubstitute ``python`` with your specific version if required, for example\n``python3``, or ``pypy3``.\n\nTo exclude tests that require an internet connection (and which may take a\nlong time), use the ``--offline`` option::\n\nIf you need to do additional configuration, e.g. changing the install\ndirectory prefix, please type ``python setup.py``.\n\nTesting\n=======\n\nBiopython includes a suite of regression tests to check if everything is\nrunning correctly. To run the tests, go to the biopython source code\ndirectory and type::\n\nIf you want to skip the online tests (which is recommended when doing repeated\ntesting), use::\n\nDo not panic if you see messages warning of skipped tests::\n\nThis most likely means that a package is not installed.  You can\nignore this if it occurs in the tests for a module that you were not\nplanning on using.  If you did want to use that module, please install\nthe required dependency and re-run the tests.\n\nSome of the tests may fail due to network issues, this is often down to\nchance or a service outage. If the problem does not go away on\nre-running the tests, you can use the ``--offline`` option.\n\nThere is more testing information in the Biopython Tutorial & Cookbook.\n\nExperimental code\n================="}, {"name": "biopython", "tags": ["cli", "dev", "math", "web"], "summary": "Freely available tools for computational molecular biology.", "text": "Biopython 1.61 introduced a new warning, ``Bio.BiopythonExperimentalWarning``,\nwhich is used to mark any experimental code included in the otherwise\nstable Biopython releases. Such 'beta' level code is ready for wider\ntesting, but still likely to change, and should only be tried by early\nadopters in order to give feedback via the biopython-dev mailing list.\n\nWe'd expect such experimental code to reach stable status within one or two\nreleases, at which point our normal policies about trying to preserve\nbackwards compatibility would apply.\n\nBugs\n====\n\nWhile we try to ship a robust package, bugs inevitably pop up.  If you are\nhaving problems that might be caused by a bug in Biopython, it is possible\nthat it has already been identified. Update to the latest release if you are\nnot using it already, and retry. If the problem persists, please search our\nbug database and our mailing lists to see if it has already been reported\n(and hopefully fixed), and if not please do report the bug. We can't fix\nproblems we don't know about ;)\n\nIssue tracker: https://github.com/biopython/biopython/issues\n\nIf you suspect the problem lies within a parser, it is likely that the data\nformat has changed and broken the parsing code.  (The text BLAST and GenBank\nformats seem to be particularly fragile.)  Thus, the parsing code in\nBiopython is sometimes updated faster than we can build Biopython releases.\nYou can get the most recent parser by pulling the relevant files (e.g. the\nones in ``Bio.SeqIO`` or ``Bio.Blast``) from our git repository. However, be\ncareful when doing this, because the code in github is not as well-tested\nas released code, and may contain new dependencies.\n\nIn any bug report, please let us know:\n\n1. Which operating system and hardware (32 bit or 64 bit) you are using\n2. Python version\n3. Biopython version (or git commit/date)\n4. Traceback that occurs (the full error message)\n\nAnd also ideally:\n\n5. Example code that breaks\n6. A data file that causes the problem\n\nContributing, Bug Reports\n=========================\n\nBiopython is run by volunteers from all over the world, with many types of\nbackgrounds. We are always looking for people interested in helping with code\ndevelopment, web-site management, documentation writing, technical\nadministration, and whatever else comes up.\n\nIf you wish to contribute, please first read `CONTRIBUTING.rst\n`_ here,\nvisit our web site http://biopython.org and join our mailing list:\n\nDistribution Structure\n======================"}, {"name": "biopython", "tags": ["cli", "dev", "math", "web"], "summary": "Freely available tools for computational molecular biology.", "text": "- ``README.rst``  -- This file.\n- ``NEWS.rst``    -- Release notes and news.\n- ``LICENSE.rst`` -- What you can do with the code.\n- ``CONTRIB.rst`` -- An (incomplete) list of people who helped Biopython in\n  one way or another.\n- ``CONTRIBUTING.rst`` -- An overview about how to contribute to Biopython.\n- ``DEPRECATED.rst`` -- Contains information about modules in Biopython that\n  were removed or no longer recommended for use, and how to update code that\n  uses those modules.\n- ``MANIFEST.in`` -- Configures which files to include in releases.\n- ``setup.py``    -- Installation file.\n- ``Bio/``        -- The main code base code.\n- ``BioSQL/``     -- Code for using Biopython with BioSQL databases.\n- ``Doc/``        -- Documentation.\n- ``Scripts/``    -- Miscellaneous, possibly useful, standalone scripts.\n- ``Tests/``      -- Regression testing code including sample data files."}, {"name": "biopython", "tags": ["cli", "dev", "math", "web"], "summary": "Freely available tools for computational molecular biology.", "text": "This library is used to provide a comprehensive set of tools for computational molecular biology, enabling developers to parse and manipulate biological data in Python. With biopython, developers can automate tasks such as file format conversion, database querying, and sequence analysis."}, {"name": "biothings-client", "tags": ["math", "web"], "summary": "Python Client for BioThings API services.", "text": ".. image:: https://img.shields.io/pypi/pyversions/biothings-client.svg\n\n.. image:: https://img.shields.io/pypi/format/biothings-client.svg\n\n.. image:: https://img.shields.io/pypi/dm/biothings_client.svg\n   :alt: PyPI Downloads\n   :target: https://pypistats.org/packages/biothings_client\n\n   :alt: Documentation Status\n\nIntro\n=====\n\n*biothings_client* is an easy-to-use Python wrapper to access any Biothings.api_\n-based backend service. Currently, the following clients are available:\n\n.. _t.biothings.io: https://t.biothings.io\n.. _Biothings.api: https://biothings.io\n.. _MyGene.Info: https://mygene.info\n.. _MyVariant.Info: https://myvariant.info\n.. _MyChem.Info: https://mychem.info\n.. _MyDisease.Info: https://mydisease.info\n.. _MyGeneset.Info: https://mygeneset.info\n\nSupport\n============\n\nOptional dependencies\n======================\n\nInstallation\n=============\n\nVersion history\n===============\n\nTutorial\n=========\n\nDocumentation\n=============\n\nUsage\n=====\n\n* Synchronous client.\n\n* Asynchronous client.\n\nContact\n========\nDrop us any feedback `@biothingsapi `_"}, {"name": "biothings-client", "tags": ["math", "web"], "summary": "Python Client for BioThings API services.", "text": "This library is used to provide a Python wrapper for accessing various Biothings API services, allowing developers to easily integrate and interact with these APIs in their applications. With biothings-client, developers can leverage a range of data sources, including MyGene.Info, MyVariant.Info, and more, to enrich their projects with biological knowledge."}, {"name": "biotite", "tags": ["math", "web"], "summary": "A comprehensive library for computational molecular biology", "text": ".. image:: https://img.shields.io/pypi/v/biotite.svg\n   :target: https://pypi.python.org/pypi/biotite\n   :alt: Biotite at PyPI\n.. image:: https://img.shields.io/pypi/pyversions/biotite.svg\n   :alt: Python version\n\n   :target: https://github.com/biotite-dev/biotite/actions/workflows/test_and_deploy.yml\n   :alt: Test status\n\n.. image:: https://www.biotite-python.org/_static/assets/general/biotite_logo_m.png\n   :alt: The Biotite Project\n\nBiotite project\n===============\n\n*Biotite* is your Swiss army knife for bioinformatics.\nWhether you want to identify homologous sequence regions in a protein family\nor you would like to find disulfide bonds in a protein structure: *Biotite*\nhas the right tool for you.\nThis package bundles popular tasks in computational molecular biology\ninto a uniform *Python* library.\nIt can handle a major part of the typical workflow\nfor sequence and biomolecular structure data:\n\n   - Searching and fetching data from biological databases\n   - Reading and writing popular sequence/structure file formats\n   - Analyzing and editing sequence/structure data\n   - Visualizing sequence/structure data\n   - Interfacing external applications for further analysis\n\n*Biotite* internally stores most of the data as *NumPy* `ndarray` objects,\nenabling\n\n   - fast C-accelerated analysis,\n   - intuitive usability through *NumPy*-like indexing syntax,\n   - extensibility through direct access of the internal *NumPy* arrays.\n\nAs a result the user can skip writing code for basic functionality (like\nfile parsers) and can focus on what their code makes unique - from\nsmall analysis scripts to entire bioinformatics software packages.\n\nIf you use *Biotite* in a scientific publication, please cite:\n\nKunzmann, P. & Hamacher, K. BMC Bioinformatics (2018) 19:346.\n``_\n\nInstallation\n------------\n\n*Biotite* requires the following packages:\n\n   - **numpy**\n   - **requests**\n   - **msgpack**\n   - **networkx**\n\nSome functions require some extra packages:\n\n   - **matplotlib** - Required for plotting purposes.\n\n*Biotite* can be installed via *Conda*...\n\n.. code-block:: console\n\n   $ conda install -c conda-forge biotite\n\n... or *pip*\n\n.. code-block:: console\n\n   $ pip install biotite\n\nUsage\n-----\n\nHere is a small example that downloads two protein sequences from the\n*NCBI Entrez* database and aligns them:\n\n.. code-block:: python\n\n   import biotite.sequence.align as align\n   import biotite.sequence.io.fasta as fasta\n   import biotite.database.entrez as entrez\n\n   # Download FASTA file for the sequences of avidin and streptavidin\n   file_name = entrez.fetch_single_file(\n   )\n\n   # Parse the downloaded FASTA file\n   # and create 'ProteinSequence' objects from it\n   fasta_file = fasta.FastaFile.read(file_name)\n   avidin_seq, streptavidin_seq = fasta.get_sequences(fasta_file).values()\n\n   # Align sequences using the BLOSUM62 matrix with affine gap penalty\n   matrix = align.SubstitutionMatrix.std_protein_matrix()\n   alignments = align.align_optimal(\n   )\n   print(alignments[0])\n\n.. code-block::\n\n   MVHATSPLLLLLLLSLALVAPGLSAR------KCSLTGKWDNDLGSNMTIGAVNSKGEFTGTYTTAV-TA\n   -------------------DPSKESKAQAAVAEAGITGTWYNQLGSTFIVTA-NPDGSLTGTYESAVGNA\n\n   TSNEIKESPLHGTQNTINKRTQPTFGFTVNWKFS----ESTTVFTGQCFIDRNGKEV-LKTMWLLRSSVN\n   ESRYVLTGRYDSTPATDGSGT--ALGWTVAWKNNYRNAHSATTWSGQYV---GGAEARINTQWLLTSGTT\n\n   DIGDDWKATRVGINIFTRLRTQKE---------------------\n   -AANAWKSTLVGHDTFTKVKPSAASIDAAKKAGVNNGNPLDAVQQ\n\nMore documentation, including a tutorial, an example gallery and the API\nreference is available at ``_.\n\nContribution\n------------\n\nInterested in improving *Biotite*?\nHave a look at the\n`contribution guidelines `_.\nFeel free to join our community chat on `Discord `_."}, {"name": "biotite", "tags": ["math", "web"], "summary": "A comprehensive library for computational molecular biology", "text": "This library is used to perform various tasks in computational molecular biology, including identifying homologous sequence regions and finding disulfide bonds in protein structures. It offers a set of tools for bioinformatics analysis that can be easily integrated into Python applications."}, {"name": "biotraj", "tags": ["math"], "summary": "Basic trajectory file format functionality for Biotite; forked from MDTraj", "text": "biotraj\n=======\n\nLoading and saving raw data in trajectory formats.\n\nThis package is small excerpt from https://github.com/mdtraj/mdtraj containing only the\nfunctionality to read and write trajectory files.\nIt is not intended to be used as a separate package, but is a dependency for\nIt is not integrated into the Biotite package directly due to an incompatible license.\n\nCurrently supported formats:\n\n- *XTC*\n- *TRR*\n- *NetCDF*\n- *DCD*"}, {"name": "biotraj", "tags": ["math"], "summary": "Basic trajectory file format functionality for Biotite; forked from MDTraj", "text": "This library is used to read and write trajectory files in various formats, including XTC, TRR, NetCDF, and DCD. With biotraj, developers can load and save raw data from molecular dynamics simulations, facilitating the analysis and manipulation of trajectory data."}, {"name": "bitsandbytes", "tags": ["math", "ml"], "summary": "k-bit optimizers and matrix multiplication routines.", "text": "System Requirements\nbitsandbytes has the following minimum requirements for all platforms:\n\n* Python 3.10+\n* [PyTorch](https://pytorch.org/get-started/locally/) 2.3+\n  * _Note: While we aim to provide wide backwards compatibility, we recommend using the latest version of PyTorch for the best experience._\n\nAccelerator support:\n\nNote: this table reflects the status of the current development branch. For the latest stable release, see the\n[document in the 0.49.0 tag](https://github.com/bitsandbytes-foundation/bitsandbytes/blob/0.49.0/README.md#accelerator-support).\n\nLegend:\n = In Development,\n\u3030\ufe0f = Partially Supported,\n = Supported,\n = Slow Implementation Supported,\n = Not Supported\n\n:book: Documentation\n* [Official Documentation](https://huggingface.co/docs/bitsandbytes/main)\n*  [Transformers](https://huggingface.co/docs/transformers/quantization/bitsandbytes)\n*  [Diffusers](https://huggingface.co/docs/diffusers/quantization/bitsandbytes)\n*  [PEFT](https://huggingface.co/docs/peft/developer_guides/quantization#quantize-a-model)\n\n:heart: Sponsors\nThe continued maintenance and development of `bitsandbytes` is made possible thanks to the generous support of our sponsors. Their contributions help ensure that we can keep improving the project and delivering valuable updates to the community.\n\n&nbsp;\n\nLicense\n`bitsandbytes` is MIT licensed.\n\nHow to cite us\nIf you found this library useful, please consider citing our work:\n\nQLoRA\n\nLLM.int8()\n\n8-bit Optimizers"}, {"name": "bitsandbytes", "tags": ["math", "ml"], "summary": "k-bit optimizers and matrix multiplication routines.", "text": "This library is used to accelerate matrix multiplication and other k-bit operations, leveraging optimized routines for improved performance. It can be integrated with PyTorch 2.3+ to enhance the efficiency of neural network computations."}, {"name": "blis", "tags": ["math", "web"], "summary": "The Blis BLAS-like linear algebra library, as a self-contained C-extension.", "text": "Cython BLIS: Fast BLAS-like operations from Python and Cython, without the tears\n\nThis repository provides the\n[Blis linear algebra](https://github.com/flame/blis) routines as a\nself-contained Python C-extension.\n\nCurrently, we only supports single-threaded execution, as this is actually best\nfor our workloads (ML inference).\n\n(https://github.com/explosion/cython-blis/actions/workflows/tests.yml)\n(https://pypi.python.org/pypi/blis)\n(https://anaconda.org/conda-forge/cython-blis)\n(https://github.com/explosion/wheelwright/releases)\n\nInstallation\n\nYou can install the package via pip, first making sure that `pip`, `setuptools`,\nand `wheel` are up-to-date:\n\nWheels should be available, so installation should be fast. If you want to\ninstall from source and you're on Windows, you'll need to install LLVM.\n\nBuilding BLIS for alternative architectures\n\nThe provided wheels should work on x86_64 architectures. Unfortunately we do not\ncurrently know a way to provide different wheels for alternative architectures,\nand we cannot provide a single binary that works everywhere. So if the wheel\ndoesn't work for your CPU, you'll need to specify source distribution, and tell\nBlis your CPU architecture using the `BLIS_ARCH` environment variable.\n\na) Installing with generic arch support\n\nb) Building specific support\n\nIn order to compile Blis, `cython-blis` bundles makefile scripts for specific\narchitectures, that are compiled by running the Blis build system and logging\nthe commands. We do not yet have logs for every architecture, as there are some\narchitectures we have not had access to.\n\n[See here](https://github.com/flame/blis/blob/0.5.1/config_registry) for list of\narchitectures. For example, here's how to build support for the ARM architecture\n`cortexa57`:\n\nFingers crossed, this will build you a wheel that supports your platform. You\ncould then [submit a PR](https://github.com/explosion/cython-blis/pulls) with\nthe `blis/_src/make/linux-cortexa57.jsonl` and\n`blis/_src/include/linux-cortexa57/blis.h` files so that you can run:\n\nUsage\n\nTwo APIs are provided: a high-level Python API, and direct\n[Cython](http://cython.org) access, which provides fused-type, nogil Cython\nbindings to the underlying Blis linear algebra library. Fused types are a simple\ntemplate mechanism, allowing just a touch of compile-time generic programming:\n\nBindings have been added as we've needed them. Please submit pull requests if\nthe library is missing some functions you require.\n\nDevelopment\n\nTo build the source package, you should run the following command:\n\nThis populates the `blis/_src` folder for the various architectures, using the\n`flame-blis` submodule.\n\nUpdating the build files\n\nIn order to compile the Blis sources, we use jsonl files that provide the\nexplicit compiler flags. We build these jsonl files by running Blis's build\nsystem, and then converting the log. This avoids us having to replicate the\nbuild system within Python: we just use the jsonl to make a bunch of subprocess\ncalls. To support a new OS/architecture combination, we have to provide the\njsonl file and the header.\n\nLinux\n\nThe Linux build files need to be produced from within the manylinux1 docker\ncontainer, so that they will be compatible with the wheel building process.\n\nFirst, install docker. Then do the following to start the container:\n\nOnce within the container, the following commands should check out the repo and\nbuild the jsonl files for the generic arch:\n\nThen from a new terminal, retrieve the two files we need out of the container:"}, {"name": "blis", "tags": ["math", "web"], "summary": "The Blis BLAS-like linear algebra library, as a self-contained C-extension.", "text": "This library is used to perform BLAS-like linear algebra operations from Python and Cython at high speeds, without the complexities of native C development. With this library, developers can seamlessly integrate efficient linear algebra functionality into their projects with minimal setup required."}, {"name": "bokeh", "tags": ["math", "ui", "visualization", "web"], "summary": "Interactive plots and applications in the browser from Python", "text": "Installation\n\nTo install Bokeh and its required dependencies using `pip`, enter the following command at a Bash or Windows command prompt:\n\nTo install using `conda`, enter the following command at a Bash or Windows command prompt:\n\nRefer to the [installation documentation](https://docs.bokeh.org/en/latest/docs/first_steps/installation.html) for more details.\n\nResources\n\nOnce Bokeh is installed, check out the [first steps guides](https://docs.bokeh.org/en/latest/docs/first_steps.html#first-steps-guides).\n\nVisit the [full documentation site](https://docs.bokeh.org) to view the [User's Guide](https://docs.bokeh.org/en/latest/docs/user_guide.html) or [checkout the Bokeh tutorial repository](https://github.com/bokeh/tutorial/) to learn about Bokeh in live Jupyter Notebooks.\n\nCommunity support is available on the [Project Discourse](https://discourse.bokeh.org).\n\nIf you would like to contribute to Bokeh, please review the [Contributor Guide](https://docs.bokeh.org/en/latest/docs/dev_guide.html) and [request an invitation to the Bokeh Dev Slack workspace](https://slack-invite.bokeh.org/).\n\n*Note: Everyone who engages in the Bokeh project's discussion forums, codebases, and issue trackers is expected to follow the [Code of Conduct](https://github.com/bokeh/bokeh/blob/HEAD/docs/CODE_OF_CONDUCT.md).*\n\nSupport\n\nFiscal Support\n\nThe Bokeh project is grateful for [individual contributions](https://opencollective.com/bokeh), as well as for present and past monetary support from the organizations and companies listed below:\n\n  \n  \n\n  \n  \n\n  \n  \n\n \n \n\n  \n  \n\n  \n  \n\n  \n  \n\n  \n  \n\nIf your company uses Bokeh and is able to sponsor the project, please contact info@bokeh.org\n\n*Bokeh is a Sponsored Project of NumFOCUS, a 501(c)(3) nonprofit charity in the United States. NumFOCUS provides Bokeh with fiscal, legal, and administrative support to help ensure the health and sustainability of the project. Visit [numfocus.org](https://numfocus.org) for more information.*\n\n*Donations to Bokeh are managed by NumFOCUS. For donors in the United States, your gift is tax-deductible to the extent provided by law. As with any donation, you should consult with your tax adviser about your particular tax situation.*\n\nIn-kind Support\n\nNon-monetary support can help with development, collaboration, infrastructure, security, and vulnerability management. The Bokeh project is grateful to the following companies for their donation of services:\n\n* [Amazon Web Services](https://aws.amazon.com/)\n* [GitGuardian](https://gitguardian.com/)\n* [GitHub](https://github.com/)\n* [makepath](https://makepath.com/)\n* [Pingdom](https://www.pingdom.com/website-monitoring)\n* [Slack](https://slack.com)\n* [QuestionScout](https://www.questionscout.com/)\n* [1Password](https://1password.com/)"}, {"name": "bokeh", "tags": ["math", "ui", "visualization", "web"], "summary": "Interactive plots and applications in the browser from Python", "text": "This library is used to create interactive plots and web applications from Python, enabling developers to visualize data and share insights with others. With Bokeh, developers can build custom browser-based visualizations that facilitate real-time interaction and exploration of complex data."}, {"name": "boolean-py", "tags": ["math"], "summary": "Define boolean algebras, create and parse boolean expressions and create custom boolean DSL.", "text": "This library helps you deal with boolean expressions and algebra with variables\nand the boolean functions AND, OR, NOT.\n\nYou can parse expressions from strings and simplify and compare expressions.\nYou can also easily create your custom algreba and mini DSL and create custom\ntokenizers to handle custom expressions.\n\nFor extensive documentation look either into the docs directory or view it online, at\n\nCopyright (c) 2009-2020 Sebastian Kraemer, basti.kr@gmail.com and others\nSPDX-License-Identifier: BSD-2-Clause"}, {"name": "boolean-py", "tags": ["math"], "summary": "Define boolean algebras, create and parse boolean expressions and create custom boolean DSL.", "text": "This library is used to create and parse boolean expressions and algebra with variables, as well as simplify and compare them. It also allows developers to define custom boolean DSLs and handle specific expression formats using custom tokenizers."}, {"name": "boost-histogram", "tags": ["cli", "math", "ui", "web"], "summary": "The Boost::Histogram Python wrapper.", "text": "boost-histogram for Python\n\n[![PyPI version][pypi-version]][pypi-link]\n\n[![PyPI platforms][pypi-platforms]][pypi-link]\n\n[\n\nPython bindings for [Boost::Histogram][] ([source][Boost::Histogram source]), a C++14 library. This is one of the [fastest libraries][] for\nhistogramming, while still providing the power of a full histogram object. See\n[what's new](./docs/CHANGELOG.md).\n\nOther members of the boost-histogram family include:\n\n- [Hist][]: The first-party analyst-friendly histogram library that extends\n  boost-histogram with named axes, many new shortcuts including UHI+, plotting\n  shortcuts, and more.\n- [UHI][]: Specification for Histogram library interop, especially for plotting.\n- [mplhep][]: Plotting extension for matplotlib with support for UHI histograms.\n- [histoprint][]: Histogram display library for the command line with support for UHI.\n- [dask-histogram][]: Dask support for boost-histogram.\n\nUsage\n\nText intro (click to expand)\n\nWe support the [uhi][] [PlottableHistogram][] protocol, so boost-histogram/[Hist][]\nhistograms can be plotted via any compatible library, such as [mplhep][].\n\nCheatsheet\n\nSimplified list of features (click to expand)"}, {"name": "boost-histogram", "tags": ["cli", "math", "ui", "web"], "summary": "The Boost::Histogram Python wrapper.", "text": "- Many axis types (all support `metadata=...`)\n  - `bh.axis.Regular(n, start, stop, ...)`: Make a regular axis. Options listed below.\n  - `bh.axis.Integer(start, stop, *, underflow=True, overflow=True, growth=False, circular=False)`: Special high-speed version of `regular` for evenly spaced bins of width 1\n  - `bh.axis.Variable([start, edge1, edge2, ..., stop], *, underflow=True, overflow=True, circular=False)`: Uneven bin spacing\n  - `bh.axis.IntCategory([...], *, growth=False)`: Integer categories\n  - `bh.axis.StrCategory([...], *, growth=False)`: String categories\n  - `bh.axis.Boolean()`: A True/False axis\n- Axis features:\n  - `.index(value)`: The index at a point (or points) on the axis\n  - `.value(index)`: The value for a fractional bin (or bins) in the axis\n  - `.bin(i)`: The bin edges (continuous axis) or a bin value (discrete axis)\n  - `.centers`: The N bin centers (if continuous)\n  - `.edges`: The N+1 bin edges (if continuous)\n  - `.extent`: The number of bins (including under/overflow)\n  - `.metadata`: Anything a user wants to store\n  - `.traits`: The options set on the axis\n  - `.size`: The number of bins (not including under/overflow)\n  - `.widths`: The N bin widths\n- Many storage types\n  - `bh.storage.Double()`: Doubles for weighted values (default)\n  - `bh.storage.Int64()`: 64-bit unsigned integers\n  - `bh.storage.Unlimited()`: Starts small, but can go up to unlimited precision ints or doubles.\n  - `bh.storage.AtomicInt64()`: Threadsafe filling, experimental. Does not support growing axis in threads.\n  - `bh.storage.Weight()`: Stores a weight and sum of weights squared.\n  - `bh.storage.Mean()`: Accepts a sample and computes the mean of the samples (profile).\n  - `bh.storage.WeightedMean()`: Accepts a sample and a weight. It computes the weighted mean of the samples.\n- Accumulators\n  - `bh.accumulator.Sum`: High accuracy sum (Neumaier) - used by the sum method when summing a numerical histogram\n  - `bh.accumulator.WeightedSum`: Tracks a weighted sum and variance\n  - `bh.accumulator.Mean`: Running count, mean, and variance (Welfords's incremental algorithm)\n  - `bh.accumulator.WeightedMean`: Tracks a weighted sum, mean, and variance (West's incremental algorithm)\n- Histogram operations\n  - `h.ndim`: The number of dimensions\n  - `h.size or len(h)`: The number of bins\n  - `+`: Add two histograms (storages must match types currently)\n  - `*=`: Multiply by a scaler (not all storages) (`hist * scalar` and `scalar * hist` supported too)\n  - `/=`: Divide by a scaler (not all storages) (`hist / scalar` supported too)\n  - `.kind`: Either `bh.Kind.COUNT` or `bh.Kind.MEAN`, depending on storage\n  - `.storage_type`: Fetch the histogram storage type\n  - `.sum(flow=False)`: The total count of all bins\n  - `.project(ax1, ax2, ...)`: Project down to listed axis (numbers). Can also reorder axes.\n  - `.to_numpy(flow=False, view=False)`: Convert to a NumPy style tuple (with or without under/overflow bins)\n  - `.view(flow=False)`: Get a view on the bin contents (with or without under/overflow bins)\n  - `.values(flow=False)`: Get a view on the values (counts or means, depending on storage)\n  - `.variances(flow=False)`: Get the variances if available\n  - `.counts(flow=False)`: Get the effective counts for all storage types\n  - `.reset()`: Set counters to 0 (growing axis remain the same size)\n  - `.empty(flow=False)`: Check to see if the histogram is empty (can check flow bins too if asked)\n  - `.copy(deep=False)`: Make a copy of a histogram\n  - `.axes`: Get the axes as a tuple-like (all properties of axes are available too)\n- Indexing - Supports [UHI Indexing](https://uhi.readthedocs.io/en/latest/indexing.html)\n  - Bin content access / setting\n  - Slicing to get histogram or set array of values\n  - Special accessors\n  - Special actions (third item in slice)\n- NumPy compatibility\n  - `bh.numpy` provides faster [drop in replacements](https://boost-histogram.readthedocs.io/en/latest/usage/numpy.html) for NumPy histogram functions\n  - Histograms follow the buffer interface, and provide `.view()`\n  - Histograms can be converted to NumPy style output tuple with `.to_numpy()`\n- Details\n  - All objects support copy/deepcopy/pickle\n  - Fully statically typed, tested with MyPy."}, {"name": "boost-histogram", "tags": ["cli", "math", "ui", "web"], "summary": "The Boost::Histogram Python wrapper.", "text": "Installation\n\nYou can install this library from [PyPI](https://pypi.org/project/boost-histogram/) with pip:\n\nAll the normal best-practices for Python apply; Pip should not be very old (Pip\n9 is very old), you should be in a virtual environment, etc. Python 3.9+ is\nrequired; for older versions of Python (3.5 and 2.7), `0.13` will be installed\ninstead, which is API equivalent to 1.0, but will not be gaining new features.\n1.3.x was the last series to support Python 3.6. 1.4.x was the last series to\nsupport Python 3.7. 1.5.x was the last series to support Python 3.8.\n\nBinaries available:\n\nThe easiest way to get boost-histogram is to use a binary wheel, which happens\nwhen you run the above command on a supported platform. Wheels are produced using\n[cibuildwheel](https://cibuildwheel.readthedocs.io/en/stable/); all common\nplatforms have wheels provided in boost-histogram:\n\nSystem\n-------------\nmanylinux2014\nmanylinux2014\nmusllinux_1_1\nmacOS\nmacOS\nWindows\nWindows\n\nPowerPC or IBM-Z wheels are not provided but are available on request.\n\nIf you are on a Linux system that is not part of the \"many\" in manylinux or musl in musllinux, such as ClearLinux, building from source is usually fine, since the compilers on those systems are often quite new. It will just take longer to install when it is using the sdist instead of a wheel. All dependencies are header-only and included.\n\nConda-Forge\n\nThe boost-histogram package is available on [conda-forge](https://github.com/conda-forge/boost-histogram-feedstock), as well. All supported variants are available.\n\nSource builds\n\nFor a source build, for example from an \"SDist\" package, the only requirements are a C++14 compatible compiler. The compiler requirements are dictated by Boost.Histogram's C++ requirements: gcc >= 5.5, clang >= 3.8, or msvc >= 14.1.\n\nBoost is not required or needed (this only depends on included header-only dependencies). You can install directly from GitHub if you would like.\n\nDeveloping\n\nSee [CONTRIBUTING.md](.github/CONTRIBUTING.md) for details on how to set up a development environment.\n\nContributors\n\nWe would like to acknowledge the contributors that made this project possible ([emoji key](https://allcontributors.org/docs/en/emoji-key)):\n\nThis project follows the [all-contributors](https://github.com/all-contributors/all-contributors) specification.\n\nTalks and other documentation/tutorial sources\n\nThe [official documentation is here](https://boost-histogram.readthedocs.io/en/latest/index.html), and includes a [quickstart](https://boost-histogram.readthedocs.io/en/latest/usage/quickstart.html).\n\n---\n\nAcknowledgements\n\nThis library was primarily developed by Henry Schreiner and Hans Dembinski.\n\nSupport for this work was provided by the National Science Foundation cooperative agreement OAC-1836650 (IRIS-HEP) and OAC-1450377 (DIANA/HEP). Any opinions, findings, conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the National Science Foundation."}, {"name": "boost-histogram", "tags": ["cli", "math", "ui", "web"], "summary": "The Boost::Histogram Python wrapper.", "text": "This library is used to quickly and efficiently create and manipulate histograms in Python, leveraging the speed of a C++14 library. With boost-histogram, developers can focus on data analysis tasks without worrying about performance constraints."}, {"name": "botbuilder-core", "tags": ["math", "ml", "web"], "summary": "Microsoft Bot Framework Bot Builder", "text": "==============================\nBotBuilder-Core SDK for Python\n==============================\n\n.. image:: https://dev.azure.com/FuseLabs/SDK_v4/_apis/build/status/Python/Python-CI-PR-yaml?branchName=master\n   :target:  https://dev.azure.com/FuseLabs/SDK_v4/_apis/build/status/Python/Python-CI-PR-yaml?branchName=master\n   :align: right\n   :alt: Azure DevOps status for master branch\n\n   :alt: Latest PyPI package version\n\nWithin the Bot Framework, BotBuilder-core enables you to build bots that exchange messages with users on channels that are configured in the Bot Framework Portal.\n\nHow to Install\n==============\n\n.. code-block:: python\n  \n\nDocumentation/Wiki\n==================\n\nYou can find more information on the botbuilder-python project by visiting our `Wiki`_.\n\nRequirements\n============\n\n* `Python >= 3.7.0`_\n\nSource Code\n===========\nThe latest developer version is available in a github repository:\n\nContributing\n============\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.microsoft.com.\n\nWhen you submit a pull request, a CLA-bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., label, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the `Microsoft Open Source Code of Conduct`_.\nFor more information see the `Code of Conduct FAQ`_ or\ncontact `opencode@microsoft.com`_ with any additional questions or comments.\n\nReporting Security Issues\n=========================\n\nSecurity issues and bugs should be reported privately, via email, to the Microsoft Security\nResponse Center (MSRC) at `secure@microsoft.com`_. You should\nreceive a response within 24 hours. If for some reason you do not, please follow up via\nemail to ensure we received your original message. Further information, including the\n`MSRC PGP`_ key, can be found in\nthe `Security TechCenter`_.\n\nLicense\n=======\n\nCopyright (c) Microsoft Corporation. All rights reserved.\n\nLicensed under the MIT_ License.\n\n.. _Wiki: https://github.com/Microsoft/botbuilder-python/wiki\n.. _Python >= 3.7.0: https://www.python.org/downloads/\n.. _MIT: https://github.com/Microsoft/vscode/blob/master/LICENSE.txt\n.. _Microsoft Open Source Code of Conduct: https://opensource.microsoft.com/codeofconduct/\n.. _Code of Conduct FAQ: https://opensource.microsoft.com/codeofconduct/faq/\n.. _opencode@microsoft.com: mailto:opencode@microsoft.com\n.. _secure@microsoft.com: mailto:secure@microsoft.com\n.. _MSRC PGP: https://technet.microsoft.com/en-us/security/dn606155\n.. _Security TechCenter: https://github.com/Microsoft/vscode/blob/master/LICENSE.txt\n\n.. `_"}, {"name": "botbuilder-core", "tags": ["math", "ml", "web"], "summary": "Microsoft Bot Framework Bot Builder", "text": "This library is used to enable developers to build bots that exchange messages with users on configured channels through the Bot Framework Portal. With this library, you can create sophisticated chatbots for various platforms and use cases, such as customer support or conversational interfaces."}, {"name": "botbuilder-schema", "tags": ["math", "ml", "web"], "summary": "BotBuilder Schema", "text": "=================\nBotBuilder-Schema\n=================\n\n.. image:: https://dev.azure.com/FuseLabs/SDK_v4/_apis/build/status/Python/Python-CI-PR-yaml?branchName=master\n   :target:  https://dev.azure.com/FuseLabs/SDK_v4/_apis/build/status/Python/Python-CI-PR-yaml?branchName=master\n   :align: right\n   :alt: Azure DevOps status for master branch\n\n   :alt: Latest PyPI package version\n\nBotBuilder-schema contains the serialized data sent across the wire between user and bot when using Bot Framework.\n\nHow to Install\n==============\n\n.. code-block:: python\n  \n\nDocumentation/Wiki\n==================\n\nYou can find more information on the botbuilder-python project by visiting our `Wiki`_.\n\nRequirements\n============\n\n* `Python >= 3.7.0`_\n\nSource Code\n===========\nThe latest developer version is available in a github repository:\n\nContributing\n============\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.microsoft.com.\n\nWhen you submit a pull request, a CLA-bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., label, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the `Microsoft Open Source Code of Conduct`_.\nFor more information see the `Code of Conduct FAQ`_ or\ncontact `opencode@microsoft.com`_ with any additional questions or comments.\n\nReporting Security Issues\n=========================\n\nSecurity issues and bugs should be reported privately, via email, to the Microsoft Security\nResponse Center (MSRC) at `secure@microsoft.com`_. You should\nreceive a response within 24 hours. If for some reason you do not, please follow up via\nemail to ensure we received your original message. Further information, including the\n`MSRC PGP`_ key, can be found in\nthe `Security TechCenter`_.\n\nLicense\n=======\n\nCopyright (c) Microsoft Corporation. All rights reserved.\n\nLicensed under the MIT_ License.\n\n.. _Wiki: https://github.com/Microsoft/botbuilder-python/wiki\n.. _Python >= 3.7.0: https://www.python.org/downloads/\n.. _MIT: https://github.com/Microsoft/vscode/blob/master/LICENSE.txt\n.. _Microsoft Open Source Code of Conduct: https://opensource.microsoft.com/codeofconduct/\n.. _Code of Conduct FAQ: https://opensource.microsoft.com/codeofconduct/faq/\n.. _opencode@microsoft.com: mailto:opencode@microsoft.com\n.. _secure@microsoft.com: mailto:secure@microsoft.com\n.. _MSRC PGP: https://technet.microsoft.com/en-us/security/dn606155\n.. _Security TechCenter: https://github.com/Microsoft/vscode/blob/master/LICENSE.txt\n\n.. `_"}, {"name": "botbuilder-schema", "tags": ["math", "ml", "web"], "summary": "BotBuilder Schema", "text": "This library is used to serialize data sent between a user and bot when using Bot Framework, enabling seamless communication between the two entities. Developers can achieve streamlined integration with Bot Framework by utilizing this library's serialization capabilities."}, {"name": "botframework-streaming", "tags": ["math", "ml", "web"], "summary": "Microsoft Bot Framework Bot Builder", "text": "=================================\nBotFramework-Streaming for Python\n=================================\n\n.. image:: https://fuselabs.visualstudio.com/SDK_v4/_apis/build/status/Python/SDK_v4-Python-CI?branchName=master\n   :target:  https://fuselabs.visualstudio.com/SDK_v4/_apis/build/status/Python/SDK_v4-Python-CI\n   :align: right\n   :alt: Azure DevOps status for master branch\n\n   :alt: Latest PyPI package version\n\nStreaming Extensions libraries for BotFramework.\n\nHow to Install\n==============\n\n.. code-block:: bash\n  \n\nDocumentation/Wiki\n==================\n\nYou can find more information on the botframework-python project by visiting our `Wiki`_.\n\nRequirements\n============\n\n* `Python >= 3.7.0`_\n\nSource Code\n===========\nThe latest developer version is available in a github repository:\n\nContributing\n============\n\nThis project welcomes contributions and suggestions.  Most contributions require you to agree to a\nContributor License Agreement (CLA) declaring that you have the right to, and actually do, grant us\nthe rights to use your contribution. For details, visit https://cla.microsoft.com.\n\nWhen you submit a pull request, a CLA-bot will automatically determine whether you need to provide\na CLA and decorate the PR appropriately (e.g., label, comment). Simply follow the instructions\nprovided by the bot. You will only need to do this once across all repos using our CLA.\n\nThis project has adopted the `Microsoft Open Source Code of Conduct`_.\nFor more information see the `Code of Conduct FAQ`_ or\ncontact `opencode@microsoft.com`_ with any additional questions or comments.\n\nReporting Security Issues\n=========================\n\nSecurity issues and bugs should be reported privately, via email, to the Microsoft Security\nResponse Center (MSRC) at `secure@microsoft.com`_. You should\nreceive a response within 24 hours. If for some reason you do not, please follow up via\nemail to ensure we received your original message. Further information, including the\n`MSRC PGP`_ key, can be found in\nthe `Security TechCenter`_.\n\nLicense\n=======\n\nCopyright (c) Microsoft Corporation. All rights reserved.\n\nLicensed under the MIT_ License.\n\n.. _Wiki: https://github.com/Microsoft/botframework-python/wiki\n.. _Python >= 3.7.0: https://www.python.org/downloads/\n.. _MIT: https://github.com/Microsoft/vscode/blob/master/LICENSE.txt\n.. _Microsoft Open Source Code of Conduct: https://opensource.microsoft.com/codeofconduct/\n.. _Code of Conduct FAQ: https://opensource.microsoft.com/codeofconduct/faq/\n.. _opencode@microsoft.com: mailto:opencode@microsoft.com\n.. _secure@microsoft.com: mailto:secure@microsoft.com\n.. _MSRC PGP: https://technet.microsoft.com/en-us/security/dn606155\n.. _Security TechCenter: https://github.com/Microsoft/vscode/blob/master/LICENSE.txt\n\n.. `_"}, {"name": "botframework-streaming", "tags": ["math", "ml", "web"], "summary": "Microsoft Bot Framework Bot Builder", "text": "This library is used to extend the Bot Framework with streaming capabilities, enabling developers to build scalable and efficient conversational AI experiences. With this library, developers can leverage the power of Python to create robust and dynamic chatbots that integrate seamlessly with the Bot Framework."}, {"name": "botorch", "tags": ["dev", "math", "ml", "web"], "summary": "Bayesian Optimization in PyTorch", "text": "Why BoTorch ?\nBoTorch\n* Provides a modular and easily extensible interface for composing Bayesian\n  optimization primitives, including probabilistic models, acquisition functions,\n  and optimizers.\n* Harnesses the power of PyTorch, including auto-differentiation, native support\n  for highly parallelized modern hardware (e.g. GPUs) using device-agnostic code,\n  and a dynamic computation graph.\n* Supports Monte Carlo-based acquisition functions via the\n  [reparameterization trick](https://arxiv.org/abs/1312.6114), which makes it\n  straightforward to implement new ideas without having to impose restrictive\n  assumptions about the underlying model.\n* Enables seamless integration with deep and/or convolutional architectures in PyTorch.\n* Has first-class support for state-of-the art probabilistic models in\n  [GPyTorch](http://www.gpytorch.ai/), including support for multi-task Gaussian\n  Processes (GPs) deep kernel learning, deep GPs, and approximate inference.\n\nTarget Audience\n\nThe primary audience for hands-on use of BoTorch are researchers and\nsophisticated practitioners in Bayesian Optimization and AI.\nWe recommend using BoTorch as a low-level API for implementing new algorithms\nfor [Ax](https://ax.dev). Ax has been designed to be an easy-to-use platform\nfor end-users, which at the same time is flexible enough for Bayesian\nOptimization researchers to plug into for handling of feature transformations,\n(meta-)data management, storage, etc.\nWe recommend that end-users who are not actively doing research on Bayesian\nOptimization simply use Ax.\n\nInstallation\n\n**Installation Requirements**\n- Python >= 3.10\n- PyTorch >= 2.0.1\n- gpytorch >= 1.14\n- linear_operator >= 0.6\n- pyro-ppl >= 1.8.4\n- scipy\n- multiple-dispatch\n\nOption 1: Installing the latest release\n\nThe latest release of BoTorch is easily installed via `pip`:\n\n_Note_: Make sure the `pip` being used is actually the one from the newly created\nConda environment. If you're using a Unix-based OS, you can use `which pip` to check.\n\nBoTorch [stopped publishing](https://github.com/meta-pytorch/botorch/discussions/2613#discussion-7431533)\nan official Anaconda package to the `pytorch` channel after the 0.12 release. However,\nusers can still use the package published to the `conda-forge` channel and install botorch via\n\nOption 2: Installing from latest main branch\n\nIf you would like to try our bleeding edge features (and don't mind potentially\nrunning into the occasional bug here or there), you can install the latest\ndevelopment version directly from GitHub. You may also want to install the\ncurrent `gpytorch` and `linear_operator` development versions:\n\nOption 3: Editable/dev install\n\nIf you want to [contribute](CONTRIBUTING.md) to BoTorch, you will want to install editably so that you can change files and have the\nchanges reflected in your local install.\n\nIf you want to install the current `gpytorch` and `linear_operator` development versions, as in Option 2, do that\nbefore proceeding.\n\nOption 3a: Bare-bones editable install\n\nOption 3b: Editable install with development and tutorials dependencies\n\n* `dev`: Specifies tools necessary for development\n  (testing, linting, docs building; see [Contributing](#contributing) below).\n* `tutorials`: Also installs all packages necessary for running the tutorial notebooks.\n* You can also install either the dev or tutorials dependencies without installing both, e.g. by changing the last command to `pip install -e \".[dev]\"`.\n\nGetting Started\n\nHere's a quick run down of the main components of a Bayesian optimization loop.\nFor more details see our [Documentation](https://botorch.org/docs/introduction) and the\n[Tutorials](https://botorch.org/docs/tutorials).\n\n1. Fit a Gaussian Process model to data\n  \n\n2. Construct an acquisition function\n  \n\n3. Optimize the acquisition function\n\nCiting BoTorch\n\nIf you use BoTorch, please cite the following paper:\n> [M. Balandat, B. Karrer, D. R. Jiang, S. Daulton, B. Letham, A. G. Wilson, and E. Bakshy. BoTorch: A Framework for Efficient Monte-Carlo Bayesian Optimization. Advances in Neural Information Processing Systems 33, 2020.](https://arxiv.org/abs/1910.06403)\n\nSee [here](https://botorch.org/docs/papers) for an incomplete selection of peer-reviewed papers that build off of BoTorch.\n\nContributing\nSee the [CONTRIBUTING](CONTRIBUTING.md) file for how to help out.\n\nLicense\nBoTorch is MIT licensed, as found in the [LICENSE](LICENSE) file."}, {"name": "botorch", "tags": ["dev", "math", "ml", "web"], "summary": "Bayesian Optimization in PyTorch", "text": "This library is used to compose Bayesian optimization primitives, enabling efficient search of optimal solutions using probabilistic models and acquisition functions. Developers can use botorch to implement robust and scalable Bayesian optimization in their PyTorch-based applications."}, {"name": "bottleneck", "tags": ["dev", "math"], "summary": "Fast NumPy array functions written in C", "text": "Bottleneck is a collection of fast NumPy array functions written in C.\n\nLet's give it a try. Create a NumPy array:\n\n.. code-block:: pycon\n\nFind the nanmean:\n\n.. code-block:: pycon\n\nMoving window mean:\n\n.. code-block:: pycon\n\nBenchmark\n=========\n\nBottleneck comes with a benchmark suite:\n\n.. code-block:: pycon\n\nYou can also run a detailed benchmark for a single function using, for\nexample, the command:\n\n.. code-block:: pycon\n\nOnly arrays with data type (dtype) int32, int64, float32, and float64 are\naccelerated. All other dtypes result in calls to slower, unaccelerated\nfunctions. In the rare case of a byte-swapped input array (e.g. a big-endian\narray on a little-endian operating system) the function will not be\naccelerated regardless of dtype.\n\nWhere\n=====\n\n===================   ========================================================\n download             https://pypi.python.org/pypi/Bottleneck\n docs                 https://bottleneck.readthedocs.io\n code                 https://github.com/pydata/bottleneck\n mailing list         https://groups.google.com/group/bottle-neck\n===================   ========================================================\n\nLicense\n=======\n\nBottleneck is distributed under a Simplified BSD license. See the LICENSE file\nand LICENSES directory for details.\n\nInstall\n=======\n\nBottleneck provides binary wheels on PyPI for all the most common platforms.\nBinary packages are also available in conda-forge. We recommend installing binaries\nwith ``pip``, ``uv``, ``conda`` or similar - it's faster and easier than building\nfrom source.\n\nInstalling from source\n----------------------\n\nRequirements:\n\n======================== ============================================================================\nBottleneck               Python >=3.9; NumPy 1.16.0+\nCompile                  gcc, clang, MinGW or MSVC\nUnit tests               pytest\nDocumentation            sphinx, numpydoc\n======================== ============================================================================\n\nTo install Bottleneck on Linux, Mac OS X, et al.:\n\n.. code-block:: console\n\nTo install bottleneck on Windows, first install MinGW and add it to your\nsystem path. Then install Bottleneck with the command:\n\n.. code-block:: console\n\nUnit tests\n==========\n\nAfter you have installed Bottleneck, run the suite of unit tests:\n\n.. code-block:: pycon\n\n  In [1]: import bottleneck as bn\n\n  In [2]: bn.test()\n  ============================= test session starts =============================\n  platform linux -- Python 3.7.4, pytest-4.3.1, py-1.8.0, pluggy-0.12.0\n  hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase('/home/chris/code/bottleneck/.hypothesis/examples')\n  rootdir: /home/chris/code/bottleneck, inifile: setup.cfg\n  plugins: openfiles-0.3.2, remotedata-0.3.2, doctestplus-0.3.0, mock-1.10.4, forked-1.0.2, cov-2.7.1, hypothesis-4.32.2, xdist-1.26.1, arraydiff-0.3\n  collected 190 items\n  \n  bottleneck/tests/input_modification_test.py ........................... [ 14%]\n  ..                                                                      [ 15%]\n  bottleneck/tests/list_input_test.py .............................       [ 30%]\n  bottleneck/tests/move_test.py .................................         [ 47%]\n  bottleneck/tests/nonreduce_axis_test.py ....................            [ 58%]\n  bottleneck/tests/nonreduce_test.py ..........                           [ 63%]\n  bottleneck/tests/reduce_test.py ....................................... [ 84%]\n  ............                                                            [ 90%]\n  bottleneck/tests/scalar_input_test.py ..................                [100%]\n  \n  ========================= 190 passed in 46.42 seconds =========================\n  Out[2]: True\n\nIf developing in the git repo, simply run ``py.test``"}, {"name": "bottleneck", "tags": ["dev", "math"], "summary": "Fast NumPy array functions written in C", "text": "This library is used to speed up NumPy array operations by leveraging optimized C code, providing significant performance improvements for common array functions. By utilizing Bottleneck's accelerated functions, developers can achieve faster computation times for tasks such as finding the mean of arrays with missing values and calculating moving window means."}, {"name": "cached-path", "tags": ["data", "math", "ml", "web"], "summary": "A file utility for accessing both local and remote files through a unified interface", "text": "[cached-path](https://cached-path.readthedocs.io/)\n\nA file utility library that provides a unified, simple interface for accessing both local and remote files.\nThis can be used behind other APIs that need to access files agnostic to where they are located.\n\nQuick links\n\nInstallation\n\n**cached-path** requires Python 3.7 or later.\n\nInstalling with `pip`\n\n**cached-path** is available [on PyPI](https://pypi.org/project/cached-path/). Just run\n\nInstalling from source\n\nTo install **cached-path** from source, first clone [the repository](https://github.com/allenai/cached_path):\n\nThen run\n\nUsage\n\nGiven something that might be a URL or local path, `cached_path()` determines which.\nIf it's a remote resource, it downloads the file and caches it to the [cache directory](#cache-directory), and\nthen returns the path to the cached file. If it's already a local path,\nit makes sure the file exists and returns the path.\n\nFor URLs, `http://`, `https://`, `s3://` (AWS S3), `gs://` (Google Cloud Storage), and `hf://` (HuggingFace Hub) are all supported out-of-the-box.\nOptionally `beaker://` URLs in the form of `beaker://{user_name}/{dataset_name}/{file_path}` are supported, which requires [beaker-py](https://beaker-py-docs.allen.ai) to be installed.\n\nFor example, to download the PyTorch weights for the model `epwalsh/bert-xsmall-dummy`\non HuggingFace, you could do:\n\nFor paths or URLs that point to a tarfile or zipfile, you can also add a path\nto a specific file to the `url_or_filename` preceeded by a \"!\", and the archive will\nbe automatically extracted (provided you set `extract_archive` to `True`),\nreturning the local path to the specific file. For example:\n\nUsing custom headers for HTTP requests\n\nYou can provide custom headers for HTTP requests, which is useful for accessing private resources that require authentication:\n\nThis is particularly useful for downloading private files from services like Hugging Face, GitHub, or any other API that uses Bearer token authentication.\n\nCache directory\n\nBy default the cache directory is `~/.cache/cached_path/`, however there are several ways to override this setting:\n- set the environment variable `CACHED_PATH_CACHE_ROOT`,\n- call `set_cache_dir()`, or\n- set the `cache_dir` argument each time you call `cached_path()`.\n\nTeam\n\n**cached-path** is developed and maintained by the AllenNLP team, backed by [the Allen Institute for Artificial Intelligence (AI2)](https://allenai.org/).\nAI2 is a non-profit institute with the mission to contribute to humanity through high-impact AI research and engineering.\nTo learn more about who specifically contributed to this codebase, see [our contributors](https://github.com/allenai/cached_path/graphs/contributors) page.\n\nLicense\n\n**cached-path** is licensed under [Apache 2.0](https://www.apache.org/licenses/LICENSE-2.0).\nA full copy of the license can be found [on GitHub](https://github.com/allenai/cached_path/blob/main/LICENSE)."}, {"name": "cached-path", "tags": ["data", "math", "ml", "web"], "summary": "A file utility for accessing both local and remote files through a unified interface", "text": "This library is used to provide a unified interface for accessing both local and remote files from any location. This can be used behind other APIs that need to access files without knowing where they are located, simplifying file retrieval logic."}, {"name": "canmatrix", "tags": ["math"], "summary": null, "text": "Canmatrix implements a \"Python Can Matrix Object\" which describes the can-communication\nand the needed objects (Boardunits, Frames, Signals, Values, ...) Canmatrix also includes\ntwo Tools (canconvert and cancompare) for converting and comparing CAN databases.\nThere are also some extract and merge options for dealing with can databases.\n\n**supported file formats for import:**\n\n**supported file formats for export:**"}, {"name": "canmatrix", "tags": ["math"], "summary": null, "text": "This library is used to create and manipulate Can Matrix Objects that describe CAN communication, including importing and exporting various database formats. It also provides tools for converting and comparing CAN databases, as well as extracting and merging data within these databases."}, {"name": "canopen", "tags": ["dev", "math"], "summary": "CANopen stack implementation", "text": "CANopen for Python\n==================\n\nA Python implementation of the CANopen_ standard.\nThe aim of the project is to support the most common parts of the CiA 301\nstandard in a simple Pythonic interface. It is mainly targeted for testing and\nautomation tasks rather than a standard compliant master implementation.\n\nThe library supports Python 3.8 or newer.\n\nFeatures\n--------\n\nThe library is mainly meant to be used as a master.\n\n* NMT master\n* SDO client\n* PDO producer/consumer\n* SYNC producer\n* EMCY consumer\n* TIME producer\n* LSS master\n* Object Dictionary from EDS\n* 402 profile support\n\nIncomplete support for creating slave nodes also exists.\n\n* SDO server\n* PDO producer/consumer\n* NMT slave\n* EMCY producer\n* Object Dictionary from EDS\n\nInstallation\n------------\n\nInstall from PyPI_ using ``pip``::\n\nInstall from latest ``master`` on GitHub::\n\nIf you want to be able to change the code while using it, clone it then install\nit in `develop mode`_::\n\nUnit tests can be run using the pytest_ framework::\n\nYou can also use ``unittest`` standard library module::\n\nDocumentation\n-------------\n\nDocumentation can be found on Read the Docs:\n\nIt can also be generated from a local clone using Sphinx_::\n\nHardware support\n----------------\n\nThis library supports multiple hardware and drivers through the python-can_ package.\nSee `the list of supported devices `_.\n\nIt is also possible to integrate this library with a custom backend.\n\nQuick start\n-----------\n\nHere are some quick examples of what you can do:\n\nThe PDOs can be access by three forms:\n\n**1st:** :code:`node.tpdo[n]` or :code:`node.rpdo[n]`\n\n**2nd:** :code:`node.pdo.tx[n]` or :code:`node.pdo.rx[n]`\n\n**3rd:** :code:`node.pdo[0x1A00]` or :code:`node.pdo[0x1600]`\n\nThe :code:`n` is the PDO index (normally 1 to 4). The second form of access is for backward compatibility.\n\n.. code-block:: python\n\nDebugging\n---------\n\nIf you need to see what's going on in better detail, you can increase the\nlogging_ level:\n\n.. code-block:: python\n\n.. _PyPI: https://pypi.org/project/canopen/\n.. _CANopen: https://www.can-cia.org/canopen/\n.. _python-can: https://python-can.readthedocs.org/en/stable/\n.. _Sphinx: http://www.sphinx-doc.org/\n.. _develop mode: https://packaging.python.org/distributing/#working-in-development-mode\n.. _logging: https://docs.python.org/3/library/logging.html\n.. _pytest: https://docs.pytest.org/"}, {"name": "canopen", "tags": ["dev", "math"], "summary": "CANopen stack implementation", "text": "This library is used to implement a CANopen master interface in Python, supporting features such as NMT management and SDO communication for automation tasks. With this library, developers can easily integrate their applications with devices that follow the CiA 301 standard, simplifying testing and development processes."}, {"name": "cartopy", "tags": ["math", "ui", "visualization"], "summary": "A Python library for cartographic visualizations with Matplotlib", "text": "Table of contents\n\n markdown-toc -i --bullets='-' README.md\n\nNOTE: This entire README can be markdown linted with\n-->\n\n- [Overview](#overview)\n- [Get in touch](#get-in-touch)\n- [License and copyright](#license-and-copyright)\n\nOverview\n\nCartopy is a Python package designed to make drawing maps for data\nanalysis and visualisation easy.\n\nIt features:\n\n- object oriented projection definitions\n- point, line, polygon and image transformations between projections\n- integration to expose advanced mapping in Matplotlib with a simple and\n  intuitive interface\n- powerful vector data handling by integrating shapefile reading with Shapely\n  capabilities\n\nDocumentation can be found at .\n\nGet in touch\n\n- Ask usage questions on\n  [StackOverflow](https://stackoverflow.com/questions/tagged/cartopy).\n- For less well defined questions, ideas, general discussion or announcements of\n  related projects use the\n  [Cartopy category on Matplotlib's Discourse](https://discourse.matplotlib.org/c/3rdparty/cartopy/19).\n- Report bugs, suggest features or view the source code on\n  [GitHub](https://github.com/SciTools/cartopy).\n- To chat with developers and other users you can use the\n  [Gitter Chat](https://gitter.im/SciTools/cartopy).\n\nCredits, copyright and license\n\nCartopy is developed collaboratively under the SciTools umberella.\n\nA full list of codecontributors (\"Cartopy contributors\") can be found at\n\nCode is just one of many ways of positively contributing to Cartopy, please see\nour [contributing guide](.github/CONTRIBUTING.md) for more details on how\nyou can get involved.\n\nCartopy is released under the 3-Clause BSD license with a shared copyright model.\nSee [LICENSE](LICENSE) for full terms.\n\nThe [Met Office](https://metoffice.gov.uk) has made a significant\ncontribution to the development, maintenance and support of this library.\nAll Met Office contributions are copyright on behalf of the British Crown."}, {"name": "cartopy", "tags": ["math", "ui", "visualization"], "summary": "A Python library for cartographic visualizations with Matplotlib", "text": "This library is used to create cartographic visualizations for data analysis and visualization, leveraging object-oriented projection definitions and integration with Matplotlib. With Cartopy, developers can easily draw maps and transform geographic data between different projections using a simple and intuitive interface."}, {"name": "cftime", "tags": ["math", "web"], "summary": "Time-handling functionality from netcdf4-python", "text": "cftime\nTime-handling functionality from netcdf4-python\n\n(https://github.com/Unidata/cftime/actions)\n(http://python.org/pypi/cftime)\n(https://coveralls.io/github/Unidata/cftime?branch=master)\n(https://github.com/Unidata/cftime/tags)\n(https://github.com/Unidata/cftime/releases)\n(https://github.com/UniData/cftime/commits/master)\n\nNews\nFor details on the latest updates, see the [Changelog](https://github.com/Unidata/cftime/blob/master/Changelog).\n\n10/13/2025:  Version 1.6.5 release.  Minor bugfixes/optimizations, wheels for python 3.14, no more wheels for 3.8/3.9.\n\n6/7/2024:  Version 1.6.4 release.  Wheels for muslinux and aarch64, numpy 2.0 compatibility.\n\n10/20/2023:  Version 1.6.3 released.  Support for python 3.12, cython 3.0, strptime formats without separators.\n \n9/18/2022:  Version 1.6.2 released.  strptime method added, fix for num2date failure on\nempty integer array, date2num 'longdouble' keyword added. New wheel building workflow.\n\n6/30/2022:  Version 1.6.1 released.  Fixes for numpy 1.23.0, updated CI/CD.\n\n3/4/2022:  Version 1.6.0 released.  Big speed-ups for num2date, date2index bugfix for select != 'exact' when select='exact' works, fix for date2num with masked array inputs.\n\n1/22/2022: Version 1.5.2 released (wheels for Apple M1 available on pypi for python 3.8,3.9 and 3.10). is_leap_year\nfunction added (issue #259).\n\n10/31/2021: Version 1.5.1.1 released (new binary wheels for python 3.10).\n\n10/1/2021:  Version 1.5.1 released. Changed default behavior of ``proleptic_gregorian``\nto has_year_zero=T (since it is allowed in ISO-8601 and CF does not specify the\nyear zero convention for this calendar). Raise warning message when trying\nto create a calendar that is not supported by CF version 1.9 (no years < 1\nallowed for 'standard'/'gregorian' or 'julian'  calendars).\nAdded support for \"common_year\" and \"common_years\" units for \"noleap\" \nand \"365_day\" calendars.\n \n5/20/2021:  Version 1.5.0 released.  Includes support for astronomical year numbering\n(including the year zero) for real-world calendars ('julian', 'gregorian'/'standard',\nand 'proleptic_gregorian') using 'has_year_zero' `cftime.datetime` kwarg.\nAdded a 'change_calendar' `cftime.datetime` method to switch to another \n'real-world' calendar to enable comparison of instances with different calendars.\nSome legacy classes and functions removed (`utime`, `JulianDayFromDate` and\n`DateFromJulianDay`). The functionality of `JulianDayFromDate` and \n`DateFromJulianDay` is now available from the methods `cftime.datetime.toordinal`\nand `cftime.datetime.fromordinal`.\n\n2/2/2021:  Version 1.4.1 released. Restore use of calendar-specific subclasses\nin `cftime.num2date`, `cftime.datetime.__add__`, and `cftime.datetime.__sub__`.\nThe use of this will be removed in a later release.\nAdd 'fromordinal' static method to create a cftime.datetime instance\nfrom a julian day ordinal and calendar (inverse of 'toordinal').\n\n2/1/2021:  Version 1.4.0 released.  License changed to MIT (GPL'ed code replaced).\nRoundtrip accuracy improved for units other than microseconds. Added \ncftime.datetime.toordinal method, returns integer julian day number.\n\n1/17/2021: Version 1.3.1 released.\n\n11/16/2020:  Version 1.3.0 released. **API change**: The `cftime.datetime` constructor now creates \n 'calendar-aware' instances (default is `'standard'` calendar, if `calendar=''` or `None` the instance\n is not calendar aware and some methods, like `dayofwk`, `dayofyr`, `__add__` and `__sub__`, will not work)\n See discussion for issue [#198](https://github.com/Unidata/cftime/issues/198).\n The calendar specific sub-classes are now deprecated, but remain for now\n as stubs that just instantiate the base class and override `__repr__`.\n The default calendar in `cftime.date2num` has been changed from `'standard'` to `None`\n (the calendar associated with first input datetime object is used to define the calendar).\n\n07/20/2020: Version 1.2.1 released.  Fixes a couple of regressions introduced in 1.2.0. See Changelog for details.\n\n7/06/2020:  version 1.2.0 released. New microsecond accurate algorithm for date2num/num2date contributed by [spencerkclark](https://github.com/spencerkclark). Bugs fixed in masked array handling."}, {"name": "cftime", "tags": ["math", "web"], "summary": "Time-handling functionality from netcdf4-python", "text": "5/12/2020:  version 1.1.3 released.  Add isoformat method for compatibility with python datetime (issue #152).\n Make 'standard' default calendar for cftime.datetime so that dayofwk,dayofyr methods don't fail (issue #169).\n\n4/20/2020:  version 1.1.2 released.  Code optimization, fix logic so `only_use_cftime_datetimes=False` works as \n expected (issues [#158](https://github.com/Unidata/cftime/issues/158) and [#165](https://github.com/Unidata/cftime/issues/165)).\n\n3/16/2020:  version 1.1.1 released.  Fix bug in microsecond formatting, ensure identical num2date results if input is an array of times, or a single scalar ([issue #143](https://github.com/Unidata/cftime/issues/143)).\n\n2/12/2020:  version 1.1.0 released.  `cftime.datetime` instances are returned by default from `num2date`\n(instead of returning python datetime instances where possible ([issue #136](https://github.com/Unidata/cftime/issues/136))).  `num2pydate`\nconvenience function added (always returns python datetime instances, [issue #134](https://github.com/Unidata/cftime/issues/134)). Fix for\nfraction seconds in reference date string ([issue #140](https://github.com/Unidata/cftime/issues/140)). Added `daysinmonth` attribute \n([issue #137](https://github.com/Unidata/cftime/issues/137)).\n\n10/25/2019:  version 1.0.4.2 released (fix for [issue #126](https://github.com/Unidata/cftime/issues/126)).\n\n10/21/2019:  version 1.0.4 released.\n\n12/05/2018:  version 1.0.3.4 released (just to fix a problem with the source \ntarball on pypi).\n\n12/05/2018:  version 1.0.3.1 released.  Bugfix release (fixed issue with installation\nwhen cython not installed, regression on 32-bit platforms, workaround for pandas \ncompatibility).\n\n12/01/2018:  version 1.0.3 released. Test coverage with coveralls.io, improved round-tripping accuracy for non-real world calendars (like `360_day`).\n\n10/27/2018:  version 1.0.2 released. Improved accuracy (from approximately 1000 microseconds to 10 microseconds on x86\nplatforms). Refactored calendar calculations now allow for negative reference years. num2date function now more than an\norder of magnitude faster. `months since` units now allowed, but only for `360_day` calendar.\n\n08/15/2018:  version 1.0.1 released.\n\n11/8/2016: `cftime` was split out of the [netcdf4-python](https://github.com/Unidata/netcdf4-python) package.\n\nQuick Start\n* Clone GitHub repository (`git clone https://github.com/Unidata/cftime.git`), or get source tarball from [PyPI](https://pypi.python.org/pypi/cftime). Links to Windows and OS X precompiled binary packages are also available on [PyPI](https://pypi.python.org/pypi/cftime).\n\n* Make sure [numpy](http://www.numpy.org/) and [Cython](http://cython.org/) are\n  installed and you have [Python](https://www.python.org) 2.7 or newer.\n\n* Run `python setup.py build`, then `python setup.py install` (with `sudo` if necessary).\n\n* To run all the tests, execute `py.test`.\n\nDocumentation\nSee the online [docs](http://unidata.github.io/cftime) for more details."}, {"name": "cftime", "tags": ["math", "web"], "summary": "Time-handling functionality from netcdf4-python", "text": "This library is used to handle time-related functionality and provide a consistent interface for working with dates and times in Python. With cftime, developers can easily convert between different date and time representations, perform date arithmetic, and more."}, {"name": "chainlit", "tags": ["math", "ml", "web"], "summary": "Build Conversational AI.", "text": "Installation\n\nOpen a terminal and run:\n\nIf this opens the `hello app` in your browser, you're all set!\n\nDevelopment version\n\nThe latest in-development version can be installed straight from GitHub with:\n\n(Requires Node and pnpm installed on the system.)\n\nQuickstart\n\nPure Python\n\nCreate a new file `demo.py` with the following code:\n\nNow run it!\n\nMore Examples - Cookbook\n\nYou can find various examples of Chainlit apps [here](https://github.com/Chainlit/cookbook) that leverage tools and services such as OpenAI, Anthropi\u0441, LangChain, LlamaIndex, ChromaDB, Pinecone and more.\n\nTell us what you would like to see added in Chainlit using the Github issues or on [Discord](https://discord.gg/k73SQ3FyUh).\n\nContributing\n\nAs an open-source initiative in a rapidly evolving domain, we welcome contributions, be it through the addition of new features or the improvement of documentation.\n\nFor detailed information on how to contribute, see [here](/CONTRIBUTING.md).\n\nLicense\n\nChainlit is open-source and licensed under the [Apache 2.0](LICENSE) license."}, {"name": "chainlit", "tags": ["math", "ml", "web"], "summary": "Build Conversational AI.", "text": "This library is used to quickly prototype and build Conversational AI applications with ease. It provides a streamlined development process through its simple installation and example-driven documentation, allowing developers to focus on creating innovative conversational experiences."}, {"name": "chdb", "tags": ["cli", "data", "math", "ui", "web"], "summary": "chDB is an in-process OLAP SQL Engine powered by ClickHouse", "text": "chDB\n\n> chDB is an in-process SQL OLAP Engine powered by ClickHouse  [^1]\n> For more details: [The birth of chDB](https://auxten.com/the-birth-of-chdb/)\n\nFeatures\n\n* In-process SQL OLAP Engine, powered by ClickHouse\n* No need to install ClickHouse\n* Minimized data copy from C++ to Python with [python memoryview](https://docs.python.org/3/c-api/memoryview.html)\n* Input&Output support Parquet, CSV, JSON, Arrow, ORC and 60+[more](https://clickhouse.com/docs/en/interfaces/formats) formats, [samples](tests/format_output.py)\n* Support Python DB API 2.0, [example](examples/dbapi.py)\n\nArch\n\nGet Started\nGet started with **chdb** using our [Installation and Usage Examples](https://clickhouse.com/docs/en/chdb)\n\nInstallation\nCurrently, chDB supports Python 3.8+ on macOS and Linux (x86_64 and ARM64).\n\nUsage\n\nRun in command line\n> `python3 -m chdb SQL [OutputFormat]`\n\nData Input\nThe following methods are available to access on-disk and in-memory data formats:\n\nFor more details, see [examples/connect.py](examples/connect.py).\n\nYou can execute SQL and return desired format data.\n\nWork with Parquet or CSV\n\nPandas dataframe output\n\nQuery On Pandas DataFrame\n\n  \ufe0f Query with Stateful Session\n\nsee also: [test_stateful.py](tests/test_stateful.py).\n\nSome notes on chDB Python UDF(User Defined Function) decorator.\n1. The function should be stateless. So, only UDFs are supported, not UDAFs(User Defined Aggregation Function).\n2. Default return type is String. If you want to change the return type, you can pass in the return type as an argument.\n3. The function should take in arguments of type String. As the input is TabSeparated, all arguments are strings.\n4. The function will be called for each line of input. Something like this:\n5. The function should be pure python function. You SHOULD import all python modules used IN THE FUNCTION.\n6. Python interpertor used is the same as the one used to run the script. Get from `sys.executable`\n\nsee also: [test_udf.py](tests/test_udf.py).\n\nProcess large datasets with constant memory usage through chunked streaming.\n\n**Important Note**: When using streaming queries, if the `StreamingResult` is not fully consumed (due to errors or early termination), you must explicitly call `stream_result.close()` to release resources, or use the `with` statement for automatic cleanup. Failure to do so may block subsequent queries.\n\nFor more details, see [test_streaming_query.py](tests/test_streaming_query.py) and [test_arrow_record_reader_deltalake.py](tests/test_arrow_record_reader_deltalake.py).\n\nQuery on Pandas DataFrame\n\nQuery on Arrow Table\n\nQuery on chdb.PyReader class instance\n\n1. You must inherit from chdb.PyReader class and implement the `read` method.\n2. The `read` method should:\n3. An optional `get_schema` method can be implemented to return the schema of the table. The prototype is `def get_schema(self) -> List[Tuple[str, str]]:`, the return value is a list of tuples, each tuple contains the column name and the column type. The column type should be one of the following: https://clickhouse.com/docs/en/sql-reference/data-types\n\nsee also: [test_query_py.py](tests/test_query_py.py) and [test_query_json.py](tests/test_query_json.py).\n\nJSON Type Inference\n\nchDB automatically converts Python dictionary objects to ClickHouse JSON types from these sources:\n\n1. **Pandas DataFrame**\n\n2. **chdb.PyReader**\n\nWhen converting Python dictionary objects to JSON columns:\n\n1. **Nested Structures**\n\n2. **Primitive Types**\n\n3. **Complex Objects**\n\nLimitations\n\n1. Column types supported: pandas.Series, pyarrow.array, chdb.PyReader\n1. Data types supported: Int, UInt, Float, String, Date, DateTime, Decimal\n1. Python Object type will be converted to String\n1. Pandas DataFrame performance is all of the best, Arrow Table is better than PyReader\n\nFor more examples, see [examples](examples) and [tests](tests).\n\nDemos and Examples\n\n- [Project Documentation](https://clickhouse.com/docs/en/chdb) and [Usage Examples](https://clickhouse.com/docs/en/chdb/install/python)\n- [Colab Notebooks](https://colab.research.google.com/drive/1-zKB6oKfXeptggXi0kUX87iR8ZTSr4P3?usp=sharing) and other [Script Examples](examples)\n\nBenchmark\n\n- [ClickBench of embedded engines](https://benchmark.clickhouse.com/#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)\n\n- [chDB vs Pandas](https://colab.research.google.com/drive/1FogLujJ_-ds7RGurDrUnK-U0IW8a8Qd0)\n\n- [Benchmark on DataFrame: chDB Pandas DuckDB Polars](https://benchmark.clickhouse.com/#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)\n\nDocumentation\n- For chdb specific examples and documentation refer to [chDB docs](https://clickhouse.com/docs/en/chdb)\n- For SQL syntax, please refer to [ClickHouse SQL Reference](https://clickhouse.com/docs/en/sql-reference/syntax)\n\nEvents\n\n- Demo chDB at [ClickHouse v23.7 livehouse!](https://t.co/todc13Kn19) and [Slides](https://docs.google.com/presentation/d/1ikqjOlimRa7QAg588TAB_Fna-Tad2WMg7_4AgnbQbFA/edit?usp=sharing)\n\nContributing\nContributions are what make the open source community such an amazing place to be learn, inspire, and create. Any contributions you make are **greatly appreciated**.\nThere are something you can help:\n- [ ] Help test and report bugs\n- [ ] Help improve documentation\n- [ ] Help improve code quality and performance\n\nBindings\n\nWe welcome bindings for other languages, please refer to [bindings](bindings.md) for more details.\n\nVersion Guide\n\nPlease refer to [VERSION-GUIDE.md](VERSION-GUIDE.md) for more details.\n\nPaper\n\n- [ClickHouse - Lightning Fast Analytics for Everyone](https://www.vldb.org/pvldb/vol17/p3731-schulze.pdf)\n\nLicense\nApache 2.0, see [LICENSE](LICENSE.txt) for more information.\n\nAcknowledgments\nchDB is mainly based on [ClickHouse](https://github.com/ClickHouse/ClickHouse) [^1]\nfor trade mark and other reasons, I named it chDB.\n\nContact\n- Discord: [https://discord.gg/D2Daa2fM5K](https://discord.gg/D2Daa2fM5K)\n- Email: auxten@clickhouse.com\n- Twitter: [@chdb](https://twitter.com/chdb_io)\n\n[^1]: ClickHouse\u00ae is a trademark of ClickHouse Inc. All trademarks, service marks, and logos mentioned or depicted are the property of their respective owners. The use of any third-party trademarks, brand names, product names, and company names does not imply endorsement, affiliation, or association with the respective owners."}, {"name": "chdb", "tags": ["cli", "data", "math", "ui", "web"], "summary": "chDB is an in-process OLAP SQL Engine powered by ClickHouse", "text": "This library is used to enable in-process SQL OLAP capabilities within a Python application, leveraging the power of ClickHouse without requiring a separate installation. With chDB, developers can perform complex analytics tasks directly within their Python code, utilizing various input/output formats and supported by the Python DB API 2.0 standard."}, {"name": "chex", "tags": ["dev", "math", "ml", "web"], "summary": "Chex: Testing made fun, in JAX!", "text": "Chex\n\nChex is a library of utilities for helping to write reliable JAX code.\n\nThis includes utils to help:\n\n* Instrument your code (e.g. assertions, warnings)\n* Debug (e.g. transforming `pmaps` in `vmaps` within a context manager).\n* Test JAX code across many `variants` (e.g. jitted vs non-jitted).\n\nInstallation\n\nYou can install the latest released version of Chex from PyPI via:\n\nor you can install the latest development version from GitHub:\n\nModules Overview\n\nDataclass ([dataclass.py](https://github.com/deepmind/chex/blob/master/chex/_src/dataclass.py))\n\nDataclasses are a popular construct introduced by Python 3.7 to allow to\neasily specify typed data structures with minimal boilerplate code. They are\nnot, however, compatible with JAX and\n[dm-tree](https://github.com/deepmind/tree) out of the box.\n\nIn Chex we provide a JAX-friendly dataclass implementation reusing python [dataclasses](https://docs.python.org/3/library/dataclasses.html#module-dataclasses).\n\nChex implementation of `dataclass` registers dataclasses as internal [_PyTree_\nnodes](https://jax.readthedocs.io/en/latest/pytrees.html) to ensure\ncompatibility with JAX data structures.\n\nIn addition, we provide a class wrapper that exposes dataclasses as\n`collections.Mapping` descendants which allows to process them\n(e.g. (un-)flatten) in `dm-tree` methods as usual Python dictionaries.\nSee [`@mappable_dataclass`](https://github.com/deepmind/chex/blob/master/chex/_src/dataclass.py#L27)\ndocstring for more details.\n\nExample:\n\n**NOTE**: Unlike standard Python 3.7 dataclasses, Chex\ndataclasses cannot be constructed using positional arguments. They support\nconstruction arguments provided in the same format as the Python dict\nconstructor. Dataclasses can be converted to tuples with the `from_tuple` and\n`to_tuple` methods if necessary.\n\nAssertions ([asserts.py](https://github.com/deepmind/chex/blob/master/chex/_src/asserts.py))\n\nOne limitation of PyType annotations for JAX is that they do not support the\nspecification of `DeviceArray` ranks, shapes or dtypes. Chex includes a number\nof functions that allow flexible and concise specification of these properties.\n\nE.g. suppose you want to ensure that all tensors `t1`, `t2`, `t3` have the same\nshape, and that tensors `t4`, `t5` have rank `2` and (`3` or `4`), respectively.\n\nMore examples:\n\nSee `asserts.py`\n[documentation](https://chex.readthedocs.io/en/latest/api.html#assertions) to\nfind all supported assertions.\n\nIf you cannot find a specific assertion, please consider making a pull request\nor openning an issue on\n[the bug tracker](https://github.com/deepmind/chex/issues).\n\nOptional Arguments\n\nAll chex assertions support the following optional kwargs for manipulating the\nemitted exception messages:\n\n* `custom_message`: A string to include into the emitted exception messages.\n* `include_default_message`: Whether to include the default Chex message into\n  the emitted exception messages.\n* `exception_type`: An exception type to use. `AssertionError` by default.\n\nFor example, the following code:\n\nwill raise a `ValueError` that includes a step number when `params` get polluted\nwith `NaNs` or `None`s.\n\nStatic and Value (aka *Runtime*) Assertions\n\nChex divides all assertions into 2 classes: ***static*** and ***value***\nassertions.\n\n1.  ***static*** assertions use anything except concrete values of tensors.\n\n2.  ***value*** assertions require access to tensor values, which are not\n\nTo enable value assertions in a jitted function, it can be decorated with\n`chex.chexify()` wrapper. Example:\n\nSee\n[this docstring](https://chex.readthedocs.io/en/latest/api.html#chex.chexify)\nfor more detail on `chex.chexify()`.\n\nJAX Tracing Assertions\n\nJAX re-traces JIT'ted function every time the structure of passed arguments\nchanges. Often this behavior is inadvertent and leads to a significant\nperformance drop which is hard to debug. [@chex.assert_max_traces](https://github.com/deepmind/chex/blob/master/chex/_src/asserts.py#L44)\ndecorator asserts that the function is not re-traced more than `n` times during\nprogram execution."}, {"name": "chex", "tags": ["dev", "math", "ml", "web"], "summary": "Chex: Testing made fun, in JAX!", "text": "Global trace counter can be cleared by calling\n`chex.clear_trace_counter()`. This function be used to isolate unittests relying\non `@chex.assert_max_traces`.\n\nExamples:\n\nCan be used with `jax.pmap()` as well:\n\nSee\n[HowJAX primitives work](https://jax.readthedocs.io/en/latest/notebooks/How_JAX_primitives_work.html)\nsection for more information about tracing.\n\nWarnings ([warnigns.py](https://github.com/deepmind/chex/blob/master/chex/_src/warnings.py))\n\nIn addition to hard assertions Chex also offers utilities to add common\nwarnings, such as specific types of deprecation warnings.\n\nTest variants ([variants.py](https://github.com/deepmind/chex/blob/master/chex/_src/variants.py))\n\nJAX relies extensively on code transformation and compilation, meaning that it\ncan be hard to ensure that code is properly tested. For instance, just testing a\npython function using JAX code will not cover the actual code path that is\nexecuted when jitted, and that path will also differ whether the code is jitted\nfor CPU, GPU, or TPU. This has been a source of obscure and hard to catch bugs\nwhere XLA changes would lead to undesirable behaviours that however only\nmanifest in one specific code transformation.\n\nVariants make it easy to ensure that unit tests cover different \u2018variations\u2019 of\na function, by providing a simple decorator that can be used to repeat any test\nunder all (or a subset) of the relevant code transformations.\n\nE.g. suppose you want to test the output of a function `fn` with or without jit.\nYou can use `chex.variants` to run the test with both the jitted and non-jitted\nversion of the function by simply decorating a test method with\n`@chex.variants`, and then using `self.variant(fn)` in place of `fn` in the body\nof the test.\n\nIf you define the function in the test method, you may also use `self.variant`\nas a decorator in the function definition. For example:\n\nExample of parameterized test:\n\nChex currently supports the following variants:\n\n* `with_jit` -- applies `jax.jit()` transformation to the function.\n* `without_jit` -- uses the function as is, i.e. identity transformation.\n* `with_device` -- places all arguments (except specified in `ignore_argnums`\n   argument) into device memory before applying the function.\n* `without_device` -- places all arguments in RAM before applying the function.\n* `with_pmap` -- applies `jax.pmap()` transformation to the function (see notes below).\n\nSee documentation in [variants.py](https://github.com/deepmind/chex/blob/master/chex/_src/variants.py) for more details on the supported variants.\nMore examples can be found in [variants_test.py](https://github.com/deepmind/chex/blob/master/chex/_src/variants_test.py).\n\nVariants notes\n\n* Test classes that use `@chex.variants` must inherit from\n`chex.TestCase` (or any other base class that unrolls tests generators\nwithin `TestCase`, e.g. `absl.testing.parameterized.TestCase`).\n\n* **[`jax.vmap`]** All variants can be applied to a vmapped function;\nplease see an example in [variants_test.py](https://github.com/deepmind/chex/blob/master/chex/_src/variants_test.py) (`test_vmapped_fn_named_params` and\n`test_pmap_vmapped_fn`).\n\n* **[`@chex.all_variants`]** You can get all supported variants\nby using the decorator `@chex.all_variants`."}, {"name": "chex", "tags": ["dev", "math", "ml", "web"], "summary": "Chex: Testing made fun, in JAX!", "text": "* **[`with_pmap` variant]** `jax.pmap(fn)`\n([doc](https://jax.readthedocs.io/en/latest/jax.html#jax.pmap)) performs\nparallel map of `fn` onto multiple devices. Since most tests run in a\nsingle-device environment (i.e. having access to a single CPU or GPU), in which\ncase `jax.pmap` is a functional equivalent to `jax.jit`, ` with_pmap` variant is\nskipped by default (although it works fine with a single device). Below we\ndescribe  a way to properly test `fn` if it is supposed to be used in\nmulti-device environments (TPUs or multiple CPUs/GPUs). To disable skipping\n`with_pmap` variants in case of a single device, add\n`--chex_skip_pmap_variant_if_single_device=false` to your test command.\n\nFakes ([fake.py](https://github.com/deepmind/chex/blob/master/chex/_src/fake.py))\n\nDebugging in JAX is made more difficult by code transformations such as `jit`\nand `pmap`, which introduce optimizations that make code hard to inspect and\ntrace. It can also be difficult to disable those transformations during\ndebugging as they can be called at several places in the underlying\ncode. Chex provides tools to globally replace `jax.jit` with a no-op\ntransformation and `jax.pmap` with a (non-parallel) `jax.vmap`, in order to more\neasily debug code in a single-device context.\n\nFor example, you can use Chex to fake `pmap` and have it replaced with a `vmap`.\nThis can be achieved by wrapping your code with a context manager:\n\nThe same functionality can also be invoked with `start` and `stop`:\n\nIn addition, you can fake a real multi-device test environment with a\nmulti-threaded CPU. See section **Faking multi-device test environments** for\nmore details.\n\nSee documentation in [fake.py](https://github.com/deepmind/chex/blob/master/chex/_src/fake.py) and examples in [fake_test.py](https://github.com/deepmind/chex/blob/master/chex/_src/fake_test.py) for more details.\n\nFaking multi-device test environments\n\nIn situations where you do not have easy access to multiple devices, you can\nstill test parallel computation using single-device multi-threading.\n\nIn particular, one can force XLA to use a single CPU's threads as separate\ndevices, i.e. to fake a real multi-device environment with a multi-threaded one.\nThese two options are theoretically equivalent from XLA perspective because they\nexpose the same interface and use identical abstractions.\n\nChex has a flag `chex_n_cpu_devices` that specifies a number of CPU threads to\nuse as XLA devices.\n\nTo set up a multi-threaded XLA environment for `absl` tests, define\n`setUpModule` function in your test module:\n\nNow you can launch your test with `python test.py --chex_n_cpu_devices=N` to run\nit in multi-device regime. Note that **all** tests within a module will have an\naccess to `N` devices.\n\nMore examples can be found in [variants_test.py](https://github.com/deepmind/chex/blob/master/chex/_src/variants_test.py), [fake_test.py](https://github.com/deepmind/chex/blob/master/chex/_src/fake_test.py) and [fake_set_n_cpu_devices_test.py](https://github.com/deepmind/chex/blob/master/chex/_src/fake_set_n_cpu_devices_test.py).\n\nUsing named dimension sizes.\n\nChex comes with a small utility that allows you to package a collection of\ndimension sizes into a single object. The basic idea is:\n\nString lookups are translated integer tuples. For instance, let's say\n`batch_size == 3`, `sequence_len = 5` and `embedding_dim = 7`, then\n\nYou can also assign dimension sizes dynamically as follows:\n\nFor more examples, see [chex.Dimensions](https://chex.readthedocs.io/en/latest/api.html#chex.Dimensions)\ndocumentation.\n\nCiting Chex\n\nThis repository is part of the [DeepMind JAX Ecosystem], to cite Chex please use\nthe following citation:"}, {"name": "chex", "tags": ["dev", "math", "ml", "web"], "summary": "Chex: Testing made fun, in JAX!", "text": "This library is used to simplify the process of writing reliable JAX code by providing utilities for instrumentation, debugging, and testing. It helps developers write robust and efficient code with features like assertion and warning handling, debug transformations, and multi-variant testing."}, {"name": "clarifai-grpc", "tags": ["data", "math", "ml", "web"], "summary": "Clarifai gRPC API Client", "text": "Clarifai Python gRPC Client\n\nThis is the official Clarifai gRPC Python client for interacting with our powerful recognition\n[API](https://docs.clarifai.com).\nClarifai provides a platform for data scientists, developers, researchers and enterprises to master the entire\nartificial intelligence lifecycle. Gather valuable business insights from images, video and text using computer vision\nand natural language processing.\n\n* Try the Clarifai demo at: https://clarifai.com/demo\n* Sign up for a free account at: https://portal.clarifai.com/signup\n* Read the documentation at: https://docs.clarifai.com/\n\n(https://pypi.python.org/pypi/clarifai-grpc)\n(https://github.com/Clarifai/clarifai-python-grpc/actions)\n\nInstallation\n\nVersioning\n\nThis library doesn't use semantic versioning. The first two version numbers (`X.Y` out of `X.Y.Z`) follow the API (backend) versioning, and\nwhenever the API gets updated, this library follows it.\n\nThe third version number (`Z` out of `X.Y.Z`) is used by this library for any independent releases of library-specific improvements and bug fixes.\n\nGetting started\n\nConstruct the `V2Stub` object using which you'll access all the Clarifai API functionality:\n\n> Alternatives to the encrypted gRPC channel (`ClarifaiChannel.get_grpc_channel()`) are:\n> - the HTTPS+JSON channel (`ClarifaiChannel.get_json_channel()`), and\n> - the unencrypted gRPC channel (`ClarifaiChannel.get_insecure_grpc_channel()`).\n>\n> We only recommend them in special cases.\n\nPredict concepts in an image:\n\nSee [the Clarifai API documentation](https://docs.clarifai.com/) for all available functionality.\n\nTroubleshooting\n\nI get the following error when installing the library: `Failed building wheel for grpcio`\n\nTry upgrading **setuptools** to a version `40.7.1` or higher.\n\nSource: https://github.com/grpc/grpc/issues/17829"}, {"name": "clarifai-grpc", "tags": ["data", "math", "ml", "web"], "summary": "Clarifai gRPC API Client", "text": "This library is used to enable developers to interact with the Clarifai gRPC API and integrate AI-powered computer vision and natural language processing capabilities into their applications, unlocking valuable business insights from images, video, and text. With this library, developers can build a range of AI-driven projects, from image classification and object detection to text analysis and sentiment analysis."}, {"name": "clearml-agent", "tags": ["cli", "data", "math", "ml", "ui", "web"], "summary": "ClearML Agent - Auto-Magical DevOps for Deep Learning", "text": "ClearML-Agent\n\n*Formerly known as Trains Agent*\n\n* Run jobs (experiments) on any local or cloud based resource\n* Implement optimized resource utilization policies\n* Deploy execution environments with either virtualenv or fully docker containerized with zero effort\n* Launch-and-Forget service containers\n* [Cloud autoscaling](https://clear.ml/docs/latest/docs/guides/services/aws_autoscaler)\n* [Customizable cleanup](https://clear.ml/docs/latest/docs/guides/services/cleanup_service)\n* Advanced [pipeline building and execution](https://clear.ml/docs/latest/docs/guides/frameworks/pytorch/notebooks/table/tabular_training_pipeline)\n\nIt is a zero configuration fire-and-forget execution agent, providing a full ML/DL cluster solution.\n\n**Full Automation in 5 steps**\n\n1. ClearML Server [self-hosted](https://github.com/clearml/clearml-server)\n   or [free tier hosting](https://app.clear.ml)\n2. `pip install clearml-agent` ([install](#installing-the-clearml-agent) the ClearML Agent on any GPU machine:\n   on-premises / cloud / ...)\n3. Create a [job](https://clear.ml/docs/latest/docs/apps/clearml_task) or\n   add [ClearML](https://github.com/clearml/clearml) to your code with just 2 lines of code\n4. Change the [parameters](#using-the-clearml-agent) in the UI & schedule for [execution](#using-the-clearml-agent) (or\n   automate with an [AutoML pipeline](#automl-and-orchestration-pipelines-))\n5. :chart_with_downwards_trend: :chart_with_upwards_trend: :eyes:  :beer:\n\n\"All the Deep/Machine-Learning DevOps your research needs, and then some... Because ain't nobody got time for that\"\n\n**Try ClearML now** [Self Hosted](https://github.com/clearml/clearml-server)\nor [Free tier Hosting](https://app.clear.ml)\n\nSimple, Flexible Experiment Orchestration\n\n**The ClearML Agent was built to address the DL/ML R&D DevOps needs:**\n\n* Easily add & remove machines from the cluster\n* Reuse machines without the need for any dedicated containers or images\n* **Combine GPU resources across any cloud and on-prem**\n* **No need for yaml / json / template configuration of any kind**\n* **User friendly UI**\n* Manageable resource allocation that can be used by researchers and engineers\n* Flexible and controllable scheduler with priority support\n* Automatic instance spinning in the cloud\n\n**Using the ClearML Agent, you can now set up a dynamic cluster with \\*epsilon DevOps**\n\n*epsilon - Because we are :triangular_ruler: and nothing is really zero work\n\nKubernetes Integration (Optional)\n\nWe think Kubernetes is awesome, but it is not a must to get started with remote execution agents and cluster management.\nWe designed `clearml-agent` so you can run both bare-metal and on top of Kubernetes, in any combination that fits your environment.\n\nYou can find the Dockerfiles in the [docker folder](./docker) and the helm Chart in https://github.com/clearml/clearml-helm-charts\n\nBenefits of integrating existing Kubernetes cluster with ClearML\n\n- ClearML-Agent adds the missing scheduling capabilities to your Kubernetes cluster\n- Users do not need to have direct Kubernetes access!\n- Easy learning curve with UI and CLI requiring no DevOps knowledge from end users\n- Unlike other solutions, ClearML-Agents work in tandem with other customers of your Kubernetes cluster \n- Allows for more flexible automation from code, building pipelines and visibility\n- A programmatic interface for easy CI/CD workflows, enabling GitOps to trigger jobs inside your cluster\n- Seamless integration with the ClearML ML/DL/GenAI experiment manager\n- Web UI for customization, scheduling & prioritization of jobs\n- **Enterprise Features**: RBAC, vault, multi-tenancy, scheduler, quota management, fractional GPU support"}, {"name": "clearml-agent", "tags": ["cli", "data", "math", "ml", "ui", "web"], "summary": "ClearML Agent - Auto-Magical DevOps for Deep Learning", "text": "**Run the agent in Kubernetes Glue mode an map ClearML jobs directly to K8s jobs:**\n- Use the [ClearML Agent Helm Chart](https://github.com/clearml/clearml-helm-charts/tree/main/charts/clearml-agent) to spin an agent pod acting as a controller\n  - Or run the [clearml-k8s glue](https://github.com/clearml/clearml-agent/blob/master/examples/k8s_glue_example.py) on\n- The clearml-k8s glue pulls jobs from the ClearML job execution queue and prepares a Kubernetes job (based on provided\n  yaml template)\n- Inside each pod the clearml-agent will install the job (experiment) environment and spin and monitor the\n  experiment's process, fully visible in the clearml UI\n- Benefits: Kubernetes full view of all running jobs in the system\n- **Enterprise Features**\n  - Full scheduler features added on Top of Kubernetes, with quota/over-quota management, priorities and order.\n  - Fractional GPU support, allowing multiple isolated containers sharing the same GPU with memory/compute limit per container\n\nSLURM (Optional)\n\nYes! Slurm integration is available, check the [documentation](https://clear.ml/docs/latest/docs/clearml_agent/#slurm) for further details\n\nUsing the ClearML Agent\n\n**Full scale HPC with a click of a button**\n\nThe ClearML Agent is a job scheduler that listens on job queue(s), pulls jobs, sets the job environments, executes the\njob and monitors its progress.\n\nAny 'Draft' experiment can be scheduled for execution by a ClearML agent.\n\nA previously run experiment can be put into 'Draft' state by either of two methods:\n\n* Using the **'Reset'** action from the experiment right-click context menu in the ClearML UI - This will clear any\n  results and artifacts the previous run had created.\n* Using the **'Clone'** action from the experiment right-click context menu in the ClearML UI - This will create a new \n  'Draft' experiment with the same configuration as the original experiment.\n\nAn experiment is scheduled for execution using the **'Enqueue'** action from the experiment right-click context menu in\nthe ClearML UI and selecting the execution queue.\n\nSee [creating an experiment and enqueuing it for execution](#from-scratch).\n\nOnce an experiment is enqueued, it will be picked up and executed by a ClearML Agent monitoring this queue.\n\nThe ClearML UI Workers & Queues page provides ongoing execution information:\n\n- Workers Tab: Monitor you cluster\n- Queues Tab:\n\nWhat The ClearML Agent Actually Does\n\nThe ClearML Agent executes experiments using the following process:\n\n- Create a new virtual environment (or launch the selected docker image)\n- Clone the code into the virtual-environment (or inside the docker)\n- Install python packages based on the package requirements listed for the experiment\n- Execute the code, while monitoring the process\n- Log all stdout/stderr in the ClearML UI, including the cloning and installation process, for easy debugging\n- Monitor the execution and allow you to manually abort the job using the ClearML UI (or, in the unfortunate case of a\n  code crash, catch the error and signal the experiment has failed)\n\nSystem Design & Flow\n\nInstalling the ClearML Agent\n\nClearML Agent Usage Examples\n\nFull Interface and capabilities are available with\n\nConfiguring the ClearML Agent\n\nNote: The ClearML Agent uses a cache folder to cache pip packages, apt packages and cloned repositories. The default\nClearML Agent cache folder is `~/.clearml`."}, {"name": "clearml-agent", "tags": ["cli", "data", "math", "ml", "ui", "web"], "summary": "ClearML Agent - Auto-Magical DevOps for Deep Learning", "text": "See full details in your configuration file at `~/clearml.conf`.\n\nNote: The **ClearML Agent** extends the **ClearML** configuration file `~/clearml.conf`.\nThey are designed to share the same configuration file, see example [here](docs/clearml.conf)\n\nRunning the ClearML Agent\n\nFor debug and experimentation, start the ClearML agent in `foreground` mode, where all the output is printed to screen:\n\nFor actual service mode, all the stdout will be stored automatically into a temporary file (no need to pipe).\nNotice: with `--detached` flag, the *clearml-agent* will be running in the background\n\nGPU allocation is controlled via the standard OS environment `NVIDIA_VISIBLE_DEVICES` or `--gpus` flag (or disabled\nwith `--cpu-only`).\n\nIf no flag is set, and `NVIDIA_VISIBLE_DEVICES` variable doesn't exist, all GPUs will be allocated for\nthe `clearml-agent`. \nIf `--cpu-only` flag is set, or `NVIDIA_VISIBLE_DEVICES=\"none\"`, no gpu will be allocated for\nthe `clearml-agent`.\n\nExample: spin two agents, one per GPU on the same machine:\n\nNotice: with `--detached` flag, the *clearml-agent* will run in the background\n\nExample: spin two agents, pulling from dedicated `dual_gpu` queue, two GPUs per agent\n\nStarting the ClearML Agent in docker mode\n\nFor debug and experimentation, start the ClearML agent in `foreground` mode, where all the output is printed to screen\n\nFor actual service mode, all the stdout will be stored automatically into a file (no need to pipe).\nNotice: with `--detached` flag, the *clearml-agent* will run in the background\n\nExample: spin two agents, one per gpu on the same machine, with default `nvidia/cuda:11.0.3-cudnn8-runtime-ubuntu20.04`\ndocker:\n\nExample: spin two agents, pulling from dedicated `dual_gpu` queue, two GPUs per agent, with default \n`nvidia/cuda:11.0.3-cudnn8-runtime-ubuntu20.04` docker:\n\nStarting the ClearML Agent - Priority Queues\n\nPriority Queues are also supported, example use case:\n\nHigh priority queue: `important_jobs`, low priority queue: `default`\n\nThe **ClearML Agent** will first try to pull jobs from the `important_jobs` queue, and only if it is empty, the agent \nwill try to pull from the `default` queue.\n\nAdding queues, managing job order within a queue, and moving jobs between queues, is available using the Web UI, see\nexample on our [free server](https://app.clear.ml/workers-and-queues/queues)\n\nStopping the ClearML Agent\n\nTo stop a **ClearML Agent** running in the background, run the same command line used to start the agent with `--stop`\nappended. For example, to stop the first of the above shown same machine, single gpu agents:\n\nHow do I create an experiment on the ClearML Server?\n\n* Integrate [ClearML](https://github.com/clearml/clearml) with your code\n* Execute the code on your machine (Manually / PyCharm / Jupyter Notebook)\n* As your code is running, **ClearML** creates an experiment logging all the necessary execution information:\n\nYou now have a 'template' of your experiment with everything required for automated execution\n\n* In the ClearML UI, right-click on the experiment and select 'clone'. A copy of your experiment will be created.\n* You now have a new draft experiment cloned from your original experiment, feel free to edit it\n* Schedule the newly created experiment for execution: right-click the experiment and select 'enqueue'\n\nClearML-Agent Services Mode"}, {"name": "clearml-agent", "tags": ["cli", "data", "math", "ml", "ui", "web"], "summary": "ClearML Agent - Auto-Magical DevOps for Deep Learning", "text": "ClearML-Agent Services is a special mode of ClearML-Agent that provides the ability to launch long-lasting jobs that\npreviously had to be executed on local / dedicated machines. It allows a single agent to launch multiple dockers (Tasks)\nfor different use cases: \n* Auto-scaler service (spinning instances when the need arises and the budget allows)\n* Controllers (Implementing pipelines and more sophisticated DevOps logic)\n* Optimizer (such as Hyperparameter Optimization or sweeping)\n* Application (such as interactive Bokeh apps for increased data transparency)\n\nClearML-Agent Services mode will spin **any** task enqueued into the specified queue. Every task launched by\nClearML-Agent Services will be registered as a new node in the system, providing tracking and transparency capabilities.\nCurrently, clearml-agent in services-mode supports CPU only configuration. ClearML-Agent services mode can be launched\nalongside GPU agents.\n\n**Note**: It is the user's responsibility to make sure the proper tasks are pushed into the specified queue.\n\nAutoML and Orchestration Pipelines\n\nThe ClearML Agent can also be used to implement AutoML orchestration and Experiment Pipelines in conjunction with the\nClearML package.\n\nSample AutoML & Orchestration examples can be found in the\nClearML [example/automation](https://github.com/clearml/clearml/tree/master/examples/automation) folder.\n\nAutoML examples:\n\n- [Toy Keras training experiment](https://github.com/clearml/clearml/blob/master/examples/optimization/hyper-parameter-optimization/base_template_keras_simple.py)\n- [Random Search over the above Keras experiment-template](https://github.com/clearml/clearml/blob/master/examples/automation/manual_random_param_search_example.py)\n\nExperiment Pipeline examples:\n\n- [First step experiment](https://github.com/clearml/clearml/blob/master/examples/automation/task_piping_example.py)\n- [Second step experiment](https://github.com/clearml/clearml/blob/master/examples/automation/toy_base_task.py)\n\nLicense\n\nApache License, Version 2.0 (see the [LICENSE](https://www.apache.org/licenses/LICENSE-2.0.html) for more information)"}, {"name": "clearml-agent", "tags": ["cli", "data", "math", "ml", "ui", "web"], "summary": "ClearML Agent - Auto-Magical DevOps for Deep Learning", "text": "This library is used to automate the execution of machine learning and deep learning jobs on local or cloud-based resources with optimized resource utilization policies. It provides a zero-configuration, fire-and-forget execution agent for building and executing full ML/DL cluster solutions in just 5 steps."}, {"name": "click", "tags": ["cli"], "summary": "Composable command line interface toolkit", "text": "Click\n\nClick is a Python package for creating beautiful command line interfaces\nin a composable way with as little code as necessary. It's the \"Command\nLine Interface Creation Kit\". It's highly configurable but comes with\nsensible defaults out of the box.\n\nIt aims to make the process of writing command line tools quick and fun\nwhile also preventing any frustration caused by the inability to\nimplement an intended CLI API.\n\nClick in three points:\n\n-   Arbitrary nesting of commands\n-   Automatic help page generation\n-   Supports lazy loading of subcommands at runtime\n\nA Simple Example\n\nDonate\n\nThe Pallets organization develops and supports Click and other popular\npackages. In order to grow the community of contributors and users, and\nallow the maintainers to devote more time to the projects, [please\ndonate today][].\n\nContributing\n\nSee our [detailed contributing documentation][contrib] for many ways to\ncontribute, including reporting issues, requesting features, asking or answering\nquestions, and making PRs."}, {"name": "click", "tags": ["cli"], "summary": "Composable command line interface toolkit", "text": "This library is used to create beautiful and customizable command line interfaces in a composable way with minimal code required, while also generating automatic help pages for user convenience. With Click, developers can quickly build CLI tools with features such as arbitrary nesting of commands and lazy loading of subcommands at runtime."}, {"name": "clickhouse-sqlalchemy", "tags": ["data", "math"], "summary": "Simple ClickHouse SQLAlchemy Dialect", "text": "ClickHouse SQLAlchemy\n=====================\n\nClickHouse dialect for SQLAlchemy to `ClickHouse database `_.\n\n.. image:: https://img.shields.io/pypi/v/clickhouse-sqlalchemy.svg\n\n.. image:: https://img.shields.io/pypi/l/clickhouse-sqlalchemy.svg\n\n.. image:: https://img.shields.io/pypi/pyversions/clickhouse-sqlalchemy.svg\n\n.. image:: https://img.shields.io/pypi/dm/clickhouse-sqlalchemy.svg\n\n   :target: https://github.com/xzkostyan/clickhouse-sqlalchemy/actions/workflows/actions.yml\n\nDocumentation\n=============\n\nDocumentation is available at https://clickhouse-sqlalchemy.readthedocs.io.\n\nUsage\n=====\n\nSupported interfaces:\n\n- **native** [recommended] (TCP) via `clickhouse-driver `\n- **async native** (TCP) via `asynch `\n- **http** via requests\n\nDefine table\n\nInsert some data\n\nAnd query inserted data\n\nLicense\n=======\n\nClickHouse SQLAlchemy is distributed under the `MIT license\n`_."}, {"name": "clickhouse-sqlalchemy", "tags": ["data", "math"], "summary": "Simple ClickHouse SQLAlchemy Dialect", "text": "This library is used to seamlessly integrate ClickHouse databases into SQLAlchemy-enabled applications, allowing developers to easily interact with ClickHouse tables and execute SQL queries. With this library, developers can leverage the full power of ClickHouse in their Python projects using a familiar SQLAlchemy interface."}, {"name": "cloudpickle", "tags": ["dev", "math", "web"], "summary": "Pickler class to extend the standard pickle.Pickler functionality", "text": "cloudpickle\n\n(https://github.com/cloudpipe/cloudpickle/actions)\n(https://codecov.io/github/cloudpipe/cloudpickle?branch=master)\n\n`cloudpickle` makes it possible to serialize Python constructs not supported\nby the default `pickle` module from the Python standard library.\n\n`cloudpickle` is especially useful for **cluster computing** where Python\ncode is shipped over the network to execute on remote hosts, possibly close\nto the data.\n\nAmong other things, `cloudpickle` supports pickling for **lambda functions**\nalong with **functions and classes defined interactively** in the\n`__main__` module (for instance in a script, a shell or a Jupyter notebook).\n\nCloudpickle can only be used to send objects between the **exact same version\nof Python**.\n\nUsing `cloudpickle` for **long-term object storage is not supported and\nstrongly discouraged.**\n\n**Security notice**: one should **only load pickle data from trusted sources** as\notherwise `pickle.load` can lead to arbitrary code execution resulting in a critical\nsecurity vulnerability.\n\nInstallation\n------------\n\nThe latest release of `cloudpickle` is available from\n[pypi](https://pypi.python.org/pypi/cloudpickle):\n\nExamples\n--------\n\nPickling a lambda expression:\n\nPickling a function interactively defined in a Python shell session\n(in the `__main__` module):\n\nOverriding pickle's serialization mechanism for importable constructs:\n----------------------------------------------------------------------\n\nAn important difference between `cloudpickle` and `pickle` is that\n`cloudpickle` can serialize a function or class **by value**, whereas `pickle`\ncan only serialize it **by reference**. Serialization by reference treats\nfunctions and classes as attributes of modules, and pickles them through\ninstructions that trigger the import of their module at load time.\nSerialization by reference is thus limited in that it assumes that the module\ncontaining the function or class is available/importable in the unpickling\nenvironment. This assumption breaks when pickling constructs defined in an\ninteractive session, a case that is automatically detected by `cloudpickle`,\nthat pickles such constructs **by value**.\n\nAnother case where the importability assumption is expected to break is when\ndeveloping a module in a distributed execution environment: the worker\nprocesses may not have access to the said module, for example if they live on a\ndifferent machine than the process in which the module is being developed. By\nitself, `cloudpickle` cannot detect such \"locally importable\" modules and\nswitch to serialization by value; instead, it relies on its default mode, which\nis serialization by reference. However, since `cloudpickle 2.0.0`, one can\nexplicitly specify modules for which serialization by value should be used,\nusing the\n`register_pickle_by_value(module)`/`/unregister_pickle_by_value(module)` API:\n\nUsing this API, there is no need to re-install the new version of the module on\nall the worker nodes nor to restart the workers: restarting the client Python\nprocess with the new source code is enough.\n\nNote that this feature is still **experimental**, and may fail in the following\nsituations:\n\n- If the body of a function/class pickled by value contains an `import` statement:\n  \n\n- If a function pickled by reference uses a function pickled by value during its execution.\n\nRunning the tests\n-----------------\n\n- With `tox`, to test run the tests for all the supported versions of\n  Python and PyPy:\n\n  or alternatively for a specific environment:\n\n- With `pytest` to only run the tests for your current version of\n  Python:\n\nHistory\n-------\n\n`cloudpickle` was initially developed by [picloud.com](http://web.archive.org/web/20140721022102/http://blog.picloud.com/2013/11/17/picloud-has-joined-dropbox/) and shipped as part of\nthe client SDK.\n\nA copy of `cloudpickle.py` was included as part of PySpark, the Python\ninterface to [Apache Spark](https://spark.apache.org/). Davies Liu, Josh\nRosen, Thom Neale and other Apache Spark developers improved it significantly,\nmost notably to add support for PyPy and Python 3.\n\nThe aim of the `cloudpickle` project is to make that work available to a wider\naudience outside of the Spark ecosystem and to make it easier to improve it\nfurther notably with the help of a dedicated non-regression test suite."}, {"name": "cloudpickle", "tags": ["dev", "math", "web"], "summary": "Pickler class to extend the standard pickle.Pickler functionality", "text": "This library is used to serialize and deserialize complex Python data structures such as lambda functions and classes defined interactively, making it ideal for cluster computing and distributed environments. It allows developers to easily ship and execute Python code on remote hosts, while maintaining compatibility across the same version of Python."}, {"name": "cma", "tags": ["cli", "math", "ml", "web"], "summary": "CMA-ES, Covariance Matrix Adaptation Evolution Strategy for non-linear numerical optimization in Python", "text": "CMA-ES Covariance Matrix Adaptation Evolution Strategy\n======================================================\n\nA stochastic numerical optimization algorithm for difficult (non-convex,\nill-conditioned, multi-modal, rugged, noisy) optimization problems in\ncontinuous or mixed-integer search spaces, implemented in Python.\n\nTypical domain of application are unconstrained or bound-constrained\nobjective functions with:\n\n* search space dimension between, say, five and a few hundred,\n* no gradients available,\n* at least, say, 100 times dimension function evaluations needed\n  to get satisfactory solutions,\n* non-separable, ill-conditioned, or rugged/multi-modal landscapes.\n\nNonlinear constraints handling is available too.\nThe CMA-ES is quite reliable, however for small budgets (fewer function\nevaluations than, say, 100 times dimension) or in very small dimensions\nfaster methods are available.\n\nThe ``pycma`` module provides two independent implementations of the \nCMA-ES algorithm in the classes `cma.CMAEvolutionStrategy`_ and \n`cma.purecma.CMAES`_.\n\nInstallation\n------------\nIn the terminal command line type ::\n\nThe package will be downloaded and installed automatically. To **upgrade**\nan existing installation, '``install``' must be replaced by '``install -U``'. For\nthe documentation of ``pip``, `see here`_.\n\n.. _`see here`: http://www.pip-installer.org\n\nAlternatively, download and unpack the ``cma-...tar.gz`` file under the\nabove ``Download files link``. The folder ``cma`` from the ``tar`` archive\ncan be used without any installation (for ``import`` to find it, it must be\nin the current folder or the Python search paths) or can be installed by\n``pip install -e .``.\n\nUsage Example\n-------------\nIn a Python shell:\n\n.. code-block:: python\n\noptimizes the 8-dimensional Rosenbrock function with initial solution all\nzeros and initial ``sigma = 0.5``.\n\nPretty much the same can be achieved with the \"one-liner\"\n\n.. code-block:: python\n\nwhere `cma.fmin2`_ provides also options for restarts.\n\nThe same can be run via the **ask-and-tell interface** which gives the user direct\ncontrol over the iteration loop of the algorithm:\n\n.. code-block:: python\n\n.. figure:: http://www.cmap.polytechnique.fr/~nikolaus.hansen/rosen12.png\n   \n\nThe `CMAOptions`_ class manages the options for `CMAEvolutionStrategy`_.\nThe options class allows for substring search.\nFor example, verbosity options can be found like\n\n.. code-block:: python\n\nOptions are passed as another argument, after ``sigma``, to `cma.fmin2`_ or\n`cma.CMAEvolutionStrategy`_ like\n\n.. code-block:: python\n\n.. _`cma.CMAEvolutionStrategy`: https://cma-es.github.io/apidocs-pycma/cma.evolution_strategy.CMAEvolutionStrategy.html\n.. _`cma.purecma.CMAES`: https://cma-es.github.io/apidocs-pycma/cma.purecma.CMAES.html\n.. _`CMAOptions`: https://cma-es.github.io/apidocs-pycma/cma.options_parameters.CMAOptions.html\n.. _`CMAEvolutionStrategy`: https://cma-es.github.io/apidocs-pycma/cma.evolution_strategy.CMAEvolutionStrategy.html\n.. _`cma.fmin2`: https://cma-es.github.io/apidocs-pycma/cma.evolution_strategy.html#fmin2\n\nDocumentations\n--------------\nThe full package API documentation:\n\n* `current`_\n* `version 1.x`_\n\n.. _`current`: https://cma-es.github.io/apidocs-pycma/\n.. _`version 1.x`: http://www.cmap.polytechnique.fr/~nikolaus.hansen/html-pythoncma/\n\nSee also\n\n* `Links to more documentation`_\n* An `FAQ`_ (under development)\n* `General CMA-ES source code page`_ with practical hints\n* `CMA-ES on Wikipedia`_\n\n.. _`Links to more documentation`: https://github.com/CMA-ES/pycma/tree/development?tab=readme-ov-file#documentation-and-getting-started-links\n.. _`FAQ`: https://github.com/CMA-ES/pycma/issues?q=is:issue+label:FAQ\n.. _`General CMA-ES source code page`: https://cma-es.github.io/cmaes_sourcecode_page.html\n.. _`CMA-ES on Wikipedia`: http://en.wikipedia.org/wiki/CMA-ES\n\nDependencies\n------------\n\n* required (unless for `cma.purecma`): ``numpy`` -- array processing for numbers, strings, records, and objects\n* optional (highly recommended): ``matplotlib`` -- Python plotting package (includes ``pylab``)\n\nUse ``pip install numpy`` etc. for installation. The `cma.purecma` submodule can be used without any dependencies installed.\n\nLicense: BSD-3-Clause"}, {"name": "cma", "tags": ["cli", "math", "ml", "web"], "summary": "CMA-ES, Covariance Matrix Adaptation Evolution Strategy for non-linear numerical optimization in Python", "text": "This library is used to efficiently solve difficult, non-linear optimization problems in Python with a focus on unconstrained or bound-constrained objective functions. It provides a robust solution for complex search spaces with no gradients available and limited budget of function evaluations."}, {"name": "cmdstanpy", "tags": ["dev", "math"], "summary": "Python interface to CmdStan", "text": "CmdStanPy\n\n(https://codecov.io/gh/stan-dev/cmdstanpy)\n\nCmdStanPy is a lightweight pure-Python interface to CmdStan which provides access to the Stan compiler and all inference algorithms.  It supports both development and production workflows. Because model development and testing may require many iterations, the defaults favor development mode and therefore output files are stored on a temporary filesystem. Non-default options allow all aspects of a run to be specified so that scripts can be used to distributed analysis jobs across nodes and machines.\n\nCmdStanPy is distributed via PyPi: https://pypi.org/project/cmdstanpy/\n\nor Conda Forge: https://anaconda.org/conda-forge/cmdstanpy\n\nGoals\n\n- Clean interface to Stan services so that CmdStanPy can keep up with Stan releases.\n\n- Provide access to all CmdStan inference methods.\n\n- Easy to install,\n  + minimal Python library dependencies: numpy, pandas\n  + Python code doesn't interface directly with c++, only calls compiled executables\n\n- Modular - CmdStanPy produces a MCMC sample (or point estimate) from the posterior; other packages do analysis and visualization.\n\n- Low memory overhead - by default, minimal memory used above that required by CmdStanPy; objects run CmdStan programs and track CmdStan input and output files.\n\nSource Repository\n\nCmdStanPy and CmdStan are available from GitHub: https://github.com/stan-dev/cmdstanpy and https://github.com/stan-dev/cmdstan\n\nDocs\n\nThe latest release documentation is hosted on  https://mc-stan.org/cmdstanpy, older release versions are available from readthedocs:  https://cmdstanpy.readthedocs.io\n\nLicensing\n\nThe CmdStanPy, CmdStan, and the core Stan C++ code are licensed under new BSD.\n\nExample"}, {"name": "cmdstanpy", "tags": ["dev", "math"], "summary": "Python interface to CmdStan", "text": "This library is used to provide a lightweight pure-Python interface to CmdStan, enabling developers to easily access the Stan compiler and inference algorithms. With this library, developers can automate model development and testing workflows by specifying run options and distributing analysis jobs across nodes and machines."}, {"name": "coloredlogs", "tags": ["math", "ui", "web"], "summary": "Colored terminal output for Python's logging module", "text": "coloredlogs: Colored terminal output for Python's logging module\n================================================================\n\n.. image:: https://travis-ci.org/xolox/python-coloredlogs.svg?branch=master\n   :target: https://travis-ci.org/xolox/python-coloredlogs\n\n:target: https://coveralls.io/github/xolox/python-coloredlogs?branch=master\n\nThe `coloredlogs` package enables colored terminal output for Python's logging_\nmodule. The ColoredFormatter_ class inherits from `logging.Formatter`_ and uses\n`ANSI escape sequences`_ to render your logging messages in color. It uses only\nstandard colors so it should work on any UNIX terminal. It's currently tested\non Python 2.7, 3.5+ and PyPy (2 and 3). On Windows `coloredlogs` automatically\ntries to enable native ANSI support (on up-to-date Windows 10 installations)\nand falls back on using colorama_ (if installed). Here is a screen shot of the\ndemo that is printed when the command ``coloredlogs --demo`` is executed:\n\n.. image:: https://coloredlogs.readthedocs.io/en/latest/_images/defaults.png\n\nNote that the screenshot above includes custom logging levels defined by my\nverboselogs_ package: if you install both `coloredlogs` and `verboselogs` it\nwill Just Work (`verboselogs` is of course not required to use\n`coloredlogs`).\n\n.. contents::\n   :local:\n\nInstallation\n------------\n\nThe `coloredlogs` package is available on PyPI_ which means installation should\nbe as simple as:\n\n.. code-block:: console\n\n$ pip install coloredlogs\n\nThere's actually a multitude of ways to install Python packages (e.g. the `per\nuser site-packages directory`_, `virtual environments`_ or just installing\nsystem wide) and I have no intention of getting into that discussion here, so\nif this intimidates you then read up on your options before returning to these\ninstructions .\n\nOptional dependencies\n~~~~~~~~~~~~~~~~~~~~~\n\nNative ANSI support on Windows requires an up-to-date Windows 10 installation.\nIf this is not working for you then consider installing the colorama_ package:\n\n.. code-block:: console\n\n$ pip install colorama\n\nOnce colorama_ is installed it will be used automatically.\n\nUsage\n-----\n\nHere's an example of how easy it is to get started:\n\n.. code-block:: python\n\nimport coloredlogs, logging\n\n# Create a logger object.\n   logger = logging.getLogger(__name__)\n\n# By default the install() function installs a handler on the root logger,\n   # this means that log messages from your code and log messages from the\n   # libraries that you use will all show up on the terminal.\n   coloredlogs.install(level='DEBUG')\n\n# If you don't want to see log messages from libraries, you can pass a\n   # specific logger object to the install() function. In this case only log\n   # messages originating from that logger will show up on the terminal.\n   coloredlogs.install(level='DEBUG', logger=logger)\n\n# Some examples.\n   logger.debug(\"this is a debugging message\")\n   logger.info(\"this is an informational message\")\n   logger.warning(\"this is a warning message\")\n   logger.error(\"this is an error message\")\n   logger.critical(\"this is a critical message\")\n\nFormat of log messages\n----------------------\n\nThe ColoredFormatter_ class supports user defined log formats so you can use\nany log format you like. The default log format is as follows::\n\n%(asctime)s %(hostname)s %(name)s[%(process)d] %(levelname)s %(message)s\n\nThis log format results in the following output::\n\n2015-10-23 03:32:22 peter-macbook coloredlogs.demo[30462] DEBUG message with level 'debug'\n 2015-10-23 03:32:23 peter-macbook coloredlogs.demo[30462] VERBOSE message with level 'verbose'\n 2015-10-23 03:32:24 peter-macbook coloredlogs.demo[30462] INFO message with level 'info'\n ...\n\nYou can customize the log format and styling using environment variables as\nwell as programmatically, please refer to the `online documentation`_ for\ndetails."}, {"name": "coloredlogs", "tags": ["math", "ui", "web"], "summary": "Colored terminal output for Python's logging module", "text": "Enabling millisecond precision\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nIf you're switching from `logging.basicConfig()`_ to `coloredlogs.install()`_\nyou may notice that timestamps no longer include milliseconds. This is because\ncoloredlogs doesn't output milliseconds in timestamps unless you explicitly\ntell it to. There are three ways to do that:\n\n1. The easy way is to pass the `milliseconds` argument to `coloredlogs.install()`_::\n\nThis became supported in `release 7.1`_ (due to `#16`_).\n\n2. Alternatively you can change the log format `to include 'msecs'`_::\n\nHere's what the call to `coloredlogs.install()`_ would then look like::\n\nCustomizing the log format also enables you to change the delimiter that\n   separates seconds from milliseconds (the comma above). This became possible\n   in `release 3.0`_ which added support for user defined log formats.\n\n3. If the use of ``%(msecs)d`` isn't flexible enough you can instead add ``%f``\n   to the date/time format, it will be replaced by the value of ``%(msecs)03d``.\n   Support for the ``%f`` directive was added to `release 9.3`_ (due to `#45`_).\n\nCustom logging fields\n~~~~~~~~~~~~~~~~~~~~~\n\nThe following custom log format fields are supported:\n\n- ``%(hostname)s`` provides the hostname of the local system.\n- ``%(programname)s`` provides the name of the currently running program.\n- ``%(username)s`` provides the username of the currently logged in user.\n\nWhen `coloredlogs.install()`_ detects that any of these fields are used in the\nformat string the applicable logging.Filter_ subclasses are automatically\nregistered to populate the relevant log record fields.\n\nChanging text styles and colors\n-------------------------------\n\nThe online documentation contains `an example of customizing the text styles and\ncolors `_.\n\nColored output from cron\n------------------------\n\nWhen `coloredlogs` is used in a cron_ job, the output that's e-mailed to you by\ncron won't contain any ANSI escape sequences because `coloredlogs` realizes\nthat it's not attached to an interactive terminal. If you'd like to have colors\ne-mailed to you by cron there are two ways to make it happen:\n\n.. contents::\n   :local:\n\nModifying your crontab\n~~~~~~~~~~~~~~~~~~~~~~\n\nHere's an example of a minimal crontab::\n\nThe ``coloredlogs`` program is installed when you install the `coloredlogs`\nPython package. When you execute ``coloredlogs --to-html your-command`` it runs\n``your-command`` under the external program ``script`` (you need to have this\ninstalled). This makes ``your-command`` think that it's attached to an\ninteractive terminal which means it will output ANSI escape sequences which\nwill then be converted to HTML by the ``coloredlogs`` program. Yes, this is a\nbit convoluted, but it works great :-)\n\nModifying your Python code\n~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nThe ColoredCronMailer_ class provides a context manager that automatically\nenables HTML output when the ``$CONTENT_TYPE`` variable has been correctly set\nin the crontab.\n\nThis requires my capturer_ package which you can install using ``pip install\n'coloredlogs[cron]'``. The ``[cron]`` extra will pull in capturer_ 2.4 or newer\nwhich is required to capture the output while silencing it - otherwise you'd\nget duplicate output in the emails sent by ``cron``."}, {"name": "coloredlogs", "tags": ["math", "ui", "web"], "summary": "Colored terminal output for Python's logging module", "text": "The context manager can also be used to retroactively silence output that has\nalready been produced, this can be useful to avoid spammy cron jobs that have\nnothing useful to do but still email their output to the system administrator\nevery few minutes :-).\n\nContact\n-------\n\nThe latest version of `coloredlogs` is available on PyPI_ and GitHub_. The\n`online documentation`_ is available on Read The Docs and includes a\nchangelog_. For bug reports please create an issue on GitHub_. If you have\nquestions, suggestions, etc. feel free to send me an e-mail at\n`peter@peterodding.com`_.\n\nLicense\n-------\n\nThis software is licensed under the `MIT license`_.\n\n\u00a9 2020 Peter Odding.\n\n.. External references:\n.. _#16: https://github.com/xolox/python-coloredlogs/issues/16\n.. _#45: https://github.com/xolox/python-coloredlogs/issues/45\n.. _ANSI escape sequences: https://en.wikipedia.org/wiki/ANSI_escape_code#Colors\n.. _capturer: https://pypi.org/project/capturer\n.. _changelog: https://coloredlogs.readthedocs.org/en/latest/changelog.html\n.. _colorama: https://pypi.org/project/colorama\n.. _ColoredCronMailer: https://coloredlogs.readthedocs.io/en/latest/api.html#coloredlogs.converter.ColoredCronMailer\n.. _ColoredFormatter: https://coloredlogs.readthedocs.io/en/latest/api.html#coloredlogs.ColoredFormatter\n.. _coloredlogs.install(): https://coloredlogs.readthedocs.io/en/latest/api.html#coloredlogs.install\n.. _cron: https://en.wikipedia.org/wiki/Cron\n.. _GitHub: https://github.com/xolox/python-coloredlogs\n.. _logging.basicConfig(): https://docs.python.org/2/library/logging.html#logging.basicConfig\n.. _logging.Filter: https://docs.python.org/3/library/logging.html#filter-objects\n.. _logging.Formatter: https://docs.python.org/2/library/logging.html#logging.Formatter\n.. _logging: https://docs.python.org/2/library/logging.html\n.. _MIT license: https://en.wikipedia.org/wiki/MIT_License\n.. _online documentation: https://coloredlogs.readthedocs.io/\n.. _per user site-packages directory: https://www.python.org/dev/peps/pep-0370/\n.. _peter@peterodding.com: peter@peterodding.com\n.. _PyPI: https://pypi.org/project/coloredlogs\n.. _release 3.0: https://coloredlogs.readthedocs.io/en/latest/changelog.html#release-3-0-2015-10-23\n.. _release 7.1: https://coloredlogs.readthedocs.io/en/latest/changelog.html#release-7-1-2017-07-15\n.. _release 9.3: https://coloredlogs.readthedocs.io/en/latest/changelog.html#release-9-3-2018-04-29\n.. _to include 'msecs': https://stackoverflow.com/questions/6290739/python-logging-use-milliseconds-in-time-format\n.. _verboselogs: https://pypi.org/project/verboselogs\n.. _virtual environments: http://docs.python-guide.org/en/latest/dev/virtualenvs/"}, {"name": "coloredlogs", "tags": ["math", "ui", "web"], "summary": "Colored terminal output for Python's logging module", "text": "This library is used to provide colored output for log messages in the terminal, allowing developers to visually distinguish between different types of logging events. With this library, developers can enhance their application's logging capabilities and make it easier to diagnose issues by highlighting important information in a more readable format."}, {"name": "convertdate", "tags": ["math", "web"], "summary": "Converts between Gregorian dates and other calendar systems", "text": "Armenian\n\nThe Armenian calendar begins on 11 July 552 (Julian) and has two modes of\nreckoning. The first is the invariant-length version consisting of 12 months\nof 30 days each and five epagomenal days; the second is the version\nestablished by Yovhannes Sarkawag in 1084, which fixed the first day of the\nyear with respect to the Julian calendar and added a sixth epagomenal day\nevery four years.\n\nBy default the invariant calendar is used, but the Sarkawag calendar can be\nused beginning with the Armenian year 533 (11 August 1084) by passing the\nparameter `method='sarkawag'` to the relevant functions.\n\nFrench Republican\n\nLeap year calculations in the French Republican calendar are a matter of\ndispute. By default, `convertdate` calculates leap years using the\nautumnal equinox. You can also use one of three more systematic methods\nproposed over the years.\n\n-   Romme, a co-creator of the calendar, proposed leap years in years\n-   Some concordances were drawn up in the 19th century that gave leap\n-   Von M&auml;dler proposed leap years in years divisible by four, except\n\nYou can specify any of these three methods with the method keyword\nargument in `french_republican` conversion functions.\n\nAll the conversion methods correctly assign the leap years implemented\nwhile calendar was in use (3, 7, 11).\n\nBaha'i\n------\n\nThe Bah&aacute;'&iacute; (Bad&iacute;) calendar has an intercalary period, Ayyam-i-H&aacute;, which occurs between the 18th and 19th months.\nDates in this period are returned as month 19, and the month of &lsquo;Al&aacute; is reported as month 20.\n\nBefore the Common Era\n---------------------\n\nFor dates before the Common Era (year 1), `convertdate` uses\nastronomical notation: 1 BC is recorded as 0, 2 BC is -1, etc. This\nmakes arithmatic much easier at the expense of ignoring custom.\n\nNote that for dates before 4 CE, `convertdate` uses the [proleptic\nJulian\ncalendar](https://en.wikipedia.org/wiki/Proleptic_Julian_calendar). The\nJulian Calendar was in use from 45 BC, but before 4 CE the leap year\nleap year pattern was irregular.\n\nThe [proleptic Gregorian\ncalendar](https://en.wikipedia.org/wiki/Proleptic_Gregorian_calendar) is\nused for dates before 1582 CE, the year of the Gregorian calendar\nreform.\n\nHolidays\n--------\n\nNorth American holidays are the current focus of the `holidays` module,\nbut pull requests are welcome.\n\nUtils\n-----\n\nConvertdate includes some utilities for manipulating and calculating\ndates.\n\nNote that when calculating weekdays, convertdate uses the convention of\nthe calendar and time modules: Monday is 0, Sunday is 6.\n\nOther utility functions:\n\n-   nearest\\_weekday\n-   next\\_or\\_current\\_weekday\n-   previous\\_weekday\n-   previous\\_or\\_current\\_weekday"}, {"name": "convertdate", "tags": ["math", "web"], "summary": "Converts between Gregorian dates and other calendar systems", "text": "This library is used to convert dates between different calendars, including the Armenian and French Republican systems, allowing for seamless integration with international date calculations. This enables developers to easily handle diverse calendar formats in their applications."}, {"name": "coola", "tags": ["math", "web"], "summary": "Library to check equality between two complex/nested objects", "text": "coola\n\nOverview\n\n`coola` is a Python library that provides simple functions to check in a single line if two\ncomplex/nested objects are equal or not.\n`coola` was initially designed to work\nwith [PyTorch `Tensor`s](https://pytorch.org/docs/stable/tensors.html)\nand [NumPy `ndarray`](https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html), but it\nis possible to extend it\nto [support other data structures](https://durandtibo.github.io/coola/customization).\n\n- [Motivation](#motivation)\n- [Documentation](https://durandtibo.github.io/coola/)\n- [Installation](#installation)\n- [Contributing](#contributing)\n- [API stability](#api-stability)\n- [License](#license)\n- [Security](SECURITY.md)\n\nMotivation\n\nLet's imagine you have the following dictionaries that contain both a PyTorch `Tensor` and a\nNumPy `ndarray`.\nYou want to check if the two dictionaries are equal or not.\nBy default, Python does not provide an easy way to check if the two dictionaries are equal or not.\nIt is not possible to use the default equality operator `==` because it will raise an error.\nThe `coola` library was developed to fill this gap. `coola` provides a function `objects_are_equal`\nthat can indicate if two complex/nested objects are equal or not.\n\n`coola` also provides a function `objects_are_allclose` that can indicate if two complex/nested\nobjects are equal within a tolerance or not.\n\n`coola` supports the following types:\n\nPlease check the [quickstart page](https://durandtibo.github.io/coola/quickstart) to learn more on\nhow to use `coola`.\n\nDocumentation\n\n- [latest (stable)](https://durandtibo.github.io/coola/): documentation from the latest stable\n  release.\n- [main (unstable)](https://durandtibo.github.io/coola/main/): documentation associated to the main\n  branch of the repo. This documentation may contain a lot of work-in-progress/outdated/missing\n  parts.\n\nInstallation\n\nWe highly recommend installing\na [virtual environment](https://packaging.python.org/guides/installing-using-pip-and-virtual-environments/).\n`coola` can be installed from pip using the following command:\n\nTo make the package as slim as possible, only the minimal packages required to use `coola` are\ninstalled.\nTo include all the dependencies, you can use the following command:\n\nPlease check the [get started page](https://durandtibo.github.io/coola/get_started) to see how to\ninstall only some specific dependencies or other alternatives to install the library.\nThe following is the corresponding `coola` versions and tested dependencies.\n\n`coola`\n----------\n`main`\n`0.11.0`\n`0.10.0`\n`0.9.1`\n\n`coola`\n----------\n`0.7.4`\n\nContributing\n\nPlease check the instructions in [CONTRIBUTING.md](CONTRIBUTING.md).\n\nSuggestions and Communication\n\nEveryone is welcome to contribute to the community.\nIf you have any questions or suggestions, you can\nsubmit [Github Issues](https://github.com/durandtibo/coola/issues).\nWe will reply to you as soon as possible. Thank you very much.\n\nAPI stability\n\n:warning: While `coola` is in development stage, no API is guaranteed to be stable from one\nrelease to the next.\nIn fact, it is very likely that the API will change multiple times before a stable 1.0.0 release.\nIn practice, this means that upgrading `coola` to a new version will possibly break any code that\nwas using the old version of `coola`.\n\nLicense\n\n`coola` is licensed under BSD 3-Clause \"New\" or \"Revised\" license available in [LICENSE](LICENSE)\nfile."}, {"name": "coola", "tags": ["math", "web"], "summary": "Library to check equality between two complex/nested objects", "text": "This library is used to check equality between two complex/nested objects, including those with nested structures such as lists and dictionaries. With coola, developers can easily verify if two objects are identical in a single line of code."}, {"name": "coolprop", "tags": ["math"], "summary": "Open-source thermodynamic and transport properties database", "text": "Welcome to CoolProp\n\nCoolProp is a thermophysical property database and wrappers for a selection of programming environments.\nIt offers similar functionality to REFPROP, but CoolProp is open-source and free.\nIt was originally developed by Ian Bell, at the time a post-doc at the University of Liege, in Liege, Belgium.\n\n* CoolProp has flexible licensing terms: Commercial - ok! Academic? - ok! (https://github.com/CoolProp/CoolProp/blob/master/LICENSE)\n\n* For Python, get the latest release via `pip install coolprop` (http://pypi.python.org/pypi/CoolProp/) (http://pypi.python.org/pypi/CoolProp/)\n\n* ... other binaries are available from [SourceForge](http://sourceforge.net/projects/coolprop/files) (http://sourceforge.net/projects/coolprop/files) \n\n* There is also a bleeding edge nightly build of the [development version](http://sourceforge.net/projects/coolprop/files/CoolProp/nightly).\n\n* The documentation is available for the [latest release](http://www.coolprop.org) and the [development version](https://coolprop.github.io/devdocs/)\n\n* For any kind of question regarding CoolProp and its usage, you can ask the [CoolProp Discussions](https://github.com/CoolProp/CoolProp/discussions)\n\n* ... you might also find answers in our [FAQ](https://github.com/CoolProp/CoolProp/blob/master/FAQ.md)\n\n* If you found a bug or have an issue that requires the developers to become active, please file an issue in our [issue tracker](https://github.com/CoolProp/CoolProp/issues)\n\n* [Contributions](https://github.com/CoolProp/CoolProp/blob/master/.github/CONTRIBUTING.md) to this project are welcomed and encouraged! If you wish to [contribute](https://github.com/CoolProp/CoolProp/blob/master/.github/CONTRIBUTING.md) bug fixes, patches, or new features, wrappers, or material properties, please submit a Pull Request with your code.\n\n* If you are new to Git and Github, please see the [CoolProp Wiki](https://github.com/CoolProp/CoolProp/wiki) for guidance on becoming a contributor to the project.\n\n* Accelerate development of things you really need implemented by posting at [Bountysource](https://www.bountysource.com/teams/coolprop)\n\n* Have a look at the coverity stats (https://scan.coverity.com/projects/coolprop)"}, {"name": "coolprop", "tags": ["math"], "summary": "Open-source thermodynamic and transport properties database", "text": "This library is used to access and calculate thermodynamic and transport properties for various fluids and substances, allowing developers to integrate accurate physical property calculations into their applications. With CoolProp, developers can create software that accurately models the behavior of different materials under various conditions."}, {"name": "coremltools", "tags": ["dev", "math", "ml", "web"], "summary": "Community Tools for Core ML", "text": "coremltools\n===========\n\n`Core ML `_\nis an Apple framework that allows developers to easily integrate\nmachine learning (ML) models into apps. Core ML is available on iOS, iPadOS,\nwatchOS, macOS, and tvOS. Core ML introduces a public file format (.mlmodel)\nfor a broad set of ML methods including deep neural networks (convolutional\nand recurrent), tree ensembles (boosted trees, random forest, decision trees),\nand generalized linear models. Core ML models can be directly integrated into\napps within Xcode.\n\n:code:`coremltools` is a python package for creating, examining, and testing models in\nthe .mlmodel format. In particular, it can be used to:\n\n- Convert trained models from popular machine learning tools into Core ML format\n  (.mlmodel).\n- Write models to Core ML format with a simple API.\n- Making predictions using the Core ML framework (on select platforms) to\n  verify conversion.\n\nMore Information\n----------------\n\n- `coremltools user guide and examples `_\n- `Core ML framework documentation `_\n- `Machine learning at Apple `_\n\nLicense\n-------\nCopyright (c) 2020, Apple Inc. All rights reserved.\n\nUse of this source code is governed by the\n`3-Clause BSD License `_\nthat can be found in the LICENSE.txt file."}, {"name": "coremltools", "tags": ["dev", "math", "ml", "web"], "summary": "Community Tools for Core ML", "text": "This library is used to create and convert machine learning models to the Core ML format for integration into Apple apps. With coremltools, developers can easily import and export models in .mlmodel format, streamlining the process of integrating AI capabilities into their iOS or macOS projects."}, {"name": "correctionlib", "tags": ["data", "math", "web"], "summary": "A generic correction library", "text": "correctionlib\n\n[![PyPI version][pypi-version]][pypi-link]\n[![PyPI platforms][pypi-platforms]][pypi-link]\n\nIntroduction\nThe purpose of this library is to provide a well-structured JSON data format for a\nwide variety of ad-hoc correction factors encountered in a typical HEP analysis and\na companion evaluation tool suitable for use in C++ and python programs.\nHere we restrict our definition of correction factors to a class of functions with\nscalar inputs that produce a scalar output.\n\nIn python, the function signature is:\n\nIn C++, the evaluator implements this currently as:\n\nThe supported function classes include:\n\n  * multi-dimensional binned lookups;\n  * binned lookups pointing to multi-argument formulas with a restricted\n  * categorical (string or integer enumeration) maps;\n  * input transforms (updating one input value in place); and\n  * compositions of the above.\n\nEach function type is represented by a \"node\" in a call graph and holds all\nof its parameters in a JSON structure, described by the JSON schema.\nPossible future extension nodes might include weigted sums (which, when composed with\nthe others, could represent a BDT) and perhaps simple MLPs.\n\nThe tool should provide:\n\n  * standardized, versioned [JSON schemas](https://json-schema.org/);\n  * forward-porting tools (to migrate data written in older schema versions); and\n  * a well-optimized C++ evaluator and python bindings (with numpy vectorization support).\n\nThis tool will definitely not provide:\n\n  * support for `TLorentzVector` or other object-type inputs (such tools should be written\n\nFormula support currently includes a mostly-complete subset of the ROOT library `TFormula` class,\nand is implemented in a threadsafe standalone manner. The parsing grammar is formally defined\nand parsed through the use of a header-only [PEG parser library](https://github.com/yhirose/cpp-peglib).\nThe supported features mirror CMSSW's [reco::formulaEvaluator](https://github.com/cms-sw/cmssw/pull/11516)\nand fully passes the test suite for that utility with the purposeful exception of the `TMath::` namespace.\nThe python bindings may be able to call into [numexpr](https://numexpr.readthedocs.io/en/latest/user_guide.html),\nthough, due to the tree-like structure of the corrections, it may prove difficult to exploit vectorization\nat levels other than the entrypoint.\n\nDetailed instructions for installing and using this package are provided in the [documentation][rtd-link].\n\nCreating new corrections\n\nA demo/tutorial of the features is available in the [documentation][rtd-link] and also available interactively\non [binder](https://mybinder.org/v2/gh/cms-nanoAOD/correctionlib/HEAD?labpath=binder%2Fcorrectionlib_tutorial.ipynb)\n\nThe `correctionlib.schemav2` module provides a helpful framework for defining correction objects\nand `correctionlib.convert` includes select conversion routines for common types. Nodes can be type-checked as they are\nconstructed using the [parse_obj](https://pydantic-docs.helpmanual.io/usage/models/#helper-functions)\nclass method or by directly constructing them using keyword arguments.\n\nDeveloping\nSee CONTRIBUTING.md"}, {"name": "correctionlib", "tags": ["data", "math", "web"], "summary": "A generic correction library", "text": "This library is used to provide a standardized JSON data format for various ad-hoc correction factors in HEP analysis, enabling seamless integration and evaluation across C++ and Python programs. Developers can utilize this library to define and apply complex correction functions, including multi-dimensional lookups, formula evaluations, and categorical mappings."}, {"name": "country-converter", "tags": ["data", "dev", "math", "web"], "summary": "The country converter (coco) - a Python package for converting country names between different classifications schemes", "text": "country converter\n\nThe country converter (coco) is a Python package to convert and match country names between different classifications and between different naming versions. Internally it uses regular expressions to match country names. Coco can also be used to build aggregation concordance matrices between different classification schemes.\n\n(https://pypi.python.org/pypi/country_converter/)\n(https://anaconda.org/conda-forge/country_converter)\n(https://doi.org/10.5281/zenodo.838035)\n(http://joss.theoj.org/papers/af694f2e5994b8aacbad119c4005e113)\n\n(https://github.com/IndEcol/country_converter/actions)\n(https://coveralls.io/github/IndEcol/country_converter?branch=master)\n(https://www.gnu.org/licenses/gpl-3.0)\n\nMotivation\n\nTo date, there is no single standard of how to name or specify\nindividual countries in a (meta) data description. While some data\nsources follow ISO 3166, this standard defines a two and a three letter\ncode in addition to a numerical classification. To further complicate\nthe matter, instead of using one of the existing standards, many\ndatabases use unstandardised country names to classify countries.\n\nThe country converter (coco) automates the conversion from different\nstandards and version of country names. Internally, coco is based on a\ntable specifying the different ISO and UN standards per country together\nwith the official name and a regular expression which aim to match all\nEnglish versions of a specific country name. In addition, coco includes\nclassification based on UN-, EU-, OECD-membership, UN regions\nspecifications, continents and various MRIO and IAM databases (see\n[Classification schemes](#classification-schemes) below).\n\nInstallation\n\nCountry_converter is registered at PyPI. From the command line:\n\nThe country converter is also available from the [conda\nforge](https://conda-forge.org/) and can be installed using conda with\n(if you don't have the conda_forge channel added to your conda config\nadd \"-c conda-forge\", see [the install instructions\nhere](https://github.com/conda-forge/country_converter-feedstock)):\n\nAlternatively, the source code is available on\n[GitHub](https://github.com/IndEcol/country_converter).\n\nThe package depends on [Pandas](http://pandas.pydata.org/); for testing\n[pytest](http://pytest.org/) is required. For further information on\nrunning the tests see [CONTRIBUTING.md](CONTRIBUTING.md).\n\nUsage\n\nBasic usage\n\nUse within Python\n\nConvert various country names to some standard names:\n\nWhich results in \\['Tanzania', 'Germany', 'Cabo Verde', 'Tunisia',\n'Myanmar', 'Congo Republic', 'Iran', 'South Korea', 'North Korea'\\]. The\ninput format is determined automatically, based on ISO two letter, ISO\nthree letter, ISO numeric or regular expression matching. In case of any\nambiguity, the source format can be specified with the parameter 'src'.\n\nIn case of multiple conversion, better performance can be achieved by\ninstantiating a single CountryConverter object for all conversions:\n\nIn order to more efficiently convert Pandas Series, the `pandas_convert()` method can be used. The\nperformance gain is especially significant for large Series. For a series containing 1 million rows\na 4000x speedup can be achieved, compared to `convert()`.\n\nConvert between classification schemes:\n\nWhich results in \\['US', 'VU', 'TK', 'AT', 'not found'\\]\n\nThe not found indication can be specified (e.g. not_found = 'not\nthere'), if None is passed for 'not_found', the original entry gets\npassed through:\n\nresults in \\['US', 'VU', 'TK', 'AT', 'XXX'\\]\n\nInternally the data is stored in a Pandas DataFrame, which can be\naccessed directly. For example, this can be used to filter countries for\nmembership organisations (per year). Note: for this, an instance of\nCountryConverter is required.\n\npython\nimport logging\nimport country_converter as coco\nlogging.basicConfig(level=logging.INFO)\ncoco.convert(\"asdf\")\n\nWARNING:country_converter.country_converter:asdf not found in regex\n\nOut: 'not found'\n\ncoco_logger = coco.logging.getLogger()\ncoco_logger.setLevel(logging.CRITICAL)\ncoco.convert(\"asdf\")"}, {"name": "country-converter", "tags": ["data", "dev", "math", "web"], "summary": "The country converter (coco) - a Python package for converting country names between different classifications schemes", "text": "Out: 'not found'\n matlab\npy.print(py.sys.version)\n matlab\n matlab\ncoco = py.country_converter.CountryConverter()\ncountries = {'The Swedish Kingdom', 'Norway is a Kingdom too', 'Peoples Republic of China', 'Republic of China'};\nISO2_pythontype = coco.convert(countries, pyargs('to', 'ISO2'));\nISO2_cellarray = cellfun(@char,cell(ISO2_pythontype),'UniformOutput',false);\n matlab\nshort_names = cellfun(@char, cell(py.country_converter.convert({56, 276}, pyargs('src', 'UNcode', 'to', 'name_short'))), 'UniformOutput',false);\n matlab\ncoco = py.country_converter.CountryConverter();\ncoco.EU27\nEU27ISO3 = coco.EU27as('ISO3');\n matlab\nEU27ISO3.values\n python\nimport country_converter as coco\ncc = coco.CountryConverter()\ncc_UN = coco.CountryConverter(only_UNmember=True)\ncc_all = coco.CountryConverter(include_obsolete=True)\n\ncc.convert(['PSE', 'XKX', 'EAZ', 'FRA'], to='name_short')\ncc_UN.convert(['PSE', 'XKX', 'EAZ', 'FRA'], to='name_short')\ncc_all.convert(['PSE', 'XKX', 'EAZ', 'FRA'], to='name_short')\n```\n\ncc results in \\['Palestine', 'Kosovo', 'not found', 'France'\\], whereas\ncc_UN converts to \\['not found', 'not found', 'not found', 'France'\\]\nand cc_all converts to \\['Palestine', 'Kosovo', 'Zanzibar', 'France'\\]\nNote that the underlying dataframe is available at the attribute .data\n(e.g. cc_all.data).\n\nData sources and further reading\n\nMost of the underlying data can be found in Wikipedia, the page describing [ISO\n3166-1](https://en.wikipedia.org/wiki/ISO_3166-1) is a good starting point. The\npage on the [ISO2 codes](https://en.wikipedia.org/wiki/ISO_3166-1_alpha-2)\nincludes a section \"Imperfect Implementations\" explaining the GB/UK and EL/GR\nissue.\nUN regions/codes are given on the United Nation\nStatistical Division\n([unstats](http://unstats.un.org/unsd/methods/m49/m49regin.htm))\nwebpage. The differences between the ISO numeric and UN (M.49) codes are\n[also explained at wikipedia](https://en.wikipedia.org/wiki/UN_M.49).\n[EXIOBASE](http://exiobase.eu/), [WIOD](http://www.wiod.org/home) and\n[Eora](http://www.worldmrio.com/) classification were extracted from the\nrespective databases. For [Eora](http://www.worldmrio.com/), the names\nare based on the 'Country names' csv file provided on the webpage, but\nupdated for different names used in the Eora26 database. The MESSAGE\nclassification follows the 11-region aggregation given in the\n[MESSAGE](http://www.iiasa.ac.at/web/home/research/researchPrograms/Energy/MESSAGE-model-regions.en.html)\nmodel regions description. The\n[IMAGE](https://models.pbl.nl/image/index.php/Welcome_to_IMAGE_3.0_Documentation)\nclassification is based on the \"[region classification\nmap](https://models.pbl.nl/image/index.php/Region_classification_map)\",\nfor\n[REMIND](https://www.pik-potsdam.de/en/institute/departments/transformation-pathways/models/remind)\nwe received a country mapping from the model developers.\n\nThe membership of\n[OECD](http://www.oecd.org/about/membersandpartners/list-oecd-member-countries.htm)\nand [UN](http://www.un.org/en/members/) can be found at the membership\norganisations' webpages, information about obsolete country codes on the\n[Statoids](http://www.statoids.com/w3166his.html) webpage.\n[Council of Europe](https://www.coe.int/en/web/portal/members-states) membership information\n\nThe situation for the\n[EU](https://ec.europa.eu/eurostat/statistics-explained/index.php/Glossary:EU_enlargements)\ngot complicated due to the Brexit process. For the naming, coco follows\nthe [Eurostat\nglossary](https://ec.europa.eu/eurostat/statistics-explained/index.php/Glossary:EU_enlargements),\nthus EU27 refers to the EU without UK, whereas EU27_2007 refers to the\nEU without Croatia (the status after the 2007 enlargement). The shortcut\nEU always links to the most recent classification. The\n[EEA](https://ec.europa.eu/eurostat/statistics-explained/index.php/Glossary:European_Economic_Area_(EEA))\nagreements for the UK ended by 2021-01-01 (which also affects Guernsey, Isle of Man, Jersey and Gibraltar). \nSwitzerland is not part of the EEA but member of the single market.\n\nThe Global Burden of Disease country codes were extracted form the [GBD\ncode book available\nhere.](https://ghdx.healthdata.org/sites/default/files/ihme_query_tool/IHME_GBD_2019_CODEBOOK.zip)\n\nCommunication, issues, bugs and enhancements\n\nPlease use the issue tracker for documenting bugs, proposing\nenhancements and all other communication related to coco.\n\nYou can follow me on [mastodon - @kst@qoto.org](https://qoto.org/@kst) and [twitter](https://twitter.com/kst_stadler) to get\nthe latest news about all my open-source and research projects (and\noccasionally some random retweets/toots).\n\nContributing\n\nWant to contribute? Great! Please check\n[CONTRIBUTING.md](CONTRIBUTING.md) if you want to help to improve coco\nand for some pointer for how to add classifications.\n\nRelated software"}, {"name": "country-converter", "tags": ["data", "dev", "math", "web"], "summary": "The country converter (coco) - a Python package for converting country names between different classifications schemes", "text": "The package [pycountry](https://pypi.python.org/pypi/pycountry) provides\naccess to the official ISO databases for historic countries, country\nsubdivisions, languages and currencies. In case you need to convert\nnon-English country names,\n[countrynames](https://github.com/occrp/countrynames) includes an\nextensive database of country names in different languages and functions\nto convert them to the different ISO 3166 standards.\n[Python-iso3166](https://github.com/deactivated/python-iso3166) focuses\non conversion between the two-letter, three-letter and three-digit codes\ndefined in the ISO 3166 standard.\n\nIf you are using R, you should have a look at\n[countrycode](https://github.com/vincentarelbundock/countrycode).\n\nCiting the country converter\n\nVersion 0.5 of the country converter was published in the [Journal of\nOpen Source Software](http://joss.theoj.org/). To cite the country\nconverter in publication please use:\n\nStadler, K. (2017). The country converter coco - a Python package for\nconverting country names between different classification schemes. The\nJournal of Open Source Software. doi:\n[10.21105/joss.00332](http://dx.doi.org/10.21105/joss.00332)\n\nFor the full bibtex key see [CITATION](CITATION)\n\nAcknowledgements\n\nThis package was inspired by (and the regular expression are mostly\nbased on) the R-package\n[countrycode](https://github.com/vincentarelbundock/countrycode) by\n[Vincent Arel-Bundock](http://arelbundock.com/) and his (defunct) port\nto Python (pycountrycode). Many thanks to [Robert\nGieseke](https://github.com/rgieseke) for the review of the source code\nand paper for the publication in the [Journal of Open Source\nSoftware](http://joss.theoj.org/)."}, {"name": "country-converter", "tags": ["data", "dev", "math", "web"], "summary": "The country converter (coco) - a Python package for converting country names between different classifications schemes", "text": "This library is used to convert and match country names between different classification schemes and naming versions using regular expressions for efficient matching. With this library, developers can build aggregation concordance matrices between various classification schemes, facilitating data analysis and integration across different datasets."}, {"name": "courlan", "tags": ["math", "web"], "summary": "Clean, filter and sample URLs to optimize data collection \u2013 includes spam, content type and language filters.", "text": "coURLan: Clean, filter, normalize, and sample URLs\n\n(https://pypi.python.org/pypi/courlan)\n(https://pypi.python.org/pypi/courlan)\n(https://codecov.io/gh/adbar/courlan)\n(https://github.com/psf/black)\n\nWhy coURLan?\n\n> \"It is important for the crawler to visit 'important' pages first,\n> so that the fraction of the Web that is visited (and kept up to date)\n> is more meaningful.\" (Cho et al. 1998)\n>\n> \"Given that the bandwidth for conducting crawls is neither infinite\n> nor free, it is becoming essential to crawl the Web in not only a\n> scalable, but efficient way, if some reasonable measure of quality or\n> freshness is to be maintained.\" (Edwards et al. 2001)\n\nThis library provides an additional \"brain\" for web crawling, scraping\nand document management. It facilitates web navigation through a set of\nfilters, enhancing the quality of resulting document collections:\n\n- Save bandwidth and processing time by steering clear of pages deemed\n  low-value\n- Identify specific pages based on language or text content\n- Pinpoint pages relevant for efficient link gathering\n\nAdditional utilities needed include URL storage, filtering, and\ndeduplication.\n\nFeatures\n\nSeparate the wheat from the chaff and optimize document discovery and\nretrieval:\n\n- URL handling\n   - Validation\n   - Normalization\n   - Sampling\n- Heuristics for link filtering\n   - Spam, trackers, and content-types\n   - Locales and internationalization\n   - Web crawling (frontier, scheduling)\n- Data store specifically designed for URLs\n- Usable with Python or on the command-line\n\n**Let the coURLan fish up juicy bits for you!**\n\nHere is a [courlan](https://en.wiktionary.org/wiki/courlan) (source:\n[Limpkin at Harn's Marsh by\nRuss](https://commons.wikimedia.org/wiki/File:Limpkin,_harns_marsh_(33723700146).jpg),\nCC BY 2.0).\n\nInstallation\n\nThis package is compatible with with all common versions of Python, it\nis tested on Linux, macOS and Windows systems.\n\nCourlan is available on the package repository [PyPI](https://pypi.org/)\nand can notably be installed with the Python package manager `pip`:\n\nThe last version to support Python 3.6 and 3.7 is `courlan==1.2.0`.\n\nPython\n\nMost filters revolve around the `strict` and `language` arguments.\n\ncheck_url()\n\nAll useful operations chained in `check_url(url)`:\n\nLanguage-aware heuristics, notably internationalization in URLs, are\navailable in `lang_filter(url, language)`:\n\nDefine stricter restrictions on the expected content type with\n`strict=True`. This also blocks certain platforms and page types\nwhere machines get lost.\n\nSampling by domain name\n\nWeb crawling and URL handling\n\nLink extraction and preprocessing:\n\nThe `filter_links()` function provides additional filters for crawling purposes:\nuse of robots.txt rules and link priorization. See `courlan.core` for details.\n\nDetermine if a link leads to another host:\n\nOther useful functions dedicated to URL handling:\n\n-   `extract_domain(url, fast=True)`: find domain and subdomain or just\n-   `get_base_url(url)`: strip the URL of some of its parts\n-   `get_host_and_path(url)`: decompose URLs in two parts: protocol +\n-   `get_hostinfo(url)`: extract domain and host info (protocol +\n-   `fix_relative_urls(baseurl, url)`: prepend necessary information to\n\nOther filters dedicated to crawl frontier management:\n\n-   `is_not_crawlable(url)`: check for deep web or pages generally not\n-   `is_navigation_page(url)`: check for navigation and overview pages\n\nSee also [URL management page](https://trafilatura.readthedocs.io/en/latest/url-management.html)\nof the Trafilatura documentation.\n\nPython helpers\n\nHelper function, scrub and normalize:\n\nBasic scrubbing only:\n\nBasic canonicalization/normalization only, i.e. modifying and\nstandardizing URLs in a consistent manner:\n\nBasic URL validation only:\n\nTroubleshooting"}, {"name": "courlan", "tags": ["math", "web"], "summary": "Clean, filter and sample URLs to optimize data collection \u2013 includes spam, content type and language filters.", "text": "Courlan uses an internal cache to speed up URL parsing. It can be reset\nas follows:\n\nUrlStore class\n\nThe `UrlStore` class allow for storing and retrieving domain-classified\nURLs, where a URL like `https://example.org/path/testpage` is stored as\nthe path `/path/testpage` within the domain `https://example.org`. It\nfeatures the following methods:\n\n- URL management\n   - `add_urls(urls=[], appendleft=None, visited=False)`: Add a\n   - `add_from_html(htmlstring, url, external=False, lang=None, with_nav=True)`:\n   - `discard(domains)`: Declare domains void and prune the store.\n   - `dump_urls()`: Return a list of all known URLs.\n   - `print_urls()`: Print all URLs in store (URL + TAB + visited or not).\n   - `print_unvisited_urls()`: Print all unvisited URLs in store.\n   - `get_all_counts()`: Return all download counts for the hosts in store.\n   - `get_known_domains()`: Return all known domains as a list.\n   - `get_unvisited_domains()`: Find all domains for which there are unvisited URLs.\n   - `total_url_number()`: Find number of all URLs in store.\n   - `is_known(url)`: Check if the given URL has already been stored.\n   - `has_been_visited(url)`: Check if the given URL has already been visited.\n   - `filter_unknown_urls(urls)`: Take a list of URLs and return the currently unknown ones.\n   - `filter_unvisited_urls(urls)`: Take a list of URLs and return the currently unvisited ones.\n   - `find_known_urls(domain)`: Get all already known URLs for the\n   - `find_unvisited_urls(domain)`: Get all unvisited URLs for the given domain.\n   - `get_unvisited_domains()`: Return all domains which have not been all visited.\n   - `reset()`: Re-initialize the URL store.\n\n- Crawling and downloads\n   - `get_url(domain)`: Retrieve a single URL and consider it to\n   - `get_rules(domain)`: Return the stored crawling rules for the given website.\n   - `store_rules(website, rules=None)`: Store crawling rules for a given website.\n   - `get_crawl_delay()`: Return the delay as extracted from robots.txt, or a given default.\n   - `get_download_urls(max_urls=100, time_limit=10)`: Get a list of immediately\n   - `establish_download_schedule(max_urls=100, time_limit=10)`:\n   - `download_threshold_reached(threshold)`: Find out if the\n   - `unvisited_websites_number()`: Return the number of websites\n   - `is_exhausted_domain(domain)`: Tell if all known URLs for\n\n- Persistance\n   - `write(filename)`: Save the store to disk.\n   - `load_store(filename)`: Read a UrlStore from disk (separate function, not class method).\n\n- Optional settings:\n   - `compressed=True`: activate compression of URLs and rules\n   - `language=XX`: focus on a particular target language (two-letter code)\n   - `strict=True`: stricter URL filtering\n   - `verbose=True`: dump URLs if interrupted (requires use of `signal`)\n\nCommand-line\n\nThe main fonctions are also available through a command-line utility:\n\nLicense\n\n*coURLan* is distributed under the [Apache 2.0\nlicense](https://www.apache.org/licenses/LICENSE-2.0.html).\n\nVersions prior to v1 were under GPLv3+ license.\n\nSettings\n\n`courlan` is optimized for English and German but its generic approach\nis also usable in other contexts.\n\nDetails of strict URL filtering can be reviewed and changed in the file\n`settings.py`. To override the default settings, clone the repository and\n[re-install the package\nlocally](https://packaging.python.org/tutorials/installing-packages/#installing-from-a-local-src-tree).\n\nContributing\n\n[Contributions](https://github.com/adbar/courlan/blob/master/CONTRIBUTING.md)\nare welcome!\n\nFeel free to file issues on the [dedicated\npage](https://github.com/adbar/courlan/issues).\n\nAuthor"}, {"name": "courlan", "tags": ["math", "web"], "summary": "Clean, filter and sample URLs to optimize data collection \u2013 includes spam, content type and language filters.", "text": "Developed with practical applications of academic research in mind, this software\nis part of a broader effort to derive information from web documents.\nExtracting and pre-processing web texts to the exacting standards of\nscientific research presents a substantial challenge.\nThis software package simplifies text data collection and enhances corpus quality,\nit is currently used to build [text databases for research](https://www.dwds.de/d/k-web).\n\n- Barbaresi, A. \"[Trafilatura: A Web Scraping Library and\n  Command-Line Tool for Text Discovery and\n  Extraction](https://aclanthology.org/2021.acl-demo.15/).\"\n  *Proceedings of ACL/IJCNLP 2021: System Demonstrations*, 2021, pp. 122-131.\n\nContact: see [homepage](https://adrien.barbaresi.eu/).\n\nSoftware ecosystem: see [this\ngraphic](https://github.com/adbar/trafilatura/blob/master/docs/software-ecosystem.png).\n\nSimilar work\n\nThese Python libraries perform similar handling and normalization tasks\nbut do not entail language or content filters. They also do not\nprimarily focus on crawl optimization:\n\nReferences\n\n-   Cho, J., Garcia-Molina, H., & Page, L. (1998). Efficient crawling\n-   Edwards, J., McCurley, K. S., and Tomlin, J. A. (2001). \"An"}, {"name": "courlan", "tags": ["math", "web"], "summary": "Clean, filter and sample URLs to optimize data collection \u2013 includes spam, content type and language filters.", "text": "This library is used to optimize data collection from the web by cleaning, filtering, and sampling URLs to exclude irrelevant or spam content. Developers can use coURLan to crawl the web in an efficient and scalable way while maintaining reasonable measures of quality or freshness."}, {"name": "crcmod", "tags": ["dev", "math", "ui"], "summary": "CRC Generator", "text": "===========================\ncrcmod for Calculating CRCs\n===========================\n\nThe software in this package is a Python module for generating objects that\ncompute the Cyclic Redundancy Check (CRC).  There is no attempt in this package\nto explain how the CRC works.  There are a number of resources on the web that\ngive a good explanation of the algorithms.  Just do a Google search for \"crc\ncalculation\" and browse till you find what you need.  Another resource can be\nfound in chapter 20 of the book \"Numerical Recipes in C\" by Press et. al.\n\nThis package allows the use of any 8, 16, 24, 32, or 64 bit CRC.  You can\ngenerate a Python function for the selected polynomial or an instance of the\nCrc class which provides the same interface as the ``md5`` and ``sha`` modules\nfrom the Python standard library.  A ``Crc`` class instance can also generate\nC/C++ source code that can be used in another application.\n\n----------\nGuidelines\n----------\n\nDocumentation is available from the doc strings.  It is up to you to decide\nwhat polynomials to use in your application.  If someone has not specified the\npolynomials to use, you will need to do some research to find one suitable for\nyour application.  Examples are available in the unit test script ``test.py``.\nYou may also use the ``predefined`` module to select one of the standard\npolynomials.\n\nIf you need to generate code for another language, I suggest you subclass the\n``Crc`` class and replace the method ``generateCode``.  Use ``generateCode`` as\na model for the new version.\n\n------------\nDependencies\n------------\n\nPython Version\n^^^^^^^^^^^^^^\n\nThe package has separate code to support the 2.x and 3.x Python series.\n\nFor the 2.x versions of Python, these versions have been tested:\n\n* 2.4\n* 2.5\n* 2.6\n* 2.7\n\nIt may still work on earlier versions of Python 2.x, but these have not been\nrecently tested.\n\nFor the 3.x versions of Python, these versions have been tested:\n\n* 3.1\n\nBuilding C extension\n^^^^^^^^^^^^^^^^^^^^\n\nTo build the C extension, the appropriate compiler tools for your platform must\nbe installed. Refer to the Python documentation for building C extensions for\ndetails.\n\n------------\nInstallation\n------------\n\nThe crcmod package is installed using ``distutils``.\nRun the following command::\n\nIf the extension module builds, it will be installed.  Otherwise, the\ninstallation will include the pure Python version.  This will run significantly\nslower than the extension module but will allow the package to be used.\n\nFor Windows users who want to use the mingw32 compiler, run this command::\n\nFor Python 3.x, the install process is the same but you need to use the 3.x\ninterpreter.\n\n------------\nUnit Testing\n------------\n\nThe ``crcmod`` package has a module ``crcmod.test``, which contains unit\ntests for both ``crcmod`` and ``crcmod.predefined``.\n\nWhen you first install ``crcmod``, you should run the unit tests to make sure\neverything is installed properly.  The test script performs a number of tests\nincluding a comparison to the direct method which uses a class implementing\npolynomials over the integers mod 2.\n\nTo run the unit tests on Python >=2.5::\n\nAlternatively, in the ``test`` directory run::\n\n---------------\nCode Generation\n---------------\n\nThe crcmod package is capable of generating C functions that can be compiled\nwith a C or C++ compiler.  In the test directory, there is an examples.py\nscript that demonstrates how to use the code generator.  The result of this is\nwritten out to the file ``examples.c``.  The generated code was checked to make\nsure it compiles with the GCC compiler.\n\n-------\nLicense\n-------\n\nThe ``crcmod`` package is released under the MIT license. See the ``LICENSE``\nfile for details.\n\n------------\nContributors\n------------\n\nCraig McQueen"}, {"name": "crcmod", "tags": ["dev", "math", "ui"], "summary": "CRC Generator", "text": "This library is used to generate Cyclic Redundancy Checks (CRCs) in various bit lengths, allowing developers to calculate CRC values for different polynomial requirements. This can be useful for data integrity checks and error detection in a variety of applications."}, {"name": "ctranslate2", "tags": ["data", "math", "ml", "web"], "summary": "Fast inference engine for Transformer models", "text": "CTranslate2\n\nCTranslate2 is a C++ and Python library for efficient inference with Transformer models.\n\nThe project implements a custom runtime that applies many performance optimization techniques such as weights quantization, layers fusion, batch reordering, etc., to [accelerate and reduce the memory usage](#benchmarks) of Transformer models on CPU and GPU.\n\nThe following model types are currently supported:\n\n* Encoder-decoder models: Transformer base/big, M2M-100, NLLB, BART, mBART, Pegasus, T5, Whisper\n* Decoder-only models: GPT-2, GPT-J, GPT-NeoX, OPT, BLOOM, MPT, Llama, Mistral, Gemma, CodeGen, GPTBigCode, Falcon, Qwen2\n* Encoder-only models: BERT, DistilBERT, XLM-RoBERTa\n\nCompatible models should be first converted into an optimized model format. The library includes converters for multiple frameworks:\n\n* [OpenNMT-py](https://opennmt.net/CTranslate2/guides/opennmt_py.html)\n* [OpenNMT-tf](https://opennmt.net/CTranslate2/guides/opennmt_tf.html)\n* [Fairseq](https://opennmt.net/CTranslate2/guides/fairseq.html)\n* [Marian](https://opennmt.net/CTranslate2/guides/marian.html)\n* [OPUS-MT](https://opennmt.net/CTranslate2/guides/opus_mt.html)\n* [Transformers](https://opennmt.net/CTranslate2/guides/transformers.html)\n\nThe project is production-oriented and comes with [backward compatibility guarantees](https://opennmt.net/CTranslate2/versioning.html), but it also includes experimental features related to model compression and inference acceleration.\n\nKey features\n\n* **Fast and efficient execution on CPU and GPU**The execution [is significantly faster and requires less resources](#benchmarks) than general-purpose deep learning frameworks on supported models and tasks thanks to many advanced optimizations: layer fusion, padding removal, batch reordering, in-place operations, caching mechanism, etc.\n* **Quantization and reduced precision**The model serialization and computation support weights with [reduced precision](https://opennmt.net/CTranslate2/quantization.html): 16-bit floating points (FP16), 16-bit brain floating points (BF16), 16-bit integers (INT16), 8-bit integers (INT8) and AWQ quantization (INT4).\n* **Multiple CPU architectures support**The project supports x86-64 and AArch64/ARM64 processors and integrates multiple backends that are optimized for these platforms: [Intel MKL](https://software.intel.com/content/www/us/en/develop/tools/oneapi/components/onemkl.html), [oneDNN](https://github.com/oneapi-src/oneDNN), [OpenBLAS](https://www.openblas.net/), [Ruy](https://github.com/google/ruy), and [Apple Accelerate](https://developer.apple.com/documentation/accelerate).\n* **Automatic CPU detection and code dispatch**One binary can include multiple backends (e.g. Intel MKL and oneDNN) and instruction set architectures (e.g. AVX, AVX2) that are automatically selected at runtime based on the CPU information.\n* **Parallel and asynchronous execution**Multiple batches can be processed in parallel and asynchronously using multiple GPUs or CPU cores.\n* **Dynamic memory usage**The memory usage changes dynamically depending on the request size while still meeting performance requirements thanks to caching allocators on both CPU and GPU.\n* **Lightweight on disk**Quantization can make the models 4 times smaller on disk with minimal accuracy loss.\n* **Simple integration**The project has few dependencies and exposes simple APIs in [Python](https://opennmt.net/CTranslate2/python/overview.html) and C++ to cover most integration needs.\n* **Configurable and interactive decoding**[Advanced decoding features](https://opennmt.net/CTranslate2/decoding.html) allow autocompleting a partial sequence and returning alternatives at a specific location in the sequence.\n* **Support tensor parallelism for distributed inference**Very large model can be split into multiple GPUs. Following this [documentation](docs/parallel.md#model-and-tensor-parallelism) to set up the required environment.\n\nSome of these features are difficult to achieve with standard deep learning frameworks and are the motivation for this project.\n\nInstallation and usage\n\nCTranslate2 can be installed with pip:\n\nThe Python module is used to convert models and can translate or generate text with few lines of code:\n\nSee the [documentation](https://opennmt.net/CTranslate2) for more information and examples.\n\nBenchmarks\n\nWe translate the En->De test set *newstest2014* with multiple models:\n\n* [OpenNMT-tf WMT14](https://opennmt.net/Models-tf/#translation): a base Transformer trained with OpenNMT-tf on the WMT14 dataset (4.5M lines)\n* [OpenNMT-py WMT14](https://opennmt.net/Models-py/#translation): a base Transformer trained with OpenNMT-py on the WMT14 dataset (4.5M lines)\n* [OPUS-MT](https://github.com/Helsinki-NLP/OPUS-MT-train/tree/master/models/en-de#opus-2020-02-26zip): a base Transformer trained with Marian on all OPUS data available on 2020-02-26 (81.9M lines)\n\nThe benchmark reports the number of target tokens generated per second (higher is better). The results are aggregated over multiple runs. See the [benchmark scripts](tools/benchmark) for more details and reproduce these numbers.\n\n**Please note that the results presented below are only valid for the configuration used during this benchmark: absolute and relative performance may change with different settings.**\n\nCPU\n\nTokens per second\n---\n**OpenNMT-tf WMT14 model**\nOpenNMT-tf 2.31.0 (with TensorFlow 2.11.0)\n**OpenNMT-py WMT14 model**\nOpenNMT-py 3.0.4 (with PyTorch 1.13.1)\n- int8\nCTranslate2 3.6.0\n- int16\n- int8\n- int8 + vmap\n**OPUS-MT model**\nTransformers 4.26.1 (with PyTorch 1.13.1)\nMarian 1.11.0\n- int16\n- int8\nCTranslate2 3.6.0\n- int16\n- int8\n\nExecuted with 4 threads on a [*c5.2xlarge*](https://aws.amazon.com/ec2/instance-types/c5/) Amazon EC2 instance equipped with an Intel(R) Xeon(R) Platinum 8275CL CPU.\n\nGPU\n\nTokens per second\n---\n**OpenNMT-tf WMT14 model**\nOpenNMT-tf 2.31.0 (with TensorFlow 2.11.0)\n**OpenNMT-py WMT14 model**\nOpenNMT-py 3.0.4 (with PyTorch 1.13.1)\nFasterTransformer 5.3\n- float16\nCTranslate2 3.6.0\n- int8\n- float16\n- int8 + float16\n**OPUS-MT model**\nTransformers 4.26.1 (with PyTorch 1.13.1)\nMarian 1.11.0\n- float16\nCTranslate2 3.6.0\n- int8\n- float16\n- int8 + float16\n\nExecuted with CUDA 11 on a [*g5.xlarge*](https://aws.amazon.com/ec2/instance-types/g5/) Amazon EC2 instance equipped with a NVIDIA A10G GPU (driver version: 510.47.03).\n\nAdditional resources\n\n* [Documentation](https://opennmt.net/CTranslate2)\n* [Forum](https://forum.opennmt.net)\n* [Gitter](https://gitter.im/OpenNMT/CTranslate2)"}, {"name": "ctranslate2", "tags": ["data", "math", "ml", "web"], "summary": "Fast inference engine for Transformer models", "text": "This library is used to accelerate and reduce memory usage of various Transformer models on CPU and GPU through a custom runtime that applies performance optimization techniques. Developers can use CTranslate2 to efficiently perform inference with a wide range of pre-trained models in Python or C++."}, {"name": "cuda-bindings", "tags": ["data", "math", "web"], "summary": "Python bindings for CUDA", "text": ".. SPDX-FileCopyrightText: Copyright (c) 2025 NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n.. SPDX-License-Identifier: LicenseRef-NVIDIA-SOFTWARE-LICENSE\n\n****************************************\ncuda-bindings: Low-level CUDA interfaces\n****************************************\n\n   :target: https://www.nvidia.com/\n   :alt: NVIDIA\n\n`cuda.bindings `_ is a standard set of low-level interfaces, providing full coverage of and 1:1 access to the CUDA host APIs from Python. Checkout the `Overview `_ for the workflow and performance results.\n\n* `Repository `_\n* `Documentation `_\n* `Examples `_\n* `Issue tracker `_\n\nFor the installation instruction, please refer to the `Installation `_ page."}, {"name": "cuda-bindings", "tags": ["data", "math", "web"], "summary": "Python bindings for CUDA", "text": "This library is used to provide low-level access to CUDA host APIs from Python, allowing developers to leverage the full capabilities of NVIDIA GPUs in their applications. With cuda-bindings, developers can write high-performance CUDA code using a standard set of interfaces and a familiar Python programming language."}, {"name": "cuda-python", "tags": ["math", "ui", "web"], "summary": "CUDA Python: Performance meets Productivity", "text": ".. SPDX-FileCopyrightText: Copyright (c) 2025 NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n.. SPDX-License-Identifier: LicenseRef-NVIDIA-SOFTWARE-LICENSE\n\n**************************************************************\ncuda-python: Metapackage collection of CUDA Python subpackages\n**************************************************************\n\nCUDA Python is the home for accessing NVIDIA's CUDA platform from Python. It consists of multiple components:\n\n* `cuda.core `_: Pythonic access to CUDA Runtime and other core functionality\n* `cuda.bindings `_: Low-level Python bindings to CUDA C APIs\n* `cuda.pathfinder `_: Utilities for locating CUDA components installed in the user's Python environment\n* `cuda.coop `_: A Python module providing CCCL's reusable block-wide and warp-wide *device* primitives for use within Numba CUDA kernels\n* `cuda.compute `_: A Python module for easy access to CCCL's highly efficient and customizable parallel algorithms, like ``sort``, ``scan``, ``reduce``, ``transform``, etc. that are callable on the *host*\n* `numba.cuda `_: A Python DSL that exposes CUDA **SIMT** programming model and compiles a restricted subset of Python code into CUDA kernels and device functions\n* `cuda.tile `_: A new Python DSL that exposes CUDA **Tile** programming model and allows users to write NumPy-like code in CUDA kernels\n* `nvmath-python `_: Pythonic access to NVIDIA CPU & GPU Math Libraries, with `host `_, `device `_, and `distributed `_ APIs. It also provides low-level Python bindings to host C APIs (`nvmath.bindings `_).\n* `nvshmem4py `_: Pythonic interface to the NVSHMEM library, enabling Python applications to leverage NVSHMEM's high-performance PGAS (Partitioned Global Address Space) programming model for GPU-accelerated computing\n* `Nsight Python `_: Python kernel profiling interface that automates performance analysis across multiple kernel configurations using NVIDIA Nsight Tools\n* `CUPTI Python `_: Python APIs for creation of profiling tools that target CUDA Python applications via the CUDA Profiling Tools Interface (CUPTI)\n\nCUDA Python is currently undergoing an overhaul to improve existing and introduce new components. All of the previously available functionality from the ``cuda-python`` package will continue to be available, please refer to the `cuda.bindings `_ documentation for installation guide and further detail.\n\ncuda-python as a metapackage\n============================\n\n``cuda-python`` is being restructured to become a metapackage that contains a collection of subpackages. Each subpackage is versioned independently, allowing installation of each component as needed.\n\nSubpackage: cuda.core\n---------------------\n\nThe ``cuda.core`` package offers idiomatic, pythonic access to CUDA Runtime and other functionality.\n\nThe goals are to\n\n1. Provide **idiomatic (\"pythonic\")** access to CUDA Driver, Runtime, and JIT compiler toolchain\n2. Focus on **developer productivity** by ensuring end-to-end CUDA development can be performed quickly and entirely in Python\n3. **Avoid homegrown** Python abstractions for CUDA for new Python GPU libraries starting from scratch\n4. **Ease** developer **burden of maintaining** and catching up with latest CUDA features\n5. **Flatten the learning curve** for current and future generations of CUDA developers\n\nSubpackage: cuda.bindings\n-------------------------\n\nThe ``cuda.bindings`` package is a standard set of low-level interfaces, providing full coverage of and access to the CUDA host APIs from Python.\n\nThe list of available interfaces is:\n\n* CUDA Driver\n* CUDA Runtime\n* NVRTC\n* nvJitLink\n* NVVM\n* cuFile"}, {"name": "cuda-python", "tags": ["math", "ui", "web"], "summary": "CUDA Python: Performance meets Productivity", "text": "This library is used to provide a collection of Python subpackages that offer direct access to NVIDIA's CUDA platform, enabling developers to utilize its performance capabilities from within their Python applications. With cuda-python, developers can efficiently integrate CUDA functionality into their projects, streamlining the development process while maintaining high-performance computing requirements."}, {"name": "cupy-cuda12x", "tags": ["math"], "summary": "CuPy: NumPy & SciPy for GPU", "text": ".. image:: https://raw.githubusercontent.com/cupy/cupy/main/docs/image/cupy_logo_1000px.png\n   :width: 400\n\nCuPy : NumPy & SciPy for GPU\n============================\n\n`CuPy `_ is a NumPy/SciPy-compatible array library for GPU-accelerated computing with Python.\n\nThis is a CuPy wheel (precompiled binary) package for CUDA 12.x.\nYou need to install `CUDA Toolkit 12.x `_ to use these packages.\n\nIf you have another version of CUDA, or want to build from source, refer to the `Installation Guide `_ for instructions."}, {"name": "cupy-cuda12x", "tags": ["math"], "summary": "CuPy: NumPy & SciPy for GPU", "text": "This library is used to accelerate NumPy and SciPy operations on a GPU with Python, allowing developers to leverage high-performance computing. With cupy-cuda12x, developers can migrate CPU-bound computations to run efficiently on NVIDIA GPUs."}, {"name": "cvxopt", "tags": ["math"], "summary": "Convex optimization package", "text": "CVXOPT is a free software package for convex optimization based on the\nPython programming language. It can be used with the interactive Python\ninterpreter, on the command line by executing Python scripts, or\nintegrated in other software via Python extension modules. Its main\npurpose is to make the development of software for convex optimization\napplications straightforward by building on Python's extensive standard\nlibrary and on the strengths of Python as a high-level programming\nlanguage."}, {"name": "cvxopt", "tags": ["math"], "summary": "Convex optimization package", "text": "This library is used to develop software for convex optimization applications, leveraging the strengths of Python as a high-level programming language. It enables developers to create efficient and effective solutions for complex optimization problems in various fields such as operations research, machine learning, and data science."}, {"name": "cymem", "tags": ["math"], "summary": "Manage calls to calloc/free through Cython", "text": "cymem: A Cython Memory Helper\n\ncymem provides two small memory-management helpers for Cython. They make it easy\nto tie memory to a Python object's life-cycle, so that the memory is freed when\nthe object is garbage collected.\n\n(https://github.com/explosion/cymem/actions/workflows/tests.yml)\n(https://pypi.python.org/pypi/cymem)\n(https://anaconda.org/conda-forge/cymem)\n(https://github.com/explosion/wheelwright/releases)\n\nOverview\n\nThe most useful is `cymem.Pool`, which acts as a thin wrapper around the calloc\nfunction:\n\nThe `Pool` object saves the memory addresses internally, and frees them when the\nobject is garbage collected. Typically you'll attach the `Pool` to some cdef'd\nclass. This is particularly handy for deeply nested structs, which have\ncomplicated initialization functions. Just pass the `Pool` object into the\ninitializer, and you don't have to worry about freeing your struct at all \u2014 all\nof the calls to `Pool.alloc` will be automatically freed when the `Pool`\nexpires.\n\nInstallation\n\nInstallation is via [pip](https://pypi.python.org/pypi/pip), and requires\n[Cython](http://cython.org). Before installing, make sure that your `pip`,\n`setuptools` and `wheel` are up to date.\n\nExample Use Case: An array of structs\n\nLet's say we want a sequence of sparse matrices. We need fast access, and a\nPython list isn't performing well enough. So, we want a C-array or C++ vector,\nwhich means we need the sparse matrix to be a C-level struct \u2014 it can't be a\nPython class. We can write this easily enough in Cython:\n\nWe wrap the data structure in a Python ref-counted class at as low a level as we\ncan, given our performance constraints. This allows us to allocate and free the\nmemory in the `__cinit__` and `__dealloc__` Cython special methods.\n\nHowever, it's very easy to make mistakes when writing the `__dealloc__` and\n`sparse_matrix_free` functions, leading to memory leaks. cymem prevents you from\nwriting these deallocators at all. Instead, you write as follows:\n\nAll that the `Pool` class does is remember the addresses it gives out. When the\n`MatrixArray` object is garbage-collected, the `Pool` object will also be\ngarbage collected, which triggers a call to `Pool.__dealloc__`. The `Pool` then\nfrees all of its addresses. This saves you from walking back over your nested\ndata structures to free them, eliminating a common class of errors.\n\nCustom Allocators\n\nSometimes external C libraries use private functions to allocate and free\nobjects, but we'd still like the laziness of the `Pool`.\n\nThread Safety\n\nAs of version 2.0.12, `cymem.Pool` is thread-safe when used with CPython 3.13+\nfree-threaded builds (PEP 703). All operations on the Pool, including `alloc()`,\n`free()`, and `realloc()`, can be safely called from multiple threads concurrently.\n\n**Key guarantees:**\n- Multiple threads can safely call `alloc()`, `free()`, and `realloc()` on the\n- The Pool's internal bookkeeping (`addresses` dict and `size` accounting) is\n\n**Important notes:**\n- Individual Pool instances are thread-safe, but you are still responsible for\n- Custom memory allocators need to be thread-safe themselves."}, {"name": "cymem", "tags": ["math"], "summary": "Manage calls to calloc/free through Cython", "text": "This library is used to automatically manage memory allocation and deallocation through Cython using a thin wrapper around calloc, allowing developers to tie memory to the life-cycle of Python objects. By attaching the cymem.Pool to a C-extended class, developers can easily manage complex memory usage patterns without manually tracking memory addresses."}, {"name": "cytoolz", "tags": ["math", "web"], "summary": "Cython implementation of Toolz: High performance functional utilities", "text": "CyToolz\n=======\n\nBuild Status\n\nCython implementation of the\nliteral toolz\nprovides high performance utility functions for iterables, functions,\nand dictionaries.\n\n.. |literal toolz| replace:: ``toolz``\n.. _literal toolz: https://github.com/pytoolz/toolz\n\n``toolz`` is a pure Python package that borrows heavily from contemporary\nfunctional languanges.  It is designed to interoperate seamlessly with other\nlibraries including ``itertools``, ``functools``, and third party libraries.\nHigh performance functional data analysis is possible with builtin types\nlike ``list`` and ``dict``, and user-defined data structures; and low memory\nusage is achieved by using the iterator protocol and returning iterators\nwhenever possible.\n\n``cytoolz`` implements the same API as ``toolz``.  The main differences are\nthat ``cytoolz`` is faster (typically 2-5x faster with a few spectacular\nexceptions) and ``cytoolz`` offers a C API that is accessible to other\nprojects developed in Cython.  Since ``toolz`` is able to process very\nlarge (potentially infinite) data sets, the performance increase gained by\nusing ``cytoolz`` can be significant.\n\nSee the PyToolz documentation at https://toolz.readthedocs.io and the full\n`API Documentation `__\nfor more details.\n\n*CyToolz has experimental support for Python free-threading introduced in\nPython 3.13 and provides wheels for free-threaded Python. CyToolz has not\nyet been developed or tested for thread-safety, so use at your own risk!\nIf you encounter any issues, please*\n`let us know `__.\n\nLICENSE\n-------\n\nNew BSD. See `License File `__.\n\nInstall\n-------\n\n``cytoolz`` is on the Python Package Index (PyPI):\n\n::\n\nDependencies\n------------\n\n``cytoolz`` supports Python 3.9+ with a common codebase.\nIt is developed in Cython, but requires no dependecies other than CPython\nand a C compiler.  Like ``toolz``, it is a light weight dependency.\n\nContributions Welcome\n---------------------\n\n``toolz`` (and ``cytoolz``) aims to be a repository for utility functions,\nparticularly those that come from the functional programming and list\nprocessing traditions. We welcome contributions that fall within this scope\nand encourage users to scrape their ``util.py`` files for functions that are\nbroadly useful.\n\nPlease take a look at our issue pages for\n`toolz `__ and\n`cytoolz `__\nfor contribution ideas.\n\nCommunity\n---------\n\nSee our `mailing list `__.\nWe're friendly.\n\n   :target: https://github.com/pytoolz/cytoolz/actions"}, {"name": "cytoolz", "tags": ["math", "web"], "summary": "Cython implementation of Toolz: High performance functional utilities", "text": "This library is used to add high performance functional utilities for iterables, functions, and dictionaries to Python applications, allowing for efficient data analysis with low memory usage. It provides a Cython implementation of the Toolz package, enabling seamless integration with other libraries such as itertools, functools, and third-party libraries."}, {"name": "dash", "tags": ["data", "math", "visualization", "web"], "summary": "A Python framework for building reactive web-apps. Developed by Plotly.", "text": "Dash\n\n(https://circleci.com/gh/plotly/dash)\n(https://github.com/plotly/dash/blob/master/LICENSE)\n(https://pypi.org/project/dash/)\n(https://pypi.org/project/dash/)\n(https://github.com/plotly/dash/graphs/contributors)\n\n*Dash is the most downloaded, trusted Python framework for building ML & data science web apps*.\n\nBuilt on top of [Plotly.js](https://github.com/plotly/plotly.js), [React](https://reactjs.org/) and [Flask](https://palletsprojects.com/p/flask/), Dash ties modern UI elements like dropdowns, sliders, and graphs directly to your analytical Python code. Read [our tutorial](https://dash.plotly.com/getting-started) (proudly crafted \ufe0f with Dash itself).\n\n- [Docs](https://dash.plotly.com/getting-started): Create your first Dash app in under 5 minutes\n\n- [dash.gallery](https://dash.gallery): Dash app gallery with Python & R code\n\nDash App Examples\n\nDash App\n---\nHere\u2019s a simple example of a Dash App that ties a Dropdown to a Plotly Graph. As the user selects a value in the Dropdown, the application code dynamically exports data from Google Finance into a Pandas DataFrame. This app was written in just **43** lines of code ([view the source](https://gist.github.com/chriddyp/3d2454905d8f01886d651f207e2419f0)).\nDash app code is declarative and reactive, which makes it easy to build complex apps that contain many interactive elements. Here\u2019s an example with 5 inputs, 3 outputs, and cross filtering. This app was composed in just 160 lines of code, all of which were Python.\nDash uses [Plotly.js](https://github.com/plotly/plotly.js) for charting. About 50 chart types are supported, including maps.\nDash isn't just for dashboards. You have full control over the look and feel of your applications. Here's a Dash App that's styled to look like a PDF report.\n\nTo learn more about Dash, read the [extensive announcement letter](https://medium.com/@plotlygraphs/introducing-dash-5ecf7191b503) or [jump in with the user guide](https://plotly.com/dash).\n\nDash OSS & Dash Enterprise\n\nWith Dash Open Source, Dash apps run on your local laptop or workstation, but cannot be easily accessed by others in your organization.\n\nScale up with Dash Enterprise when your Dash app is ready for department or company-wide consumption. Or, launch your initiative with Dash Enterprise from the start to unlock developer productivity gains and hands-on acceleration from Plotly's team.\n\nML Ops Features: A one-stop shop for ML Ops: Horizontally scalable hosting, deployment, and authentication for your Dash apps. No IT or DevOps required.\n\nLow-Code Features: Low-code Dash app capabilities that supercharge developer productivity.\n\nEnterprise AI Features: Everything that your data science team needs to rapidly deliver AI/ML research and business initiatives.\n\nSee [https://plotly.com/contact-us/](https://plotly.com/contact-us/) to get in touch."}, {"name": "dash", "tags": ["data", "math", "visualization", "web"], "summary": "A Python framework for building reactive web-apps. Developed by Plotly.", "text": "This library is used to build reactive web-apps for data science and machine learning, allowing developers to combine modern UI elements with their analytical Python code. With Dash, developers can create fully interactive web applications in a matter of minutes."}, {"name": "dask-expr", "tags": ["math", "web"], "summary": "High Level Expressions for Dask", "text": "Dask Expressions\n================\n\nDask DataFrames with query optimization.\n\nThis is a rewrite of Dask DataFrame that includes query\noptimization and generally improved organization.\n\nMore in our blog posts:\n- [Dask Expressions overview](https://blog.dask.org/2023/08/25/dask-expr-introduction)\n- [TPC-H benchmark results vs. Dask DataFrame](https://docs.coiled.io/blog/tpch.html)\n\nExample\n-------\n\nQuery Representation\n--------------------\n\nDask-expr encodes user code in an expression tree:\n\nThis expression tree will be optimized and modified before execution:\n\nStability\n---------\n\nThis is the default backend for dask.DataFrame since version 2024.3.0.\n\nAPI Coverage\n------------\n\nDask-Expr covers almost everything of the Dask DataFrame API. The only missing features are:\n\n- named GroupBy Aggregations"}, {"name": "dask-expr", "tags": ["math", "web"], "summary": "High Level Expressions for Dask", "text": "This library is used to optimize and execute complex data queries on large datasets, rewriting user code into an optimized expression tree for efficient execution with Dask DataFrames. It provides a high-level interface for query optimization and improved organization of Dask DataFrame operations, covering almost the entire Dask DataFrame API."}, {"name": "datasets", "tags": ["data", "math", "ml", "web"], "summary": "HuggingFace community-driven open-source library of datasets", "text": "Installation\n\nWith pip\n\n Datasets can be installed from PyPi and has to be installed in a virtual environment (venv or conda for instance)\n\nWith conda\n\n Datasets can be installed using conda as follows:\n\nFollow the installation pages of TensorFlow and PyTorch to see how to install them with conda.\n\nFor more details on installation, check the installation page in the documentation: https://huggingface.co/docs/datasets/installation\n\nInstallation to use with Machine Learning & Data frameworks frameworks\n\nIf you plan to use  Datasets with PyTorch (2.0+), TensorFlow (2.6+) or JAX (3.14+) you should also install PyTorch, TensorFlow or JAX.\n Datasets is also well integrated with data frameworks like PyArrow, Pandas, Polars and Spark, which should be installed separately.\n\nFor more details on using the library with these frameworks, check the quick start page in the documentation: https://huggingface.co/docs/datasets/quickstart\n\nUsage\n\n Datasets is made to be very simple to use - the API is centered around a single function, `datasets.load_dataset(dataset_name, **kwargs)`, that instantiates a dataset.\n\nThis library can be used for text/image/audio/etc. datasets. Here is an example to load a text dataset:\n\nHere is a quick example:\n\nIf your dataset is bigger than your disk or if you don't want to wait to download the data, you can use streaming:\n\nFor more details on using the library, check the quick start page in the documentation: https://huggingface.co/docs/datasets/quickstart and the specific pages on:\n\n- etc.\n\nAdd a new dataset to the Hub\n\nWe have a very detailed step-by-step guide to add a new dataset to the  datasets already provided on the [HuggingFace Datasets Hub](https://huggingface.co/datasets).\n\nYou can find:\n- [how to upload a dataset to the Hub using your web browser or Python](https://huggingface.co/docs/datasets/upload_dataset) and also\n- [how to upload it using Git](https://huggingface.co/docs/datasets/share).\n\nDisclaimers\n\nYou can use  Datasets to load datasets based on versioned git repositories maintained by the dataset authors. For reproducibility reasons, we ask users to pin the `revision` of the repositories they use.\n\nIf you're a dataset owner and wish to update any part of it (description, citation, license, etc.), or do not want your dataset to be included in the Hugging Face Hub, please get in touch by opening a discussion or a pull request in the Community tab of the dataset page. Thanks for your contribution to the ML community!\n\nBibTeX\n\nIf you want to cite our  Datasets library, you can use our [paper](https://huggingface.co/papers/2109.02846):\n\nIf you need to cite a specific version of our  Datasets library for reproducibility, you can use the corresponding version Zenodo DOI from this [list](https://zenodo.org/search?q=conceptrecid:%224817768%22&sort=-version&all_versions=True)."}, {"name": "datasets", "tags": ["data", "math", "ml", "web"], "summary": "HuggingFace community-driven open-source library of datasets", "text": "This library is used to provide a unified interface for loading and managing various types of datasets in machine learning pipelines. With this library, developers can easily access and utilize a wide range of datasets from the Hugging Face community, streamlining their data preparation workflows."}, {"name": "datasketch", "tags": ["data", "math", "web"], "summary": "Probabilistic data structures for processing and searching very large datasets", "text": "datasketch: Big Data Looks Small\n================================\n\n   :target: https://zenodo.org/doi/10.5281/zenodo.598238\n\ndatasketch gives you probabilistic data structures that can process and\nsearch very large amount of data super fast, with little loss of\naccuracy.\n\nThis package contains the following data sketches:\n\n+-------------------------+-----------------------------------------------+\nData Sketch\n+=========================+===============================================+\n`MinHash`_\n+-------------------------+-----------------------------------------------+\n`Weighted MinHash`_\n+-------------------------+-----------------------------------------------+\n`HyperLogLog`_\n+-------------------------+-----------------------------------------------+\n`HyperLogLog++`_\n+-------------------------+-----------------------------------------------+\n\nThe following indexes for data sketches are provided to support\nsub-linear query time:\n\n+---------------------------+-----------------------------+------------------------+\nIndex\n+===========================+=============================+========================+\n`MinHash LSH`_\n+---------------------------+-----------------------------+------------------------+\n`LSHBloom`_\n+---------------------------+-----------------------------+------------------------+\n`MinHash LSH Forest`_\n+---------------------------+-----------------------------+------------------------+\n`MinHash LSH Ensemble`_\n+---------------------------+-----------------------------+------------------------+\n`HNSW`_\n+---------------------------+-----------------------------+------------------------+\n\ndatasketch must be used with Python 3.9 or above, NumPy 1.11 or above, and Scipy.\n\nNote that `MinHash LSH`_ and `MinHash LSH Ensemble`_ also support Redis and Cassandra \nstorage layer (see `MinHash LSH at Scale`_).\n\nInstall\n-------\n\nTo install datasketch using ``pip``:\n\n.. code-block:: bash\n\nThis will also install NumPy as dependency.\n\nTo install with Redis dependency:\n\n.. code-block:: bash\n\nTo install with Cassandra dependency:\n\n.. code-block:: bash\n\nTo install with Bloom filter dependency:\n\n.. code-block:: bash\n\n.. _`MinHash`: https://ekzhu.github.io/datasketch/minhash.html\n.. _`Weighted MinHash`: https://ekzhu.github.io/datasketch/weightedminhash.html\n.. _`HyperLogLog`: https://ekzhu.github.io/datasketch/hyperloglog.html\n.. _`HyperLogLog++`: https://ekzhu.github.io/datasketch/hyperloglog.html#hyperloglog-plusplus\n.. _`MinHash LSH`: https://ekzhu.github.io/datasketch/lsh.html\n.. _`MinHash LSH Forest`: https://ekzhu.github.io/datasketch/lshforest.html\n.. _`MinHash LSH Ensemble`: https://ekzhu.github.io/datasketch/lshensemble.html\n.. _`LSHBloom`: https://ekzhu.github.io/datasketch/lshbloom.html\n.. _`Minhash LSH at Scale`: http://ekzhu.github.io/datasketch/lsh.html#minhash-lsh-at-scale\n.. _`HNSW`: https://ekzhu.github.io/datasketch/documentation.html#hnsw\n\nContributing\n------------\n\nWe welcome contributions from everyone. Whether you're fixing bugs, adding features, improving documentation, or helping with tests, your contributions are valuable.\n\nDevelopment Setup\n^^^^^^^^^^^^^^^^^\n\nThe project uses `uv` for fast and reliable Python package management. Follow these steps to set up your development environment:\n\n1. **Install uv**: Follow the official installation guide at https://docs.astral.sh/uv/getting-started/installation/\n\n2. **Clone the repository**:\n\n   .. code-block:: bash\n\n3. **Set up the environment**:\n\n   .. code-block:: bash\n\n4. **Verify installation**:\n\n   .. code-block:: bash\n\n5. **Optional dependencies** (for specific development needs):\n\n   .. code-block:: bash\n\nLearn more about `uv` at https://docs.astral.sh/uv/\n\nDevelopment Workflow\n^^^^^^^^^^^^^^^^^^^^\n\n1. **Fork the repository** on GitHub if you haven't already.\n\n2. **Create a feature branch** for your changes:\n\n   .. code-block:: bash\n\n3. **Make your changes** following the project's coding standards.\n\n4. **Run the tests** to ensure nothing is broken:\n\n   .. code-block:: bash\n\n5. **Check code quality** with ruff:\n\n   .. code-block:: bash\n\n6. **Commit your changes** with a clear, descriptive commit message:\n\n   .. code-block:: bash\n\n7. **Push to your fork** and create a pull request on GitHub:\n\n   .. code-block:: bash\n\n8. **Respond to feedback** from maintainers and iterate on your changes.\n\nGuidelines\n^^^^^^^^^^\n\n- Follow PEP 8 style guidelines\n- Write tests for new features\n- Update documentation as needed\n- Keep commits focused and atomic\n- Be respectful in discussions\n\nFor more information, check the `GitHub issues `_ for current priorities or areas needing help. You can also join the discussion on `project roadmap and priorities `_."}, {"name": "datasketch", "tags": ["data", "math", "web"], "summary": "Probabilistic data structures for processing and searching very large datasets", "text": "This library is used to efficiently process and search large datasets with minimal loss of accuracy. With datasketch, developers can quickly identify similarities or differences between very large amounts of data."}, {"name": "dawg-python", "tags": ["math", "web"], "summary": "Pure-python reader for DAWGs (DAFSAs) created by dawgdic C++ library or DAWG Python extension.", "text": "DAWG-Python\n===========\n\n.. image:: https://travis-ci.org/kmike/DAWG-Python.png?branch=master\n\nThis pure-python package provides read-only access for files\ncreated by `dawgdic`_ C++ library and `DAWG`_ python package.\n\n.. _dawgdic: https://code.google.com/p/dawgdic/\n.. _DAWG: https://github.com/kmike/DAWG\n\nThis package is not capable of creating DAWGs. It works with DAWGs built by\n`dawgdic`_ C++ library or `DAWG`_ Python extension module. The main purpose\nof DAWG-Python is to provide an access to DAWGs without requiring compiled\nextensions. It is also quite fast under PyPy (see benchmarks).\n\nInstallation\n============\n\nUsage\n=====\n\nThe aim of DAWG-Python is to be API- and binary-compatible\nwith `DAWG`_ when it is possible.\n\nFirst, you have to create a dawg using DAWG_ module::\n\nAnd then this dawg can be loaded without requiring C extensions::\n\nPlease consult `DAWG`_ docs for detailed usage. Some features\n(like constructor parameters or ``save`` method) are intentionally\nunsupported.\n\nBenchmarks\n==========\n\nBenchmark results (100k unicode words, integer values (lenghts of the words),\nPyPy 1.9, macbook air i5 1.8 Ghz)::\n\nUnder CPython expect it to be about 50x slower.\nMemory consumption of DAWG-Python should be the same as of `DAWG`_.\n\n.. _marisa-trie: https://github.com/kmike/marisa-trie\n\nCurrent limitations\n===================\n\n* This package is not capable of creating DAWGs;\n* all the limitations of `DAWG`_ apply.\n\nContributions are welcome!\n\nContributing\n============\n\nDevelopment happens at github: https://github.com/kmike/DAWG-Python\nIssue tracker: https://github.com/kmike/DAWG-Python/issues\n\nFeel free to submit ideas, bugs or pull requests.\n\nRunning tests and benchmarks\n----------------------------\n\nMake sure `tox`_ is installed and run\n\n::\n\nfrom the source checkout. Tests should pass under python 2.6, 2.7, 3.2, 3.3,\n3.4 and PyPy >= 1.9.\n\nIn order to run benchmarks, type\n\n::\n\nThis runs benchmarks under PyPy (they are about 50x slower under CPython).\n\n.. _tox: http://tox.testrun.org\n\nAuthors & Contributors\n----------------------\n\n* Mikhail Korobov \n\nThe algorithms are from `dawgdic`_ C++ library by Susumu Yata & contributors.\n\nLicense\n=======\n\nThis package is licensed under MIT License.\n\nChanges\n=======\n\n0.7.2 (2015-04-18)\n------------------\n\n- minor speedup;\n- bitbucket mirror is no longer maintained.\n\n0.7.1 (2014-06-05)\n------------------\n\n- Switch to setuptools;\n- upload wheel tp pypi;\n- check Python 3.4 compatibility.\n\n0.7 (2013-10-13)\n----------------\n\nIntDAWG and IntCompletionDAWG are implemented.\n\n0.6 (2013-03-23)\n----------------\n\nUse less shared state internally. This should fix thread-safety bugs and\nmake iterkeys/iteritems reenterant.\n\n0.5.1 (2013-03-01)\n------------------\n\nInternal tweaks: memory usage is reduced; something is a bit faster,\nsomething is a bit slower.\n\n0.5 (2012-10-08)\n----------------\n\nStorage scheme is updated to match DAWG==0.5. This enables\nthe alphabetical ordering of ``BytesDAWG`` and ``RecordDAWG`` items.\n\nIn order to read ``BytesDAWG`` or ``RecordDAWG`` created with\nversions of DAWG >> BytesDAWG(payload_separator=b'\\xff').load('old.dawg')\n\n0.3.1 (2012-10-01)\n------------------\n\nBug with empty DAWGs is fixed.\n\n0.3 (2012-09-26)\n----------------\n\n- ``iterkeys`` and ``iteritems`` methods.\n\n0.2 (2012-09-24)\n----------------\n\n``prefixes`` support.\n\n0.1 (2012-09-20)\n----------------\n\nInitial release."}, {"name": "dawg-python", "tags": ["math", "web"], "summary": "Pure-python reader for DAWGs (DAFSAs) created by dawgdic C++ library or DAWG Python extension.", "text": "This library is used to provide read-only access for files created by DAWGs (DAFSAs) built by the `dawgdic` C++ library or `DAWG` Python extension, without requiring compiled extensions. It enables developers to use DAWGs in their Python applications quickly and efficiently, with API- and binary-compatibility with the `DAWG` module when possible."}, {"name": "descartes", "tags": ["data", "math"], "summary": "Use geometric objects as matplotlib paths and patches", "text": "Descartes\n=========\n\nUse Shapely_ or GeoJSON-like geometric objects as matplotlib paths and patches\n\n.. image:: http://farm4.static.flickr.com/3662/4555372019_9bbed1f956_o_d.png\n   :width: 800\n   :height: 320\n\nRequires: matplotlib, numpy, and optionally Shapely 1.2+.\n\nExample::\n\n  from matplotlib import pyplot\n  from shapely.geometry import LineString\n  from descartes import PolygonPatch\n\n  BLUE = '#6699cc'\n  GRAY = '#999999'\n\n  def plot_line(ax, ob):\n\n  line = LineString([(0, 0), (1, 1), (0, 2), (2, 2), (3, 1), (1, 0)])\n\n  fig = pyplot.figure(1, figsize=(10, 4), dpi=180)\n\n  # 1\n  ax = fig.add_subplot(121)\n\n  plot_line(ax, line)\n\n  dilated = line.buffer(0.5)\n  patch1 = PolygonPatch(dilated, fc=BLUE, ec=BLUE, alpha=0.5, zorder=2)\n  ax.add_patch(patch1)\n\n  #2\n  ax = fig.add_subplot(122)\n\n  patch2a = PolygonPatch(dilated, fc=GRAY, ec=GRAY, alpha=0.5, zorder=1)\n  ax.add_patch(patch2a)\n\n  eroded = dilated.buffer(-0.3)\n\n  # GeoJSON-like data works as well\n\n  polygon = eroded.__geo_interface__\n  # >>> geo['type']\n  # 'Polygon'\n  # >>> geo['coordinates'][0][:2]\n  # ((0.50502525316941682, 0.78786796564403572), (0.5247963548222736, 0.8096820147509064))\n  patch2b = PolygonPatch(polygon, fc=BLUE, ec=BLUE, alpha=0.5, zorder=2)\n  ax.add_patch(patch2b)\n\n  pyplot.show()\n\nSee also: examples/patches.py.\n\nDescartes is not associated with the identically named and apparently defunct\nproject at http://descartes.sourceforge.net/.\n\n.. _Shapely: http://gispython.org/lab/wiki/Shapely"}, {"name": "descartes", "tags": ["data", "math"], "summary": "Use geometric objects as matplotlib paths and patches", "text": "This library is used to seamlessly integrate Shapely or GeoJSON-like geometric objects with matplotlib paths and patches for enhanced visualization capabilities. With descartes, developers can easily convert complex geographic shapes into beautiful, interactive plots that combine the strengths of both geometric calculations and data visualization."}, {"name": "diffusers", "tags": ["math", "ml", "web"], "summary": "State-of-the-art diffusion in PyTorch and JAX.", "text": "Installation\n\nWe recommend installing  Diffusers in a virtual environment from PyPI or Conda. For more details about installing [PyTorch](https://pytorch.org/get-started/locally/), please refer to their official documentation.\n\nPyTorch\n\nWith `pip` (official package):\n\nWith `conda` (maintained by the community):\n\nApple Silicon (M1/M2) support\n\nPlease refer to the [How to use Stable Diffusion in Apple Silicon](https://huggingface.co/docs/diffusers/optimization/mps) guide.\n\nQuickstart\n\nGenerating outputs is super easy with  Diffusers. To generate an image from text, use the `from_pretrained` method to load any pretrained diffusion model (browse the [Hub](https://huggingface.co/models?library=diffusers&sort=downloads) for 30,000+ checkpoints):\n\nYou can also dig into the models and schedulers toolbox to build your own diffusion system:\n\nCheck out the [Quickstart](https://huggingface.co/docs/diffusers/quicktour) to launch your diffusion journey today!\n\nHow to navigate the documentation\n\n**Documentation**\n---------------------------------------------------------------------\n[Tutorial](https://huggingface.co/docs/diffusers/tutorials/tutorial_overview)\n[Loading](https://huggingface.co/docs/diffusers/using-diffusers/loading)\n[Pipelines for inference](https://huggingface.co/docs/diffusers/using-diffusers/overview_techniques)\n[Optimization](https://huggingface.co/docs/diffusers/optimization/fp16)\n[Training](https://huggingface.co/docs/diffusers/training/overview)\n\nContribution\n\nWe \ufe0f  contributions from the open-source community!\nIf you want to contribute to this library, please check out our [Contribution guide](https://github.com/huggingface/diffusers/blob/main/CONTRIBUTING.md).\nYou can look out for [issues](https://github.com/huggingface/diffusers/issues) you'd like to tackle to contribute to the library.\n\nAlso, say  in our public Discord channel . We discuss the hottest trends about diffusion models, help each other with contributions, personal projects or just hang out \u2615.\n\nPopular Tasks & Pipelines\n\nPopular libraries using  Diffusers\n\n- +14,000 other amazing GitHub repositories \n\nThank you for using us \ufe0f.\n\nCredits\n\nThis library concretizes previous work by many different authors and would not have been possible without their great research and implementations. We'd like to thank, in particular, the following implementations which have helped us in our development and without which the API could not have been as polished today:\n\nWe also want to thank @heejkoo for the very helpful overview of papers, code and resources on diffusion models, available [here](https://github.com/heejkoo/Awesome-Diffusion-Models) as well as @crowsonkb and @rromb for useful discussions and insights.\n\nCitation"}, {"name": "diffusers", "tags": ["math", "ml", "web"], "summary": "State-of-the-art diffusion in PyTorch and JAX.", "text": "This library is used to generate high-quality outputs such as images from text using state-of-the-art diffusion models. Developers can easily integrate these models into their applications using a simple `from_pretrained` method."}, {"name": "dill", "tags": ["math", "ui"], "summary": "serialize all of Python", "text": "-----------------------------\ndill: serialize all of Python\n-----------------------------\n\nAbout Dill\n==========\n\n``dill`` extends Python's ``pickle`` module for serializing and de-serializing\nPython objects to the majority of the built-in Python types. Serialization\nis the process of converting an object to a byte stream, and the inverse\nof which is converting a byte stream back to a Python object hierarchy.\n\n``dill`` provides the user the same interface as the ``pickle`` module, and\nalso includes some additional features. In addition to pickling Python\nobjects, ``dill`` provides the ability to save the state of an interpreter\nsession in a single command.  Hence, it would be feasible to save an\ninterpreter session, close the interpreter, ship the pickled file to\nanother computer, open a new interpreter, unpickle the session and\nthus continue from the 'saved' state of the original interpreter\nsession.\n\n``dill`` can be used to store Python objects to a file, but the primary\nusage is to send Python objects across the network as a byte stream.\n``dill`` is quite flexible, and allows arbitrary user defined classes\nand functions to be serialized.  Thus ``dill`` is not intended to be\nsecure against erroneously or maliciously constructed data. It is\nleft to the user to decide whether the data they unpickle is from\na trustworthy source.\n\n``dill`` is part of ``pathos``, a Python framework for heterogeneous computing.\n``dill`` is in active development, so any user feedback, bug reports, comments,\nor suggestions are highly appreciated.  A list of issues is located at\n\nMajor Features\n==============\n\n``dill`` can pickle the following standard types:\n\n``dill`` can also pickle more 'exotic' standard types:\n\n``dill`` cannot yet pickle these standard types:\n\n``dill`` also provides the capability to:\n\nCurrent Release\n===============\n\nThe latest released version of ``dill`` is available from:\n\n``dill`` is distributed under a 3-clause BSD license.\n\nDevelopment Version\n===================\n\nYou can get the latest development version with all the shiny new features at:\n\nIf you have a new contribution, please submit a pull request.\n\nInstallation\n============\n\n``dill`` can be installed with ``pip``::\n\nTo optionally include the ``objgraph`` diagnostic tool in the install::\n\nTo optionally include the ``gprof2dot`` diagnostic tool in the install::\n\nFor windows users, to optionally install session history tools::\n\nRequirements\n============\n\n``dill`` requires:\n\nOptional requirements:\n\nBasic Usage\n===========\n\n``dill`` is a drop-in replacement for ``pickle``. Existing code can be\nupdated to allow complete pickling using::\n\nor::\n\n``dumps`` converts the object to a unique byte string, and ``loads`` performs\nthe inverse operation::\n\nThere are a number of options to control serialization which are provided\nas keyword arguments to several ``dill`` functions:"}, {"name": "dill", "tags": ["math", "ui"], "summary": "serialize all of Python", "text": "* with *protocol*, the pickle protocol level can be set. This uses the\n  same value as the ``pickle`` module, *DEFAULT_PROTOCOL*.\n* with *byref=True*, ``dill`` to behave a lot more like pickle with\n  certain objects (like modules) pickled by reference as opposed to\n  attempting to pickle the object itself.\n* with *recurse=True*, objects referred to in the global dictionary are\n  recursively traced and pickled, instead of the default behavior of\n  attempting to store the entire global dictionary.\n* with *fmode*, the contents of the file can be pickled along with the file\n  handle, which is useful if the object is being sent over the wire to a\n  remote system which does not have the original file on disk. Options are\n  *HANDLE_FMODE* for just the handle, *CONTENTS_FMODE* for the file content\n  and *FILE_FMODE* for content and handle.\n* with *ignore=False*, objects reconstructed with types defined in the\n  top-level script environment use the existing type in the environment\n  rather than a possibly different reconstructed type.\n\nThe default serialization can also be set globally in *dill.settings*.\nThus, we can modify how ``dill`` handles references to the global dictionary\nlocally or globally::\n\n``dill`` also includes source code inspection, as an alternate to pickling::\n\nTo aid in debugging pickling issues, use *dill.detect* which provides\ntools like pickle tracing::\n\nWith trace, we see how ``dill`` stored the lambda (``F1``) by first storing\n``_create_function``, the underlying code object (``Co``) and ``_create_code``\n(which is used to handle code objects), then we handle the reference to\nthe global dict (``D2``) plus other dictionaries (``D1`` and ``D2``) that\nsave the lambda object's state. A ``#`` marks when the object is actually stored.\n\nMore Information\n================\n\nProbably the best way to get started is to look at the documentation at\ndemonstrate how ``dill`` can serialize different Python objects. You can\nrun the test suite with ``python -m dill.tests``. The contents of any\npickle file can be examined with ``undill``.  As ``dill`` conforms to\nthe ``pickle`` interface, the examples and documentation found at\nif one will ``import dill as pickle``. The source code is also generally\nwell documented, so further questions may be resolved by inspecting the\ncode itself. Please feel free to submit a ticket on github, or ask a\nquestion on stackoverflow (**@Mike McKerns**).\nIf you would like to share how you use ``dill`` in your work, please send\nan email (to **mmckerns at uqfoundation dot org**).\n\nCitation\n========\n\nIf you use ``dill`` to do research that leads to publication, we ask that you\nacknowledge use of ``dill`` by citing the following in your publication::\n\nPlease see https://uqfoundation.github.io/project/pathos or"}, {"name": "dill", "tags": ["math", "ui"], "summary": "serialize all of Python", "text": "This library is used to serialize and de-serialize Python objects to a byte stream, supporting the majority of built-in Python types. This allows developers to store and transfer complex Python data structures between systems or save the state of an interpreter session for later use."}, {"name": "distributed", "tags": ["math"], "summary": "Distributed scheduler for Dask", "text": "Distributed\n===========\n\nTest Status\n\nA library for distributed computation.  See documentation_ for more details.\n\n.. _documentation: https://distributed.dask.org\n\n   :target: https://github.com/dask/distributed/actions?query=workflow%3ATests+branch%3Amain\n\n   :target: https://dask.github.io/distributed/test_report.html\n   :alt: Longitudinal test report (full version)\n\n   :target: https://dask.github.io/distributed/test_short_report.html\n   :alt: Longitudinal test report (short version)\n\n   :target: https://codecov.io/gh/dask/distributed/branch/main\n   :alt: Coverage status\n\n   :target: https://distributed.dask.org\n   :alt: Documentation Status\n.. |Discourse| image:: https://img.shields.io/discourse/users?logo=discourse&server=https%3A%2F%2Fdask.discourse.group\n   :alt: Discuss Dask-related things and ask for help\n   :target: https://dask.discourse.group\n.. |Version Status| image:: https://img.shields.io/pypi/v/distributed.svg\n   :target: https://pypi.python.org/pypi/distributed/\n\n   :target: https://www.numfocus.org/"}, {"name": "distributed", "tags": ["math"], "summary": "Distributed scheduler for Dask", "text": "This library is used to enable distributed computation and scheduling capabilities for Dask applications, allowing developers to scale their tasks across multiple machines. By leveraging this library, developers can efficiently manage and execute large-scale computations in a distributed environment."}, {"name": "django-forge-ai", "tags": ["math", "ml", "web"], "summary": "A comprehensive AI toolbox for Django - seamless integration of LLM functionalities, RAG systems, and AI agents", "text": "DjangoForgeAI\n\nA comprehensive, plug-and-play toolkit that seamlessly integrates modern AI/LLM functionalities directly into your Django projects. DjangoForgeAI provides \"batteries-included\" solutions for common AI engineering tasks, including AI-powered admin fields, native RAG system integration, and configurable AI agents, all manageable through the Django admin interface.\n\nDemos\n\n**Demo 1**: [Watch on YouTube](https://youtu.be/Xa9G_AY5_ic)\n\n**Demo 2**: [Watch on YouTube](https://youtu.be/bmWhTSAQr5A)\n\nFeatures\n\n1. AI-Powered Admin & Model Fields\n\n- **AICharField / AITextField**: Custom model fields that can automatically generate summaries or content from other fields in the model using an LLM.\n- **AIAdminMixin**: A mixin for ModelAdmin classes that adds a \"Generate with AI\" button next to specified text fields.\n- **AIModeratedField**: A field that automatically runs AI-powered content moderation upon save.\n\n2. Native RAG System Integration\n\n- **Knowledge Base Management**: Upload documents, define data sources (e.g., website URLs), and manage vector stores through the Django admin.\n- **Vector DB Connectors**: Pluggable backends for popular vector databases (ChromaDB, Qdrant, PGVector).\n- **SemanticSearchMixin**: A mixin for QuerySets that enables semantic search against your RAG knowledge base.\n\n3. Configurable AI Agents & Automation\n\n- **Agent Management Dashboard**: Define agents (persona, tools, goals) and assign them to specific tasks through the admin.\n- **AgentTaskQueue**: Integration with Celery to execute agent tasks in the background and log their progress.\n\nInstallation\n\nOr install from source:\n\nQuick Start\n\n1. Add to INSTALLED_APPS\n\n2. Configure Settings\n\n3. Run Migrations\n\n4. Use AI-Powered Fields\n\n5. Use AIAdminMixin\n\n6. Set Up RAG System\n\n1. Create a Knowledge Base in the Django admin\n2. Upload documents or add URLs\n3. Generate embeddings (admin action)\n4. Use semantic search:\n\n7. Create AI Agents\n\n1. Create an Agent Configuration in the Django admin\n2. Define persona, goals, and tools\n3. Create tasks and execute them (async with Celery)\n\nConfiguration\n\nLLM Providers\n\nDjangoForgeAI supports multiple LLM providers:\n\n- **OpenAI**: Set `DJANGO_FORGE_AI_LLM_PROVIDER = \"openai\"` and provide `DJANGO_FORGE_AI_OPENAI_API_KEY`\n- **Anthropic**: Set `DJANGO_FORGE_AI_LLM_PROVIDER = \"anthropic\"` and provide `DJANGO_FORGE_AI_ANTHROPIC_API_KEY`\n\nVector Databases\n\n- **ChromaDB**: Default, file-based storage\n- **Qdrant**: Requires Qdrant server running\n- **PGVector**: Requires PostgreSQL with pgvector extension\n\nCelery Integration\n\nFor async task execution, configure Celery:\n\nRequirements\n\n- Python >= 3.8\n- Django >= 3.2\n- OpenAI API key or Anthropic API key\n- (Optional) Celery for async tasks\n- (Optional) Vector database (ChromaDB, Qdrant, or PostgreSQL with pgvector)\n\nLicense\n\nMIT License\n\nContributing\n\nContributions are welcome! Please feel free to submit a Pull Request.\n\nSupport\n\nFor issues and questions, please open an issue on GitHub."}, {"name": "django-forge-ai", "tags": ["math", "ml", "web"], "summary": "A comprehensive AI toolbox for Django - seamless integration of LLM functionalities, RAG systems, and AI agents", "text": "This library is used to seamlessly integrate modern AI and Large Language Model (LLM) functionalities into Django projects, providing pre-built solutions for common AI engineering tasks. With django-forge-ai, developers can easily add AI-powered features such as auto-generated summaries and content to their models and admin interfaces."}, {"name": "django", "tags": ["web"], "summary": "A high-level Python web framework that encourages rapid development and clean, pragmatic design.", "text": "======\nDjango\n======\n\nDjango is a high-level Python web framework that encourages rapid development\nand clean, pragmatic design. Thanks for checking it out.\n\nAll documentation is in the \"``docs``\" directory and online at\nhere's how we recommend you read the docs:\n\n* First, read ``docs/intro/install.txt`` for instructions on installing Django.\n\n* Next, work through the tutorials in order (``docs/intro/tutorial01.txt``,\n  ``docs/intro/tutorial02.txt``, etc.).\n\n* If you want to set up an actual deployment server, read\n  ``docs/howto/deployment/index.txt`` for instructions.\n\n* You'll probably want to read through the topical guides (in ``docs/topics``)\n  next; from there you can jump to the HOWTOs (in ``docs/howto``) for specific\n  problems, and check out the reference (``docs/ref``) for gory details.\n\n* See ``docs/README`` for instructions on building an HTML version of the docs.\n\nDocs are updated rigorously. If you find any problems in the docs, or think\nthey should be clarified in any way, please take 30 seconds to fill out a\nticket here: https://code.djangoproject.com/newticket\n\nTo get more help:\n\n* Join the `Django Discord community `_.\n\n* Join the community on the `Django Forum `_.\n\nTo contribute to Django:\n\n* Check out https://docs.djangoproject.com/en/dev/internals/contributing/ for\n  information about getting involved.\n\nTo run Django's test suite:\n\n* Follow the instructions in the \"Unit tests\" section of\n  ``docs/internals/contributing/writing-code/unit-tests.txt``, published online at\n\nSupporting the Development of Django\n====================================\n\nDjango's development depends on your contributions.\n\nIf you depend on Django, remember to support the Django Software Foundation: https://www.djangoproject.com/fundraising/"}, {"name": "django", "tags": ["web"], "summary": "A high-level Python web framework that encourages rapid development and clean, pragmatic design.", "text": "This library is used to build high-performance web applications quickly with a focus on clean design. With Django, developers can rapidly create scalable and maintainable web projects using Python's robust framework."}, {"name": "dm-tree", "tags": ["math", "ml"], "summary": "Tree is a library for working with nested data structures.", "text": "Tree\n\n`tree` is a library for working with nested data structures. In a way, `tree`\ngeneralizes the builtin `map` function which only supports flat sequences,\nand allows to apply a function to each \"leaf\" preserving the overall\nstructure.\n\n`tree` is backed by an optimized C++ implementation suitable for use in\ndemanding applications, such as machine learning models.\n\nInstallation\n\nFrom PyPI:\n\nDirectly from github using pip:\n\nBuild from source:\n\nSupport\n\nIf you are having issues, please let us know by filing an issue on our\n[issue tracker](https://github.com/deepmind/tree/issues).\n\nLicense\n\nThe project is licensed under the Apache 2.0 license."}, {"name": "dm-tree", "tags": ["math", "ml"], "summary": "Tree is a library for working with nested data structures.", "text": "This library is used to generalize and optimize the application of functions to nested data structures, preserving their overall structure. This enables developers to efficiently manipulate complex, hierarchical data in various applications, such as machine learning models."}, {"name": "docling-core", "tags": ["data", "dev", "math", "web"], "summary": "A python library to define and validate data types in Docling.", "text": "Docling Core\n\n(https://pypi.org/project/docling-core/)\n\n(https://github.com/astral-sh/uv)\n(https://github.com/psf/black)\n(https://pycqa.github.io/isort/)\n(https://mypy-lang.org/)\n(https://pydantic.dev)\n(https://github.com/pre-commit/pre-commit)\n(https://opensource.org/licenses/MIT)\n\nDocling Core is a library that defines core data types and transformations in [Docling](https://github.com/docling-project/docling).\n\nInstallation\n\nTo use Docling Core, simply install `docling-core` from your package manager, e.g. pip:\n\nDevelopment setup\n\nTo develop for Docling Core, you need Python\u202f3.9 through 3.14 and the `uv` package. You can then install it from your local clone's root directory:\n\nTo run the pytest suite, execute:\n\nMain features\n\nDocling Core provides the foundational DoclingDocument data model and API, as well as\nadditional APIs for tasks like serialization and chunking, which are key to developing\ngenerative AI applications using Docling.\n\nDoclingDocument\n\nDocling Core defines the DoclingDocument as a Pydantic model, allowing for advanced\ndata model control, customizability, and interoperability.\n\nIn addition to specifying the schema, it provides a handy API for building documents,\nas well as for basic operations, e.g. exporting to various formats, like Markdown, HTML,\nand others.\n\n More details:\n- [Architecture docs](https://docling-project.github.io/docling/concepts/architecture/)\n- [DoclingDocument docs](https://docling-project.github.io/docling/concepts/docling_document/)\n\nSerialization\n\nDifferent users can have varying requirements when it comes to serialization.\nTo address this, the Serialization API introduces a design that allows easy extension,\nwhile providing feature-rich built-in implementations (on which the respective\nDoclingDocument helpers are actually based).\n\n More details:\n- [Serialization docs](https://docling-project.github.io/docling/concepts/serialization/)\n- [Serialization example](https://docling-project.github.io/docling/examples/serialization/)\n\nChunking\n\nSimilarly to above, the Chunking API provides built-in chunking capabilities as well as\na design that enables easy extension, this way tackling customization requirements of\ndifferent use cases.\n\n More details:\n\nContributing\n\nPlease read [Contributing to Docling Core](./CONTRIBUTING.md) for details.\n\nReferences\n\nIf you use Docling Core in your projects, please consider citing the following:\n\nLicense\n\nThe Docling Core codebase is under MIT license.\nFor individual model usage, please refer to the model licenses found in the original packages."}, {"name": "docling-core", "tags": ["data", "dev", "math", "web"], "summary": "A python library to define and validate data types in Docling.", "text": "This library is used to define and validate custom data types in the Docling framework. It enables developers to create robust data models with built-in validation and transformation capabilities."}, {"name": "docling-ibm-models", "tags": ["data", "math", "ml"], "summary": "This package contains the AI models used by the Docling PDF conversion package", "text": "Docling IBM models\n\nAI modules to support the Docling PDF document conversion project.\n\n- TableFormer is an AI module that recognizes the structure of a table and the bounding boxes of the table content.\n- Layout model is an AI model that provides among other things ability to detect tables on the page. This package contains inference code for Layout model.\n\nInstall\n\nThe package provides two variants which allow to seemlessly switch between `opencv-python` and `opencv-python-headless`.\n\nPipeline Overview\n\nDatasets\nBelow we list datasets used with their description, source, and ***\"TableFormer Format\"***. The TableFormer Format is our processed version of the version of the original format to work with the dataloader out of the box, and to augment the dataset when necassary to add missing groundtruth (bounding boxes for empty cells).\n\nName\n-------------\nPubTabNet\nFinTabNet\nTableBank\n\nModels\n\nTableModel04:\n\n**TableModel04rs (OTSL)** is our SOTA method that using transformers in order to predict table structure and bounding box.\n\nConfiguration file\n\nExample configuration can be found inside test `tests/test_tf_predictor.py`\nThese are the main sections of the configuration file:\n\n- `dataset`: The directory for prepared data and the parameters used during the data loading.\n- `model`: The type, name and hyperparameters of the model. Also the directory to save/load the\n  trained checkpoint files.\n- `train`: Parameters for the training of the model.\n- `predict`: Parameters for the evaluation of the model.\n- `dataset_wordmap`: Very important part that contains token maps.\n\nModel weights\n\nYou can download the model weights and config files from the links:\n\n- [TableFormer Checkpoint](https://huggingface.co/ds4sd/docling-models/tree/main/model_artifacts/tableformer)\n- [beehive_v0.0.5](https://huggingface.co/ds4sd/docling-models/tree/main/model_artifacts/layout/beehive_v0.0.5)\n\nInference Tests\n\nYou can run the inference tests for the models with:\n\nThis will also generate prediction and matching visualizations that can be found here:\n`tests\\test_data\\viz\\`\n\nVisualization outlines:\n- `Light Pink`: border of recognized table\n- `Grey`: OCR cells\n- `Green`: prediction bboxes\n- `Red`: OCR cells matched with prediction\n- `Blue`: Post processed, match\n- `Bold Blue`: column header\n- `Bold Magenta`: row header\n- `Bold Brown`: section row (if table have one)\n\nDemo\n\nA demo application allows to apply the `LayoutPredictor` on a directory `` that contains\n`png` images and visualize the predictions inside another directory ``.\n\nFirst download the model weights (see above), then run:\n\ne.g."}, {"name": "docling-ibm-models", "tags": ["data", "math", "ml"], "summary": "This package contains the AI models used by the Docling PDF conversion package", "text": "This library is used to provide AI-powered table detection and structure recognition capabilities for PDF document conversion. It includes two key modules: TableFormer for recognizing table structures and content bounding boxes, and a Layout model for detecting tables on a page."}, {"name": "docling", "tags": ["data", "math", "ml", "visualization", "web"], "summary": "SDK and CLI for parsing PDF, DOCX, HTML, and more, to a unified document representation for powering downstream workflows such as gen AI applications.", "text": "Docling\n\n(https://arxiv.org/abs/2408.09869)\n(https://docling-project.github.io/docling/)\n(https://pypi.org/project/docling/)\n(https://pypi.org/project/docling/)\n(https://github.com/astral-sh/uv)\n(https://github.com/astral-sh/ruff)\n(https://pydantic.dev)\n(https://github.com/pre-commit/pre-commit)\n(https://opensource.org/licenses/MIT)\n(https://pepy.tech/projects/docling)\n(https://apify.com/vancura/docling)\n(https://app.dosu.dev/097760a8-135e-4789-8234-90c8837d7f1c/ask?utm_source=github)\n(https://docling.ai/discord)\n(https://www.bestpractices.dev/projects/10101)\n(https://lfaidata.foundation/projects/)\n\nDocling simplifies document processing, parsing diverse formats \u2014 including advanced PDF understanding \u2014 and providing seamless integrations with the gen AI ecosystem.\n\nFeatures\n\n* \ufe0f Parsing of [multiple document formats][supported_formats] incl. PDF, DOCX, PPTX, XLSX, HTML, WAV, MP3, VTT, images (PNG, TIFF, JPEG, ...), and more\n*  Advanced PDF understanding incl. page layout, reading order, table structure, code, formulas, image classification, and more\n*  Unified, expressive [DoclingDocument][docling_document] representation format\n* \u21aa\ufe0f Various [export formats][supported_formats] and options, including Markdown, HTML, [DocTags](https://arxiv.org/abs/2503.11576) and lossless JSON\n*  Local execution capabilities for sensitive data and air-gapped environments\n*  Plug-and-play [integrations][integrations] incl. LangChain, LlamaIndex, Crew AI & Haystack for agentic AI\n*  Extensive OCR support for scanned PDFs and images\n*  Support of several Visual Language Models ([GraniteDocling](https://huggingface.co/ibm-granite/granite-docling-258M))\n* \ufe0f Audio support with Automatic Speech Recognition (ASR) models\n*  Connect to any agent using the [MCP server](https://docling-project.github.io/docling/usage/mcp/)\n*  Simple and convenient CLI\n\nWhat's new\n*  Structured [information extraction][extraction] \\[ beta\\]\n*  New layout model (**Heron**) by default, for faster PDF parsing\n*  [MCP server](https://docling-project.github.io/docling/usage/mcp/) for agentic applications\n*  Parsing of Web Video Text Tracks (WebVTT) files\n\nComing soon\n\n*  Metadata extraction, including title, authors, references & language\n*  Chart understanding (Barchart, Piechart, LinePlot, etc)\n*  Complex chemistry understanding (Molecular structures)\n\nInstallation\n\nTo use Docling, simply install `docling` from your package manager, e.g. pip:\n\nWorks on macOS, Linux and Windows environments. Both x86_64 and arm64 architectures.\n\nMore [detailed installation instructions](https://docling-project.github.io/docling/installation/) are available in the docs.\n\nGetting started\n\nTo convert individual documents with python, use `convert()`, for example:\n\nMore [advanced usage options](https://docling-project.github.io/docling/usage/advanced_options/) are available in\nthe docs.\n\nCLI\n\nDocling has a built-in CLI to run conversions.\n\nYou can also use [GraniteDocling](https://huggingface.co/ibm-granite/granite-docling-258M) and other VLMs via Docling CLI:\n\nThis will use MLX acceleration on supported Apple Silicon hardware.\n\nRead more [here](https://docling-project.github.io/docling/usage/)\n\nDocumentation\n\nCheck out Docling's [documentation](https://docling-project.github.io/docling/), for details on\ninstallation, usage, concepts, recipes, extensions, and more.\n\nExamples\n\nGo hands-on with our [examples](https://docling-project.github.io/docling/examples/),\ndemonstrating how to address different application use cases with Docling.\n\nIntegrations\n\nTo further accelerate your AI application development, check out Docling's native\n[integrations](https://docling-project.github.io/docling/integrations/) with popular frameworks\nand tools.\n\nGet help and support\n\nPlease feel free to connect with us using the [discussion section](https://github.com/docling-project/docling/discussions).\n\nTechnical report\n\nFor more details on Docling's inner workings, check out the [Docling Technical Report](https://arxiv.org/abs/2408.09869).\n\nContributing\n\nPlease read [Contributing to Docling](https://github.com/docling-project/docling/blob/main/CONTRIBUTING.md) for details.\n\nReferences\n\nIf you use Docling in your projects, please consider citing the following:\n\nLicense\n\nThe Docling codebase is under MIT license.\nFor individual model usage, please refer to the model licenses found in the original packages.\n\nLF AI & Data\n\nDocling is hosted as a project in the [LF AI & Data Foundation](https://lfaidata.foundation/projects/).\n\nIBM \ufe0f Open Source AI\n\nThe project was started by the AI for knowledge team at IBM Research Zurich."}, {"name": "docling", "tags": ["data", "math", "ml", "visualization", "web"], "summary": "SDK and CLI for parsing PDF, DOCX, HTML, and more, to a unified document representation for powering downstream workflows such as gen AI applications.", "text": "This library is used to parse documents from various formats, such as PDF, DOCX, HTML, into a unified representation for downstream applications like generative AI. With docling, developers can simplify document processing and seamlessly integrate with the gen AI ecosystem."}, {"name": "doit", "tags": ["cli", "math", "web"], "summary": "doit - Automation Tool", "text": "*doit* comes from the idea of bringing the power of build-tools to execute any\nkind of task\n\n*doit* can be uses as a simple **Task Runner** allowing you to easily define ad hoc\ntasks, helping you to organize all your project related tasks in an unified\neasy-to-use & discoverable way.\n\n*doit* scales-up with an efficient execution model like a **build-tool**.\n*doit* creates a DAG (direct acyclic graph) and is able to cache task results.\nIt ensures that only required tasks will be executed and in the correct order\n(aka incremental-builds).\n\nThe *up-to-date* check to cache task results is not restricted to looking for\nfile modification on dependencies.  Nor it requires \"target\" files.\nSo it is also suitable to handle **workflows** not handled by traditional build-tools.\n\nTasks' dependencies and creation can be done dynamically during it is execution\nmaking it suitable to drive complex workflows and **pipelines**.\n\n*doit* is build with a plugin architecture allowing extensible commands, custom\noutput, storage backend and \"task loader\". It also provides an API allowing\nusers to create new applications/tools leveraging *doit* functionality like a framework.\n\n*doit* is a mature project being actively developed for more than 10 years.\nIt includes several extras like: parallel execution, auto execution (watch for file\nchanges), shell tab-completion, DAG visualisation, IPython integration, and more.\n\nSample Code\n===========\n\nDefine functions returning python dict with task's meta-data.\n\nSnippet from `tutorial `_:\n\n.. code:: python\n\n  def task_imports():\n\n  def task_dot():\n\n  def task_draw():\n\nRun from terminal::\n\n  $ doit list\n  dot       generate a graphviz's dot graph from module imports\n  draw      generate image from a dot file\n  imports   find imports from a python module\n  $ doit\n  .  imports:requests.models\n  .  imports:requests.__init__\n  .  imports:requests.help\n  (...)\n  .  dot\n  .  draw\n\nProject Details\n===============\n\nlicense\n=======\n\nThe MIT License\nCopyright (c) 2008-2022 Eduardo Naufel Schettino"}, {"name": "doit", "tags": ["cli", "math", "web"], "summary": "doit - Automation Tool", "text": "This library, doit, is used to automate tasks and workflows in a project by providing a unified and efficient way to execute tasks in the correct order. It enables developers to easily define tasks, manage dependencies, and cache results for incremental builds and up-to-date checks."}, {"name": "donfig", "tags": ["math", "web"], "summary": "Python package for configuring a python package", "text": "Donfig\n======\n\n   :target: https://codecov.io/gh/pytroll/donfig\n\n   :target: https://anaconda.org/conda-forge/donfig/\n\n   :target: https://results.pre-commit.ci/latest/github/pytroll/donfig/main\n   :alt: pre-commit.ci status\n\nDonfig is a python library meant to make configuration easier for other\npython packages. Donfig can be configured programmatically, by\nenvironment variables, or from YAML files in standard locations. The\nbelow examples show the basics of using donfig. For more details see the\nofficial `documentation `_.\n\nInstallation\n------------\n\nDonfig can be installed from PyPI using pip:\n\n.. code-block:: bash\n\nOr with conda using the conda-forge channel:\n\n.. code-block:: bash\n\nUsing Donfig\n------------\n\nCreate the package-wide configuration object for your package named `mypkg`:\n\n.. code-block:: python\n\nUse the configuration object:\n\n.. code-block:: python\n\nSet configuration in Python\n---------------------------\n\nConfiguration can be modified in python before code using it is called:\n\n.. code-block:: python\n\nDonfig configurations can also be changed as a context manager:\n\n.. code-block:: python\n\nConfigure from environment variables\n------------------------------------\n\nEnvironment variables are automatically loaded when the Config object is\ncreated. Any environment variable starting with the name of the config\nobject in all capital letters and an underscore will be loaded in to\nthe config object:\n\n.. code-block:: bash\n\nAnd can be accessed in python:\n\n.. code-block:: python\n\nConfigure from YAML file\n------------------------\n\nDonfig will also automatically load any YAML configuration files found in\nspecific paths. The default paths:\n\n- ~/.config//\n- /etc//\n- /etc//\n\nNote the `/etc//` directory can also be specified with the\nenvironment variable `DASK_ROOT_CONFIG`. Also note that\n`~/.config/` (or other location specified with `DASK_CONFIG`)\ncan be created as a custom user configuration file for easier user\ncustomization (see documentation for details).\n\nHistory\n-------\n\nDonfig is based on the original configuration logic of the `dask` library.\nThe code has been modified to use a config object instead of a global\nconfiguration dictionary. This makes the configuration logic of dask available\nto everyone. The name \"donfig\" is a shortening of \"dask.config\", the original\ndask module that implemented this functionality.\n\nLicense\n-------\n\nOriginal code from the dask library was distributed under the license\nspecified in `DASK_LICENSE.txt`. In November 2018 this code was migrated to\nthe Donfig project under the MIT license described in `LICENSE.txt`. The full\ncopyright for this project is therefore::"}, {"name": "donfig", "tags": ["math", "web"], "summary": "Python package for configuring a python package", "text": "This library is used to make configuration easier for other Python packages by allowing developers to configure settings programmatically, through environment variables, or from YAML files. With donfig, developers can automate and streamline their package's setup process with minimal code."}, {"name": "duckdb", "tags": ["data", "math", "web"], "summary": "DuckDB in-process database", "text": "DuckDB: A Fast, In-Process, Portable, Open Source, Analytical Database System\n\n* **Simple**: DuckDB is easy to install and deploy. It has zero external dependencies and runs in-process in its host application or as a single binary.\n* **Portable**: DuckDB runs on Linux, macOS, Windows, Android, iOS and all popular hardware architectures. It has idiomatic client APIs for major programming languages.\n* **Feature-rich**: DuckDB offers a rich SQL dialect. It can read and write file formats such as CSV, Parquet, and JSON, to and from the local file system and remote endpoints such as S3 buckets.\n* **Fast**: DuckDB runs analytical queries at blazing speed thanks to its columnar engine, which supports parallel execution and can process larger-than-memory workloads.\n* **Extensible**: DuckDB is extensible by third-party features such as new data types, functions, file formats and new SQL syntax. User contributions are available as community extensions.\n* **Free**: DuckDB and its core extensions are open-source under the permissive MIT License. The intellectual property of the project is held by the DuckDB Foundation.\n\nInstallation\n\nInstall the latest release of DuckDB directly from [PyPI](https://pypi.org/project/duckdb/):\n\nInstall with all optional dependencies:\n\nContributing\n\nSee the [CONTRIBUTING.md](CONTRIBUTING.md) for instructions on how to set up a development environment."}, {"name": "duckdb", "tags": ["data", "math", "web"], "summary": "DuckDB in-process database", "text": "This library is used to provide a fast, in-process analytical database system that allows developers to run complex SQL queries on large datasets with ease. With DuckDB, developers can read and write various file formats and execute parallelized queries across multiple hardware architectures."}, {"name": "ephem", "tags": ["cli", "math", "web"], "summary": "Compute positions of the planets and stars", "text": "==============\nPyEphem README\n==============\n\n.. _ephem: http://pypi.python.org/pypi/ephem/\n.. _pyephem: http://pypi.python.org/pypi/pyephem/\n.. _XEphem: https://xephem.github.io/XEphem/Site/xephem\n.. _Quick Reference: http://rhodesmill.org/pyephem/quick\n.. _Tutorial: http://rhodesmill.org/pyephem/tutorial\n.. _PyEphem web site: http://rhodesmill.org/pyephem/\n\nPyEphem provides an ``ephem`` Python package\nfor performing high-precision astronomy computations.\nThe underlying numeric routines are coded in C\nand are the same ones that drive the popular `XEphem`_ astronomy application,\nwhose author, Elwood Charles Downey,\ngenerously gave permission for their use in PyEphem.\nThe name *ephem* is short for the word *ephemeris*,\nwhich is the traditional term for a table\ngiving the position of a planet, asteroid, or comet for a series of dates.\n\nThe `PyEphem web site`_ offers documentation\nand also links to the project bug tracker, user support forum,\nand source code repository.\nIf you have a C compiler and the\n`pip Python installer tool `_\non your system,\nthen installing PyEphem should be as easy as::\n\nThere are also Windows installers in the downloads section below.\n\nThe design of PyEphem emphasizes convenience and ease of use.\nBoth celestial bodies and the observer's location on Earth\nare represented by Python objects,\nwhile dates and angles automatically print themselves\nin standard astronomical formats::\n\n >>> import ephem\n >>> mars = ephem.Mars()\n >>> mars.compute('2008/1/1')\n >>> print(mars.ra)\n 5:59:27.35\n >>> print(mars.dec)\n 26:56:27.4\n\nThe documentation includes both a `Quick Reference`_ and a `Tutorial`_,\nwhich are included in text files within the module itself\nas well as being available on the `PyEphem web site`_.\n\nThe features provided by PyEphem include:\n\n* Find where a planet, comet, or asteroid is in the sky.\n\n  * High-precision orbital routines are provdied\n  * The user can supply the orbital elements of a comet, asteroid,\n  * The positions of 94 bright stars come built-in,\n\n* Determine where in the sky an object appears for a particular observer.\n\n  * The user can supply the longitude, latitude, and altitude\n  * For convenience, a small database of longitudes and latitudes\n  * For specified weather conditions (temperature and pressure),\n\n* Compute when a body will rise, transit overhead, and set\n  from a particular location.\n\n* Parse and use orbital data in either the traditional XEphem file format,\n  or the standard TLE format used for tracking Earth-orbiting satellites.\n\n* Determine the dates of the equinoxes and solstices.\n\n* Compute the dates of the various phases of the Moon.\n\n* Convert from the Greenwich Time (more precisely, Ephemeris Time)\n  which PyEphem uses to the local time of the user.\n\n* Convert positions between the equatorial, ecliptic, and galactic\n  coordinate systems.\n\n* Determine on which page of the Uranometria or the Millennium Star Atlas\n  a particular star should appear.\n\n* Return the Julian Date corresponding to any calendar date.\n\nDevelopers\n----------\n\nIf you are interested in learning about how PyEphem works or in\nexploring its source code, check out this repository from GitHub.  It is\nhosted at:\n\nIf you lack expertise with version control, you can instead simply\ndownload a static copy of the most recent source code using this link:\n\nTo run its source code in place, create a `virtual environment\n`_, activate\nit, change directory to the root of the PyEphem source code, and run::\n\nYou can then run the PyEphem test suite to see whether all of its\nfeatures are working correctly on your operating system and platform::\n\nPyEphem\u2019s documentation is organized as a standard `Sphinx\n`_ document project.  You can\nbuild the documentation either with the Sphinx command line::\n\n\u2014 or, more typically, by invoking one of the targets in the\ndocumentation\u2019s Makefile::"}, {"name": "ephem", "tags": ["cli", "math", "web"], "summary": "Compute positions of the planets and stars", "text": "This library is used to compute high-precision positions of planets and stars using astronomy computations. It provides an interface to perform precise calculations that are the same as those used in the XEphem application, allowing developers to integrate accurate astronomical data into their projects."}, {"name": "equinox", "tags": ["math", "ml"], "summary": "Elegant easy-to-use neural networks in JAX.", "text": "Installation\n\nRequires Python 3.10+.\n\nEquinox is also available through a community-supported build on [conda-forge](https://github.com/conda-forge/equinox-feedstock).\n\nDocumentation\n\nAvailable at [https://docs.kidger.site/equinox](https://docs.kidger.site/equinox).\n\nQuick example\n\nModels are defined using PyTorch-like syntax:\n\nand are fully compatible with normal JAX operations:\n\nFinally, there's no magic behind the scenes. All `eqx.Module` does is register your class as a PyTree. From that point onwards, JAX already knows how to work with PyTrees.\n\nCitation\n\nIf you found this library to be useful in academic work, then please cite: ([arXiv link](https://arxiv.org/abs/2111.00254))\n\n(Also consider starring the project on GitHub.)\n\nSee also: other libraries in the JAX ecosystem\n\n**Always useful**  \n[jaxtyping](https://github.com/patrick-kidger/jaxtyping): type annotations for shape/dtype of arrays.  \n\n**Deep learning**  \n[Optax](https://github.com/deepmind/optax): first-order gradient (SGD, Adam, ...) optimisers.  \n[Orbax](https://github.com/google/orbax): checkpointing (async/multi-host/multi-device).  \n[Levanter](https://github.com/stanford-crfm/levanter): scalable+reliable training of foundation models (e.g. LLMs).  \n[paramax](https://github.com/danielward27/paramax): parameterizations and constraints for PyTrees.\n\n**Scientific computing**  \n[Diffrax](https://github.com/patrick-kidger/diffrax): numerical differential equation solvers.  \n[Optimistix](https://github.com/patrick-kidger/optimistix): root finding, minimisation, fixed points, and least squares.  \n[Lineax](https://github.com/patrick-kidger/lineax): linear solvers.  \n[BlackJAX](https://github.com/blackjax-devs/blackjax): probabilistic+Bayesian sampling.  \n[sympy2jax](https://github.com/patrick-kidger/sympy2jax): SymPyJAX conversion; train symbolic expressions via gradient descent.  \n[PySR](https://github.com/milesCranmer/PySR): symbolic regression. (Non-JAX honourable mention!)  \n\n**Awesome JAX**  \n[Awesome Equinox](https://docs.kidger.site/equinox/awesome-list/)  \n[Awesome JAX](https://github.com/lockwo/awesome-jax): a longer list of other JAX projects."}, {"name": "equinox", "tags": ["math", "ml"], "summary": "Elegant easy-to-use neural networks in JAX.", "text": "This library is used to create and work with neural networks in JAX using an elegant, PyTorch-like syntax. It allows developers to define models that are fully compatible with normal JAX operations, making it easy to integrate into existing projects."}, {"name": "evaluate", "tags": ["math", "ml"], "summary": "HuggingFace community-driven open-source library of evaluation", "text": "Installation\n\nWith pip\n\n Evaluate can be installed from PyPi and has to be installed in a virtual environment (venv or conda for instance)\n\nUsage\n\n Evaluate's main methods are:\n\n- `evaluate.list_evaluation_modules()` to list the available metrics, comparisons and measurements\n- `evaluate.load(module_name, **kwargs)` to instantiate an evaluation module\n- `results = module.compute(*kwargs)` to compute the result of an evaluation module\n\nAdding a new evaluation module\n\nFirst install the necessary dependencies to create a new metric with the following command:\n\nThen you can get started with the following command which will create a new folder for your metric and display the necessary steps:\n\nSee this [step-by-step guide](https://huggingface.co/docs/evaluate/creating_and_sharing) in the documentation for detailed instructions.\n\nCredits\n\nThanks to [@marella](https://github.com/marella) for letting us use the `evaluate` namespace on PyPi previously used by his [library](https://github.com/marella/evaluate)."}, {"name": "evaluate", "tags": ["math", "ml"], "summary": "HuggingFace community-driven open-source library of evaluation", "text": "This library is used to provide a collection of evaluation metrics, comparisons, and measurements that can be easily integrated into machine learning workflows. With Evaluate, developers can seamlessly compute results for various evaluation tasks using its simple and intuitive API."}, {"name": "exchange-calendars", "tags": ["data", "math", "web"], "summary": "Calendars for securities exchanges", "text": "exchange_calendars\n\n(https://pypi.org/project/exchange-calendars/)   (https://github.com/astral-sh/ruff) (https://results.pre-commit.ci/latest/github/gerrymanoim/exchange_calendars/master)\n\nA Python library for defining and querying calendars for security exchanges.\n\nCalendars for more than [50 exchanges](#Calendars) available out-the-box! If you still can't find the calendar you're looking for, [create a new one](#How-can-I-create-a-new-calendar)!\n\nInstallation\n\nQuick Start\n\nGet a list of available calendars:\n\nGet a calendar:\n\nQuery the schedule:\n\nWorking with **sessions**\n\nSee the [sessions tutorial](docs/tutorials/sessions.ipynb) for a deeper dive into sessions.\n\nWorking with **minutes**\n\nCheck out the [minutes tutorial](docs/tutorials/minutes.ipynb) for a deeper dive that includes an explanation of the concept of 'minutes' and how the \"side\" option determines which minutes are treated as trading minutes.\n\nTutorials\n* [sessions.ipynb](docs/tutorials/sessions.ipynb) - all things [sessions](#Working-with-sessions).\n* [minutes.ipynb](docs/tutorials/minutes.ipynb) - all things [minutes](#Working-with-minutes). Don't miss this one!\n* [calendar_properties.ipynb](docs/tutorials/calendar_properties.ipynb) - calendar constrution and a walk through the schedule and all other calendar properties.\n* [calendar_methods.ipynb](docs/tutorials/calendar_methods.ipynb) - a walk through all the methods available to interrogate a calendar.\n* [trading_index.ipynb](docs/tutorials/trading_index.ipynb) - a method that warrants a tutorial all of its own.\n\nHopefully you'll find that `exchange_calendars` has the method you need to get the information you want. If it doesn't, either [PR](https://github.com/gerrymanoim/exchange_calendars/pulls) it or [raise an issue](https://github.com/gerrymanoim/exchange_calendars/issues) and let us know!\n\nCommand Line Usage\nPrint a unix-cal like calendar straight from the command line (holidays are indicated by brackets)...\n\nFrequently Asked Questions\n\n**How can I create a new calendar?**\n\nFirst off, make sure the calendar you're after hasn't already been defined; exchange calendars comes with over [50 pre-defined calendars](#Calendars), including major security exchanges.\n\nIf you can't find what you're after, a custom calendar can be created as a subclass of [ExchangeCalendar](exchange_calendars/exchange_calendar.py). [This workflow](.github/pull_request_template.md) describes the process to add a new calendar to `exchange_calendars`. Just follow the relevant parts.\n\nTo access the new calendar via `get_calendar` call either `xcals.register_calendar` or `xcals.register_calendar_type` to register, respectively, a specific calendar instance or a calendar factory (i.e. the subclass).\n\n**Can I contribute a new calendar to exchange calendars?**\n\nYes please! The workflow can be found [here](.github/pull_request_template.md).\n\n**`` is missing a holiday, has a wrong time, should have a break etc...**\n\n**All** of the exchange calendars are maintained by user contributions. If a calendar you care about needs revising, please open a [PR](https://github.com/gerrymanoim/exchange_calendars/pulls) - that's how this thing works! (Never contributed to a project before and it all seems a bit daunting? Check [this out](https://github.com/firstcontributions/first-contributions/blob/main/README.md) and don't look back!)\n\nYou'll find the workflow to modify an existing calendar [here](.github/pull_request_template.md).\n\n**What times are considered open and closed?**\n\n`exchange_calendars` attempts to be broadly useful by considering an exchange to be open only during periods of regular trading. During any pre-trading, post-trading or auction period the exchange is treated as closed. An exchange is also treated as closed during any observed lunch break.\n\nSee the [minutes tutorial](docs/tutorials/minutes.ipynb) for a detailed explanation of which minutes an exchange is considered open over. If you previously used `trading_calendars`, or `exchange_calendars` prior to release 3.4, then this is the place to look for answers to questions of how the definition of trading minutes has changed over time (and is now stable and flexible!).\n\nCalendars"}, {"name": "exchange-calendars", "tags": ["data", "math", "web"], "summary": "Calendars for securities exchanges", "text": "Exchange\n---------------------------------\nNew York Stock Exchange\nCBOE Futures\nChicago Mercantile Exchange\nICE US\nToronto Stock Exchange\nBMF Bovespa\nLondon Stock Exchange\nEuronext Amsterdam\nEuronext Brussels\nEuronext Lisbon\nEuronext Paris\nFrankfurt Stock Exchange\nSIX Swiss Exchange\nTokyo Stock Exchange\nAustralian Securities Exchange\nBolsa de Madrid\nBorsa Italiana\nNew Zealand Exchange\nWiener Borse\nHong Kong Stock Exchange\nCopenhagen Stock Exchange\nHelsinki Stock Exchange\nStockholm Stock Exchange\nOslo Stock Exchange\nIrish Stock Exchange\nBombay Stock Exchange\nSingapore Exchange\nShanghai Stock Exchange\nKorea Exchange\nIceland Stock Exchange\nPoland Stock Exchange\nSantiago Stock Exchange\nColombia Securities Exchange\nMexican Stock Exchange\nLima Stock Exchange\nPrague Stock Exchange\nBudapest Stock Exchange\nAthens Stock Exchange\nIstanbul Stock Exchange\nJohannesburg Stock Exchange\nMalaysia Stock Exchange\nMoscow Exchange\nPhilippine Stock Exchange\nStock Exchange of Thailand\nIndonesia Stock Exchange\nTaiwan Stock Exchange Corp.\nBuenos Aires Stock Exchange\nPakistan Stock Exchange\nXetra\nTel Aviv Stock Exchange\nAstana International Exchange\nBucharest Stock Exchange\nSaudi Stock Exchange\nEuropean Energy Exchange AG\nHamburg Stock Exchange\nDuesseldorf Stock Exchange\nLuxembourg Stock Exchange\nTallinn Stock Exchange\nRiga Stock Exchange\nVilnius Stock Exchange\nCyprus Stock Exchange\nBermuda Stock Exchange\nZagreb Stock Exchange\nLjubljana Stock Exchange\nBratislava Stock Exchange\nBelgrade Stock Exchange\n\n> Note that exchange calendars are defined by their [ISO-10383](https://www.iso20022.org/10383/iso-10383-market-identifier-codes) market identifier code.\n\n[`market-prices`](https://github.com/maread99/market_prices)\nMuch of the post v3 development of `exchange_calendars` has been driven by the [`market_prices`](https://github.com/maread99/market_prices) library. Check it out if you like the idea of using `exchange_calendars` to create meaningful OHLCV datasets. It works out-the-box with freely available data!\n\nDeprecations and Renaming\n\nMethods renamed in version 4.0.3 and removed in 4.3\nPrevious name\n-------------\nbound_start\nbound_end\n\nMethods deprecated in 4.0 and removed in 4.3\nDeprecated method\n-----------------\nsessions_closes\nsessions_opens\n\nMethods with a parameter renamed in 4.0\nMethod\n------\nis_session\nis_open_on_minute\nminutes_in_range\nminutes_window\nnext_close\nnext_minute\nnext_open\nprevious_close\nprevious_minute\nprevious_open\nsession_break_end\nsession_break_start\nsession_close\nsession_open\nsessions_in_range\nsessions_window\n\nMethods renamed in version 3.4 and removed in 4.0\nPrevious name\n-------------\nall_minutes\nall_minutes_nanos\nall_sessions\nbreak_start_and_end_for_session\ndate_to_session_label\nfirst_trading_minute\nfirst_trading_session\nhas_breaks\nlast_trading_minute\nlast_trading_session\nnext_session_label\nopen_and_close_for_session\nprevious_session_label\nmarket_break_ends_nanos\nmarket_break_starts_nanos\nmarket_closes_nanos\nmarket_opens_nanos\nminute_index_to_session_labels\nminute_to_session_label\nminutes_count_for_sessions_in_range\nminutes_for_session\nminutes_for_sessions_in_range\nsession_closes_in_range\nsession_distance\nsession_opens_in_range\n\nOther methods deprecated in 3.4 and removed in 4.0\nRemoved Method\n-----------------\nexecution_minute_for_session\nexecution_minute_for_sessions_in_range\nexecution_time_from_close\nexecution_time_from_open"}, {"name": "exchange-calendars", "tags": ["data", "math", "web"], "summary": "Calendars for securities exchanges", "text": "This library is used to define and query calendars for various security exchanges, providing access to pre-built calendars for over 50 exchanges. With this library, developers can easily retrieve and manipulate exchange schedules, sessions, and minutes."}, {"name": "faiss-cpu", "tags": ["math", "ml", "web"], "summary": "A library for efficient similarity search and clustering of dense vectors.", "text": "faiss-wheels \n\n(https://github.com/kyamagu/faiss-wheels/actions/workflows/build.yml)\n(https://pypi.org/project/faiss-cpu/)\n\nFaiss Python wheel packages.\n\n- [faiss](https://github.com/facebookresearch/faiss)\n\nOverview\n\nThis repository provides CI scripts to build wheel packages for the\n[faiss](https://github.com/facebookresearch/faiss) library.\n\n- Support various build options for customization.\n\nFeatures\n\nThe PyPI distributed wheels include the following extentions.\n\nPlatform\n--------\nLinux\nLinux\nmacOS\nmacOS\nWindows\nWindows\n\n> **Note**\n> GPU binary package is discontinued as of 1.7.3 release. Build a custom wheel to support GPU features.\n\nInstall\n\nInstall the CPU-only package by:\n\nNote that the package name is `faiss-cpu`.\n\nUsage\n\nCheck [the official documentation at the upstream](https://faiss.ai/) for general usage.\n\nIndex portability\n\nOne caveat is that faiss indices built in a specific environment is not always compatible in the other environment. For example, indices built and saved in the x86_64 architecture is not always compatible in the arm64 environment. In addition, SIMD features can lead to incompatibility. Indices built in the AVX2 extension are not compatible in the generic extension. Faiss automatically detects the CPU instruction set and loads extensions. This tends to be an issue in the containerized environment where CPU features are not correctly detected due to driver issues.\n\nIf you encounter a segfault or weird argument errors, set the following environment variable to force or disable the specific SIMD extension:\n\nBuilding customized wheels\n\nThe PyPI wheels do not support GPU by default. To support GPU methods or use faiss with a different build configuration, build a custom wheel. For building a wheel package, there are a few requirements.\n\n- BLAS: There must be a BLAS implementation available on the Linux and Windows platforms.\n- OpenMP: macOS requires `libomp` (available via Homebrew).\n- CUDA or ROCm: A GPU development toolkit is necessary to support GPU features.\n\nSee `scripts/install_*` scripts for details.\n\nBuild instruction\n\nClone the repository with submodules.\n\nYou can use a standard Python environment manager like `pipx` to build a wheel.\n\nAny build backend supporting `scikit-build-core` can build wheels. For example, you can use `uv` to build wheels.\n\nBuild options\n\nYou can set environment variables to customize the build options. The following example builds a wheel with AVX2 and CUDA support.\n\nAlternatively, you may directly pass CMake options via the command line. See [the scikit-build-core documentation](https://scikit-build-core.readthedocs.io/en/latest/configuration/index.html#configuring-cmake-arguments-and-defines) for details on how to specify CMake defines.\n\nThe following options are available for configuration.\n\n- `FAISS_OPT_LEVELS`: Optimization levels. You may set a semicolon-separated list of values from ``. For example, setting `generic,avx2` will include both `generic` and `avx2` binary extensions in the resulting wheel. This option offers more flexibility than the upstream config variable `FAISS_OPT_LEVEL`, which cannot specify arbitrary combinations.\n- `FAISS_GPU_SUPPORT`: GPU support. You may set a value from ``. For example, setting `CUDA` will enable CUDA support. For CUDA, you will need the [CUDA toolkit](https://developer.nvidia.com/cuda-toolkit) installed on the system. For ROCm, you will need the [ROCm](https://rocm.docs.amd.com/en/latest/).\n- `FAISS_ENABLE_MKL`: Intel MKL support. Default is `OFF`. Setting `FAISS_ENABLE_MKL=ON` links Intel oneAPI Math Kernel Library on Linux. You will need to install [Intel oneAPI MKL](https://www.intel.com/content/www/us/en/developer/tools/oneapi/onemkl.html) before building a wheel. When `OFF`, the system needs a BLAS backend that CMake can find, such as OpenBLAS.\n- `FAISS_USE_LTO`: Enable link time optimization. Default is `ON`. Set `FAISS_USE_LTO=OFF` to disable.\n\nSee also the list of supported build-time options in [the upstream documentation](https://github.com/facebookresearch/faiss/blob/main/INSTALL.md#step-1-invoking-cmake). Do not directly set `FAISS_OPT_LEVEL` and `FAISS_ENABLE_GPU` when building a wheel via this project, as that will confuse the build process.\n\nYou might want to overwrite the default wheel package name `faiss-cpu` depending on the build option. Manually rewrite the name field in `pyproject.toml` file, or launch the following script to update the project name in `pyproject.toml`.\n\nDevelopment\n\nThis repository is intended to support PyPI distribution for the official [faiss](https://github.com/facebookresearch/faiss) library.\nThe repository contains the CI workflow based on [cibuildwheel](https://github.com/pypa/cibuildwheel/).\nFeel free to make a pull request to fix packaging problems.\n\nCurrently, GPU wheels result in a large binary size that exceeds the file size limit of PyPI.\n\nOther relevant resources:"}, {"name": "faiss-cpu", "tags": ["math", "ml", "web"], "summary": "A library for efficient similarity search and clustering of dense vectors.", "text": "This library is used to efficiently perform similarity search and clustering of dense vectors in CPU-based applications. Developers can leverage this library to build high-performance vector similarity search models without requiring a GPU."}, {"name": "fastapi", "tags": ["cli", "data", "ui", "web"], "summary": "FastAPI framework, high performance, easy to learn, fast to code, ready for production", "text": "Sponsors\n\nKeystone Sponsor\n\nGold and Silver Sponsors\n\nOther sponsors\n\nOpinions\n\n\"_[...] I'm using **FastAPI** a ton these days. [...] I'm actually planning to use it for all of my team's **ML services at Microsoft**. Some of them are getting integrated into the core **Windows** product and some **Office** products._\"\n\nKabir Khan - Microsoft (ref)\n\n---\n\n\"_We adopted the **FastAPI** library to spawn a **REST** server that can be queried to obtain **predictions**. [for Ludwig]_\"\n\nPiero Molino, Yaroslav Dudin, and Sai Sumanth Miryala - Uber (ref)\n\n---\n\n\"_**Netflix** is pleased to announce the open-source release of our **crisis management** orchestration framework: **Dispatch**! [built with **FastAPI**]_\"\n\nKevin Glisson, Marc Vilanova, Forest Monsen - Netflix (ref)\n\n---\n\n\"_I\u2019m over the moon excited about **FastAPI**. It\u2019s so fun!_\"\n\nBrian Okken - Python Bytes podcast host (ref)\n\n---\n\n\"_Honestly, what you've built looks super solid and polished. In many ways, it's what I wanted **Hug** to be - it's really inspiring to see someone build that._\"\n\nTimothy Crosley - Hug creator (ref)\n\n---\n\n\"_If you're looking to learn one **modern framework** for building REST APIs, check out **FastAPI** [...] It's fast, easy to use and easy to learn [...]_\"\n\n\"_We've switched over to **FastAPI** for our **APIs** [...] I think you'll like it [...]_\"\n\nInes Montani - Matthew Honnibal - Explosion AI founders - spaCy creators (ref) - (ref)\n\n---\n\n\"_If anyone is looking to build a production Python API, I would highly recommend **FastAPI**. It is **beautifully designed**, **simple to use** and **highly scalable**, it has become a **key component** in our API first development strategy and is driving many automations and services such as our Virtual TAC Engineer._\"\n\nDeon Pillsbury - Cisco (ref)\n\n---\n\n**Typer**, the FastAPI of CLIs\n\nIf you are building a CLI app to be used in the terminal instead of a web API, check out **Typer**.\n\n**Typer** is FastAPI's little sibling. And it's intended to be the **FastAPI of CLIs**. \u2328\ufe0f\n\nRequirements\n\nFastAPI stands on the shoulders of giants:\n\n* Starlette for the web parts.\n* Pydantic for the data parts.\n\nInstallation\n\nCreate and activate a virtual environment and then install FastAPI:\n\n**Note**: Make sure you put `\"fastapi[standard]\"` in quotes to ensure it works in all terminals.\n\nExample\n\nCreate it\n\nCreate a file `main.py` with:\n\nOr use async def...\n\nIf your code uses `async` / `await`, use `async def`:\n\n**Note**:\n\nIf you don't know, check the _\"In a hurry?\"_ section about `async` and `await` in the docs.\n\nRun it\n\nRun the server with:\n\nAbout the command fastapi dev main.py...\n\nThe command `fastapi dev` reads your `main.py` file, detects the **FastAPI** app in it, and starts a server using Uvicorn.\n\nBy default, `fastapi dev` will start with auto-reload enabled for local development.\n\nYou can read more about it in the FastAPI CLI docs.\n\nCheck it\n\nOpen your browser at http://127.0.0.1:8000/items/5?q=somequery.\n\nYou will see the JSON response as:\n\nYou already created an API that:"}, {"name": "fastapi", "tags": ["cli", "data", "ui", "web"], "summary": "FastAPI framework, high performance, easy to learn, fast to code, ready for production", "text": "* Receives HTTP requests in the _paths_ `/` and `/items/{item_id}`.\n* Both _paths_ take `GET` operations (also known as HTTP _methods_).\n* The _path_ `/items/{item_id}` has a _path parameter_ `item_id` that should be an `int`.\n* The _path_ `/items/{item_id}` has an optional `str` _query parameter_ `q`.\n\nInteractive API docs\n\nNow go to http://127.0.0.1:8000/docs.\n\nYou will see the automatic interactive API documentation (provided by Swagger UI):\n\nAlternative API docs\n\nAnd now, go to http://127.0.0.1:8000/redoc.\n\nYou will see the alternative automatic documentation (provided by ReDoc):\n\nExample upgrade\n\nNow modify the file `main.py` to receive a body from a `PUT` request.\n\nDeclare the body using standard Python types, thanks to Pydantic.\n\nThe `fastapi dev` server should reload automatically.\n\nInteractive API docs upgrade\n\nNow go to http://127.0.0.1:8000/docs.\n\n* The interactive API documentation will be automatically updated, including the new body:\n\n* Click on the button \"Try it out\", it allows you to fill the parameters and directly interact with the API:\n\n* Then click on the \"Execute\" button, the user interface will communicate with your API, send the parameters, get the results and show them on the screen:\n\nAlternative API docs upgrade\n\nAnd now, go to http://127.0.0.1:8000/redoc.\n\n* The alternative documentation will also reflect the new query parameter and body:\n\nRecap\n\nIn summary, you declare **once** the types of parameters, body, etc. as function parameters.\n\nYou do that with standard modern Python types.\n\nYou don't have to learn a new syntax, the methods or classes of a specific library, etc.\n\nJust standard **Python**.\n\nFor example, for an `int`:\n\nor for a more complex `Item` model:\n\n...and with that single declaration you get:\n\n* Editor support, including:\n* Validation of data:\n* Conversion of input data: coming from the network to Python data and types. Reading from:\n* Conversion of output data: converting from Python data and types to network data (as JSON):\n* Automatic interactive API documentation, including 2 alternative user interfaces:\n\n---\n\nComing back to the previous code example, **FastAPI** will:\n\n* Validate that there is an `item_id` in the path for `GET` and `PUT` requests.\n* Validate that the `item_id` is of type `int` for `GET` and `PUT` requests.\n* Check if there is an optional query parameter named `q` (as in `http://127.0.0.1:8000/items/foo?q=somequery`) for `GET` requests.\n* For `PUT` requests to `/items/{item_id}`, read the body as JSON:\n* Convert from and to JSON automatically.\n* Document everything with OpenAPI, that can be used by:\n* Provide 2 interactive documentation web interfaces directly.\n\n---\n\nWe just scratched the surface, but you already get the idea of how it all works.\n\nTry changing the line with:\n\n...from:\n\n...to:\n\n...and see how your editor will auto-complete the attributes and know their types:\n\nFor a more complete example including more features, see the Tutorial - User Guide.\n\n**Spoiler alert**: the tutorial - user guide includes:"}, {"name": "fastapi", "tags": ["cli", "data", "ui", "web"], "summary": "FastAPI framework, high performance, easy to learn, fast to code, ready for production", "text": "* Declaration of **parameters** from other different places as: **headers**, **cookies**, **form fields** and **files**.\n* How to set **validation constraints** as `maximum_length` or `regex`.\n* A very powerful and easy to use **Dependency Injection** system.\n* Security and authentication, including support for **OAuth2** with **JWT tokens** and **HTTP Basic** auth.\n* More advanced (but equally easy) techniques for declaring **deeply nested JSON models** (thanks to Pydantic).\n* **GraphQL** integration with Strawberry and other libraries.\n* Many extra features (thanks to Starlette) as:\n\nDeploy your app (optional)\n\nYou can optionally deploy your FastAPI app to FastAPI Cloud, go and join the waiting list if you haven't.\n\nIf you already have a **FastAPI Cloud** account (we invited you from the waiting list ), you can deploy your application with one command.\n\nBefore deploying, make sure you are logged in:\n\nThen deploy your app:\n\nThat's it! Now you can access your app at that URL.\n\nAbout FastAPI Cloud\n\n**FastAPI Cloud** is built by the same author and team behind **FastAPI**.\n\nIt streamlines the process of **building**, **deploying**, and **accessing** an API with minimal effort.\n\nIt brings the same **developer experience** of building apps with FastAPI to **deploying** them to the cloud.\n\nFastAPI Cloud is the primary sponsor and funding provider for the *FastAPI and friends* open source projects.\n\nDeploy to other cloud providers\n\nFastAPI is open source and based on standards. You can deploy FastAPI apps to any cloud provider you choose.\n\nFollow your cloud provider's guides to deploy FastAPI apps with them.\n\nPerformance\n\nIndependent TechEmpower benchmarks show **FastAPI** applications running under Uvicorn as one of the fastest Python frameworks available, only below Starlette and Uvicorn themselves (used internally by FastAPI). (*)\n\nTo understand more about it, see the section Benchmarks.\n\nDependencies\n\nFastAPI depends on Pydantic and Starlette.\n\n`standard` Dependencies\n\nWhen you install FastAPI with `pip install \"fastapi[standard]\"` it comes with the `standard` group of optional dependencies:\n\nUsed by Pydantic:\n\n* email-validator - for email validation.\n\nUsed by Starlette:\n\n* httpx - Required if you want to use the `TestClient`.\n* jinja2 - Required if you want to use the default template configuration.\n* python-multipart - Required if you want to support form \"parsing\", with `request.form()`.\n\nUsed by FastAPI:\n\n* uvicorn - for the server that loads and serves your application. This includes `uvicorn[standard]`, which includes some dependencies (e.g. `uvloop`) needed for high performance serving.\n* `fastapi-cli[standard]` - to provide the `fastapi` command.\n\nWithout `standard` Dependencies\n\nIf you don't want to include the `standard` optional dependencies, you can install with `pip install fastapi` instead of `pip install \"fastapi[standard]\"`.\n\nWithout `fastapi-cloud-cli`\n\nIf you want to install FastAPI with the standard dependencies but without the `fastapi-cloud-cli`, you can install with `pip install \"fastapi[standard-no-fastapi-cloud-cli]\"`.\n\nAdditional Optional Dependencies\n\nThere are some additional dependencies you might want to install.\n\nAdditional optional Pydantic dependencies:\n\n* pydantic-settings - for settings management.\n* pydantic-extra-types - for extra types to be used with Pydantic.\n\nAdditional optional FastAPI dependencies:"}, {"name": "fastapi", "tags": ["cli", "data", "ui", "web"], "summary": "FastAPI framework, high performance, easy to learn, fast to code, ready for production", "text": "* orjson - Required if you want to use `ORJSONResponse`.\n* ujson - Required if you want to use `UJSONResponse`.\n\nLicense\n\nThis project is licensed under the terms of the MIT license."}, {"name": "fastapi", "tags": ["cli", "data", "ui", "web"], "summary": "FastAPI framework, high performance, easy to learn, fast to code, ready for production", "text": "This library is used to build high-performance web APIs quickly and easily, enabling developers to create scalable and efficient server-side applications. With FastAPI, developers can rapidly develop and deploy REST servers, integrate machine learning services, or orchestrate complex workflows with ease."}, {"name": "fastavro", "tags": ["math"], "summary": "Fast read/write of AVRO files", "text": "fastavro\n(https://github.com/fastavro/fastavro/actions)\n\n(https://codecov.io/gh/fastavro/fastavro)\n\nBecause the Apache Python `avro` package is written in pure Python, it is\nrelatively slow. In one test case, it takes about 14 seconds to iterate through\na file of 10,000 records. By comparison, the JAVA `avro` SDK reads the same file in\n1.9 seconds.\n\nThe `fastavro` library was written to offer performance comparable to the Java\nlibrary. With regular CPython, `fastavro` uses C extensions which allow it to\niterate the same 10,000 record file in 1.7 seconds. With PyPy, this drops to 1.5\nseconds (to be fair, the JAVA benchmark is doing some extra JSON\nencoding/decoding).\n\n`fastavro` supports the following Python versions:\n\n* Python 3.9\n* Python 3.10\n* Python 3.11\n* Python 3.12\n* Python 3.13\n* Python 3.14\n* PyPy3\n\nSupported Features\n\n* File Writer\n* File Reader (iterating via records or blocks)\n* Schemaless Writer\n* Schemaless Reader\n* JSON Writer\n* JSON Reader\n* Codecs (Snappy, Deflate, Zstandard, Bzip2, LZ4, XZ)\n* Schema resolution\n* Aliases\n* Logical Types\n* Parsing schemas into the canonical form\n* Schema fingerprinting\n\nMissing Features\n\n* Anything involving Avro's RPC features\n\nDocumentation\n\nDocumentation is available at http://fastavro.readthedocs.io/en/latest/\n\nInstalling\n`fastavro` is available both on [PyPI](http://pypi.python.org/pypi)\n\nand on [conda-forge](https://conda-forge.github.io) `conda` channel.\n\nContributing\n\n* Bugs and new feature requests typically start as GitHub issues where they can be discussed. I try to resolve these as time affords, but PRs are welcome from all.\n* Get approval from discussing on the GitHub issue before opening the pull request\n* Tests must be passing for pull request to be considered\n\nDeveloper requirements can be installed with `pip install -r developer_requirements.txt`.\nIf those are installed, you can run the tests with `./run-tests.sh`. If you have trouble\ninstalling those dependencies, you can run `docker build .` to run the tests inside\na Docker container. This won't test on all versions of Python or on PyPy, so it's possible\nto still get CI failures after making a pull request, but we can work through those errors\nif/when they happen. `.run-tests.sh` only covers the Cython tests. In order to test the\npure Python implementation, comment out `python setup.py build_ext --inplace`\nand re-run.\n\nNOTE: Some tests might fail when running the tests locally. An example of this\nis this codec tests. If the supporting codec library is not available, the test\nwill fail. These failures can be ignored since the tests will on pull requests\nand will be run in the correct environments with the correct dependencies set up.\n\nReleasing\n\nWe release both to [PyPI][pypi] and to [conda-forge][conda-forge].\n\nWe assume you have [twine][twine] installed and that you've created your own\nfork of [fastavro-feedstock][feedstock].\n\n* Make sure the tests pass\n* Run `make tag`\n* Wait for all artifacts to be built and published to the GitHub release.\n* Run `make publish`\n* The conda-forge PR should get created and merged automatically\n\nChanges\n\nSee the [ChangeLog]\n\nContact\n\n[Project Home](https://github.com/fastavro/fastavro)"}, {"name": "fastavro", "tags": ["math"], "summary": "Fast read/write of AVRO files", "text": "This library is used to enable high-performance read and write operations for AVRO files in Python, achieving speeds comparable to the Java SDK. By leveraging C extensions or PyPy, developers can efficiently handle large-scale AVRO data with faster iteration times."}, {"name": "faster-whisper", "tags": ["cli", "math", "ml", "ui", "web"], "summary": "Faster Whisper transcription with CTranslate2", "text": "Faster Whisper transcription with CTranslate2\n\n**faster-whisper** is a reimplementation of OpenAI's Whisper model using [CTranslate2](https://github.com/OpenNMT/CTranslate2/), which is a fast inference engine for Transformer models.\n\nThis implementation is up to 4 times faster than [openai/whisper](https://github.com/openai/whisper) for the same accuracy while using less memory. The efficiency can be further improved with 8-bit quantization on both CPU and GPU.\n\nBenchmark\n\nWhisper\n\nFor reference, here's the time and memory usage that are required to transcribe [**13 minutes**](https://www.youtube.com/watch?v=0u7tTptBo9I) of audio using different implementations:\n\n* [openai/whisper](https://github.com/openai/whisper)@[v20240930](https://github.com/openai/whisper/tree/v20240930)\n* [whisper.cpp](https://github.com/ggerganov/whisper.cpp)@[v1.7.2](https://github.com/ggerganov/whisper.cpp/tree/v1.7.2)\n* [transformers](https://github.com/huggingface/transformers)@[v4.46.3](https://github.com/huggingface/transformers/tree/v4.46.3)\n* [faster-whisper](https://github.com/SYSTRAN/faster-whisper)@[v1.1.0](https://github.com/SYSTRAN/faster-whisper/tree/v1.1.0)\n\nLarge-v2 model on GPU\n\nImplementation\n---\nopenai/whisper\nwhisper.cpp (Flash Attention)\ntransformers (SDPA)[^1]\nfaster-whisper\nfaster-whisper (`batch_size=8`)\nfaster-whisper\nfaster-whisper (`batch_size=8`)\n\ndistil-whisper-large-v3 model on GPU\n\nImplementation\n---\ntransformers (SDPA) (`batch_size=16`)\nfaster-whisper (`batch_size=16`)\n\n*GPU Benchmarks are Executed with CUDA 12.4 on a NVIDIA RTX 3070 Ti 8GB.*\n[^1]: transformers OOM for any batch size > 1\n\nSmall model on CPU\n\nImplementation\n---\nopenai/whisper\nwhisper.cpp\nwhisper.cpp (OpenVINO)\nfaster-whisper\nfaster-whisper (`batch_size=8`)\nfaster-whisper\nfaster-whisper (`batch_size=8`)\n\n*Executed with 8 threads on an Intel Core i7-12700K.*\n\nRequirements\n\n* Python 3.9 or greater\n\nUnlike openai-whisper, FFmpeg does **not** need to be installed on the system. The audio is decoded with the Python library [PyAV](https://github.com/PyAV-Org/PyAV) which bundles the FFmpeg libraries in its package.\n\nGPU\n\nGPU execution requires the following NVIDIA libraries to be installed:\n\n* [cuBLAS for CUDA 12](https://developer.nvidia.com/cublas)\n* [cuDNN 9 for CUDA 12](https://developer.nvidia.com/cudnn)\n\n**Note**: The latest versions of `ctranslate2` only support CUDA 12 and cuDNN 9. For CUDA 11 and cuDNN 8, the current workaround is downgrading to the `3.24.0` version of `ctranslate2`, for CUDA 12 and cuDNN 8, downgrade to the `4.4.0` version of `ctranslate2`, (This can be done with `pip install --force-reinstall ctranslate2==4.4.0` or specifying the version in a `requirements.txt`).\n\nThere are multiple ways to install the NVIDIA libraries mentioned above. The recommended way is described in the official NVIDIA documentation, but we also suggest other installation methods below.\n\nOther installation methods (click to expand)\n\n**Note:** For all these methods below, keep in mind the above note regarding CUDA versions. Depending on your setup, you may need to install the _CUDA 11_ versions of libraries that correspond to the CUDA 12 libraries listed in the instructions below.\n\nUse Docker\n\nThe libraries (cuBLAS, cuDNN) are installed in this official NVIDIA CUDA Docker images: `nvidia/cuda:12.3.2-cudnn9-runtime-ubuntu22.04`.\n\nInstall with `pip` (Linux only)\n\nOn Linux these libraries can be installed with `pip`. Note that `LD_LIBRARY_PATH` must be set before launching Python.\n\nDownload the libraries from Purfview's repository (Windows & Linux)\n\nPurfview's [whisper-standalone-win](https://github.com/Purfview/whisper-standalone-win) provides the required NVIDIA libraries for Windows & Linux in a [single archive](https://github.com/Purfview/whisper-standalone-win/releases/tag/libs). Decompress the archive and place the libraries in a directory included in the `PATH`.\n\nInstallation\n\nThe module can be installed from [PyPI](https://pypi.org/project/faster-whisper/):\n\nOther installation methods (click to expand)\n\nInstall the master branch\n\nInstall a specific commit\n\nUsage\n\nFaster-whisper\n\n**Warning:** `segments` is a *generator* so the transcription only starts when you iterate over it. The transcription can be run to completion by gathering the segments in a list or a `for` loop:"}, {"name": "faster-whisper", "tags": ["cli", "math", "ml", "ui", "web"], "summary": "Faster Whisper transcription with CTranslate2", "text": "Batched Transcription\nThe following code snippet illustrates how to run batched transcription on an example audio file. `BatchedInferencePipeline.transcribe` is a drop-in replacement for `WhisperModel.transcribe`\n\nFaster Distil-Whisper\n\nThe Distil-Whisper checkpoints are compatible with the Faster-Whisper package. In particular, the latest [distil-large-v3](https://huggingface.co/distil-whisper/distil-large-v3)\ncheckpoint is intrinsically designed to work with the Faster-Whisper transcription algorithm. The following code snippet \ndemonstrates how to run inference with distil-large-v3 on a specified audio file:\n\nFor more information about the distil-large-v3 model, refer to the original [model card](https://huggingface.co/distil-whisper/distil-large-v3).\n\nWord-level timestamps\n\nVAD filter\n\nThe library integrates the [Silero VAD](https://github.com/snakers4/silero-vad) model to filter out parts of the audio without speech:\n\nThe default behavior is conservative and only removes silence longer than 2 seconds. See the available VAD parameters and default values in the [source code](https://github.com/SYSTRAN/faster-whisper/blob/master/faster_whisper/vad.py). They can be customized with the dictionary argument `vad_parameters`:\n\nVad filter is enabled by default for batched transcription.\n\nLogging\n\nThe library logging level can be configured like this:\n\nGoing further\n\nSee more model and transcription options in the [`WhisperModel`](https://github.com/SYSTRAN/faster-whisper/blob/master/faster_whisper/transcribe.py) class implementation.\n\nCommunity integrations\n\nHere is a non exhaustive list of open-source projects using faster-whisper. Feel free to add your project to the list!\n\n* [speaches](https://github.com/speaches-ai/speaches) is an OpenAI compatible server using `faster-whisper`. It's easily deployable with Docker, works with OpenAI SDKs/CLI, supports streaming, and live transcription.\n* [WhisperX](https://github.com/m-bain/whisperX) is an award-winning Python library that offers speaker diarization and accurate word-level timestamps using wav2vec2 alignment\n* [whisper-ctranslate2](https://github.com/Softcatala/whisper-ctranslate2) is a command line client based on faster-whisper and compatible with the original client from openai/whisper.\n* [whisper-diarize](https://github.com/MahmoudAshraf97/whisper-diarization) is a speaker diarization tool that is based on faster-whisper and NVIDIA NeMo.\n* [whisper-standalone-win](https://github.com/Purfview/whisper-standalone-win) Standalone CLI executables of faster-whisper for Windows, Linux & macOS. \n* [asr-sd-pipeline](https://github.com/hedrergudene/asr-sd-pipeline) provides a scalable, modular, end to end multi-speaker speech to text solution implemented using AzureML pipelines.\n* [Open-Lyrics](https://github.com/zh-plus/Open-Lyrics) is a Python library that transcribes voice files using faster-whisper, and translates/polishes the resulting text into `.lrc` files in the desired language using OpenAI-GPT.\n* [wscribe](https://github.com/geekodour/wscribe) is a flexible transcript generation tool supporting faster-whisper, it can export word level transcript and the exported transcript then can be edited with [wscribe-editor](https://github.com/geekodour/wscribe-editor)\n* [aTrain](https://github.com/BANDAS-Center/aTrain) is a graphical user interface implementation of faster-whisper developed at the BANDAS-Center at the University of Graz for transcription and diarization in Windows ([Windows Store App](https://apps.microsoft.com/detail/atrain/9N15Q44SZNS2)) and Linux.\n* [Whisper-Streaming](https://github.com/ufal/whisper_streaming) implements real-time mode for offline Whisper-like speech-to-text models with faster-whisper as the most recommended back-end. It implements a streaming policy with self-adaptive latency based on the actual source complexity, and demonstrates the state of the art.\n* [WhisperLive](https://github.com/collabora/WhisperLive) is a nearly-live implementation of OpenAI's Whisper which uses faster-whisper as the backend to transcribe audio in real-time.\n* [Faster-Whisper-Transcriber](https://github.com/BBC-Esq/ctranslate2-faster-whisper-transcriber) is a simple but reliable voice transcriber that provides a user-friendly interface.\n* [Open-dubbing](https://github.com/softcatala/open-dubbing) is open dubbing is an AI dubbing system which uses machine learning models to automatically translate and synchronize audio dialogue into different languages.\n* [Whisper-FastAPI](https://github.com/heimoshuiyu/whisper-fastapi) whisper-fastapi is a very simple script that provides an API backend compatible with OpenAI, HomeAssistant, and Konele (Android voice typing) formats.\n\nModel conversion"}, {"name": "faster-whisper", "tags": ["cli", "math", "ml", "ui", "web"], "summary": "Faster Whisper transcription with CTranslate2", "text": "When loading a model from its size such as `WhisperModel(\"large-v3\")`, the corresponding CTranslate2 model is automatically downloaded from the [Hugging Face Hub](https://huggingface.co/Systran).\n\nWe also provide a script to convert any Whisper models compatible with the Transformers library. They could be the original OpenAI models or user fine-tuned models.\n\nFor example the command below converts the [original \"large-v3\" Whisper model](https://huggingface.co/openai/whisper-large-v3) and saves the weights in FP16:\n\n* The option `--model` accepts a model name on the Hub or a path to a model directory.\n* If the option `--copy_files tokenizer.json` is not used, the tokenizer configuration is automatically downloaded when the model is loaded later.\n\nModels can also be converted from the code. See the [conversion API](https://opennmt.net/CTranslate2/python/ctranslate2.converters.TransformersConverter.html).\n\nLoad a converted model\n\n1. Directly load the model from a local directory:\n\n2. [Upload your model to the Hugging Face Hub](https://huggingface.co/docs/transformers/model_sharing#upload-with-the-web-interface) and load it from its name:\n\nComparing performance against other implementations\n\nIf you are comparing the performance against other Whisper implementations, you should make sure to run the comparison with similar settings. In particular:\n\n* Verify that the same transcription options are used, especially the same beam size. For example in openai/whisper, `model.transcribe` uses a default beam size of 1 but here we use a default beam size of 5.\n* Transcription speed is closely affected by the number of words in the transcript, so ensure that other implementations have a similar WER (Word Error Rate) to this one.\n* When running on CPU, make sure to set the same number of threads. Many frameworks will read the environment variable `OMP_NUM_THREADS`, which can be set when running your script:"}, {"name": "faster-whisper", "tags": ["cli", "math", "ml", "ui", "web"], "summary": "Faster Whisper transcription with CTranslate2", "text": "This library is used to accelerate Whisper transcription by up to 4 times, allowing for faster and more efficient audio-to-text conversion while maintaining the same level of accuracy. It achieves this through a reimplementation using CTranslate2, a fast inference engine for Transformer models."}, {"name": "fasttext-langdetect", "tags": ["data", "math", "ml"], "summary": "80x faster and 95% accurate language identification with Fasttext", "text": "fasttext-langdetect\nThis library is a wrapper for the language detection model trained on fasttext by Facebook. For more information, please visit: https://fasttext.cc/docs/en/language-identification.html\n\nSupported languages\n\nInstall\n\nUsage\n`detect` method expects UTF-8 data. `low_memory` option enables getting predictions with the compressed version of the fasttext model by sacrificing the accuracy a bit.\n\nBenchmark\nWe benchmarked the fasttext model against [cld2](https://github.com/CLD2Owners/cld2), [langid](https://github.com/saffsd/langid.py), and [langdetect](https://github.com/Mimino666/langdetect) on Wili-2018 dataset.\n\nfasttext\n--------------------------\nAverage time (ms)\n139 langs - not weighted\n139 langs - pop weighted\n44 langs - not weighted\n44 langs - pop weighted\n\n- `pop weighted` means recall for each language is multipled by [its number of speakers](https://en.wikipedia.org/wiki/List_of_languages_by_total_number_of_speakers).\n- 139 languages = all languages with ISO 639-1 2-letter code\n- 44 languages = top 44 languages spoken in the world\n\nRecall per language\nlang\n-------------------------\nAfrikaans\nAlbanian\nAmharic\nArabic\nAragonese\nArmenian\nAssamese\nAvar\nAymara\nAzerbaijani\nBashkir\nBasque\nBelarusian\nBengali\nBhojpuri\nBokm\u00e5l\nBosnian\nBreton\nBulgarian\nBurmese\nCatalan\nCentral Khmer\nChechen\nChuvash\nCornish\nCorsican\nCroatian\nCzech\nDanish\nDhivehi\nDutch\nEnglish\nEsperanto\nEstonian\nFaroese\nFinnish\nFrench\nGalician\nGeorgian\nGerman\nGuarani\nGujarati\nHaitian Creole\nHausa\nHebrew\nHindi\nHungarian\nIcelandic\nIdo\nIgbo\nIndonesian\nInterlingua\nInterlingue\nIrish\nItalian\nJapanese\nJavanese\nKannada\nKazakh\nKinyarwanda\nKirghiz\nKomi\nKorean\nKurdish\nLao\nLatin\nLatvian\nLimburgan\nLingala\nLithuanian\nLuganda\nLuxembourgish\nMacedonian\nMalagasy\nMalay\nMalayalam\nMaltese\nManx\nMaori\nMarathi\nModern Greek\nMongolian\nNavajo\nNepali (macrolanguage)\nNorthern Sami\nNorwegian Nynorsk\nOccitan\nOriya\nOromo\nOssetian\nPanjabi\nPersian\nPolish\nPortuguese\nPushto\nQuechua\nRomanian\nRomansh\nRussian\nSanskrit\nSardinian\nScottish Gaelic\nSerbian\nSerbo-Croatian\nShona\nSindhi\nSinhala\nSlovak\nSlovene\nSomali\nSpanish\nStandard Chinese\nSundanese\nSwahili (macrolanguage)\nSwedish\nTagalog\nTajik\nTamil\nTatar\nTelugu\nThai\nTibetan\nTongan\nTswana\nTurkish\nTurkmen\nUighur\nUkrainian\nUrdu\nUzbek\nVietnamese\nVolap\u00fck\nWalloon\nWelsh\nWestern Frisian\nWolof\nXhosa\nYiddish\nYoruba\n\nReferences\n[1] A. Joulin, E. Grave, P. Bojanowski, T. Mikolov, [Bag of Tricks for Efficient Text Classification](https://arxiv.org/abs/1607.01759)\n\n[2] A. Joulin, E. Grave, P. Bojanowski, M. Douze, H. J\u00e9gou, T. Mikolov, [FastText.zip: Compressing text classification models](https://arxiv.org/abs/1612.03651)"}, {"name": "fasttext-langdetect", "tags": ["data", "math", "ml"], "summary": "80x faster and 95% accurate language identification with Fasttext", "text": "This library is used to perform fast and accurate language identification with a model trained on Fasttext, achieving 95% accuracy in just 80x the time of other popular libraries. Developers can utilize this library to enable their applications to identify languages efficiently, making it suitable for tasks such as text analysis, content moderation, and internationalization."}, {"name": "fasttext-wheel", "tags": ["data", "math", "web"], "summary": "fasttext Python bindings", "text": "fastText |CircleCI|\n===================\n\n`fastText `__ is a library for efficient learning\nof word representations and sentence classification.\n\nIn this document we present how to use fastText in python.\n\nTable of contents\n-----------------\n\n-  `Requirements `__\n-  `Installation `__\n-  `Usage overview `__\n-  `Word representation model `__\n-  `Text classification model `__\n-  `IMPORTANT: Preprocessing data / encoding\n   conventions `__\n-  `More examples `__\n-  `API `__\n-  `train_unsupervised parameters `__\n-  `train_supervised parameters `__\n-  `model object `__\n\nRequirements\n============\n\n`fastText `__ builds on modern Mac OS and Linux\ndistributions. Since it uses C++11 features, it requires a compiler with\ngood C++11 support. You will need `Python `__\n(version 2.7 or \u2265 3.4), `NumPy `__ &\n`SciPy `__ and\n`pybind11 `__.\n\nInstallation\n============\n\nTo install the latest release, you can do :\n\n.. code:: bash\n\nor, to get the latest development version of fasttext, you can install\nfrom our github repository :\n\n.. code:: bash\n\nUsage overview\n==============\n\nWord representation model\n-------------------------\n\nIn order to learn word vectors, as `described\nhere `__,\nwe can use ``fasttext.train_unsupervised`` function like this:\n\n.. code:: py\n\nwhere ``data.txt`` is a training file containing utf-8 encoded text.\n\nThe returned ``model`` object represents your learned model, and you can\nuse it to retrieve information.\n\n.. code:: py\n\nSaving and loading a model object\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nYou can save your trained model object by calling the function\n``save_model``.\n\n.. code:: py\n\nand retrieve it later thanks to the function ``load_model`` :\n\n.. code:: py\n\nFor more information about word representation usage of fasttext, you\ncan refer to our `word representations\ntutorial `__.\n\nText classification model\n-------------------------\n\nIn order to train a text classifier using the method `described\nhere `__,\nwe can use ``fasttext.train_supervised`` function like this:\n\n.. code:: py\n\nwhere ``data.train.txt`` is a text file containing a training sentence\nper line along with the labels. By default, we assume that labels are\nwords that are prefixed by the string ``__label__``\n\nOnce the model is trained, we can retrieve the list of words and labels:\n\n.. code:: py\n\nTo evaluate our model by computing the precision at 1 (P@1) and the\nrecall on a test set, we use the ``test`` function:\n\n.. code:: py\n\nWe can also predict labels for a specific text :\n\n.. code:: py\n\nBy default, ``predict`` returns only one label : the one with the\nhighest probability. You can also predict more than one label by\nspecifying the parameter ``k``:\n\n.. code:: py\n\nIf you want to predict more than one sentence you can pass an array of\nstrings :\n\n.. code:: py\n\nOf course, you can also save and load a model to/from a file as `in the\nword representation usage `__.\n\nFor more information about text classification usage of fasttext, you\ncan refer to our `text classification\ntutorial `__.\n\nCompress model files with quantization\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~"}, {"name": "fasttext-wheel", "tags": ["data", "math", "web"], "summary": "fasttext Python bindings", "text": "When you want to save a supervised model file, fastText can compress it\nin order to have a much smaller model file by sacrificing only a little\nbit performance.\n\n.. code:: py\n\n``model_filename.ftz`` will have a much smaller size than\n``model_filename.bin``.\n\nFor further reading on quantization, you can refer to `this paragraph\nfrom our blog\npost `__.\n\nIMPORTANT: Preprocessing data / encoding conventions\n----------------------------------------------------\n\nIn general it is important to properly preprocess your data. In\nparticular our example scripts in the `root\nfolder `__ do this.\n\nfastText assumes UTF-8 encoded text. All text must be `unicode for\nPython2 `__\nand `str for\nPython3 `__.\nThe passed text will be `encoded as UTF-8 by\npybind11 `__\nbefore passed to the fastText C++ library. This means it is important to\nuse UTF-8 encoded text when building a model. On Unix-like systems you\ncan convert text using `iconv `__.\n\nfastText will tokenize (split text into pieces) based on the following\nASCII characters (bytes). In particular, it is not aware of UTF-8\nwhitespace. We advice the user to convert UTF-8 whitespace / word\nboundaries into one of the following symbols as appropiate.\n\n-  space\n-  tab\n-  vertical tab\n-  carriage return\n-  formfeed\n-  the null character\n\nThe newline character is used to delimit lines of text. In particular,\nthe EOS token is appended to a line of text if a newline character is\nencountered. The only exception is if the number of tokens exceeds the\nMAX\\_LINE\\_SIZE constant as defined in the `Dictionary\nheader `__.\nThis means if you have text that is not separate by newlines, such as\nthe `fil9 dataset `__, it will be\nbroken into chunks with MAX\\_LINE\\_SIZE of tokens and the EOS token is\nnot appended.\n\nThe length of a token is the number of UTF-8 characters by considering\nthe `leading two bits of a\nbyte `__ to identify\n`subsequent bytes of a multi-byte\nsequence `__.\nKnowing this is especially important when choosing the minimum and\nmaximum length of subwords. Further, the EOS token (as specified in the\n`Dictionary\nheader `__)\nis considered a character and will not be broken into subwords.\n\nMore examples\n-------------\n\nIn order to have a better knowledge of fastText models, please consider\nthe main\n`README `__\nand in particular `the tutorials on our\nwebsite `__.\n\nYou can find further python examples in `the doc\nfolder `__.\n\nAs with any package you can get help on any Python function using the\nhelp function.\n\nFor example\n\n::\n\nAPI\n===\n\n``train_unsupervised`` parameters\n---------------------------------\n\n.. code:: python\n\n``train_supervised`` parameters\n-------------------------------\n\n.. code:: python\n\n``model`` object\n----------------\n\n``train_supervised``, ``train_unsupervised`` and ``load_model``\nfunctions return an instance of ``_FastText`` class, that we generaly\nname ``model`` object.\n\nThis object exposes those training arguments as properties : ``lr``,\n``dim``, ``ws``, ``epoch``, ``minCount``, ``minCountLabel``, ``minn``,\n``maxn``, ``neg``, ``wordNgrams``, ``loss``, ``bucket``, ``thread``,\n``lrUpdateRate``, ``t``, ``label``, ``verbose``, ``pretrainedVectors``.\nSo ``model.wordNgrams`` will give you the max length of word ngram used\nfor training this model.\n\nIn addition, the object exposes several functions :\n\n.. code:: python"}, {"name": "fasttext-wheel", "tags": ["data", "math", "web"], "summary": "fasttext Python bindings", "text": "The properties ``words``, ``labels`` return the words and labels from\nthe dictionary :\n\n.. code:: py\n\nThe object overrides ``__getitem__`` and ``__contains__`` functions in\norder to return the representation of a word and to check if a word is\nin the vocabulary.\n\n.. code:: py\n\nJoin the fastText community\n---------------------------\n\n-  `Facebook page `__\n-  `Stack\n   overflow `__\n-  `Google\n   group `__\n-  `GitHub `__\n\n.. |CircleCI| image:: https://circleci.com/gh/facebookresearch/fastText/tree/master.svg?style=svg\n   :target: https://circleci.com/gh/facebookresearch/fastText/tree/master"}, {"name": "fasttext-wheel", "tags": ["data", "math", "web"], "summary": "fasttext Python bindings", "text": "This library is used to efficiently learn word representations and sentence classification models in Python using the fastText library. Developers can use this library to create advanced natural language processing capabilities in their applications."}, {"name": "fasttext", "tags": ["data", "math", "web"], "summary": "fasttext Python bindings", "text": "fastText |CircleCI|\n===================\n\n`fastText `__ is a library for efficient learning\nof word representations and sentence classification.\n\nIn this document we present how to use fastText in python.\n\nTable of contents\n-----------------\n\n-  `Requirements `__\n-  `Installation `__\n-  `Usage overview `__\n-  `Word representation model `__\n-  `Text classification model `__\n-  `IMPORTANT: Preprocessing data / encoding\n   conventions `__\n-  `More examples `__\n-  `API `__\n-  `train_unsupervised parameters `__\n-  `train_supervised parameters `__\n-  `model object `__\n\nRequirements\n============\n\n`fastText `__ builds on modern Mac OS and Linux\ndistributions. Since it uses C++11 features, it requires a compiler with\ngood C++11 support. You will need `Python `__\n(version 2.7 or \u2265 3.4), `NumPy `__ &\n`SciPy `__ and\n`pybind11 `__.\n\nInstallation\n============\n\nTo install the latest release, you can do :\n\n.. code:: bash\n\nor, to get the latest development version of fasttext, you can install\nfrom our github repository :\n\n.. code:: bash\n\nUsage overview\n==============\n\nWord representation model\n-------------------------\n\nIn order to learn word vectors, as `described\nhere `__,\nwe can use ``fasttext.train_unsupervised`` function like this:\n\n.. code:: py\n\nwhere ``data.txt`` is a training file containing utf-8 encoded text.\n\nThe returned ``model`` object represents your learned model, and you can\nuse it to retrieve information.\n\n.. code:: py\n\nSaving and loading a model object\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nYou can save your trained model object by calling the function\n``save_model``.\n\n.. code:: py\n\nand retrieve it later thanks to the function ``load_model`` :\n\n.. code:: py\n\nFor more information about word representation usage of fasttext, you\ncan refer to our `word representations\ntutorial `__.\n\nText classification model\n-------------------------\n\nIn order to train a text classifier using the method `described\nhere `__,\nwe can use ``fasttext.train_supervised`` function like this:\n\n.. code:: py\n\nwhere ``data.train.txt`` is a text file containing a training sentence\nper line along with the labels. By default, we assume that labels are\nwords that are prefixed by the string ``__label__``\n\nOnce the model is trained, we can retrieve the list of words and labels:\n\n.. code:: py\n\nTo evaluate our model by computing the precision at 1 (P@1) and the\nrecall on a test set, we use the ``test`` function:\n\n.. code:: py\n\nWe can also predict labels for a specific text :\n\n.. code:: py\n\nBy default, ``predict`` returns only one label : the one with the\nhighest probability. You can also predict more than one label by\nspecifying the parameter ``k``:\n\n.. code:: py\n\nIf you want to predict more than one sentence you can pass an array of\nstrings :\n\n.. code:: py\n\nOf course, you can also save and load a model to/from a file as `in the\nword representation usage `__.\n\nFor more information about text classification usage of fasttext, you\ncan refer to our `text classification\ntutorial `__.\n\nCompress model files with quantization\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~"}, {"name": "fasttext", "tags": ["data", "math", "web"], "summary": "fasttext Python bindings", "text": "When you want to save a supervised model file, fastText can compress it\nin order to have a much smaller model file by sacrificing only a little\nbit performance.\n\n.. code:: py\n\n``model_filename.ftz`` will have a much smaller size than\n``model_filename.bin``.\n\nFor further reading on quantization, you can refer to `this paragraph\nfrom our blog\npost `__.\n\nIMPORTANT: Preprocessing data / encoding conventions\n----------------------------------------------------\n\nIn general it is important to properly preprocess your data. In\nparticular our example scripts in the `root\nfolder `__ do this.\n\nfastText assumes UTF-8 encoded text. All text must be `unicode for\nPython2 `__\nand `str for\nPython3 `__.\nThe passed text will be `encoded as UTF-8 by\npybind11 `__\nbefore passed to the fastText C++ library. This means it is important to\nuse UTF-8 encoded text when building a model. On Unix-like systems you\ncan convert text using `iconv `__.\n\nfastText will tokenize (split text into pieces) based on the following\nASCII characters (bytes). In particular, it is not aware of UTF-8\nwhitespace. We advice the user to convert UTF-8 whitespace / word\nboundaries into one of the following symbols as appropiate.\n\n-  space\n-  tab\n-  vertical tab\n-  carriage return\n-  formfeed\n-  the null character\n\nThe newline character is used to delimit lines of text. In particular,\nthe EOS token is appended to a line of text if a newline character is\nencountered. The only exception is if the number of tokens exceeds the\nMAX\\_LINE\\_SIZE constant as defined in the `Dictionary\nheader `__.\nThis means if you have text that is not separate by newlines, such as\nthe `fil9 dataset `__, it will be\nbroken into chunks with MAX\\_LINE\\_SIZE of tokens and the EOS token is\nnot appended.\n\nThe length of a token is the number of UTF-8 characters by considering\nthe `leading two bits of a\nbyte `__ to identify\n`subsequent bytes of a multi-byte\nsequence `__.\nKnowing this is especially important when choosing the minimum and\nmaximum length of subwords. Further, the EOS token (as specified in the\n`Dictionary\nheader `__)\nis considered a character and will not be broken into subwords.\n\nMore examples\n-------------\n\nIn order to have a better knowledge of fastText models, please consider\nthe main\n`README `__\nand in particular `the tutorials on our\nwebsite `__.\n\nYou can find further python examples in `the doc\nfolder `__.\n\nAs with any package you can get help on any Python function using the\nhelp function.\n\nFor example\n\n::\n\nAPI\n===\n\n``train_unsupervised`` parameters\n---------------------------------\n\n.. code:: python\n\n``train_supervised`` parameters\n-------------------------------\n\n.. code:: python\n\n``model`` object\n----------------\n\n``train_supervised``, ``train_unsupervised`` and ``load_model``\nfunctions return an instance of ``_FastText`` class, that we generaly\nname ``model`` object.\n\nThis object exposes those training arguments as properties : ``lr``,\n``dim``, ``ws``, ``epoch``, ``minCount``, ``minCountLabel``, ``minn``,\n``maxn``, ``neg``, ``wordNgrams``, ``loss``, ``bucket``, ``thread``,\n``lrUpdateRate``, ``t``, ``label``, ``verbose``, ``pretrainedVectors``.\nSo ``model.wordNgrams`` will give you the max length of word ngram used\nfor training this model.\n\nIn addition, the object exposes several functions :\n\n.. code:: python"}, {"name": "fasttext", "tags": ["data", "math", "web"], "summary": "fasttext Python bindings", "text": "The properties ``words``, ``labels`` return the words and labels from\nthe dictionary :\n\n.. code:: py\n\nThe object overrides ``__getitem__`` and ``__contains__`` functions in\norder to return the representation of a word and to check if a word is\nin the vocabulary.\n\n.. code:: py\n\nJoin the fastText community\n---------------------------\n\n-  `Facebook page `__\n-  `Stack\n   overflow `__\n-  `Google\n   group `__\n-  `GitHub `__\n\n.. |CircleCI| image:: https://circleci.com/gh/facebookresearch/fastText/tree/master.svg?style=svg\n   :target: https://circleci.com/gh/facebookresearch/fastText/tree/master"}, {"name": "fasttext", "tags": ["data", "math", "web"], "summary": "fasttext Python bindings", "text": "This library is used to efficiently learn word representations and classify text using machine learning algorithms. It provides a Python interface for training models on word representation and text classification tasks."}, {"name": "fiddle", "tags": ["math", "ml"], "summary": "Fiddle: A Python-first configuration library", "text": "Fiddle\n\nFiddle is a Python-first configuration library particularly well suited to ML\napplications. Fiddle enables deep configurability of parameters in a program,\nwhile allowing configuration to be expressed in readable and maintainable\nPython code.\n\n**Authors**: Dan Holtmann-Rice, Brennan Saeta, Sergio Guadarrama"}, {"name": "fiddle", "tags": ["math", "ml"], "summary": "Fiddle: A Python-first configuration library", "text": "This library is used to enable deep configurability of parameters in ML applications through readable and maintainable Python code. With Fiddle, developers can express complex configurations in a programmatic way that simplifies maintenance and modification of model parameters."}, {"name": "filterpy", "tags": ["dev", "math", "web"], "summary": "Kalman filtering and optimal estimation library", "text": "FilterPy - Kalman filters and other optimal and non-optimal estimation filters in Python.\n-----------------------------------------------------------------------------------------\n\n.. image:: https://img.shields.io/pypi/v/filterpy.svg\n\n**NOTE**: Imminent drop of support of Python 2.7, 3.4. See section below for details.\n\nThis library provides Kalman filtering and various related optimal and\nnon-optimal filtering software written in Python. It contains Kalman\nfilters, Extended Kalman filters, Unscented Kalman filters, Kalman\nsmoothers, Least Squares filters, fading memory filters, g-h filters,\ndiscrete Bayes, and more.\n\nThis is code I am developing in conjunction with my book Kalman and\nBayesian Filter in Python, which you can read/download at\n\nMy aim is largely pedalogical - I opt for clear code that matches the\nequations in the relevant texts on a 1-to-1 basis, even when that has a\nperformance cost. There are places where this tradeoff is unclear - for\nexample, I find it somewhat clearer to write a small set of equations\nusing linear algebra, but numpy's overhead on small matrices makes it\nrun slower than writing each equation out by hand. Furthermore, books\nsuch Zarchan present the written out form, not the linear algebra form.\nIt is hard for me to choose which presentation is 'clearer' - it depends\non the audience. In that case I usually opt for the faster implementation.\n\nI use NumPy and SciPy for all of the computations. I have experimented\nwith Numba and it yields impressive speed ups with minimal costs, but I \nam not convinced that I want to add that requirement to my project. It \nis still on my list of things to figure out, however.\n\nSphinx generated documentation lives at http://filterpy.readthedocs.org/.\nGeneration is triggered by git when I do a check in, so this will always\nbe bleeding edge development version - it will often be ahead of the\nreleased version.\n\nPlan for dropping Python 2.7 support\n------------------------------------\n\nI haven't finalized my decision on this, but NumPy is dropping\nPython 2.7 support in December 2018. I will certainly drop Python\n2.7 support by then; I will probably do it much sooner.\n\nAt the moment FilterPy is on version 1.x. I plan to fork the project\nto version 2.0, and support only Python 3.5+. The 1.x version \nwill still be available, but I will not support it. If I add something\namazing to 2.0 and someone really begs, I might backport it; more\nlikely I would accept a pull request with the feature backported\nto 1.x. But to be honest I don't forsee this happening."}, {"name": "filterpy", "tags": ["dev", "math", "web"], "summary": "Kalman filtering and optimal estimation library", "text": "Why 3.5+, and not 3.4+? 3.5 introduced the matrix multiply symbol,\nand I want my code to take advantage of it. Plus, to be honest,\nI'm being selfish. I don't want to spend my life supporting this\npackage, and moving as far into the present as possible means\na few extra years before the Python version I choose becomes\nhopelessly dated and a liability. I recognize this makes people\nrunning the default Python in their linux distribution more\npainful. All I can say is I did not decide to do the Python\n3 fork, and I don't have the time to support the bifurcation\nany longer.\n\nI am making edits to the package now in support of my book;\nonce those are done I'll probably create the 2.0 branch. \nI'm contemplating a SLAM addition to the book, and am not\nsure if I will do this in 3.5+ only or not.\n\nInstallation\n------------\n\nThe most general installation is just to use pip, which should come with\nany modern Python distribution.\n\n.. image:: https://img.shields.io/pypi/v/filterpy.svg\n::\n\nIf you prefer to download the source yourself\n\n::\n\nIf you use Anaconda, you can install from the conda-forge channel. You\nwill need to add the conda-forge channel if you haven't already done so:\n\n::\nand then install with:\n\n::\nAnd, if you want to install from the bleeding edge git version\n\n::\n\nNote: I make no guarantees that everything works if you install from here.\nI'm the only developer, and so I don't worry about dev/release branches and\nthe like. Unless I fix a bug for you and tell you to get this version because\nI haven't made a new release yet, I strongly advise not installing from git.\n\nBasic use\n---------\n\nFull documentation is at\n\nFirst, import the filters and helper functions.\n\n.. code-block:: python\n\nNow, create the filter\n\n.. code-block:: python\n\nInitialize the filter's matrices.\n\n.. code-block:: python\n\nFinally, run the filter.\n\n.. code-block:: python\n\nSorry, that is the extent of the documentation here. However, the library\nis broken up into subdirectories: gh, kalman, memory, leastsq, and so on.\nEach subdirectory contains python files relating to that form of filter.\nThe functions and methods contain pretty good docstrings on use.\n\nMy book https://github.com/rlabbe/Kalman-and-Bayesian-Filters-in-Python/\nuses this library, and is the place to go if you are trying to learn\nabout Kalman filtering and/or this library. These two are not exactly in \nsync - my normal development cycle is to add files here, test them, figure \nout how to present them pedalogically, then write the appropriate section\nor chapter in the book. So there is code here that is not discussed\nyet in the book.\n\nRequirements\n------------\n\nThis library uses NumPy, SciPy, Matplotlib, and Python.\n\nI haven't extensively tested backwards compatibility - I use the\nAnaconda distribution, and so I am on Python 3.6 and 2.7.14, along with\nwhatever version of NumPy, SciPy, and matplotlib they provide. But I am\nusing pretty basic Python - numpy.array, maybe a list comprehension in\nmy tests."}, {"name": "filterpy", "tags": ["dev", "math", "web"], "summary": "Kalman filtering and optimal estimation library", "text": "I import from **__future__** to ensure the code works in Python 2 and 3.\n\nTesting\n-------\n\nAll tests are written to work with py.test. Just type ``py.test`` at the\ncommand line.\n\nAs explained above, the tests are not robust. I'm still at the stage\nwhere visual plots are the best way to see how things are working.\nApologies, but I think it is a sound choice for development. It is easy\nfor a filter to perform within theoretical limits (which we can write a\nnon-visual test for) yet be 'off' in some way. The code itself contains\ntests in the form of asserts and properties that ensure that arrays are\nof the proper dimension, etc.\n\nReferences\n----------\n\nI use three main texts as my refererence, though I do own the majority\nof the Kalman filtering literature. First is Paul Zarchan's\n'Fundamentals of Kalman Filtering: A Practical Approach'. I think it by\nfar the best Kalman filtering book out there if you are interested in\npractical applications more than writing a thesis. The second book I use\nis Eli Brookner's 'Tracking and Kalman Filtering Made Easy'. This is an\nastonishingly good book; its first chapter is actually readable by the\nlayperson! Brookner starts from the g-h filter, and shows how all other\nfilters - the Kalman filter, least squares, fading memory, etc., all\nderive from the g-h filter. It greatly simplifies many aspects of\nanalysis and/or intuitive understanding of your problem. In contrast,\nZarchan starts from least squares, and then moves on to Kalman\nfiltering. I find that he downplays the predict-update aspect of the\nalgorithms, but he has a wealth of worked examples and comparisons\nbetween different methods. I think both viewpoints are needed, and so I\ncan't imagine discarding one book. Brookner also focuses on issues that\nare ignored in other books - track initialization, detecting and\ndiscarding noise, tracking multiple objects, an so on.\n\nI said three books. I also like and use Bar-Shalom's Estimation with\nApplications to Tracking and Navigation. Much more mathematical than the\nprevious two books, I would not recommend it as a first text unless you\nalready have a background in control theory or optimal estimation. Once\nyou have that experience, this book is a gem. Every sentence is crystal\nclear, his language is precise, but each abstract mathematical statement\nis followed with something like \"and this means...\".\n\nLicense\n-------\n\nThe MIT License (MIT)\n\nCopyright (c) 2015 Roger R. Labbe Jr\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in\nall copies or substantial portions of the Software."}, {"name": "filterpy", "tags": ["dev", "math", "web"], "summary": "Kalman filtering and optimal estimation library", "text": "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\nTHE SOFTWARE.TION OF CONTRACT,\nTORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE\nSOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE."}, {"name": "filterpy", "tags": ["dev", "math", "web"], "summary": "Kalman filtering and optimal estimation library", "text": "This library is used to implement various types of optimal and non-optimal estimation filters, including Kalman filters, Extended Kalman filters, and Unscented Kalman filters. Developers can use this library to create robust filtering algorithms in Python for tasks such as state estimation and prediction."}, {"name": "fiona", "tags": ["cli", "data", "dev", "math", "ui", "web"], "summary": "Fiona reads and writes spatial data files", "text": "=====\nFiona\n=====\n\n:target: https://github.com/Toblerity/Fiona/actions/workflows/tests.yml\n\n:target: https://github.com/Toblerity/Fiona/actions/workflows/test_gdal_latest.yml\n.. image:: https://img.shields.io/pypi/v/fiona\n   :target: https://pypi.org/project/fiona/\n\n:target: https://securityscorecards.dev/viewer/?uri=github.com/Toblerity/Fiona\n\nFiona streams simple feature data to and from GIS formats like GeoPackage and\nShapefile.\n\nFiona can read and write real-world data using multi-layered GIS formats,\nzipped and in-memory virtual file systems, from files on your hard drive or in\ncloud storage. This project includes Python modules and a command line\ninterface (CLI).\n\nFiona depends on `GDAL `__ but is different from GDAL's own\n`bindings `__. Fiona is designed to\nbe highly productive and to make it easy to write code which is easy to read.\n\nInstallation\n============\n\nFiona has several `extension modules\n`__ which link against\nlibgdal. This complicates installation. Binary distributions (wheels)\ncontaining libgdal and its own dependencies are available from the Python\nPackage Index and can be installed using pip.\n\n.. code-block:: console\n\nThese wheels are mainly intended to make installation easy for simple\napplications, not so much for production. They are not tested for compatibility\nwith all other binary wheels, conda packages, or QGIS, and omit many of GDAL's\noptional format drivers. If you need, for example, GML support you will need to\nbuild and install Fiona from a source distribution. It is possible to install\nFiona from source using pip (version >= 22.3) and the `--no-binary` option. A\nspecific GDAL installation can be selected by setting the GDAL_CONFIG\nenvironment variable.\n\n.. code-block:: console\n\nMany users find Anaconda and conda-forge a good way to install Fiona and get\naccess to more optional format drivers (like GML).\n\nFiona 1.10 requires Python 3.8 or higher and GDAL 3.4 or higher.\n\nPython Usage\n============\n\nFeatures are read from and written to file-like ``Collection`` objects returned\nfrom the ``fiona.open()`` function. Features are data classes modeled on the\nGeoJSON format. They don't have any spatial methods of their own, so if you\nwant to transform them you will need Shapely or something like it. Here is an\nexample of using Fiona to read some features from one data file, change their\ngeometry attributes using Shapely, and write them to a new data file.\n\n.. code-block:: python\n\nCLI Usage\n=========\n\nFiona's command line interface, named \"fio\", is documented at `docs/cli.rst\n`__. The CLI has a\nnumber of different commands. Its ``fio cat`` command streams GeoJSON features\nfrom any dataset.\n\n.. code-block:: console\n\nDocumentation\n=============\n\nFor more details about this project, please see:\n\n* Fiona `home page `__\n* `Docs and manual `__\n* `Examples `__\n* Main `user discussion group `__\n* `Developers discussion group `__\n\nChanges\n=======\n\nAll issue numbers are relative to https://github.com/Toblerity/Fiona/issues.\n\n1.10.1 (2024-09-16)\n-------------------\n\nBug fixes:\n\n- Logging in the CRS class no longer tries to print representations of objects\n  that may be NULL when searching for authority matches (#1445).\n\n1.10.0 (2024-09-03)\n-------------------\n\nThe package version, credits, and citation file have been updated. There have\nbeen no other changes since 1.10.0rc1. Fiona is the work of 73 contributors,\nincluding 25 new contributors since 1.9.0.\n\n1.10.0rc1 (2024-08-21)\n----------------------\n\nThis is the first release candidate for 1.10.0."}, {"name": "fiona", "tags": ["cli", "data", "dev", "math", "ui", "web"], "summary": "Fiona reads and writes spatial data files", "text": "Changes:\n\n- Mutable item access to Feature, Geometry, and Properties instances has been\n  restored (reported in #1430). This usage should be avoided as instances of\n  these classes will be immutable in a future version.\n- The setup.cfg duplicates project configuration in pyproject.toml and has been\n  removed.\n\n1.10b3 (2024-07-29)\n-------------------\n\nBug fixes:\n\n- The sketchy, semi-private Python opener interfaces of version 1.10b2 have\n  been replaced by ABCs that are exported from fiona.abc (#1415).\n- The truncate VSI plugin callback has been implemented (#1413).\n\n1.10b2 (2024-07-10)\n-------------------\n\nBug fixes:\n\n- The Pyopener registry and VSI plugin have been rewritten to avoid filename\n  conflicts and to be compatible with multithreading. Now, a new plugin handler\n  is registered for each instance of using an opener (#1408). Before GDAL 3.9.0\n  plugin handlers cannot not be removed and so it may be observed that the size\n  of the Pyopener registry grows during the execution of a program.\n- A CSLConstList ctypedef has been added and is used where appropriate (#1404).\n- Fiona model objects have a informative, printable representation again\n  (#1380).\n\nPackaging:\n\n- PyPI wheels include GDAL 3.9.1 and curl 8.8.0.\n\n1.10b1 (2024-04-16)\n-------------------\n\nBug fixes:\n\n- Fiona can again set fields with values that are instances of classes derived\n  from date, time, and datetime (#1377).  This was broken by changes in 1.10a2.\n\n1.10a2 (2024-04-05)\n-------------------\n\nDeprecations:\n\n- The FIELD_TYPES, FIELD_TYPES_MAP, and FIELD_TYPES_MAP_REV attributes of\n  fiona.schema are no longer used by the project and will be removed in version\n  2.0 (#1366).\n- The Python style of rio-filter expressions introduced in version 1.0 are\n  deprecated. Only the parenthesized list type of expression will be supported\n  by version 2.0.\n\nNew features:\n\n- All supported Fiona field types are now represented by classes in\n  fiona.schema. These classes are mapped in FIELD_TYPES_MAP2 and\n  FIELD_TYPES_MAP2_REV to OGR field type and field subtype pairs (#1366).\n- The filter, map, and reduce CLI commands from the public domain version 1.1.0\n  of fio-planet have been incorporated into Fiona's core set of commands\n  (#1362).  These commands are only available if pyparsing and shapely (each of\n  these are declared in the \"calc\" set of extra requirements) are installed.\n\nBug fixes:\n\n- Fiona's python opener VSI plugin prefix has been changed to \"vsifiopener\" to\n  not conflict with Rasterio (#1368).\n- Add a 16-bit integer type \"int16\" based on OGR's OSFTInt16 integer sub-type\n  (#1358).\n- Allow a GeoJSON collection's layer name to be set on opening in write mode\n  (#1352).\n- The legacy crs.py module which was shadowed by the new crs.pyx module has\n  been deleted (#1344).\n- Python 3.8 has been added back to the list of supported versions and\n  a dependency on Numpy added in 1.10a1 has been removed.\n- An implementation of the VSI flush callback has been added to _vsiopener.pyx.\n- Openers are now registered only by urlpath. The mode is no longer considered\n  as OGR drivers may use a mix of modes when creating a new dataset.\n\nOther changes:"}, {"name": "fiona", "tags": ["cli", "data", "dev", "math", "ui", "web"], "summary": "Fiona reads and writes spatial data files", "text": "- Feature builder and field getter/setter instances are reused when reading and\n  writing features (#1366).\n\n1.10a1 (2024-03-01)\n-------------------\n\nPython version:\n\nFiona 1.10 will require Python version 3.9 or higher.\n\nDeprecations:\n\nThe fiona.path module will be removed in version 2.0 and a deprecation warning\nis issued when the module is imported (#1334). Additionally, members of that\nmodule are no longer exported from the top level module.\n\nNew features:\n\nPython openers can now support discovery of auxiliary \"sidecar\" files like\n.aux.xml, .msk, and .tfw files for GeoTIFFs (#1331). Additionally, filesystem\nobjects, such as those from fsspec, can be used as openers. This will become\nthe recommended usage, supplanting the use of single file openers.\n\nBug fixes:\n\n- Use of pkg_resources in test_rio_info.py has been eliminated.\n- gzip, tar, and zip archive URIs containing drive letters were not always\n  parsed properly on Windows, but are now (#1334).\n\n1.9.6 (2024-03-07)\n------------------\n\n- Ensure that geometry types in a schema are translated to a linear type, as\n  geometry instances are (#1313).\n- Fix broken stable API documentation on Read The Docs (#).\n- Remove install requirement of setuptools, a regression introduced in 1.9.5.\n\n1.9.5 (2023-10-11)\n------------------\n\nBug fixes:\n\n- Expand keys in schema mismatch exception, resolving #1278.\n- Preserve the null properties and geometry of a Feature when serializing\n  (#1276).\n\nPackaging:\n\n- The distribution name is now officially \"fiona\", not \"Fiona\". The import\n  name remains \"fiona\".\n- Builds now require Cython >= 3.0.2 (#1276).\n- PyPI wheels include GDAL 3.6.4, PROJ 9.0.1, and GEOS 3.11.2.\n- PyPI wheels include curl 8.4.0, addressing CVE-2023-38545 and CVE-38546.\n- PyPI wheels are now available for Python 3.12.\n\n1.9.4.post1 (2023-05-23)\n------------------------\n\nExtraneous files were unintentionally packaged in the 1.9.4 wheels. This post1\nrelease excludes them so that wheel contents are as in version 1.9.3.\n\n1.9.4 (2023-05-16)\n------------------\n\n- The performance of Feature.from_dict() has been improved (#1267).\n- Several sources of meaningless log messages from fiona._geometry about NULL\n  geometries are avoided (#1264).\n- The Parquet driver has been added to the list of supported drivers and will\n  be available if your system's GDAL library links libarrow. Note that fiona\n  wheels on PyPI do not include libarrow as it is rather large.\n- Ensure that fiona._vendor modules are found and included.\n- Bytes type feature properties are now hex encoded when serializing to GeoJSON\n  (#1263).\n- Docstrings for listdir and listlayers have been clarified and harmonized.\n- Nose style test cases have been converted to unittest.TestCase (#1256).\n- The munch package used by fio-filter and fio-calc is now vendored and patched\n  to remove usage of the deprecated pkg_resources module (#1255).\n\n1.9.3 (2023-04-10)\n------------------\n\n- Rasterio CRS objects are compatible with the Collection constructor and are\n  now accepted (#1248).\n- Enable append mode for fio-load (#1237).\n- Reading a GeoJSON with an empty array property can result in a segmentation\n  fault since version 1.9.0. This has been fixed (#1228).\n\n1.9.2 (2023-03-20)\n------------------"}, {"name": "fiona", "tags": ["cli", "data", "dev", "math", "ui", "web"], "summary": "Fiona reads and writes spatial data files", "text": "- Get command entry points using importlib.metadata (#1220).\n- Instead of warning, transform_geom() raises an exception when some points\n  can't be reprojected unless the caller opts in to partial reprojection. This\n  restores the behavior of version 1.8.22.\n- Add support for open options to all CLI commands that call fiona.open\n  (#1215).\n- Fix a memory leak that can occur when iterating over a dataset using strides\n  (#1205).\n- ZipMemoryFile now supports zipped GDB data (#1203).\n\n1.9.1 (2023-02-09)\n------------------\n\n- Log a warning message when identically named fields are encountered (#1201).\n- Avoid dependence on listdir order in tests (#1193).\n- Prevent empty geometries arrays from appearing in __geo_interface__ (#1197).\n- setuptools added to pyproject.toml. Its pkg_resources module is used by the\n  CLI (#1191).\n\n1.9.0 (2023-01-30)\n------------------\n\n- CITATION.txt has been replaced by a new CITATION.cff file and the credits\n  have been updated.\n- In setup.py the distutils (deprecated) logger is no longer used.\n\n1.9b2 (2023-01-22)\n------------------\n\n- Add Feature.__geo_interface__ property (#1181).\n- Invalid creation options are filtered and ignored (#1180).\n- The readme doc has been shortened and freshened up, with a modern example for\n  version 1.9.0 (#1174).\n- The Geometry class now provides and looks for __geo_interface__ (#1174).\n- The top level fiona module now exports Feature, Geometry, and Properties\n  (#1174).\n- Functions that take Feature or Geometry objects will continue to take dicts\n  or objects that provide __geo_interface__ (#1177). This reverses the\n  deprecation introduced in 1.9a2.\n- Python ignores SIGPIPE by default. By never catching BrokenPipeError via\n  `except Exception` when, for example, piping the output of rio-shapes to\n  the Unix head program, we avoid getting an unhandled BrokenPipeError message\n  when the interpreter shuts down (#2689).\n\n1.9b1 (2022-12-13)\n------------------\n\nNew features:\n\n* Add listdir and listlayers method to io.MemoryFile (resolving #754).\n* Add support for TIN and triangle geometries (#1163).\n* Add an allow_unsupported_drivers option to fiona.open() (#1126).\n* Added support for the OGR StringList field type (#1141).\n\nChanges and bug fixes:\n\n* Missing and unused imports have been added or removed.\n* Make sure that errors aren't lost when a collection can't be saved properly\n  (#1169).\n* Ensure that ZipMemoryFile have the proper GDAL name after creation so that we\n  can use listdir() (#1092).\n* The fiona._loading module, which supports DLL loading on Windows,\n  has been moved into __init__.py and is no longer used anywhere else (#1168).\n* Move project metadata to pyproject.toml (#1165).\n* Update drvsupport.py to reflect new format capabilities in GDAL 3.6.0\n  (#1122).\n* Remove debug logging from env and _env modules.\n\n1.9a3 (2022-10-17)\n------------------\n\nPackaging:\n\n* Builds now require Cython >= 0.29.29 because of\n* https://github.com/cython/cython/issues/4609 (see #1143).\n* PyPI wheels now include GDAL 3.5.2, PROJ 9.0.1, and GEOS 3.11.0.\n* PyPI wheels are now available for Python 3.11.\n\n1.9a2 (2022-06-10)\n------------------\n\nDeprecations:"}, {"name": "fiona", "tags": ["cli", "data", "dev", "math", "ui", "web"], "summary": "Fiona reads and writes spatial data files", "text": "- Fiona's API methods will accept feature and geometry dicts in 1.9.0, but this\n  usage is deprecated. Instances of Feature and Geometry will be required in\n  2.0.\n- The precision keyword argument of fiona.transform.transform_geom is\n  deprecated and will be removed in version 2.0.\n- Deprecated usage has been eliminated in the project. Fiona's tests pass when\n  run with a -Werror::DeprecationWarning filter.\n\nChanges:\n\n- Fiona's FionaDeprecationWarning now sub-classes DeprecationWarning.\n- Some test modules have been re-formatted using black.\n\nNew features:\n\n- Fiona Collections now carry a context exit stack into which we can push fiona\n  Envs and MemoryFiles (#1059).\n- Fiona has a new CRS class, like rasterio's, which is compatible with the CRS\n  dicts of previous versions (#714).\n\n1.9a1 (2022-05-19)\n------------------\n\nDeprecations:\n\n- The fiona.drivers() function has been deprecated and will be removed in\n  version 2.0. It should be replaced by fiona.Env().\n- The new fiona.meta module will be renamed to fiona.drivers in version 2.0.\n\nPackaging:\n\n- Source distributions contain no C source files and require Cython to create\n  them from .pyx files (#1096).\n\nChanges:\n\n- Shims for various versions of GDAL have been removed and are replaced by\n  Cython compilation conditions (#1093).\n- Use of CURL_CA_BUNDLE environment variable is replaced by a more specific\n  GDAL/PROJ_CURL_CA_BUNDLE (#1095).\n- Fiona's feature accessors now return instances of fiona.model.Feature instead\n  of Python dicts (#787). The Feature class is compatible with code that\n  expects GeoJSON-like dicts but also provides id, geometry, and properties\n  attributes. The last two of these are instances of fiona.model.Geometry and\n  fiona.model.Properties.\n- GDAL 3.1.0 is the minimum GDAL version.\n- Drop Python 2, and establish Python 3.7 as the minimum version (#1079).\n- Remove six and reduce footprint of fiona.compat (#985).\n\nNew features:\n\n- The appropriate format driver can be detected from filename in write mode (#948).\n- Driver metadata including dataset open and dataset and layer creations\n  options are now exposed through methods of the fiona.meta module (#950).\n- CRS WKT format support (#979).\n- Add 'where' SQL clause to set attribute filter (#961, #1097).\n\nBug fixes:\n\n- Env and Session classes have been updated for parity with rasterio and to\n  resolve a credential refresh bug (#1055).\n\n1.8.22 (2022-10-14)\n-------------------\n\nBuilds now require Cython >= 0.29.29 because of\n\n1.8.21 (2022-02-07)\n-------------------\n\nChanges:\n\n- Driver mode support tests have been made more general and less susceptible to\n  driver quirks involving feature fields and coordinate values (#1060).\n- OSError is raised on attempts to open a dataset in a Python file object in\n  \"a\" mode (see #1027).\n- Upgrade attrs, cython, etc to open up Python 3.10 support (#1049).\n\nBug fixes:\n\n- Allow FieldSkipLogFilter to handle exception messages as well as strings\n  (reported in #1035).\n- Clean up VSI files left by MemoryFileBase, resolving #1041.\n- Hard-coded \"utf-8\" collection encoding added in #423 has been removed\n  (#1057).\n\n1.8.20 (2021-05-31)\n-------------------\n\nPackaging:\n\n- Wheels include GDAL 3.3.0 and GEOS 3.9.1.\n\nBug fixes:\n\n- Allow use with click 8 and higher (#1015).\n\n1.8.19 (2021-04-07)\n-------------------\n\nPackaging:"}, {"name": "fiona", "tags": ["cli", "data", "dev", "math", "ui", "web"], "summary": "Fiona reads and writes spatial data files", "text": "- Wheels include GDAL 3.2.1 and PROJ 7.2.1.\n\nBug fixes:\n\n- In fiona/env.py the GDAL data path is now configured using set_gdal_config\n  instead by setting the GDAL_DATA environment variable (#1007).\n- Spurious iterator reset warnings have been eliminatged (#987).\n\n1.8.18 (2020-11-17)\n-------------------\n\n- The precision option of transform has been fixed for the case of\n  GeometryCollections (#971, #972).\n- Added missing --co (creation) option to fio-load (#390).\n- If the certifi package can be imported, its certificate store location will\n  be passed to GDAL during import of fiona._env unless CURL_CA_BUNDLE is\n  already set.\n- Warn when feature fields named \"\" are found (#955).\n\n1.8.17 (2020-09-09)\n-------------------\n\n- To fix issue #952 the fio-cat command no longer cuts feature geometries at\n  the anti-meridian by default. A --cut-at-antimeridian option has been added\n  to allow cutting of geometries in a geographic destination coordinate\n  reference system.\n\n1.8.16 (2020-09-04)\n-------------------\n\n- More OGR errors and warnings arising in calls to GDAL C API functions are\n  surfaced (#946).\n- A circular import introduced in some cases in 1.8.15 has been fixed (#945).\n\n1.8.15 (2020-09-03)\n-------------------\n\n- Change shim functions to not return tuples (#942) as a solution for the\n  packaging problem reported in #941.\n- Raise a Python exception when VSIFOpenL fails (#937).\n\n1.8.14 (2020-08-31)\n-------------------\n\n- When creating a new Collection in a MemoryFile with a default (random) name\n  Fiona will attempt to use a format driver-supported file extension (#934).\n  When initializing a MemoryFile with bytes of data formatted for a vector\n  driver that requires a certain file name or extension, the user should\n  continue to pass an appropriate filename and/or extension.\n- Read support for FlatGeobuf has been enabled in the drvsupport module.\n- The MemoryFile implementation has been improved so that it can support multi-part\n  S3 downloads (#906). This is largely a port of code from rasterio.\n- Axis ordering for results of fiona.transform was wrong when CRS were passed\n  in the \"EPSG:dddd\" form (#919). This has been fixed by (#926).\n- Allow implicit access to the only dataset in a ZipMemoryFile. The path\n  argument of ZipMemoryFile.open() is now optional (#928).\n- Improve support for datetime types: support milliseconds (#744), timezones (#914)\n  and improve warnings if type is not supported by driver (#572).\n- Fix \"Failed to commit transaction\" TransactionError for FileGDB driver.\n- Load GDAL DLL dependencies on Python 3.8+ / Windows with add_dll_directory() (#851).\n- Do not require optional properties (#848).\n- Ensure that slice does not overflow available data (#884).\n- Resolve issue when \"ERROR 4: Unable to open EPSG support file gcs.csv.\" is raised on\n  importing fiona (#897).\n- Resolve issue resulting in possible mixed up fields names (affecting only DXF, GPX,\n  GPSTrackMacker and DGN driver) (#916).\n- Ensure crs_wkt is passed when writing to MemoryFile (#907).\n\n1.8.13.post1 (2020-02-21)\n-------------------------\n\n- This release is being made to improve binary wheel compatibility with shapely\n  1.7.0. There have been no changes to the fiona package code since 1.8.13.\n\n1.8.13 (2019-12-05)\n-------------------"}, {"name": "fiona", "tags": ["cli", "data", "dev", "math", "ui", "web"], "summary": "Fiona reads and writes spatial data files", "text": "- The Python version specs for argparse and ordereddict in 1.8.12 were wrong\n  and have been corrected (#843).\n\n1.8.12 (2019-12-04)\n-------------------\n\n- Specify Python versions for argparse, enum34, and ordereddict requirements\n  (#842).\n\n1.8.11 (2019-11-07)\n-------------------\n\n- Fix an access violation on Windows (#826).\n\n1.8.10 (2019-11-07)\n-------------------\n\nDeprecations:\n\n- Use of vfs keyword argument with open or listlayers has been previously noted\n  as deprecated, but now triggers a deprecation warning.\n\nBug fixes:\n\n- fiona.open() can now create new datasets using CRS URNs (#823).\n- listlayers() now accepts file and Path objects, like open() (#825).\n- Use new set_proj_search_path() function to set the PROJ data search path. For\n  GDAL versions before 3.0 this sets the PROJ_LIB environment variable. For\n  GDAL version 3.0 this calls OSRSetPROJSearchPaths(), which overrides\n  PROJ_LIB.\n- Remove old and unused _drivers extension module.\n- Check for header.dxf file instead of pcs.csv when looking for installed GDAL\n  data. The latter is gone with GDAL 3.0 but the former remains (#818).\n\n1.8.9.post2 (2019-10-22)\n------------------------\n\n- The 1.8.9.post1 release introduced a bug affecting builds of the package from\n  a source distribution using GDAL 2.x. This bug has been fixed in commit\n  960568d.\n\n1.8.9.post1 (2019-10-22)\n------------------------\n\n- A change has been made to the package setup script so that the shim module\n  for GDAL 3 is used when building the package from a source distribution.\n  There are no other changes to the package.\n\n1.8.9 (2019-10-21)\n------------------\n\n- A shim module and support for GDAL 3.0 has been added. The package can now be\n  built and used with GDAL 3.0 and PROJ 6.1 or 6.2. Note that the 1.8.9 wheels\n  we will upload to PyPI will contain GDAL 2.4.2 and PROJ 4.9.3 as in the 1.8.8\n  wheels.\n\n1.8.8 (2019-09-25)\n------------------\n\n- The schema of geopackage files with a geometry type code of 3000 could not be\n  reported using Fiona 1.8.7. This bug is fixed.\n\n1.8.7 (2019-09-24)\n------------------\n\nBug fixes:\n\n- Regression in handling of polygons with M values noted under version 1.8.5\n  below was in fact not fixed then (see new report #789), but is fixed in\n  version 1.8.7.\n- Windows filenames containing \"!\" are now parsed correctly, fixing issue #742.\n\nUpcoming changes:\n\n- In version 1.9.0, the objects yielded when a Collection is iterated will be\n  mutable mappings but will no longer be instances of Python's dict. Version\n  1.9 is intended to be backwards compatible with 1.8 except where user code\n  tests `isinstance(feature, dict)`. In version 2.0 the new Feature, Geometry,\n  and Properties classes will become immutable mappings. See\n  for more discussion of the upcoming changes for version 2.0.\n\n1.8.6 (2019-03-18)\n------------------\n\n- The advertisement for JSON driver enablement in 1.8.5 was false (#176), but\n  in this release they are ready for use.\n\n1.8.5 (2019-03-15)\n------------------"}, {"name": "fiona", "tags": ["cli", "data", "dev", "math", "ui", "web"], "summary": "Fiona reads and writes spatial data files", "text": "- GDAL seems to work best if GDAL_DATA is set as early as possible. Ideally it\n  is set when building the library or in the environment before importing\n  Fiona, but for wheels we patch GDAL_DATA into os.environ when fiona.env\n  is imported. This resolves #731.\n- A combination of bugs which allowed .cpg files to be overlooked has been\n  fixed (#726).\n- On entering a collection context (Collection.__enter__) a new anonymous GDAL\n  environment is created if needed and entered. This makes `with\n  fiona.open(...) as collection:` roughly equivalent to `with fiona.open(...)\n  as collection, Env():`. This helps prevent bugs when Collections are created\n  and then used later or in different scopes.\n- Missing GDAL support for TopoJSON, GeoJSONSeq, and ESRIJSON has been enabled\n  (#721).\n- A regression in handling of polygons with M values (#724) has been fixed.\n- Per-feature debug logging calls in OGRFeatureBuilder methods have been\n  eliminated to improve feature writing performance (#718).\n- Native support for datasets in Google Cloud Storage identified by \"gs\"\n  resource names has been added (#709).\n- Support has been added for triangle, polyhedral surface, and TIN geometry\n  types (#679).\n- Notes about using the MemoryFile and ZipMemoryFile classes has been added to\n  the manual (#674).\n\n1.8.4 (2018-12-10)\n------------------\n\n- 3D geometries can now be transformed with a specified precision (#523).\n- A bug producing a spurious DriverSupportError for Shapefiles with a \"time\"\n  field (#692) has been fixed.\n- Patching of the GDAL_DATA environment variable was accidentally left in place\n  in 1.8.3 and now has been removed.\n\n1.8.3 (2018-11-30)\n------------------\n\n- The RASTERIO_ENV config environment marker this project picked up from\n  Rasterio has been renamed to FIONA_ENV (#665).\n- Options --gdal-data and --proj-data have been added to the fio-env command so\n  that users of Rasterio wheels can get paths to set GDAL_DATA and PROJ_LIB\n  environment variables.\n- The unsuccessful attempt to make GDAL and PROJ support file discovery and\n  configuration automatic within collection's crs and crs_wkt properties has\n  been reverted.  Users must execute such code inside a `with Env()` block or\n  set the GDAL_DATA and PROJ_LIB environment variables needed by GDAL.\n\n1.8.2 (2018-11-19)\n------------------\n\nBug fixes:\n\n- Raise FionaValueError when an iterator's __next__ is called and the session\n  is found to be missing or inactive instead of passing a null pointer to\n  OGR_L_GetNextFeature (#687).\n\n1.8.1 (2018-11-15)\n------------------\n\nBug fixes:"}, {"name": "fiona", "tags": ["cli", "data", "dev", "math", "ui", "web"], "summary": "Fiona reads and writes spatial data files", "text": "- Add checks around OSRGetAuthorityName and OSRGetAuthorityCode calls that will\n  log problems with looking up these items.\n- Opened data sources are now released before we raise exceptions in\n  WritingSession.start (#676). This fixes an issue with locked files on\n  Windows.\n- We now ensure that an Env instance exists when getting the crs or crs_wkt\n  properties of a Collection (#673, #690). Otherwise, required GDAL and PROJ\n  data files included in Fiona wheels can not be found.\n- GDAL and PROJ data search has been refactored to improve testability (#678).\n- In the project's Cython code, void* pointers have been replaced with proper\n  GDAL types (#672).\n- Pervasive warning level log messages about ENCODING creation options (#668)\n  have been eliminated.\n\n1.8.0 (2018-10-31)\n------------------\n\nThis is the final 1.8.0 release. Thanks, everyone!\n\nBug fixes:\n\n- We cpdef Session.stop so that it has a C version that can be called safely\n  from __dealloc__, fixing a PyPy issue (#659, #553).\n\n1.8rc1 (2018-10-26)\n-------------------\n\nThere are no changes in 1.8rc1 other than more test standardization and the\nintroduction of a temporary test_collection_legacy.py module to support the\nbuild of fully tested Python 2.7 macosx wheels on Travis-CI.\n\n1.8b2 (2018-10-23)\n------------------\n\nBug fixes:\n\n- The ensure_env_with_credentials decorator will no longer clobber credentials\n  of the outer environment. This fixes a bug reported to the Rasterio project\n  and which also existed in Fiona.\n- An unused import of the packaging module and the dependency have been \n  removed (#653).\n- The Env class logged to the 'rasterio' hierarchy instead of 'fiona'. This\n  mistake has been corrected (#646).\n- The Mapping abstract base class is imported from collections.abc when\n  possible (#647).\n\nRefactoring:\n\n- Standardization of the tests on pytest functions and fixtures continues and\n  is nearing completion (#648, #649, #650, #651, #652).\n\n1.8b1 (2018-10-15)\n------------------\n\nDeprecations:\n\n- Collection slicing has been deprecated and will be prohibited in a future\n  version.\n\nBug fixes:\n\n- Rasterio CRS objects passed to transform module methods will be converted\n  to dicts as needed (#590).\n- Implicitly convert curve geometries to their linear approximations rather\n  than failing (#617).\n- Migrated unittest test cases in test_collection.py and test_layer.py to the\n  use of the standard data_dir and path_coutwildrnp_shp fixtures (#616).\n- Root logger configuration has been removed from all test scripts (#615).\n- An AWS session is created for the CLI context Env only if explicitly\n  requested, matching the behavior of Rasterio's CLI (#635).\n- Dependency on attrs is made explicit.\n- Other dependencies are pinned to known good versions in requirements files.\n- Unused arguments have been removed from the Env constructor (#637).\n\nRefactoring:\n\n- A with_context_env decorator has been added and used to set up the GDAL\n  environment for CLI commands. The command functions themselves are now\n  simplified.\n\n1.8a3 (2018-10-01)\n------------------\n\nDeprecations:\n\n- The ``fiona.drivers()`` context manager is officially deprecated. All\n  users should switch to ``fiona.Env()``, which registers format drivers and\n  manages GDAL configuration in a reversible manner.\n\nBug fixes:"}, {"name": "fiona", "tags": ["cli", "data", "dev", "math", "ui", "web"], "summary": "Fiona reads and writes spatial data files", "text": "- The Collection class now filters log messages about skipped fields to\n  a maximum of one warning message per field (#627).\n- The boto3 module is only imported when needed (#507, #629).\n- Compatibility with Click 7.0 is achieved (#633).\n- Use of %r instead of %s in a debug() call prevents UnicodeDecodeErrors\n  (#620).\n\n1.8a2 (2018-07-24)\n------------------\n\nNew features:\n\n- 64-bit integers are the now the default for int type fields (#562, #564).\n- 'http', 's3', 'zip+http', and 'zip+s3' URI schemes for datasets are now\n  supported (#425, #426).\n- We've added a ``MemoryFile`` class which supports formatted in-memory\n  feature collections (#501).\n- Added support for GDAL 2.x boolean field sub-type (#531).\n- A new ``fio rm`` command makes it possible to cleanly remove multi-file\n  datasets (#538).\n- The geometry type in a feature collection is more flexible. We can now\n  specify not only a single geometry type, but a sequence of permissible types,\n  or \"Any\" to permit any geometry type (#539).\n- Support for GDAL 2.2+ null fields has been added (#554).\n- The new ``gdal_open_vector()`` function of our internal API provides much\n  improved error handling (#557).\n\nBug fixes:\n\n- The bug involving OrderedDict import on Python 2.7 has been fixed (#533).\n- An ``AttributeError`` raised when the ``--bbox`` option of fio-cat is used\n  with more than one input file has been fixed (#543, #544).\n- Obsolete and derelict fiona.tool module has been removed.\n- Revert the change in 0a2bc7c that discards Z in geometry types when a\n  collection's schema is reported (#541).\n- Require six version 1.7 or higher (#550).\n- A regression related to \"zip+s3\" URIs has been fixed.\n- Debian's GDAL data locations are now searched by default (#583).\n\n1.8a1 (2017-11-06)\n------------------\n\nNew features:\n\n- Each call of ``writerecords()`` involves one or more transactions of up to\n  20,000 features each. This improves performance when writing GeoPackage files\n  as the previous transaction size was only 200 features (#476, #491).\n\nPackaging:\n\n- Fiona's Cython source files have been refactored so that there are no longer\n  separate extension modules for GDAL 1.x and GDAL 2.x. Instead there is a base\n  extension module based on GDAL 2.x and shim modules for installations that\n  use GDAL 1.x.\n\n1.7.11.post1 (2018-01-08)\n-------------------------\n\n- This post-release adds missing expat (and thereby GPX format) support to\n  the included GDAL library (still version 2.2.2).\n\n1.7.11 (2017-12-14)\n-------------------\n\n- The ``encoding`` keyword argument for ``fiona.open()``, which is intended\n  to allow a caller to override a data source's own and possibly erroneous\n  encoding, has not been working (#510, #512). The problem is that we weren't\n  always setting GDAL open or config options before opening the data sources.\n  This bug is resolved by a number of commits in the maint-1.7 branch and\n  the fix is demonstrated in tests/test_encoding.py.\n- An ``--encoding`` option has been added to fio-load to enable creation of\n  encoded shapefiles with an accompanying .cpg file (#499, #517).\n\n1.7.10.post1 (2017-10-30)\n-------------------------\n\n- A post-release has been made to fix a problem with macosx wheels uploaded\n  to PyPI."}, {"name": "fiona", "tags": ["cli", "data", "dev", "math", "ui", "web"], "summary": "Fiona reads and writes spatial data files", "text": "1.7.10 (2017-10-26)\n-------------------\n\nBug fixes:\n\n- An extraneous printed line from the ``rio cat --layers`` validator has been\n  removed (#478).\n\nPackaging:\n\n- Official OS X and Manylinux1 wheels (on PyPI) for this release will be\n  compatible with Shapely 1.6.2 and Rasterio 1.0a10 wheels.\n\n1.7.9.post1 (2017-08-21)\n------------------------\n\nThis release introduces no changes in the Fiona package. It upgrades GDAL\nfrom 2.2.0 to 2.2.1 in wheels that we publish to the Python Package Index.\n\n1.7.9 (2017-08-17)\n------------------\n\nBug fixes:\n\n- Acquire the GIL for GDAL error callback functions to prevent crashes when\n  GDAL errors occur when the GIL has been released by user code.\n- Sync and flush layers when closing even when the number of features is not\n  precisely known (#467).\n\n1.7.8 (2017-06-20)\n------------------\n\nBug fixes:\n\n- Provide all arguments needed by CPLError based exceptions (#456).\n\n1.7.7 (2017-06-05)\n------------------\n\nBug fixes:\n\n- Switch logger `warn()` (deprecated) calls to `warning()`.\n- Replace all relative imports and cimports in Cython modules with absolute\n  imports (#450).\n- Avoid setting `PROJ_LIB` to a non-existent directory (#439).\n\n1.7.6 (2017-04-26)\n------------------\n\nBug fixes:\n\n- Fall back to `share/proj` for PROJ_LIB (#440).\n- Replace every call to `OSRDestroySpatialReference()` with `OSRRelease()`,\n  fixing the GPKG driver crasher reported in #441 (#443).\n- Add a `DriverIOError` derived from `IOError` to use for driver-specific\n  errors such as the GeoJSON driver's refusal to overwrite existing files.\n  Also we now ensure that when this error is raised by `fiona.open()` any\n  created read or write session is deleted, this eliminates spurious \n  exceptions on teardown of broken `Collection` objects (#437, #444).\n\n1.7.5 (2017-03-20)\n------------------\n\nBug fixes:\n\n- Opening a data file in read (the default) mode with `fiona.open()` using the\n  the `driver` or `drivers` keyword arguments (to specify certain format \n  drivers) would sometimes cause a crash on Windows due to improperly\n  terminated lists of strings (#428). The fix: Fiona's buggy `string_list()`\n  has been replaced by GDAL's `CSLAddString()`.\n\n1.7.4 (2017-02-20)\n------------------\n\nBug fixes:\n\n- OGR's EsriJSON detection fails when certain keys aren't found in the first\n  6000 bytes of data passed to `BytesCollection` (#422). A .json file extension\n  is now explicitly given to the in-memory file behind `BytesCollection` when\n  the `driver='GeoJSON'` keyword argument is given (#423).\n\n1.7.3 (2017-02-14)\n------------------\n\nRoses are red.\nTan is a pug.\nSoftware regression's\nthe most embarrassing bug.\n\nBug fixes:\n\n- Use __stdcall for GDAL error handling callback on Windows as in Rasterio.\n- Turn on latent support for zip:// URLs in rio-cat and rio-info (#421).\n- The 1.7.2 release broke support for zip files with absolute paths (#418).\n  This regression has been fixed with tests to confirm.\n\n1.7.2 (2017-01-27)\n------------------\n\nFuture Deprecation:\n\n- `Collection.__next__()` is buggy in that it can lead to duplication of \n  features when used in combination with `Collection.filter()` or\n  `Collection.__iter__()`. It will be removed in Fiona 2.0. Please check for\n  usage of this deprecated feature by running your tests or programs with\n  `PYTHONWARNINGS=\"always:::fiona\"` or `-W\"always:::fiona\"` and switch from\n  `next(collection)` to `next(iter(collection))` (#301).\n\nBug fix:\n\n- Zipped streams of bytes can be accessed by `BytesCollection` (#318)."}, {"name": "fiona", "tags": ["cli", "data", "dev", "math", "ui", "web"], "summary": "Fiona reads and writes spatial data files", "text": "1.7.1.post1 (2016-12-23)\n------------------------\n- New binary wheels using version 1.2.0 of sgillies/frs-wheel-builds. See\n\n1.7.1 (2016-11-16)\n------------------\n\nBug Fixes:\n\n- Prevent Fiona from stumbling over 'Z', 'M', and 'ZM' geometry types\n  introduced in GDAL 2.1 (#384). Fiona 1.7.1 doesn't add explicit support for\n  these types, they are coerced to geometry types 1-7 ('Point', 'LineString',\n  etc.)\n- Raise an `UnsupportedGeometryTypeError` when a bogus or unsupported \n  geometry type is encountered in a new collection's schema or elsewhere\n  (#340).\n- Enable `--precision 0` for fio-cat (#370).\n- Prevent datetime exceptions from unnecessarily stopping collection iteration\n  by yielding `None` (#385)\n- Replace log.warn calls with log.warning calls (#379).\n- Print an error message if neither gdal-config or `--gdalversion` indicate\n  a GDAL C API version when running `setup.py` (#364).\n- Let dict-like subclasses through CRS type checks (#367).\n\n1.7.0post2 (2016-06-15)\n-----------------------\n\nPackaging: define extension modules for 'clean' and 'config' targets (#363).\n\n1.7.0post1 (2016-06-15)\n-----------------------\n\nPackaging: No files are copied for the 'clean' setup target (#361, #362).\n\n1.7.0 (2016-06-14)\n------------------\n\nThe C extension modules in this library can now be built and used with either\na 1.x or 2.x release of the GDAL library. Big thanks to Ren\u00e9 Buffat for\nleading this effort.\n\nRefactoring:\n\n- The `ogrext1.pyx` and `ogrext2.pyx` files now use separate\n  C APIs defined in `ogrext1.pxd` and `ogrex2.pxd`. The other extension\n  modules have been refactored so that they do not depend on either of these\n  modules and use subsets of the GDAL/OGR API compatible with both GDAL 1.x and\n  2.x (#359).\n\nPackaging:\n\n- Source distributions now contain two different sources for the\n  `ogrext` extension module. The `ogrext1.c` file will be used with GDAL 1.x\n  and the `ogrext2.c` file will be used with GDAL 2.x.\n\n1.7b2 (2016-06-13)\n------------------\n\n- New feature: enhancement of the `--layer` option for fio-cat and fio-dump\n  to allow separate layers of one or more multi-layer input files to be\n  selected (#349).\n\n1.7b1 (2016-06-10)\n------------------\n\n- New feature: support for GDAL version 2+ (#259).\n- New feature: a new fio-calc CLI command (#273).\n- New feature: `--layer` options for fio-info (#316) and fio-load (#299).\n- New feature: a `--no-parse` option for fio-collect that lets a careful user\n  avoid extra JSON serialization and deserialization (#306).\n- Bug fix: `+wktext` is now preserved when serializing CRS from WKT to PROJ.4\n  dicts (#352).\n- Bug fix: a small memory leak when opening a collection has been fixed (#337).\n- Bug fix: internal unicode errors now result in a log message and a \n  `UnicodeError` exception, not a `TypeError` (#356).\n\n1.6.4 (2016-05-06)\n------------------\n- Raise ImportError if the active GDAL library version is >= 2.0 instead of\n  failing unpredictably (#338, #341). Support for GDAL>=2.0 is coming in\n  Fiona 1.7.\n\n1.6.3.post1 (2016-03-27)\n------------------------\n- No changes to the library in this post-release version, but there is a\n  significant change to the distributions on PyPI: to help make Fiona more\n  compatible with Shapely on OS X, the GDAL shared library included in the\n  macosx (only) binary wheels now statically links the GEOS library. See"}, {"name": "fiona", "tags": ["cli", "data", "dev", "math", "ui", "web"], "summary": "Fiona reads and writes spatial data files", "text": "1.6.3 (2015-12-22)\n------------------\n- Daytime has been decreasing in the Northern Hemisphere, but is now\n  increasing again as it should.\n- Non-UTF strings were being passed into OGR functions in some situations\n  and on Windows this would sometimes crash a Python process (#303). Fiona\n  now raises errors derived from UnicodeError when field names or field\n  values can't be encoded.\n\n1.6.2 (2015-09-22)\n------------------\n- Providing only PROJ4 representations in the dataset meta property resulted in\n  loss of CRS information when using the `fiona.open(..., **src.meta) as dst`\n  pattern (#265). This bug has been addressed by adding a crs_wkt item to the`\n  meta property and extending the `fiona.open()` and the collection constructor\n  to look for and prioritize this keyword argument.\n\n1.6.1 (2015-08-12)\n------------------\n- Bug fix: Fiona now deserializes JSON-encoded string properties provided by\n  the OGR GeoJSON driver (#244, #245, #246).\n- Bug fix: proj4 data was not copied properly into binary distributions due to\n  a typo (#254).\n\nSpecial thanks to WFMU DJ Liz Berg for the awesome playlist that's fueling my\nrelease sprint. Check it out at https://wfmu.org/playlists/shows/62083. You\ncan't unhear Love Coffin.\n\n1.6.0 (2015-07-21)\n------------------\n- Upgrade Cython requirement to 0.22 (#214).\n- New BytesCollection class (#215).\n- Add GDAL's OpenFileGDB driver to registered drivers (#221).\n- Implement CLI commands as plugins (#228).\n- Raise click.abort instead of calling sys.exit, preventing surprising exits\n  (#236).\n\n1.5.1 (2015-03-19)\n------------------\n- Restore test data to sdists by fixing MANIFEST.in (#216).\n\n1.5.0 (2015-02-02)\n------------------\n- Finalize GeoJSON feature sequence options (#174).\n- Fix for reading of datasets that don't support feature counting (#190).\n- New test dataset (#188).\n- Fix for encoding error (#191).\n- Remove confusing warning (#195).\n- Add data files for binary wheels (#196).\n- Add control over drivers enabled when reading datasets (#203).\n- Use cligj for CLI options involving GeoJSON (#204).\n- Fix fio-info --bounds help (#206).\n\n1.4.8 (2014-11-02)\n------------------\n- Add missing crs_wkt property as in Rasterio (#182).\n\n1.4.7 (2014-10-28)\n------------------\n- Fix setting of CRS from EPSG codes (#149).\n\n1.4.6 (2014-10-21)\n------------------\n- Handle 3D coordinates in bounds() #178.\n\n1.4.5 (2014-10-18)\n------------------\n- Add --bbox option to fio-cat (#163).\n- Skip geopackage tests if run from an sdist (#167).\n- Add fio-bounds and fio-distrib.\n- Restore fio-dump to working order.\n\n1.4.4 (2014-10-13)\n------------------\n- Fix accidental requirement on GDAL 1.11 introduced in 1.4.3 (#164).\n\n1.4.3 (2014-10-10)\n------------------\n- Add support for geopackage format (#160).\n- Add -f and --format aliases for --driver in CLI (#162).\n- Add --version option and env command to CLI.\n\n1.4.2 (2014-10-03)\n------------------\n- --dst-crs and --src-crs options for fio cat and collect (#159).\n\n1.4.1 (2014-09-30)\n------------------\n- Fix encoding bug in collection's __getitem__ (#153).\n\n1.4.0 (2014-09-22)\n------------------\n- Add fio cat and fio collect commands (#150).\n- Return of Python 2.6 compatibility (#148).\n- Improved CRS support (#149).\n\n1.3.0 (2014-09-17)\n------------------\n- Add single metadata item accessors to fio inf (#142).\n- Move fio to setuptools entry point (#142).\n- Add fio dump and load commands (#143).\n- Remove fio translate command."}, {"name": "fiona", "tags": ["cli", "data", "dev", "math", "ui", "web"], "summary": "Fiona reads and writes spatial data files", "text": "1.2.0 (2014-09-02)\n------------------\n- Always show property width and precision in schema (#123).\n- Write datetime properties of features (#125).\n- Reset spatial filtering in filter() (#129).\n- Accept datetime.date objects as feature properties (#130).\n- Add slicing to collection iterators (#132).\n- Add geometry object masks to collection iterators (#136).\n- Change source layout to match Shapely and Rasterio (#138).\n\n1.1.6 (2014-07-23)\n------------------\n- Implement Collection __getitem__() (#112).\n- Leave GDAL finalization to the DLL's destructor (#113).\n- Add Collection keys(), values(), items(), __contains__() (#114).\n- CRS bug fix (#116).\n- Add fio CLI program.\n  \n1.1.5 (2014-05-21)\n------------------\n- Addition of cpl_errs context manager (#108).\n- Check for NULLs with '==' test instead of 'is' (#109).\n- Open auxiliary files with encoding='utf-8' in setup for Python 3 (#110).\n\n1.1.4 (2014-04-03)\n------------------\n- Convert 'long' in schemas to 'int' (#101).\n- Carefully map Python schema to the possibly munged internal schema (#105).\n- Allow writing of features with geometry: None (#71).\n\n1.1.3 (2014-03-23)\n------------------\n- Always register all GDAL and OGR drivers when entering the DriverManager\n  context (#80, #92).\n- Skip unsupported field types with a warning (#91).\n- Allow OGR config options to be passed to fiona.drivers() (#90, #93).\n- Add a bounds() function (#100).\n- Turn on GPX driver.\n\n1.1.2 (2014-02-14)\n------------------\n- Remove collection slice left in dumpgj (#88).\n\n1.1.1 (2014-02-02)\n------------------\n- Add an interactive file inspector like the one in rasterio.\n- CRS to_string bug fix (#83).\n\n1.1 (2014-01-22)\n----------------\n- Use a context manager to manage drivers (#78), a backwards compatible but\n  big change. Fiona is now compatible with rasterio and plays better with the\n  osgeo package.\n\n1.0.3 (2014-01-21)\n------------------\n- Fix serialization of +init projections (#69).\n\n1.0.2 (2013-09-09)\n------------------\n- Smarter, better test setup (#65, #66, #67).\n- Add type='Feature' to records read from a Collection (#68).\n- Skip geometry validation when using GeoJSON driver (#61).\n- Dumpgj file description reports record properties as a list (as in\n  dict.items()) instead of a dict.\n\n1.0.1 (2013-08-16)\n------------------\n- Allow ordering of written fields and preservation of field order when\n  reading (#57).\n\n1.0 (2013-07-30)\n-----------------\n- Add prop_type() function.\n- Allow UTF-8 encoded paths for Python 2 (#51). For Python 3, paths must\n  always be str, never bytes.\n- Remove encoding from collection.meta, it's a file creation option only.\n- Support for linking GDAL frameworks (#54).\n\n0.16.1 (2013-07-02)\n-------------------\n- Add listlayers, open, prop_width to __init__py:__all__.\n- Reset reading of OGR layer whenever we ask for a collection iterator (#49).\n\n0.16 (2013-06-24)\n-----------------\n- Add support for writing layers to multi-layer files.\n- Add tests to reach 100% Python code coverage.\n\n0.15 (2013-06-06)\n-----------------\n- Get and set numeric field widths (#42).\n- Add support for multi-layer data sources (#17).\n- Add support for zip and tar virtual filesystems (#45).\n- Add listlayers() function.\n- Add GeoJSON to list of supported formats (#47).\n- Allow selection of layers by index or name."}, {"name": "fiona", "tags": ["cli", "data", "dev", "math", "ui", "web"], "summary": "Fiona reads and writes spatial data files", "text": "0.14 (2013-05-04)\n-----------------\n- Add option to add JSON-LD in the dumpgj program.\n- Compare values to six.string_types in Collection constructor.\n- Add encoding to Collection.meta.\n- Document dumpgj in README.\n\n0.13 (2013-04-30)\n-----------------\n- Python 2/3 compatibility in a single package. Pythons 2.6, 2.7, 3.3 now supported.\n\n0.12.1 (2013-04-16)\n-------------------\n- Fix messed up linking of README in sdist (#39).\n\n0.12 (2013-04-15)\n-----------------\n- Fix broken installation of extension modules (#35).\n- Log CPL errors at their matching Python log levels.\n- Use upper case for encoding names within OGR, lower case in Python.\n\n0.11 (2013-04-14)\n-----------------\n- Cythonize .pyx files (#34).\n- Work with or around OGR's internal recoding of record data (#35).\n- Fix bug in serialization of int/float PROJ.4 params.\n\n0.10 (2013-03-23)\n-----------------\n- Add function to get the width of str type properties.\n- Handle validation and schema representation of 3D geometry types (#29).\n- Return {'geometry': None} in the case of a NULL geometry (#31).\n\n0.9.1 (2013-03-07)\n------------------\n- Silence the logger in ogrext.so (can be overridden).\n- Allow user specification of record field encoding (like 'Windows-1252' for\n  Natural Earth shapefiles) to help when OGR can't detect it.\n\n0.9 (2013-03-06)\n----------------\n- Accessing file metadata (crs, schema, bounds) on never inspected closed files\n  returns None without exceptions.\n- Add a dict of supported_drivers and their supported modes.\n- Raise ValueError for unsupported drivers and modes.\n- Remove asserts from ogrext.pyx.\n- Add validate_record method to collections.\n- Add helpful coordinate system functions to fiona.crs.\n- Promote use of fiona.open over fiona.collection.\n- Handle Shapefile's mix of LineString/Polygon and multis (#18).\n- Allow users to specify width of shapefile text fields (#20).\n\n0.8 (2012-02-21)\n----------------\n- Replaced .opened attribute with .closed (product of collection() is always\n  opened). Also a __del__() which will close a Collection, but still not to be\n  depended upon.\n- Added writerecords method.\n- Added a record buffer and better counting of records in a collection.\n- Manage one iterator per collection/session.\n- Added a read-only bounds property.\n\n0.7 (2012-01-29)\n----------------\n- Initial timezone-naive support for date, time, and datetime fields. Don't use\n  these field types if you can avoid them. RFC 3339 datetimes in a string field\n  are much better.\n\n0.6.2 (2012-01-10)\n------------------\n- Diagnose and set the driver property of collection in read mode.\n- Fail if collection paths are not to files. Multi-collection workspaces are\n  a (maybe) TODO.\n\n0.6.1 (2012-01-06)\n------------------\n- Handle the case of undefined crs for disk collections.\n\n0.6 (2012-01-05)\n----------------\n- Support for collection coordinate reference systems based on Proj4.\n- Redirect OGR warnings and errors to the Fiona log.\n- Assert that pointers returned from the ograpi functions are not NULL before\n  using.\n\n0.5 (2011-12-19)\n----------------\n- Support for reading and writing collections of any geometry type.\n- Feature and Geometry classes replaced by mappings (dicts).\n- Removal of Workspace class.\n\n0.2 (2011-09-16)\n----------------\n- Rename WorldMill to Fiona.\n\n0.1.1 (2008-12-04)\n------------------\n- Support for features with no geometry.\n\nCredits\n======="}, {"name": "fiona", "tags": ["cli", "data", "dev", "math", "ui", "web"], "summary": "Fiona reads and writes spatial data files", "text": "Fiona is written by:\n\n- Adam J. Stewart \n- Alan D. Snow \n- Alexandre Detiste \n- Ariel Nunez \n- Ariki \n- Bas Couwenberg \n- Brandon Liu \n- Brendan Ward \n- Chad Hawkins \n- Chris Mutel \n- Christoph Gohlke \n- Dan \"Ducky\" Little \n- daryl herzmann \n- Denis \n- Denis Rykov \n- dimlev \n- Efr\u00e9n \n- Egor Fedorov \n- Elliott Sales de Andrade \n- Even Rouault \n- Ewout ter Hoeven \n- Filipe Fernandes \n- fredj \n- Gavin S \n- G\u00e9raud \n- Hannes Gr\u00e4uler \n- Hao Lyu \n- Herz \n- Ian Rose \n- Jacob Wasserman \n- James McBride \n- James Wilshaw \n- Jelle van der Waa \n- Jesse Crocker \n- joehuanguf \n- Johan Van de Wauw \n- Joris Van den Bossche \n- Joshua Arnott \n- Juan Luis Cano Rodr\u00edguez \n- Keith Jenkins \n- Kelsey Jordahl \n- Kevin Wurster \n- lgolston \n- Lo\u00efc Dutrieux \n- Ludovic Delaun\u00e9 \n- Martijn Visser \n- Matthew Perry \n- Micah Cochran \n- Michael Weisman \n- Michele Citterio \n- Mike Taves \n- Miro Hron\u010dok \n- Oliver Tonnhofer \n- Patrick Young \n- Phillip Cloud \n- pmav99 \n- qinfeng \n- Ren\u00e9 Buffat \n- Reuben Fletcher-Costin \n- Ryan Grout \n- Ryan Munro \n- Sandro Mani \n- Sean Gillies \n- Sid Kapur \n- Simon Norris \n- Stefan Brand \n- Stefano Costa \n- Stephane Poss \n- Tim Tr\u00f6ndle \n- wilsaj \n- Yann-Sebastien Tremblay-Johnston\n\nThe GeoPandas project (Joris Van den Bossche et al.) has been a major driver\nfor new features in 1.8.0.\n\nFiona would not be possible without the great work of Frank Warmerdam and other\nGDAL/OGR developers.\n\nSome portions of this work were supported by a grant (for Pleiades_) from the\nU.S. National Endowment for the Humanities (https://www.neh.gov).\n\n.. _Pleiades: https://pleiades.stoa.org"}, {"name": "fiona", "tags": ["cli", "data", "dev", "math", "ui", "web"], "summary": "Fiona reads and writes spatial data files", "text": "This library is used to read and write spatial data files in various formats, including GeoPackage and Shapefile. It allows developers to stream simple feature data to and from GIS formats, making it easier to work with real-world geospatial data."}, {"name": "flask", "tags": ["web"], "summary": "A simple framework for building complex web applications.", "text": "Flask\n\nFlask is a lightweight [WSGI] web application framework. It is designed\nto make getting started quick and easy, with the ability to scale up to\ncomplex applications. It began as a simple wrapper around [Werkzeug]\nand [Jinja], and has become one of the most popular Python web\napplication frameworks.\n\nFlask offers suggestions, but doesn't enforce any dependencies or\nproject layout. It is up to the developer to choose the tools and\nlibraries they want to use. There are many extensions provided by the\ncommunity that make adding new functionality easy.\n\nA Simple Example\n\nDonate\n\nThe Pallets organization develops and supports Flask and the libraries\nit uses. In order to grow the community of contributors and users, and\nallow the maintainers to devote more time to the projects, [please\ndonate today].\n\nContributing\n\nSee our [detailed contributing documentation][contrib] for many ways to\ncontribute, including reporting issues, requesting features, asking or answering\nquestions, and making PRs."}, {"name": "flask", "tags": ["web"], "summary": "A simple framework for building complex web applications.", "text": "This library is used to build complex web applications quickly and easily with a lightweight framework that can scale up as needed. With Flask, developers have the flexibility to choose their own dependencies and project layout while still benefiting from a supportive community of contributors and extensions."}, {"name": "flax", "tags": ["math", "ml", "web"], "summary": "Flax: A neural network library for JAX designed for flexibility", "text": "Flax: A neural network library and ecosystem for JAX designed for flexibility\n\n(https://github.com/google/flax/actions/workflows/flax_test.yml)\n(https://pypi.org/project/flax/)\n\n[**Overview**](#overview)\n[**Quick install**](#quick-install)\n[**What does Flax look like?**](#what-does-flax-look-like)\n[**Documentation**](https://flax.readthedocs.io/)\n\nReleased in 2024, Flax NNX is a new simplified Flax API that is designed to make\nit easier to create, inspect, debug, and analyze neural networks in\n[JAX](https://jax.readthedocs.io/). It achieves this by adding first class support\nfor Python reference semantics. This allows users to express their models using\nregular Python objects, enabling reference sharing and mutability.\n\nFlax NNX evolved from the [Flax Linen API](https://flax-linen.readthedocs.io/), which\nwas released in 2020 by engineers and researchers at Google Brain in close collaboration\nwith the JAX team.\n\nYou can learn more about Flax NNX on the [dedicated Flax documentation site](https://flax.readthedocs.io/). Make sure you check out:\n\n* [Flax NNX basics](https://flax.readthedocs.io/en/latest/nnx_basics.html)\n* [MNIST tutorial](https://flax.readthedocs.io/en/latest/mnist_tutorial.html)\n* [Why Flax NNX](https://flax.readthedocs.io/en/latest/why.html)\n* [Evolution from Flax Linen to Flax NNX](https://flax.readthedocs.io/en/latest/guides/linen_to_nnx.html)\n\n**Note:** Flax Linen's [documentation has its own site](https://flax-linen.readthedocs.io/).\n\nThe Flax team's mission is to serve the growing JAX neural network\nresearch ecosystem - both within Alphabet and with the broader community,\nand to explore the use-cases where JAX shines. We use GitHub for almost\nall of our coordination and planning, as well as where we discuss\nupcoming design changes. We welcome feedback on any of our discussion,\nissue and pull request threads.\n\nYou can make feature requests, let us know what you are working on,\nreport issues, ask questions in our [Flax GitHub discussion\nforum](https://github.com/google/flax/discussions).\n\nWe expect to improve Flax, but we don't anticipate significant\nbreaking changes to the core API. We use [Changelog](https://github.com/google/flax/tree/main/CHANGELOG.md)\nentries and deprecation warnings when possible.\n\nIn case you want to reach us directly, we're at flax-dev@google.com.\n\nOverview\n\nFlax is a high-performance neural network library and ecosystem for\nJAX that is **designed for flexibility**:\nTry new forms of training by forking an example and by modifying the training\nloop, not adding features to a framework.\n\nFlax is being developed in close collaboration with the JAX team and\ncomes with everything you need to start your research, including:\n\n* **Neural network API** (`flax.nnx`): Including [`Linear`](https://flax.readthedocs.io/en/latest/api_reference/flax.nnx/nn/linear.html#flax.nnx.Linear), [`Conv`](https://flax.readthedocs.io/en/latest/api_reference/flax.nnx/nn/linear.html#flax.nnx.Conv), [`BatchNorm`](https://flax.readthedocs.io/en/latest/api_reference/flax.nnx/nn/normalization.html#flax.nnx.BatchNorm), [`LayerNorm`](https://flax.readthedocs.io/en/latest/api_reference/flax.nnx/nn/normalization.html#flax.nnx.LayerNorm), [`GroupNorm`](https://flax.readthedocs.io/en/latest/api_reference/flax.nnx/nn/normalization.html#flax.nnx.GroupNorm), [Attention](https://flax.readthedocs.io/en/latest/api_reference/flax.nnx/nn/attention.html) ([`MultiHeadAttention`](https://flax.readthedocs.io/en/latest/api_reference/flax.nnx/nn/attention.html#flax.nnx.MultiHeadAttention)), [`LSTMCell`](https://flax.readthedocs.io/en/latest/api_reference/flax.nnx/nn/recurrent.html#flax.nnx.nn.recurrent.LSTMCell), [`GRUCell`](https://flax.readthedocs.io/en/latest/api_reference/flax.nnx/nn/recurrent.html#flax.nnx.nn.recurrent.GRUCell), [`Dropout`](https://flax.readthedocs.io/en/latest/api_reference/flax.nnx/nn/stochastic.html#flax.nnx.Dropout).\n\n* **Utilities and patterns**: replicated training, serialization and checkpointing, metrics, prefetching on device.\n\n* **Educational examples**: [MNIST](https://flax.readthedocs.io/en/latest/mnist_tutorial.html), [Inference/sampling with the Gemma language model (transformer)](https://github.com/google/flax/tree/main/examples/gemma).\n\nQuick install\n\nFlax uses JAX, so do check out [JAX installation instructions on CPUs, GPUs and TPUs](https://jax.readthedocs.io/en/latest/installation.html).\n\nYou will need Python 3.8 or later. Install Flax from PyPi:\n\nTo upgrade to the latest version of Flax, you can use:\n\nTo install some additional dependencies (like `matplotlib`) that are required but not included\nby some dependencies, you can use:\n\nWhat does Flax look like?\n\nWe provide three examples using the Flax API: a simple multi-layer perceptron, a CNN and an auto-encoder.\n\nTo learn more about the `Module` abstraction, check out our [docs](https://flax.readthedocs.io/), our [broad intro to the Module abstraction](https://github.com/google/flax/blob/main/docs/linen_intro.ipynb). For additional concrete demonstrations of best practices, refer to our\n[guides](https://flax.readthedocs.io/en/latest/guides/index.html) and\n[developer notes](https://flax.readthedocs.io/en/latest/developer_notes/index.html).\n\nExample of an MLP:\n\nExample of a CNN:\n\nExample of an autoencoder:\n\nCiting Flax\n\nTo cite this repository:\n\nIn the above bibtex entry, names are in alphabetical order, the version number\nis intended to be that from [flax/version.py](https://github.com/google/flax/blob/main/flax/version.py), and the year corresponds to the project's open-source release.\n\nNote\n\nFlax is an open source project maintained by a dedicated team at Google DeepMind, but is not an official Google product."}, {"name": "flax", "tags": ["math", "ml", "web"], "summary": "Flax: A neural network library for JAX designed for flexibility", "text": "This library is used to create, inspect, debug, and analyze neural networks in JAX with added support for Python reference semantics. It enables users to express their models using regular Python objects, allowing for reference sharing and mutability."}, {"name": "folium", "tags": ["math", "visualization"], "summary": "Make beautiful maps with Leaflet.js & Python", "text": "PyPI\n\n.. |PyPI| image:: https://img.shields.io/pypi/v/folium.svg\n\n   :alt: DOI\n\n :target: https://mybinder.org/v2/gh/python-visualization/folium/main?filepath=examples\n\nfolium\n======\n\n.. image:: https://github.com/python-visualization/folium/blob/main/docs/_static/folium_logo.png\n   :height: 100px\n\nPython Data, Leaflet.js Maps\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n`folium` builds on the data wrangling strengths of the Python ecosystem and the\nmapping strengths of the Leaflet.js library. Manipulate your data in Python,\nthen visualize it in a Leaflet map via `folium`.\n\nInstallation\n------------\n\n.. code:: bash\n\nor\n\n.. code:: bash\n\nDocumentation\n-------------\n\nContributing\n------------\n\nWe love contributions!  folium is open source, built on open source,\nand we'd love to have you hang out in our community.\n\nSee `our complete contributor's guide `_ for more info.\n\nChangelog\n---------\n\n- Release notes of v0.16.0 and higher: https://github.com/python-visualization/folium/releases\n- Older `changelog `_\n\nPackages and plugins\n--------------------\n\nPackages:\n\nPlugins:"}, {"name": "folium", "tags": ["math", "visualization"], "summary": "Make beautiful maps with Leaflet.js & Python", "text": "This library is used to create interactive and customizable maps with Leaflet.js using Python data. Developers can leverage folium to visualize their data on a map, making it easier to understand complex spatial relationships and trends."}, {"name": "formulaic", "tags": ["data", "math"], "summary": "An implementation of Wilkinson formulas.", "text": "(https://pypi.org/project/formulaic/)\n\n(https://github.com/matthewwardrop/formulaic/actions?query=workflow%3A%22Run+Tox+Tests%22)\n(https://matthewwardrop.github.io/formulaic/)\n(https://codecov.io/gh/matthewwardrop/formulaic)\n(https://github.com/psf/black)\n\nFormulaic is a high-performance implementation of Wilkinson formulas for Python.\n\nIt provides:\n\n- high-performance dataframe to model-matrix conversions.\n- support for reusing the encoding choices made during conversion of one data-set on other datasets.\n- extensible formula parsing.\n- extensible data input/output plugins, with implementations for:\n  - input:\n  - output:\n- support for symbolic differentiation of formulas (and hence model matrices).\n- and much more.\n\nExample code\n\n`y = `\n\n  \n  \n  \n  \n\n`X = `\n\n  \n  \n  \n  \n\nNote that the above can be short-handed to:\n\nBenchmarks\n\nFormulaic typically outperforms R for both dense and sparse model matrices, and vastly outperforms `patsy` (the existing implementation for Python) for dense matrices (`patsy` does not support sparse model matrix output).\n\nFor more details, see [here](benchmarks/README.md).\n\nRelated projects and prior art\n\n- The work that started it all: Wilkinson, G. N., and C. E. Rogers. Symbolic description of factorial models for analysis of variance. J. Royal Statistics Society 22, pp. 392\u2013399, 1973.\n\nUsed by\n\nBelow are some of the projects that use Formulaic:\n\n- Add your project here!"}, {"name": "formulaic", "tags": ["data", "math"], "summary": "An implementation of Wilkinson formulas.", "text": "This library is used to convert dataframes into high-performance model matrices for various statistical modeling tasks. It provides efficient tools for symbolic differentiation, formula parsing, and extensible input/output plugins to support a wide range of use cases."}, {"name": "geoalchemy2", "tags": ["math"], "summary": "Using SQLAlchemy with Spatial Databases", "text": "============\nGeoAlchemy 2\n============\n\n   :target: https://github.com/geoalchemy/geoalchemy2/actions\n\n   :target: https://coveralls.io/r/geoalchemy/geoalchemy2\n\n   :alt: Documentation Status\n\n  :target: https://zenodo.org/doi/10.5281/zenodo.10808783\n\nGeoAlchemy 2 is a Python toolkit for working with spatial databases. It is\nbased on the gorgeous `SQLAlchemy `_.\n\nDocumentation is on Read the Docs: https://geoalchemy-2.readthedocs.io/en/stable."}, {"name": "geoalchemy2", "tags": ["math"], "summary": "Using SQLAlchemy with Spatial Databases", "text": "This library is used to enable developers to work with spatial data in their applications using SQLAlchemy, a popular Python SQL toolkit. With geoalchemy2, developers can easily integrate and manipulate geospatial data within their existing SQLAlchemy-based projects."}, {"name": "geocoder", "tags": ["dev", "math", "ui", "web"], "summary": "Geocoder is a simple and consistent geocoding library.", "text": "Overview\n\nMany online providers such as Google & Bing have geocoding services,\nthese providers do not include Python libraries and have different\nJSON responses between each other.\n\nIt can be very difficult sometimes to parse a particular geocoding provider\nsince each one of them have their own JSON schema.\n\nHere is a typical example of retrieving a Lat & Lng from Google using Python,\nthings shouldn't be this hard.\n\nNow lets use Geocoder to do the same task\n\nA glimpse at the API\n\nMany properties are available once the geocoder object is created.\n\nForward\n\nMultiple queries ('batch' geocoding)\n\nMultiple results\n\n> The providers currently supporting multiple results are listed in the table [below](#providers).\n\nReverse\n\nHouse Addresses\n\nIP Addresses\n\nBounding Box\n\nAccessing the JSON & GeoJSON attributes will be different\n\nCommand Line Interface\n\nProviders\n\nProvider\n:-------------------------------\n[ArcGIS][ArcGIS]\n[Baidu][Baidu]\n[Bing][Bing]\n[CanadaPost][CanadaPost]\n[FreeGeoIP][FreeGeoIP]\n[Gaode][Gaode]\n[Geocoder.ca][Geocoder.ca] (Geolytica)\n[GeocodeFarm][GeocodeFarm]\n[GeoNames][GeoNames]\n[GeoOttawa][GeoOttawa]\n[Gisgraphy][Gisgraphy]\n[Google][Google]\n[HERE][HERE]\n[IPInfo][IPInfo]\n[Komoot][Komoot] (OSM powered)\n[LocationIQ][LocationIQ]\n[Mapbox][Mapbox]\n[MapQuest][MapQuest]\n[~~Mapzen~~][Mapzen]\n[MaxMind][MaxMind]\n[OpenCage][OpenCage]\n[OpenStreetMap][OpenStreetMap]\n[Tamu][Tamu]\n[TGOS][TGOS]\n[TomTom][TomTom]\n[USCensus][USCensus]\n[What3Words][What3Words]\n[Yahoo][Yahoo]\n[Yandex][Yandex]\n\nInstallation\n\nPyPi Install\n\nTo install Geocoder, simply:\n\nGitHub Install\n\nInstalling the latest version from Github:\n\nSnap Install\n\nTo install the stable geocoder [snap](https://snapcraft.io) in any of the [supported Linux distros](https://snapcraft.io/docs/core/install):\n\nIf you want to help testing the latest changes from the master branch, you can install it from the edge channel:\n\nThe installed snap will be updated automatically every time a new version is pushed to the store.\n\nFeedback\n\nPlease feel free to give any feedback on this module.\n\nSpeak up on Twitter [@DenisCarriere](https://twitter.com/DenisCarriere) and tell me how you use this Python Geocoder. New updates will be pushed to Twitter Hashtags [#python](https://twitter.com/search?q=%23python).\n\nContribution\n\nIf you find any bugs or any enhancements to recommend please send some of your comments/suggestions to the [Github Issues Page](https://github.com/DenisCarriere/geocoder/issues).\n\nSome way to contribute, from the most generic to the most detailed:\n\nDocumenting\n\nIf you are not comfortable with development, you can still contibute with the documentation.\n\n- review the documentation of a specific provider. Most of the time they are lacking details...\n- review the parameters for a specific method, compared to what is supported by the provider\n- review documentation for command line\n\nIf you miss any feature, just create an issue accordingly. Be sure to describe clearly your use case, and to provide links to the correct sources.\n\nCoding\n\n- add support for a new provider. _Documentation TBD_, starting point possible with [wip_guide](https://geocoder.readthedocs.io/wip_guide.html).\n- extend methods for an existing support, i.e support an additionnal API). _Documentation TBD_\n- extend support of an existing API, i.e, support more (json) fields from the response, or more parameters. _Documentation TBD_\n\nChangeLog\n\nSee [CHANGELOG.md](./CHANGELOG.md)"}, {"name": "geocoder", "tags": ["dev", "math", "ui", "web"], "summary": "Geocoder is a simple and consistent geocoding library.", "text": "This library is used to simplify and standardize geocoding tasks across different online providers, allowing developers to easily parse various JSON responses and access multiple geocoding features. With Geocoder, developers can retrieve latitude and longitude coordinates for locations from a variety of sources in a consistent and straightforward manner."}, {"name": "geojson-pydantic", "tags": ["data", "math"], "summary": "Pydantic data models for the GeoJSON spec.", "text": "geojson-pydantic\n\n   Pydantic models for GeoJSON.\n\n  \n\n  \n  \n\n  \n  \n  \n  \n  \n  \n  \n  \n\n  \n\n---\n\n**Documentation**: https://developmentseed.org/geojson-pydantic/\n\n**Source Code**: https://github.com/developmentseed/geojson-pydantic\n\n---\n\nDescription\n\n`geojson_pydantic` provides a suite of Pydantic models matching the [GeoJSON specification rfc7946](https://datatracker.ietf.org/doc/html/rfc7946). Those models can be used for creating or validating geojson data.\n\nInstall\n\nOr install from source:\n\nInstall with conda from [`conda-forge`](https://anaconda.org/conda-forge/geojson-pydantic):\n\nContributing\n\nSee [CONTRIBUTING.md](https://github.com/developmentseed/geojson-pydantic/blob/main/CONTRIBUTING.md).\n\nChanges\n\nSee [CHANGES.md](https://github.com/developmentseed/geojson-pydantic/blob/main/CHANGELOG.md).\n\nAuthors\n\nInitial implementation by @geospatial-jeff; taken liberally from https://github.com/arturo-ai/stac-pydantic/\n\nSee [contributors](hhttps://github.com/developmentseed/geojson-pydantic/graphs/contributors) for a listing of individual contributors.\n\nLicense\n\nSee [LICENSE](https://github.com/developmentseed/geojson-pydantic/blob/main/LICENSE)"}, {"name": "geojson-pydantic", "tags": ["data", "math"], "summary": "Pydantic data models for the GeoJSON spec.", "text": "This library is used to create or validate GeoJSON data with a suite of Pydantic models that match the official specification. By using this library, developers can ensure their geospatial data conforms to industry standards and automate validation and serialization tasks."}, {"name": "geojson", "tags": ["data", "math"], "summary": "Python bindings and utilities for GeoJSON", "text": "geojson\n==============\n\n:target: https://github.com/jazzband/geojson/actions/workflows/test.yml\n   :alt: GitHub Actions\n.. image:: https://img.shields.io/codecov/c/github/jazzband/geojson.svg\n   :target: https://codecov.io/github/jazzband/geojson?branch=main\n   :alt: Codecov\n\n:target: https://jazzband.co/\n   :alt: Jazzband\n.. image:: https://img.shields.io/pypi/dm/geojson.svg\n   :target: https://pypi.org/project/geojson/\n   :alt: PyPI\n\nThis Python library contains:\n\n- Functions for encoding and decoding GeoJSON_ formatted data\n- Classes for all GeoJSON Objects\n- An implementation of the Python `__geo_interface__ Specification`_\n\n**Table of Contents**\n\n.. contents::\n   :backlinks: none\n   :local:\n\nInstallation\n------------\n\ngeojson is compatible with Python 3.7 - 3.13. The recommended way to install is via pip_:\n\n.. code::\n\n.. _PyPi as 'geojson': https://pypi.python.org/pypi/geojson/\n.. _pip: https://www.pip-installer.org\n\nGeoJSON Objects\n---------------\n\nThis library implements all the `GeoJSON Objects`_ described in `The GeoJSON Format Specification`_.\n\n.. _GeoJSON Objects: https://tools.ietf.org/html/rfc7946#section-3\n\nAll object keys can also be used as attributes.\n\nThe objects contained in GeometryCollection and FeatureCollection can be indexed directly.\n\nPoint\n~~~~~\n\n.. code:: python\n\n>>> from geojson import Point\n\n>>> Point((-115.81, 37.24))  # doctest: +ELLIPSIS\n  {\"coordinates\": [-115.8..., 37.2...], \"type\": \"Point\"}\n\nVisualize the result of the example above `here `__. General information about Point can be found in `Section 3.1.2`_ and `Appendix A: Points`_ within `The GeoJSON Format Specification`_.\n\n.. _Section 3.1.2: https://tools.ietf.org/html/rfc7946#section-3.1.2\n.. _Appendix A\\: Points: https://tools.ietf.org/html/rfc7946#appendix-A.1\n\nMultiPoint\n~~~~~~~~~~\n\n.. code:: python\n\n>>> from geojson import MultiPoint\n\n>>> MultiPoint([(-155.52, 19.61), (-156.22, 20.74), (-157.97, 21.46)])  # doctest: +ELLIPSIS\n  {\"coordinates\": [[-155.5..., 19.6...], [-156.2..., 20.7...], [-157.9..., 21.4...]], \"type\": \"MultiPoint\"}\n\nVisualize the result of the example above `here `__. General information about MultiPoint can be found in `Section 3.1.3`_ and `Appendix A: MultiPoints`_ within `The GeoJSON Format Specification`_.\n\n.. _Section 3.1.3: https://tools.ietf.org/html/rfc7946#section-3.1.3\n.. _Appendix A\\: MultiPoints: https://tools.ietf.org/html/rfc7946#appendix-A.4\n\nLineString\n~~~~~~~~~~\n\n.. code:: python\n\n>>> from geojson import LineString\n\n>>> LineString([(8.919, 44.4074), (8.923, 44.4075)])  # doctest: +ELLIPSIS\n  {\"coordinates\": [[8.91..., 44.407...], [8.92..., 44.407...]], \"type\": \"LineString\"}\n\nVisualize the result of the example above `here `__. General information about LineString can be found in `Section 3.1.4`_ and `Appendix A: LineStrings`_ within `The GeoJSON Format Specification`_.\n\n.. _Section 3.1.4: https://tools.ietf.org/html/rfc7946#section-3.1.4\n.. _Appendix A\\: LineStrings: https://tools.ietf.org/html/rfc7946#appendix-A.2\n\nMultiLineString\n~~~~~~~~~~~~~~~\n\n.. code:: python\n\n>>> from geojson import MultiLineString\n\n>>> MultiLineString([\n  ...     [(3.75, 9.25), (-130.95, 1.52)],\n  ...     [(23.15, -34.25), (-1.35, -4.65), (3.45, 77.95)]\n  ... ])  # doctest: +ELLIPSIS\n  {\"coordinates\": [[[3.7..., 9.2...], [-130.9..., 1.52...]], [[23.1..., -34.2...], [-1.3..., -4.6...], [3.4..., 77.9...]]], \"type\": \"MultiLineString\"}\n\nVisualize the result of the example above `here `__. General information about MultiLineString can be found in `Section 3.1.5`_ and `Appendix A: MultiLineStrings`_ within `The GeoJSON Format Specification`_.\n\n.. _Section 3.1.5: https://tools.ietf.org/html/rfc7946#section-3.1.5\n.. _Appendix A\\: MultiLineStrings: https://tools.ietf.org/html/rfc7946#appendix-A.5\n\nPolygon\n~~~~~~~\n\n.. code:: python\n\n>>> from geojson import Polygon\n\n>>> # no hole within polygon\n  >>> Polygon([[(2.38, 57.322), (-120.43, 19.15), (23.194, -20.28), (2.38, 57.322)]])  # doctest: +ELLIPSIS\n  {\"coordinates\": [[[2.3..., 57.32...], [-120.4..., 19.1...], [23.19..., -20.2...]]], \"type\": \"Polygon\"}\n\n>>> # hole within polygon\n  >>> Polygon([\n  ...     [(2.38, 57.322), (-120.43, 19.15), (23.194, -20.28), (2.38, 57.322)],\n  ...     [(-5.21, 23.51), (15.21, -10.81), (-20.51, 1.51), (-5.21, 23.51)]\n  ... ])  # doctest: +ELLIPSIS\n  {\"coordinates\": [[[2.3..., 57.32...], [-120.4..., 19.1...], [23.19..., -20.2...]], [[-5.2..., 23.5...], [15.2..., -10.8...], [-20.5..., 1.5...], [-5.2..., 23.5...]]], \"type\": \"Polygon\"}"}, {"name": "geojson", "tags": ["data", "math"], "summary": "Python bindings and utilities for GeoJSON", "text": "Visualize the results of the example above `here `__. General information about Polygon can be found in `Section 3.1.6`_ and `Appendix A: Polygons`_ within `The GeoJSON Format Specification`_.\n\n.. _Section 3.1.6: https://tools.ietf.org/html/rfc7946#section-3.1.6\n.. _Appendix A\\: Polygons: https://tools.ietf.org/html/rfc7946#appendix-A.3\n\nMultiPolygon\n~~~~~~~~~~~~\n\n.. code:: python\n\n>>> from geojson import MultiPolygon\n\n>>> MultiPolygon([\n  ...     ([(3.78, 9.28), (-130.91, 1.52), (35.12, 72.234), (3.78, 9.28)],),\n  ...     ([(23.18, -34.29), (-1.31, -4.61), (3.41, 77.91), (23.18, -34.29)],)\n  ... ])  # doctest: +ELLIPSIS\n  {\"coordinates\": [[[[3.7..., 9.2...], [-130.9..., 1.5...], [35.1..., 72.23...]]], [[[23.1..., -34.2...], [-1.3..., -4.6...], [3.4..., 77.9...]]]], \"type\": \"MultiPolygon\"}\n\nVisualize the result of the example above `here `__. General information about MultiPolygon can be found in `Section 3.1.7`_ and `Appendix A: MultiPolygons`_ within `The GeoJSON Format Specification`_.\n\n.. _Section 3.1.7: https://tools.ietf.org/html/rfc7946#section-3.1.7\n.. _Appendix A\\: MultiPolygons: https://tools.ietf.org/html/rfc7946#appendix-A.6\n\nGeometryCollection\n~~~~~~~~~~~~~~~~~~\n\n.. code:: python\n\n>>> from geojson import GeometryCollection, Point, LineString\n\n>>> my_point = Point((23.532, -63.12))\n\n>>> my_line = LineString([(-152.62, 51.21), (5.21, 10.69)])\n\n>>> geo_collection = GeometryCollection([my_point, my_line])\n\n>>> geo_collection  # doctest: +ELLIPSIS\n  {\"geometries\": [{\"coordinates\": [23.53..., -63.1...], \"type\": \"Point\"}, {\"coordinates\": [[-152.6..., 51.2...], [5.2..., 10.6...]], \"type\": \"LineString\"}], \"type\": \"GeometryCollection\"}\n\n>>> geo_collection[1]\n  {\"coordinates\": [[-152.62, 51.21], [5.21, 10.69]], \"type\": \"LineString\"}\n\n>>> geo_collection[0] == geo_collection.geometries[0]\n  True\n\nVisualize the result of the example above `here `__. General information about GeometryCollection can be found in `Section 3.1.8`_ and `Appendix A: GeometryCollections`_ within `The GeoJSON Format Specification`_.\n\n.. _Section 3.1.8: https://tools.ietf.org/html/rfc7946#section-3.1.8\n.. _Appendix A\\: GeometryCollections: https://tools.ietf.org/html/rfc7946#appendix-A.7\n\nFeature\n~~~~~~~\n\n.. code:: python\n\n>>> from geojson import Feature, Point\n\n>>> my_point = Point((-3.68, 40.41))\n\n>>> Feature(geometry=my_point)  # doctest: +ELLIPSIS\n  {\"geometry\": {\"coordinates\": [-3.68..., 40.4...], \"type\": \"Point\"}, \"properties\": {}, \"type\": \"Feature\"}\n\n>>> Feature(geometry=my_point, properties={\"country\": \"Spain\"})  # doctest: +ELLIPSIS\n  {\"geometry\": {\"coordinates\": [-3.68..., 40.4...], \"type\": \"Point\"}, \"properties\": {\"country\": \"Spain\"}, \"type\": \"Feature\"}\n\n>>> Feature(geometry=my_point, id=27)  # doctest: +ELLIPSIS\n  {\"geometry\": {\"coordinates\": [-3.68..., 40.4...], \"type\": \"Point\"}, \"id\": 27, \"properties\": {}, \"type\": \"Feature\"}\n\nVisualize the results of the examples above `here `__. General information about Feature can be found in `Section 3.2`_ within `The GeoJSON Format Specification`_.\n\n.. _Section 3.2: https://tools.ietf.org/html/rfc7946#section-3.2\n\nFeatureCollection\n~~~~~~~~~~~~~~~~~\n\n.. code:: python\n\n>>> from geojson import Feature, Point, FeatureCollection\n\n>>> my_feature = Feature(geometry=Point((1.6432, -19.123)))\n\n>>> my_other_feature = Feature(geometry=Point((-80.234, -22.532)))\n\n>>> feature_collection = FeatureCollection([my_feature, my_other_feature])\n\n>>> feature_collection # doctest: +ELLIPSIS\n  {\"features\": [{\"geometry\": {\"coordinates\": [1.643..., -19.12...], \"type\": \"Point\"}, \"properties\": {}, \"type\": \"Feature\"}, {\"geometry\": {\"coordinates\": [-80.23..., -22.53...], \"type\": \"Point\"}, \"properties\": {}, \"type\": \"Feature\"}], \"type\": \"FeatureCollection\"}\n\n>>> feature_collection.errors()\n  []\n\n>>> (feature_collection[0] == feature_collection['features'][0], feature_collection[1] == my_other_feature)\n  (True, True)\n\nVisualize the result of the example above `here `__. General information about FeatureCollection can be found in `Section 3.3`_ within `The GeoJSON Format Specification`_.\n\n.. _Section 3.3: https://tools.ietf.org/html/rfc7946#section-3.3\n\nGeoJSON encoding/decoding\n-------------------------\n\nAll of the GeoJSON Objects implemented in this library can be encoded and decoded into raw GeoJSON with the ``geojson.dump``, ``geojson.dumps``, ``geojson.load``, and ``geojson.loads`` functions. Note that each of these functions is a wrapper around the core `json` function with the same name, and will pass through any additional arguments. This allows you to control the JSON formatting or parsing behavior with the underlying core `json` functions.\n\n.. code:: python\n\n>>> import geojson\n\n>>> my_point = geojson.Point((43.24, -1.532))"}, {"name": "geojson", "tags": ["data", "math"], "summary": "Python bindings and utilities for GeoJSON", "text": ">>> my_point  # doctest: +ELLIPSIS\n  {\"coordinates\": [43.2..., -1.53...], \"type\": \"Point\"}\n\n>>> dump = geojson.dumps(my_point, sort_keys=True)\n\n>>> dump  # doctest: +ELLIPSIS\n  '{\"coordinates\": [43.2..., -1.53...], \"type\": \"Point\"}'\n\n>>> geojson.loads(dump)  # doctest: +ELLIPSIS\n  {\"coordinates\": [43.2..., -1.53...], \"type\": \"Point\"}\n\nCustom classes\n~~~~~~~~~~~~~~\n\nThis encoding/decoding functionality shown in the previous can be extended to custom classes using the interface described by the `__geo_interface__ Specification`_.\n\n.. code:: python\n\n>>> import geojson\n\n>>> class MyPoint():\n  ...     def __init__(self, x, y):\n  ...         self.x = x\n  ...         self.y = y\n  ...\n  ...     @property\n  ...     def __geo_interface__(self):\n  ...         return {'type': 'Point', 'coordinates': (self.x, self.y)}\n\n>>> point_instance = MyPoint(52.235, -19.234)\n\n>>> geojson.dumps(point_instance, sort_keys=True)  # doctest: +ELLIPSIS\n  '{\"coordinates\": [52.23..., -19.23...], \"type\": \"Point\"}'\n\nDefault and custom precision\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nGeoJSON Object-based classes in this package have an additional `precision` attribute which rounds off\ncoordinates to 6 decimal places (roughly 0.1 meters) by default and can be customized per object instance.\n\n.. code:: python\n\n>>> from geojson import Point\n\n>>> Point((-115.123412341234, 37.123412341234))  # rounded to 6 decimal places by default\n  {\"coordinates\": [-115.123412, 37.123412], \"type\": \"Point\"}\n\n>>> Point((-115.12341234, 37.12341234), precision=8)  # rounded to 8 decimal places\n  {\"coordinates\": [-115.12341234, 37.12341234], \"type\": \"Point\"}\n\nPrecision can be set at the package level by setting `geojson.geometry.DEFAULT_PRECISION`\n\n.. code:: python\n\n>>> import geojson\n\n>>> geojson.geometry.DEFAULT_PRECISION = 5\n\n>>> from geojson import Point\n\n>>> Point((-115.12341234, 37.12341234))  # rounded to 8 decimal places\n  {\"coordinates\": [-115.12341, 37.12341], \"type\": \"Point\"}\n\nAfter setting the DEFAULT_PRECISION, coordinates will be rounded off to that precision with `geojson.load` or `geojson.loads`. Following one of those with `geojson.dump` is a quick and easy way to scale down the precision of excessively precise, arbitrarily-sized GeoJSON data.\n\nHelpful utilities\n-----------------\n\ncoords\n~~~~~~\n\n:code:`geojson.utils.coords` yields all coordinate tuples from a geometry or feature object.\n\n.. code:: python\n\n>>> import geojson\n\n>>> my_line = LineString([(-152.62, 51.21), (5.21, 10.69)])\n\n>>> my_feature = geojson.Feature(geometry=my_line)\n\n>>> list(geojson.utils.coords(my_feature))  # doctest: +ELLIPSIS\n  [(-152.62..., 51.21...), (5.21..., 10.69...)]\n\nmap_coords\n~~~~~~~~~~\n\n:code:`geojson.utils.map_coords` maps a function over all coordinate values and returns a geometry of the same type. Useful for scaling a geometry.\n\n.. code:: python\n\n>>> import geojson\n\n>>> new_point = geojson.utils.map_coords(lambda x: x/2, geojson.Point((-115.81, 37.24)))\n\n>>> geojson.dumps(new_point, sort_keys=True)  # doctest: +ELLIPSIS\n  '{\"coordinates\": [-57.905..., 18.62...], \"type\": \"Point\"}'\n\nmap_tuples\n~~~~~~~~~~\n\n:code:`geojson.utils.map_tuples` maps a function over all coordinates and returns a geometry of the same type. Useful for changing coordinate order or applying coordinate transforms.\n\n.. code:: python\n\n>>> import geojson\n\n>>> new_point = geojson.utils.map_tuples(lambda c: (c[1], c[0]), geojson.Point((-115.81, 37.24)))\n\n>>> geojson.dumps(new_point, sort_keys=True)  # doctest: +ELLIPSIS\n  '{\"coordinates\": [37.24..., -115.81], \"type\": \"Point\"}'\n\nmap_geometries\n~~~~~~~~~~~~~~\n\n:code:`geojson.utils.map_geometries` maps a function over each geometry in the input.\n\n.. code:: python\n\n>>> import geojson\n\n>>> new_point = geojson.utils.map_geometries(lambda g: geojson.MultiPoint([g[\"coordinates\"]]), geojson.GeometryCollection([geojson.Point((-115.81, 37.24))]))\n\n>>> geojson.dumps(new_point, sort_keys=True)\n  '{\"geometries\": [{\"coordinates\": [[-115.81, 37.24]], \"type\": \"MultiPoint\"}], \"type\": \"GeometryCollection\"}'\n\nvalidation\n~~~~~~~~~~\n\n:code:`is_valid` property provides simple validation of GeoJSON objects.\n\n.. code:: python\n\n>>> import geojson\n\n>>> obj = geojson.Point((-3.68,40.41,25.14,10.34))\n  >>> obj.is_valid\n  False\n\n:code:`errors` method provides collection of errors when validation GeoJSON objects.\n\n.. code:: python\n\n>>> import geojson\n\n>>> obj = geojson.Point((-3.68,40.41,25.14,10.34))\n  >>> obj.errors()\n  'a position must have exactly 2 or 3 values'\n\ngenerate_random\n~~~~~~~~~~~~~~~"}, {"name": "geojson", "tags": ["data", "math"], "summary": "Python bindings and utilities for GeoJSON", "text": ":code:`geojson.utils.generate_random` yields a geometry type with random data\n\n.. code:: python\n\n>>> import geojson\n\n>>> geojson.utils.generate_random(\"LineString\")  # doctest: +ELLIPSIS\n  {\"coordinates\": [...], \"type\": \"LineString\"}\n\n>>> geojson.utils.generate_random(\"Polygon\")  # doctest: +ELLIPSIS\n  {\"coordinates\": [...], \"type\": \"Polygon\"}\n\nDevelopment\n-----------\n\nTo build this project, run :code:`python setup.py build`.\nTo run the unit tests, run :code:`python -m pip install tox && tox`.\nTo run the style checks, run :code:`flake8` (install `flake8` if needed).\n\nCredits\n-------\n\n* Sean Gillies \n* Matthew Russell \n* Corey Farwell \n* Blake Grotewold \n* Zsolt Ero \n* Sergey Romanov \n* Ray Riga\n\n.. _GeoJSON: https://geojson.org/\n.. _The GeoJSON Format Specification: https://tools.ietf.org/html/rfc7946\n.. _\\_\\_geo\\_interface\\_\\_ Specification: https://gist.github.com/sgillies/2217756"}, {"name": "geojson", "tags": ["data", "math"], "summary": "Python bindings and utilities for GeoJSON", "text": "This library is used to parse, serialize, and manipulate GeoJSON formatted data in Python, providing classes for all GeoJSON objects and functions for encoding and decoding. Developers can use this library to work with geospatial data in their applications, leveraging the widely adopted GeoJSON standard."}, {"name": "geomet", "tags": ["data", "math"], "summary": "Pure Python conversion library for common geospatial data formats", "text": "GeoMet (https://app.circleci.com/pipelines/github/geomet)\n\nPure-Python conversion library for common geospatial data formats.\nSupported formats include:\n\nInstall\n\nInstall the latest version from [PyPI](https://pypi.org/project/geomet/):\n\nFunctionality\n\nConverion functions are exposed through idiomatic `load/loads/dump/dumps`\ninterfaces.\n\nGeoMet is intended to cover all common use cases for dealing with 2D, 3D, and\n4D geometries (including 'Z', 'M', and 'ZM').\n\nGeometry\n--------\nPoint\nLineString\nPolygon\nMultiPoint\nMultiLineString\nMultiPolygon\nGeometryCollection\n\nExample usage\n\nCoverting a 'Point' GeoJSON object to WKT:\n\nConverting a 'Point' GeoJSON object to WKB:\n\nConverting a 'Point' GeoJSON object to GeoPackage Binary:\n\nConverting a 'LineString' GeoJSON object to WKT:\n\nConverting a 'LineString' GeoJSON object to WKB:\n\nConverting a 'LineString' GeoJSON object to GeoPackage Binary:\n\nConverting 'Point' WKT to GeoJSON:\n\nCoverting 'GeometryCollection' WKT to GeoJSON:\n\n[EWKT/EWKB](http://postgis.net/documentation/manual-2.1/using_postgis_dbmanagement.html#EWKB_EWKT) \nare also supported for all geometry types. This uses a custom extension\nto the GeoJSON standard in order to preserve SRID information through conversions.\nFor example:\n\nGeoPackage binary supports encoding of SRID and envelope information. If your geopackage\nhas an envelope specified, then it will be added into the resulting GeoJSON in a key \ncalled `'bbox'`:\n\nIn the same way, if a 'bbox' key is present on a `dumps`-ed geometry, it will be added to the \nheader of the GeoPackage geometry:\n\nIf an integer SRID identifier is present in a `'meta'` key (like `'meta': {'srid': 4326}`), then the SRID will be included in the\nGeoPackage header.\n\nHistory\n\nThis library was originally created as the result of a bug report related\nto another project: https://bugs.launchpad.net/openquake-old/+bug/1073909.\nThe source of this issue was largely due to a dependency on\n[GEOS](https://libgeos.org/), which is written in C/C++. Depending on GEOS\nrequires any data conversion bug fixes to happen upstream, which takes time\nand effort. Ultimately, this was the inspiration to create a more\nlightweight, pure-Python conversion library as an alterntive tool for\nreliably converting data between various geospatial formats.\n\nThe name \"GeoMet\" was inspired by \"met\", the German word for\n[mead](http://en.wikipedia.org/wiki/Mead). It is also a shortened version of\nthe word \"geometry\".\n\nLimitations\n\nOutputing \"empty\" geometries to binary formats is not supported\n\nAttempting to output an empty geometry to a binary format will result in an exception: `ValueError: Empty geometries cannot be represented in WKB. Reason: The dimensionality of the WKB would be ambiguous.` There are a few reasons for this this limitation:\n\nAs a result, GeoMet has chosen to not attempt to address these problems, and\nsimply raise an exception instead.\n\nExample:\n\nSee also"}, {"name": "geomet", "tags": ["data", "math"], "summary": "Pure Python conversion library for common geospatial data formats", "text": "This library is used to convert between various common geospatial data formats, including 2D, 3D, and 4D geometries, through idiomatic Python interfaces. Developers can use this library to easily exchange data between different formats, such as GeoJSON, WKT, WKB, and GeoPackage Binary."}, {"name": "geopandas", "tags": ["math"], "summary": "Geographic pandas extensions", "text": "GeoPandas is a project to add support for geographic data to\n`pandas`_ objects.\n\nThe goal of GeoPandas is to make working with geospatial data in\npython easier. It combines the capabilities of `pandas`_ and `shapely`_,\nproviding geospatial operations in pandas and a high-level interface\nto multiple geometries to shapely. GeoPandas enables you to easily do\noperations in python that would otherwise require a spatial database\nsuch as PostGIS.\n\n.. _pandas: https://pandas.pydata.org\n.. _shapely: https://shapely.readthedocs.io/en/latest/"}, {"name": "geopandas", "tags": ["math"], "summary": "Geographic pandas extensions", "text": "This library is used to extend the functionality of pandas objects to support geographic data, enabling developers to perform geospatial operations and manipulate multiple geometries in Python. With GeoPandas, you can easily work with geospatial data without requiring a spatial database like PostGIS."}, {"name": "geopy", "tags": ["math", "web"], "summary": "Python Geocoding Toolbox", "text": "geopy\n=====\n\n.. image:: https://img.shields.io/pypi/v/geopy.svg?style=flat-square\n\n.. image:: https://img.shields.io/github/actions/workflow/status/geopy/geopy/ci.yml?branch=master&style=flat-square\n\n.. image:: https://img.shields.io/github/license/geopy/geopy.svg?style=flat-square\n\ngeopy is a Python client for several popular geocoding web\nservices.\n\ngeopy makes it easy for Python developers to locate the coordinates of\naddresses, cities, countries, and landmarks across the globe using\nthird-party geocoders and other data sources.\n\ngeopy includes geocoder classes for the `OpenStreetMap Nominatim`_,\n`Google Geocoding API (V3)`_, and many other geocoding services.\nThe full list is available on the `Geocoders doc section`_.\nGeocoder classes are located in `geopy.geocoders`_.\n\n.. _OpenStreetMap Nominatim: https://nominatim.org\n.. _Google Geocoding API (V3): https://developers.google.com/maps/documentation/geocoding/\n.. _Geocoders doc section: https://geopy.readthedocs.io/en/latest/#geocoders\n.. _geopy.geocoders: https://github.com/geopy/geopy/tree/master/geopy/geocoders\n\ngeopy is tested against CPython (versions 3.7, 3.8, 3.9, 3.10, 3.11, 3.12)\nand PyPy3. geopy 1.x line also supported CPython 2.7, 3.4 and PyPy2.\n\n\u00a9 geopy contributors 2006-2018 (see AUTHORS) under the `MIT\nLicense `__.\n\nInstallation\n------------\n\nInstall using `pip `__ with:\n\n::\n\nOr, `download a wheel or source archive from\nPyPI `__.\n\nGeocoding\n---------\n\nTo geolocate a query to an address and coordinates:\n\n.. code:: pycon\n\nTo find the address corresponding to a set of coordinates:\n\n.. code:: pycon\n\nMeasuring Distance\n------------------\n\nGeopy can calculate geodesic distance between two points using the\n`geodesic distance\n`_ or the\n`great-circle distance\n`_,\nwith a default of the geodesic distance available as the function\n`geopy.distance.distance`.\n\nHere's an example usage of the geodesic distance, taking pair\nof :code:`(lat, lon)` tuples:\n\n.. code:: pycon\n\nUsing great-circle distance, also taking pair of :code:`(lat, lon)` tuples:\n\n.. code:: pycon\n\nDocumentation\n-------------\n\nMore documentation and examples can be found at\n`Read the Docs `__."}, {"name": "geopy", "tags": ["math", "web"], "summary": "Python Geocoding Toolbox", "text": "This library is used to locate coordinates of addresses, cities, countries, and landmarks across the globe using third-party geocoding web services. Developers can use geopy to easily integrate geolocation functionality into their applications."}, {"name": "gin-config", "tags": ["math", "ml"], "summary": "Gin-Config: A lightweight configuration library for Python", "text": "Gin\n\nGin provides a lightweight configuration framework for Python, based on\ndependency injection. Functions or classes can be decorated with\n`@gin.configurable`, allowing default parameter values to be supplied from a\nconfig file (or passed via the command line) using a simple but powerful syntax.\nThis removes the need to define and maintain configuration objects (e.g.\nprotos), or write boilerplate parameter plumbing and factory code, while often\ndramatically expanding a project's flexibility and configurability.\n\nGin is particularly well suited for machine learning experiments (e.g. using\nTensorFlow), which tend to have many parameters, often nested in complex ways.\n\n**Authors**: Dan Holtmann-Rice, Sergio Guadarrama, Nathan Silberman\n**Contributors**: Oscar Ramirez, Marek Fiser"}, {"name": "gin-config", "tags": ["math", "ml"], "summary": "Gin-Config: A lightweight configuration library for Python", "text": "This library is used to simplify configuration management for Python applications by allowing functions and classes to be easily configured from a file or command line. With Gin-Config, developers can remove boilerplate code and improve flexibility in their projects."}, {"name": "glfw", "tags": ["math", "visualization"], "summary": "A ctypes-based wrapper for GLFW3.", "text": "pyGLFW\n======\n\nThis module provides Python bindings for `GLFW `__\n(on GitHub: `glfw/glfw `__). It is a\n``ctypes`` wrapper which keeps very close to the original GLFW API,\nexcept for:\n\n-  function names use the pythonic ``words_with_underscores`` notation\n   instead of ``camelCase``\n-  ``GLFW_`` and ``glfw`` prefixes have been removed, as their function\n   is replaced by the module namespace\n   (you can use ``from glfw.GLFW import *`` if you prefer the naming\n   convention used by the GLFW C API)\n-  structs have been replaced with Python sequences and namedtuples\n-  functions like ``glfwGetMonitors`` return a list instead of a pointer\n   and an object count\n-  Gamma ramps use floats between 0.0 and 1.0 instead of unsigned shorts\n   (use ``glfw.NORMALIZE_GAMMA_RAMPS=False`` to disable this)\n-  GLFW errors are reported as ``glfw.GLFWError`` warnings if no error\n   callback is set (use ``glfw.ERROR_REPORTING=False`` to disable this,\n   set it to 'warn' instead to issue warnings, set it to 'log' to log it\n   using the 'glfw' logger or set it to a dict to define the behavior for\n   specific error codes)\n-  instead of a sequence for ``GLFWimage`` structs, PIL/pillow ``Image``\n   objects can be used\n\nInstallation\n------------\n\npyGLFW can be installed using pip:\n\n.. code:: sh\n\nWindows\n~~~~~~~\n\nThe GLFW shared library and Visual C++ runtime are included in the Python wheels.\n\nTo use a different GLFW library, you can set ``PYGLFW_LIBRARY`` to its location.\n\nmacOS\n~~~~~\n\nThe GLFW shared library for 64-bit is included in the Python wheels for macOS.\n\nIf you are using a 32-bit Python installation or otherwise cannot use the\nlibrary downloaded with the wheel, you can build and install it yourself by\n`compiling GLFW from source `__\n(use ``-DBUILD_SHARED_LIBS=ON``).\n\npyGLFW will search for the library in a list of search paths (including those\nin ``DYLD_LIBRARY_PATH``). If you want to use a specific library, you can set\nthe ``PYGLFW_LIBRARY`` environment variable to its path.\n\nLinux\n~~~~~\n\nThe GLFW shared library is included in the Python wheels for Linux. Although\npyGLFW will try to detect whether the GLFW library for Wayland or X11 should\nbe used, you can set the ``PYGLFW_LIBRARY_VARIANT`` variable to ``wayland`` or\n``x11`` to select either variant of the library.\n\nIf you cannot use these on your system, you can install the GLFW shared\nlibrary using a package management system (e.g. ``apt install libglfw3``\non Debian or Ubuntu) or you can build and install it yourself by\n`compiling GLFW from source `__\n(use ``-DBUILD_SHARED_LIBS=ON``).\n\npyGLFW will search for the library in a list of search paths (including those\nin ``LD_LIBRARY_PATH``). If you want to use a specific library, you can set\nthe ``PYGLFW_LIBRARY`` environment variable to its path.\n\ncx_Freeze / PyInstaller\n~~~~~~~~~~~~~~~~~~~~~~~\n\npyGLFW will search for the GLFW library in the current working directory, the directory\nof the executable and in the package on non-Windows platforms if running in an\nexecutable frozen with cx_Freeze or PyInstaller, unless the ``PYGLFW_LIBRARY``\nenvironment variable is set.\n\nDevelopment Version\n~~~~~~~~~~~~~~~~~~~\n\nIf you are using the development version of GLFW and would like to use wrappers\nfor currently unreleased macros and functions, you can instead install:\n\n.. code:: sh\n\nor set the ``PYGLFW_PREVIEW`` environment variable.\n\nNote, however, that there will be a slight delay between the development\nversion of GLFW and the wrappers provided by this package.\n\nExample Code\n------------\n\nThe example from the `GLFW\ndocumentation `__ ported to\npyGLFW:\n\n.. code:: python"}, {"name": "glfw", "tags": ["math", "visualization"], "summary": "A ctypes-based wrapper for GLFW3.", "text": "This library is used to provide a Python interface for the GLFW3 API, allowing developers to create cross-platform graphics applications with ease. With glfw, developers can access a wide range of features such as window and input management, monitor handling, and more."}, {"name": "google-cloud-pipeline-components", "tags": ["math", "ml", "web"], "summary": "This SDK enables a set of First Party (Google owned) pipeline components that allow users to take their experience from Vertex AI SDK and other Google Cloud services and create a corresponding pipeline using KFP or Managed Pipelines.", "text": "Google Cloud Pipeline Components\n\n(https://github.com/kubeflow/pipelines/tree/master/components/google-cloud)\n\n[Google Cloud Pipeline Components](https://cloud.google.com/vertex-ai/docs/pipelines/build-pipeline?hl=en#google-cloud-components) (GCPC) provides predefined [KFP](https://www.kubeflow.org/docs/components/pipelines/) components that can be run on Google Cloud Vertex AI Pipelines and other KFP-conformant pipeline execution backends. You can compose the components together into pipelines using the [Kubeflow Pipelines SDK](https://pypi.org/project/kfp/).\n\nDocumentation\n\nUser documentation\n\nPlease see the [Google Cloud Pipeline Components user guide](https://cloud.google.com/vertex-ai/docs/pipelines/components-introduction).\n\nAPI documentation\nPlease see the [Google Cloud Pipeline Components API reference documentation](https://google-cloud-pipeline-components.readthedocs.io/).\n\nRelease details\nFor details about previous and upcoming releases, please see the [release notes](https://github.com/kubeflow/pipelines/blob/master/components/google-cloud/RELEASE.md).\n\nExamples\n\n*   [Train an image classification model using Vertex AI AutoML](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/master/notebooks/official/pipelines/google_cloud_pipeline_components_automl_images.ipynb).\n*   [Train a classification model using tabular data and Vertex AI AutoML](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/master/notebooks/official/pipelines/automl_tabular_classification_beans.ipynb).\n*   [Train a linear regression model using tabular data and Vertex AI AutoML](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/master/notebooks/official/pipelines/google_cloud_pipeline_components_automl_tabular.ipynb).\n*   [Use the Google Cloud pipeline components to upload and deploy a model](https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/master/notebooks/official/pipelines/google_cloud_pipeline_components_model_train_upload_deploy.ipynb).\n\nInstallation\n\nRequirements\n\n-   Python >= 3.7\n-   [A Google Cloud project with the Vertex API enabled.](https://cloud.google.com/vertex-ai/docs/start/cloud-environment)\n-   An\n\nInstall latest release\n\nUse the following command to install Google Cloud Pipeline Components from [PyPI](https://pypi.org/project/google-cloud-pipeline-components/)."}, {"name": "google-cloud-pipeline-components", "tags": ["math", "ml", "web"], "summary": "This SDK enables a set of First Party (Google owned) pipeline components that allow users to take their experience from Vertex AI SDK and other Google Cloud services and create a corresponding pipeline using KFP or Managed Pipelines.", "text": "This library is used to enable the creation of pipelines that integrate with Google Cloud services, allowing users to automate tasks and workflows using Vertex AI SDK components. Developers can compose these components together into customized pipelines using the Kubeflow Pipelines SDK."}, {"name": "google-generativeai", "tags": ["math", "ml", "web"], "summary": "Google Generative AI High level API client library and tools.", "text": "[Deprecated] Google AI Python SDK for the Gemini API\n\nWith Gemini 2.0, we took the chance to create a single unified SDK for all developers who want to use Google's GenAI models (Gemini, Veo, Imagen, etc). As part of that process, we took all of the feedback from this SDK and what developers like about other SDKs in the ecosystem to create the [Google Gen AI SDK](https://github.com/googleapis/python-genai). \n\nThe full migration guide from the old SDK to new SDK is available in the [Gemini API docs](https://ai.google.dev/gemini-api/docs/migrate).\n\nThe Gemini API docs are fully updated to show examples of the new Google Gen AI SDK. We know how disruptive an SDK change can be and don't take this change lightly, but our goal is to create an extremely simple and clear path for developers to build with our models so it felt necessary to make this change.\n\nThank you for building with Gemini and [let us know](https://discuss.ai.google.dev/c/gemini-api/4) if you need any help!\n\n**Please be advised that this repository is now considered legacy.** For the latest features, performance improvements, and active development, we strongly recommend migrating to the official **[Google Generative AI SDK for Python](https://github.com/googleapis/python-genai)**.\n\n**Support Plan for this Repository:**\n\n*   **Limited Maintenance:** Development is now restricted to **critical bug fixes only**. No new features will be added.\n*   **Purpose:** This limited support aims to provide stability for users while they transition to the new SDK.\n*   **End-of-Life Date:** All support for this repository ended permanently on **November 30, 2025**.\n\nWe encourage all users to begin planning their migration to the [Google Generative AI SDK](https://github.com/googleapis/python-genai) to ensure continued access to the latest capabilities and support."}, {"name": "google-generativeai", "tags": ["math", "ml", "web"], "summary": "Google Generative AI High level API client library and tools.", "text": "This library is used to provide a unified client library and tools for accessing Google's Generative AI models, including Gemini, Veo, and Imagen. This library enables developers to easily integrate Google's GenAI capabilities into their applications with minimal complexity."}, {"name": "gprofiler-official", "tags": ["math", "web"], "summary": "Functional enrichment analysis and more via the g:Profiler toolkit", "text": "gprofiler\n\nProject description\n\nThe official Python 3 interface to the [g:Profiler](https://biit.cs.ut.ee/gprofiler/) \ntoolkit for enrichment analysis of functional (GO and other) terms, \nconversion between identifier namespaces and mapping orhologous genes in related organisms. \n\nIt has an optional dependency on pandas.\n\nInstalling gprofiler\n\nthe recommended way of installing gprofiler is using pip\n\nLegacy version \n\nThe `0.3.x` series of gprofiler-official is incompatible with the `1.0.x` series. We changed the major version number to \nsignify the breaking changes in the API. To install the previous version of `gprofiler-official`, use the command\n\nTools:\n\nTo use any of the tools in the g:Profiler toolkit, first initialize the GProfiler object.\n\ng:GOSt (profile)\n\nOutput:\n\n* `source` is the code for the datasource\n* `native` is the ID for the enriched term/functional category in its native namespace.\n* `name` is the readable name for the enriched term, `description` is the longer description if available.\n* `p_value` is the corrected p-value for the \n* `term_size`, `query_size`, `intersection_size`, `effective_domain_size` are parameters to the hypergeometric test.\n* `query` is the name of the query and is significant if multiple queries were made in one call (e.g `gp.profile(query={'query1':['NR1H4'], 'query2':['NR1H4','TRIP12']})`)\n\nSetting the parameter `no_evidences=False` would add the column `intersections` (a list of genes that are annotated to the term and are present in the query )\nand the column `evidences` (a list of lists of GO evidence codes for the intersecting genes)\n\nNB! the parameter `combined` significantly changes the output structure by packing the results of distinct queries together.\nFor example:\n\nOutput (truncated):\n\ng:Convert (convert)\n\nOutput:\n\n`incoming` column lists the input gene, `converted` lists the gene in the target namespace (Entrez Gene accession number in this case).\n\ng:Orth (orth)\n\nOutput:\n\n`incoming` is the input gene, `converted` is the canonical Ensembl ID for the input gene, \n`ortholog_ensg` is the canonical Ensembl ID for the orthologous gene in the target organism.\n\ng:SNPense (snpense)\n\nOutput:\n\n* `rs_id` is the input rs-number\n* `chromosome`, `strand`, `start` and `end` encode the position of the variation\n* `ensgs` and `gene_names` are lists of protein-encoding genes associated with the rs-number.\n* `variants` are predicted variant effects."}, {"name": "gprofiler-official", "tags": ["math", "web"], "summary": "Functional enrichment analysis and more via the g:Profiler toolkit", "text": "This library is used to perform enrichment analysis and conversion between different identifier namespaces, including functional terms and gene mappings across organisms. With this library, developers can automate tasks involving bioinformatics data and enrich their understanding of complex biological systems."}, {"name": "gradio-client", "tags": ["math", "ml", "web"], "summary": "Python library for easily interacting with trained machine learning models", "text": "`gradio_client`: Use a Gradio app as an API -- in 3 lines of Python\n\nThis directory contains the source code for `gradio_client`, a lightweight Python library that makes it very easy to use any Gradio app as an API.\n\nAs an example, consider this [Hugging Face Space that transcribes audio files](https://huggingface.co/spaces/abidlabs/whisper) that are recorded from the microphone.\n\nUsing the `gradio_client` library, we can easily use the Gradio as an API to transcribe audio files programmatically.\n\nHere's the entire code to do it:\n\nThe Gradio client works with any Gradio Space, whether it be an image generator, a stateful chatbot, or a tax calculator.\n\nInstallation\n\nIf you already have a recent version of `gradio`, then the `gradio_client` is included as a dependency.\n\nOtherwise, the lightweight `gradio_client` package can be installed from pip (or pip3) and works with Python versions 3.10 or higher:\n\nBasic Usage\n\nConnecting to a Space or a Gradio app\n\nStart by connecting instantiating a `Client` object and connecting it to a Gradio app that is running on Spaces (or anywhere else)!\n\n**Connecting to a Space**\n\nYou can also connect to private Spaces by passing in your HF token with the `hf_token` parameter. You can get your HF token here: https://huggingface.co/settings/tokens\n\n**Duplicating a Space for private use**\n\nWhile you can use any public Space as an API, you may get rate limited by Hugging Face if you make too many requests. For unlimited usage of a Space, simply duplicate the Space to create a private Space,\nand then use it to make as many requests as you'd like!\n\nThe `gradio_client` includes a class method: `Client.duplicate()` to make this process simple:\n\nIf you have previously duplicated a Space, re-running `duplicate()` will _not_ create a new Space. Instead, the Client will attach to the previously-created Space. So it is safe to re-run the `Client.duplicate()` method multiple times.\n\n**Note:** if the original Space uses GPUs, your private Space will as well, and your Hugging Face account will get billed based on the price of the GPU. To minimize charges, your Space will automatically go to sleep after 1 hour of inactivity. You can also set the hardware using the `hardware` parameter of `duplicate()`.\n\n**Connecting a general Gradio app**\n\nIf your app is running somewhere else, just provide the full URL instead, including the \"http://\" or \"https://\". Here's an example of making predictions to a Gradio app that is running on a share URL:\n\nInspecting the API endpoints\n\nOnce you have connected to a Gradio app, you can view the APIs that are available to you by calling the `.view_api()` method. For the Whisper Space, we see the following:\n\nThis shows us that we have 1 API endpoint in this space, and shows us how to use the API endpoint to make a prediction: we should call the `.predict()` method, providing a parameter `input_audio` of type `str`, which is a `filepath or URL`.\n\nWe should also provide the `api_name='/predict'` argument. Although this isn't necessary if a Gradio app has a single named endpoint, it does allow us to call different endpoints in a single app if they are available. If an app has unnamed API endpoints, these can also be displayed by running `.view_api(all_endpoints=True)`.\n\nMaking a prediction\n\nThe simplest way to make a prediction is simply to call the `.predict()` function with the appropriate arguments:\n\nIf there are multiple parameters, then you should pass them as separate arguments to `.predict()`, like this:\n\nFor certain inputs, such as images, you should pass in the filepath or URL to the file. Likewise, for the corresponding output types, you will get a filepath or URL returned.\n\nAdvanced Usage\n\nFor more ways to use the Gradio Python Client, check out our dedicated Guide on the Python client, available here: https://www.gradio.app/guides/getting-started-with-the-python-client"}, {"name": "gradio-client", "tags": ["math", "ml", "web"], "summary": "Python library for easily interacting with trained machine learning models", "text": "This library is used to interact with trained machine learning models via their Gradio apps, allowing developers to programmatically use these models in their own applications. With gradio-client, you can easily turn any Gradio Space into an API that can be accessed and utilized from your code."}, {"name": "gradio", "tags": ["cli", "dev", "math", "ml", "ui", "visualization", "web"], "summary": "Python library for easily interacting with trained machine learning models", "text": "Gradio: Build Machine Learning Web Apps \u2014 in Python\n\nGradio is an open-source Python package that allows you to quickly **build** a demo or web application for your machine learning model, API, or any arbitrary Python function. You can then **share** a link to your demo or web application in just a few seconds using Gradio's built-in sharing features. *No JavaScript, CSS, or web hosting experience needed!*\n\nIt just takes a few lines of Python to create your own demo, so let's get started\n\nInstallation\n\n**Prerequisite**: Gradio requires [Python 3.10 or higher](https://www.python.org/downloads/).\n\nWe recommend installing Gradio using `pip`, which is included by default in Python. Run this in your terminal or command prompt:\n\n> [!TIP]\n > It is best to install Gradio in a virtual environment. Detailed installation instructions for all common operating systems are provided here.\n\nBuilding Your First Demo\n\nYou can run Gradio in your favorite code editor, Jupyter notebook, Google Colab, or anywhere else you write Python. Let's write your first Gradio app:\n\n> [!TIP]\n > We shorten the imported name from gradio to gr. This is a widely adopted convention for better readability of code.\n\nNow, run your code. If you've written the Python code in a file named `app.py`, then you would run `python app.py` from the terminal.\n\nThe demo below will open in a browser on [http://localhost:7860](http://localhost:7860) if running from a file. If you are running within a notebook, the demo will appear embedded within the notebook.\n\nType your name in the textbox on the left, drag the slider, and then press the Submit button. You should see a friendly greeting on the right.\n\n> [!TIP]\n > When developing locally, you can run your Gradio app in hot reload mode, which automatically reloads the Gradio app whenever you make changes to the file. To do this, simply type in gradio before the name of the file instead of python. In the example above, you would type: `gradio app.py` in your terminal. You can also enable vibe mode by using the --vibe flag, e.g. gradio --vibe app.py, which provides an in-browser chat that can be used to write or edit your Gradio app using natural language. Learn more in the Hot Reloading Guide.\n\n**Understanding the `Interface` Class**\n\nYou'll notice that in order to make your first demo, you created an instance of the `gr.Interface` class. The `Interface` class is designed to create demos for machine learning models which accept one or more inputs, and return one or more outputs.\n\nThe `Interface` class has three core arguments:\n\n- `fn`: the function to wrap a user interface (UI) around\n- `inputs`: the Gradio component(s) to use for the input. The number of components should match the number of arguments in your function.\n- `outputs`: the Gradio component(s) to use for the output. The number of components should match the number of return values from your function."}, {"name": "gradio", "tags": ["cli", "dev", "math", "ml", "ui", "visualization", "web"], "summary": "Python library for easily interacting with trained machine learning models", "text": "The `fn` argument is very flexible -- you can pass *any* Python function that you want to wrap with a UI. In the example above, we saw a relatively simple function, but the function could be anything from a music generator to a tax calculator to the prediction function of a pretrained machine learning model.\n\nThe `inputs` and `outputs` arguments take one or more Gradio components. As we'll see, Gradio includes more than [30 built-in components](https://www.gradio.app/docs/gradio/introduction) (such as the `gr.Textbox()`, `gr.Image()`, and `gr.HTML()` components) that are designed for machine learning applications.\n\n> [!TIP]\n > For the `inputs` and `outputs` arguments, you can pass in the name of these components as a string (`\"textbox\"`) or an instance of the class (`gr.Textbox()`).\n\nIf your function accepts more than one argument, as is the case above, pass a list of input components to `inputs`, with each input component corresponding to one of the arguments of the function, in order. The same holds true if your function returns more than one value: simply pass in a list of components to `outputs`. This flexibility makes the `Interface` class a very powerful way to create demos.\n\nWe'll dive deeper into the `gr.Interface` on our series on [building Interfaces](https://www.gradio.app/main/guides/the-interface-class).\n\nSharing Your Demo\n\nWhat good is a beautiful demo if you can't share it? Gradio lets you easily share a machine learning demo without having to worry about the hassle of hosting on a web server. Simply set `share=True` in `launch()`, and a publicly accessible URL will be created for your demo. Let's revisit our example demo,  but change the last line as follows:\n\nWhen you run this code, a public URL will be generated for your demo in a matter of seconds, something like:\n\n&nbsp; `https://a23dsf231adb.gradio.live`\n\nNow, anyone around the world can try your Gradio demo from their browser, while the machine learning model and all computation continues to run locally on your computer.\n\nTo learn more about sharing your demo, read our dedicated guide on [sharing your Gradio application](https://www.gradio.app/guides/sharing-your-app).\n\nAn Overview of Gradio\n\nSo far, we've been discussing the `Interface` class, which is a high-level class that lets you build demos quickly with Gradio. But what else does Gradio include?\n\nCustom Demos with `gr.Blocks`\n\nGradio offers a low-level approach for designing web apps with more customizable layouts and data flows with the `gr.Blocks` class. Blocks supports things like controlling where components appear on the page, handling multiple data flows and more complex interactions (e.g. outputs can serve as inputs to other functions), and updating properties/visibility of components based on user interaction \u2014 still all in Python.\n\nYou can build very custom and complex applications using `gr.Blocks()`. For example, the popular image generation [Automatic1111 Web UI](https://github.com/AUTOMATIC1111/stable-diffusion-webui) is built using Gradio Blocks. We dive deeper into the `gr.Blocks` on our series on [building with Blocks](https://www.gradio.app/guides/blocks-and-event-listeners).\n\nChatbots with `gr.ChatInterface`"}, {"name": "gradio", "tags": ["cli", "dev", "math", "ml", "ui", "visualization", "web"], "summary": "Python library for easily interacting with trained machine learning models", "text": "Gradio includes another high-level class, `gr.ChatInterface`, which is specifically designed to create Chatbot UIs. Similar to `Interface`, you supply a function and Gradio creates a fully working Chatbot UI. If you're interested in creating a chatbot, you can jump straight to [our dedicated guide on `gr.ChatInterface`](https://www.gradio.app/guides/creating-a-chatbot-fast).\n\nThe Gradio Python & JavaScript Ecosystem\n\nThat's the gist of the core `gradio` Python library, but Gradio is actually so much more! It's an entire ecosystem of Python and JavaScript libraries that let you build machine learning applications, or query them programmatically, in Python or JavaScript. Here are other related parts of the Gradio ecosystem:\n\n* [Gradio Python Client](https://www.gradio.app/guides/getting-started-with-the-python-client) (`gradio_client`): query any Gradio app programmatically in Python.\n* [Gradio JavaScript Client](https://www.gradio.app/guides/getting-started-with-the-js-client) (`@gradio/client`): query any Gradio app programmatically in JavaScript.\n* [Hugging Face Spaces](https://huggingface.co/spaces): the most popular place to host Gradio applications \u2014 for free!\n\nWhat's Next?\n\nKeep learning about Gradio sequentially using the Gradio Guides, which include explanations as well as example code and embedded interactive demos. Next up: [let's dive deeper into the Interface class](https://www.gradio.app/guides/the-interface-class).\n\nOr, if you already know the basics and are looking for something specific, you can search the more [technical API documentation](https://www.gradio.app/docs/).\n\nGradio Sketch\n\nYou can also build Gradio applications without writing any code. Simply type `gradio sketch` into your terminal to open up an editor that lets you define and modify Gradio components, adjust their layouts, add events, all through a web editor. Or [use this hosted version of Gradio Sketch, running on Hugging Face Spaces](https://huggingface.co/spaces/aliabid94/Sketch).\n\nQuestions?\n\nIf you'd like to report a bug or have a feature request, please create an [issue on GitHub](https://github.com/gradio-app/gradio/issues/new/choose). For general questions about usage, we are available on [our Discord server](https://discord.com/invite/feTf9x3ZSB) and happy to help.\n\nIf you like Gradio, please leave us a \u2b50 on GitHub!\n\nOpen Source Stack\n\nGradio is built on top of many wonderful open-source libraries!\n\n[](https://huggingface.co)\n[](https://www.python.org)\n[](https://fastapi.tiangolo.com)\n[](https://www.encode.io)\n[](https://svelte.dev)\n[](https://vitejs.dev)\n[](https://pnpm.io)\n[](https://tailwindcss.com)\n[](https://storybook.js.org/)\n[](https://www.chromatic.com/)\n\nLicense\n\nGradio is licensed under the Apache License 2.0 found in the [LICENSE](LICENSE) file in the root directory of this repository.\n\nCitation\n\nAlso check out the paper _[Gradio: Hassle-Free Sharing and Testing of ML Models in the Wild](https://arxiv.org/abs/1906.02569), ICML HILL 2019_, and please cite it if you use Gradio in your work."}, {"name": "gradio", "tags": ["cli", "dev", "math", "ml", "ui", "visualization", "web"], "summary": "Python library for easily interacting with trained machine learning models", "text": "This library is used to quickly build and share web applications for machine learning models, allowing developers to showcase their models' capabilities without requiring extensive JavaScript or web hosting experience. With just a few lines of Python code, developers can create interactive demos that can be easily shared with others."}, {"name": "graphframes", "tags": ["math"], "summary": "GraphFrames: DataFrame-based Graphs", "text": "This is a package for DataFrame-based graphs on top of Apache Spark. Users can write highly expressive queries by leveraging the DataFrame API, combined with a new API for motif finding. The user also benefits from DataFrame performance optimizations within the Spark SQL engine."}, {"name": "graphframes", "tags": ["math"], "summary": "GraphFrames: DataFrame-based Graphs", "text": "This library is used to create and query graph data structures on top of Apache Spark using DataFrames, enabling users to express complex queries with high performance. Developers can leverage this library to find patterns in their graph data through motif finding capabilities."}, {"name": "graphviz", "tags": ["math", "ui", "visualization"], "summary": "Simple Python interface for Graphviz", "text": "Graphviz\n========\n\nPyPI version\n\nBuild\n\nBinder-stable\n\nThis package facilitates the creation and rendering of graph descriptions in\nthe DOT_ language of the Graphviz_ graph drawing software (`upstream repo`_)\nfrom Python.\n\nCreate a graph object, assemble the graph by adding nodes and edges, and\nretrieve its DOT source code string. Save the source code to a file and render\nit with the Graphviz installation of your system.\n\nUse the ``view`` option/method to directly inspect the resulting (PDF, PNG,\nSVG, etc.) file with its default application. Graphs can also be rendered\nand displayed within `Jupyter notebooks`_ (formerly known as\n`IPython notebooks`_,\n`example `_, `nbviewer `_)\nas well as the `Jupyter QtConsole`_.\n\nLinks\n-----\n\nInstallation\n------------\n\nThis package runs under Python 3.9+, use pip_ to install:\n\n.. code:: bash\n\nTo render the generated DOT source code, you also need to install Graphviz_\n(`download page `_,\n`archived versions `_,\n`installation procedure for Windows `_).\n\nMake sure that the directory containing the ``dot`` executable is on your\nsystems' ``PATH``\n(sometimes done by the installer;\nsetting ``PATH``\non `Linux `_,\n`Mac `_,\nand `Windows `_).\n\nAnaconda_: see the conda-forge_ package\n`conda-forge/python-graphviz `_\n(`feedstock `_),\nwhich should automatically ``conda install``\n`conda-forge/graphviz `_\n(`feedstock `_) as dependency.\n\nQuickstart\n----------\n\nCreate a graph object:\n\n.. code:: python\n\nAdd nodes and edges:\n\n.. code:: python\n\nCheck the generated source code:\n\n.. code:: python\n\nSave and render the source code (skip/ignore any ``doctest_mark_exe()`` lines):\n\n.. code:: python\n\nSave and render and view the result:\n\n.. code:: python\n\n.. image:: https://raw.github.com/xflr6/graphviz/master/docs/_static/round-table.svg\n\n**Caveat:**\nBackslash-escapes and strings of the form ````\nhave a special meaning in the DOT language.\nIf you need to render arbitrary strings (e.g. from user input),\ncheck the details in the `user guide`_.\n\nSee also\n--------\n\n- pygraphviz_ |--| full-blown interface wrapping the Graphviz C library with SWIG\n- graphviz-python_ |--| official Python bindings\n  (`documentation `_)\n- pydot_ |--| stable pure-Python approach, requires pyparsing\n\nLicense\n-------\n\nThis package is distributed under the `MIT license`_.\n\n.. _Graphviz:  https://www.graphviz.org\n.. _DOT: https://www.graphviz.org/doc/info/lang.html\n.. _upstream repo: https://gitlab.com/graphviz/graphviz/\n.. _upstream-download: https://www.graphviz.org/download/\n.. _upstream-archived: https://www2.graphviz.org/Archive/stable/\n.. _upstream-windows: https://forum.graphviz.org/t/new-simplified-installation-procedure-on-windows/224\n\n.. _set-path-windows: https://www.computerhope.com/issues/ch000549.htm\n.. _set-path-linux: https://stackoverflow.com/questions/14637979/how-to-permanently-set-path-on-linux-unix\n.. _set-path-darwin: https://stackoverflow.com/questions/22465332/setting-path-environment-variable-in-osx-permanently\n\n.. _pip: https://pip.pypa.io\n\n.. _Jupyter notebooks: https://jupyter.org\n.. _IPython notebooks: https://ipython.org/notebook.html\n.. _Jupyter QtConsole: https://qtconsole.readthedocs.io\n\n.. _notebook: https://github.com/xflr6/graphviz/blob/master/examples/graphviz-notebook.ipynb\n.. _notebook-nbviewer: https://nbviewer.org/github/xflr6/graphviz/blob/master/examples/graphviz-notebook.ipynb\n\n.. _Anaconda: https://docs.anaconda.com/anaconda/install/\n.. _conda-forge: https://conda-forge.org\n.. _conda-forge-python-graphviz: https://anaconda.org/conda-forge/python-graphviz\n.. _conda-forge-python-graphviz-feedstock: https://github.com/conda-forge/python-graphviz-feedstock\n.. _conda-forge-graphviz: https://anaconda.org/conda-forge/graphviz\n.. _conda-forge-graphviz-feedstock: https://github.com/conda-forge/graphviz-feedstock\n\n.. _user guide: https://graphviz.readthedocs.io/en/stable/manual.html\n\n.. _pygraphviz: https://pypi.org/project/pygraphviz/\n.. _graphviz-python: https://pypi.org/project/graphviz-python/\n.. _graphviz-python-docs: https://www.graphviz.org/pdf/gv.3python.pdf\n.. _pydot: https://pypi.org/project/pydot/\n\n.. _MIT license: https://opensource.org/licenses/MIT\n\n.. |--| unicode:: U+2013\n\n.. |PyPI version| image:: https://img.shields.io/pypi/v/graphviz.svg\n.. |License| image:: https://img.shields.io/pypi/l/graphviz.svg\n.. |Supported Python| image:: https://img.shields.io/pypi/pyversions/graphviz.svg\n.. |Downloads| image::  https://img.shields.io/pypi/dm/graphviz.svg"}, {"name": "graphviz", "tags": ["math", "ui", "visualization"], "summary": "Simple Python interface for Graphviz", "text": "This library is used to create and render graph descriptions in the DOT language of Graphviz from Python, enabling developers to visualize complex relationships and structures. With this library, developers can generate various types of output files (e.g., PDF, PNG, SVG) for their graphs and even display them directly within Jupyter notebooks."}, {"name": "gymnasium", "tags": ["math", "ml", "web"], "summary": "A standard API for reinforcement learning and a diverse set of reference environments (formerly Gym).", "text": "Environments\n\nGymnasium includes the following families of environments along with a wide variety of third-party environments\n* [Classic Control](https://gymnasium.farama.org/environments/classic_control/) - These are classic reinforcement learning based on real-world problems and physics.\n* [Box2D](https://gymnasium.farama.org/environments/box2d/) - These environments all involve toy games based around physics control, using box2d based physics and PyGame-based rendering\n* [Toy Text](https://gymnasium.farama.org/environments/toy_text/) - These environments are designed to be extremely simple, with small discrete state and action spaces, and hence easy to learn. As a result, they are suitable for debugging implementations of reinforcement learning algorithms.\n* [MuJoCo](https://gymnasium.farama.org/environments/mujoco/) - A physics engine based environments with multi-joint control which are more complex than the Box2D environments.\n* [Atari](https://ale.farama.org/) - Emulator of Atari 2600 ROMs simulated that have a high range of complexity for agents to learn.\n* [Third-party](https://gymnasium.farama.org/environments/third_party_environments/) - A number of environments have been created that are compatible with the Gymnasium API. Be aware of the version that the software was created for and use the `apply_env_compatibility` in `gymnasium.make` if necessary.\n\nInstallation\n\nTo install the base Gymnasium library, use `pip install gymnasium`\n\nThis does not include dependencies for all families of environments (there's a massive number, and some can be problematic to install on certain systems). You can install these dependencies for one family like `pip install \"gymnasium[atari]\"` or use `pip install \"gymnasium[all]\"` to install all dependencies.\n\nWe support and test for Python 3.10, 3.11, 3.12 and 3.13 on Linux and macOS. We will accept PRs related to Windows, but do not officially support it.\n\nAPI\n\nThe Gymnasium API models environments as simple Python `env` classes. Creating environment instances and interacting with them is very simple- here's an example using the \"CartPole-v1\" environment:\n\nNotable Related Libraries\n\nPlease note that this is an incomplete list, and just includes libraries that the maintainers most commonly point newcomers to when asked for recommendations.\n\n* [CleanRL](https://github.com/vwxyzjn/cleanrl) is a learning library based on the Gymnasium API. It is designed to cater to newer people in the field and provides very good reference implementations.\n* [PettingZoo](https://github.com/Farama-Foundation/PettingZoo) is a multi-agent version of Gymnasium with a number of implemented environments, i.e. multi-agent Atari environments.\n* The Farama Foundation also has a collection of many other [environments](https://farama.org/projects) that are maintained by the same team as Gymnasium and use the Gymnasium API.\n\nEnvironment Versioning\n\nGymnasium keeps strict versioning for reproducibility reasons. All environments end in a suffix like \"-v0\".  When changes are made to environments that might impact learning results, the number is increased by one to prevent potential confusion. These were inherited from Gym.\n\nContributing\n\nWe welcome contributions from the community!\nPlease see our [CONTRIBUTING.md](https://github.com/Farama-Foundation/Gymnasium/blob/main/CONTRIBUTING.md) for details on how to get started.\n\nSupport Gymnasium's Development\n\nIf you are financially able to do so and would like to support the development of Gymnasium, please join others in the community in [donating to us](https://github.com/sponsors/Farama-Foundation).\n\nCitation\n\nYou can cite Gymnasium using our related paper (https://arxiv.org/abs/2407.17032) as:\n\nRepository Sponsors\n\nWispr Flow\n\n  \n\nDictation that understands code\nShip 4x faster with developer-first dictation that works in every app.\n\nIf you'd like to sponsor Gymnasium or other Farama repositories and have your logo here, contact us."}, {"name": "gymnasium", "tags": ["math", "ml", "web"], "summary": "A standard API for reinforcement learning and a diverse set of reference environments (formerly Gym).", "text": "This library is used to enable the development of reinforcement learning agents in a variety of simulation environments, including classic control problems, physics-based games, and simple text-based worlds. With Gymnasium, developers can train and test their agents across multiple environment types, facilitating experimentation and improvement of reinforcement learning algorithms."}, {"name": "h3", "tags": ["math", "web"], "summary": "Uber's hierarchical hexagonal geospatial indexing system", "text": "**h3-py**: Uber's H3 Hexagonal Hierarchical Geospatial Indexing System in Python\n\n(https://pypistats.org/packages/h3)\n(https://anaconda.org/conda-forge/h3-py)\n(https://github.com/uber/h3/releases/tag/v4.4.1)\n(https://github.com/uber/h3-py/blob/master/LICENSE)\n\n(https://github.com/uber/h3-py/actions)\n(https://github.com/uber/h3-py/blob/master/.github/workflows/lint_and_coverage.yml#L31) \n\nPython bindings for the [H3 core library](https://h3geo.org/).\n\n- Documentation: [uber.github.io/h3-py](https://uber.github.io/h3-py)\n- GitHub repo: [github.com/uber/h3-py](https://github.com/uber/h3-py)\n\nInstallation\n\nFrom [PyPI](https://pypi.org/project/h3/):\n\nFrom [conda](https://github.com/conda-forge/h3-py-feedstock):\n\nUsage\n\nAPIs\n\nWe provide [multiple APIs][api_comparison] in `h3-py`.\n\n- All APIs have the same set of functions;\n  see the [API reference][api_quick].\n- The APIs differ only in their input/output formats;\n  see the [API comparison page][api_comparison].\n\nExample gallery\n\nBrowse [a collection of example notebooks](https://github.com/uber/h3-py-notebooks),\nand if you have examples or visualizations of your own, please feel free\nto contribute!\n\nWe also have an introductory [walkthrough of the API][walkthrough].\n\nVersioning\n\n`h3-py` wraps the [H3 core library](https://github.com/uber/h3),\nwhich is written in C.\nThe C and Python projects each employ\n[semantic versioning](https://semver.org/),\nwhere versions take the form `X.Y.Z`.\n\nThe `h3-py` version string is guaranteed to match the C library string\nin both *major* and *minor* numbers (`X.Y`), but may differ on the\n*patch* (`Z`) number.\nThis convention provides users with information on breaking changes and\nfeature additions, while providing downstream bindings (like this one!)\nwith the versioning freedom to fix bugs.\n\nUse `h3.versions()` to see the version numbers for both\n`h3-py` and the C library. For example,"}, {"name": "h3", "tags": ["math", "web"], "summary": "Uber's hierarchical hexagonal geospatial indexing system", "text": "This library is used to perform efficient geospatial queries and indexing on hexagonal cells, enabling developers to handle large-scale location-based data with precision. With h3, developers can index, query, and analyze geographic data with optimized performance and accuracy."}, {"name": "h5netcdf", "tags": ["data", "math", "web"], "summary": "netCDF4 via h5py", "text": "h5netcdf\n========\n\nA Python interface for the `netCDF4`_ file-format that reads and writes local or\nremote HDF5 files directly via `h5py`_ or `h5pyd`_, without relying on the Unidata\nnetCDF library.\n\n.. _netCDF4: https://docs.unidata.ucar.edu/netcdf-c/current/file_format_specifications.html#netcdf_4_spec\n.. _h5py: https://www.h5py.org/\n.. _h5pyd: https://github.com/HDFGroup/h5pyd\n\n.. why-h5netcdf\n\nWhy h5netcdf?\n-------------\n\n- It has one less binary dependency (netCDF C). If you already have h5py\n  installed, reading netCDF4 with h5netcdf may be much easier than installing\n  netCDF4-Python.\n- We've seen occasional reports of better performance with h5py than\n  netCDF4-python, though in many cases performance is identical. For\n  `one workflow`_, h5netcdf was reported to be almost **4x faster** than\n  `netCDF4-python`_.\n- Anecdotally, HDF5 users seem to be unexcited about switching to netCDF --\n  hopefully this will convince them that netCDF4 is actually quite sane!\n- Finally, side-stepping the netCDF C library (and Cython bindings to it)\n  gives us an easier way to identify the source of performance issues and\n  bugs in the netCDF libraries/specification.\n\n.. _one workflow: https://github.com/Unidata/netcdf4-python/issues/390#issuecomment-93864839\n.. _xarray: https://github.com/pydata/xarray/\n\nInstall\n-------\n\nEnsure you have a recent version of h5py installed (I recommend using `conda`_ or\nthe community effort `conda-forge`_).\nAt least version 3.0 is required. Then::\n\nOr if you are already using conda::\n\nNote:\n\nFrom version 1.2. h5netcdf tries to align with a `nep29`_-like support policy with regard\nto it's upstream dependencies.\n\n.. _conda: https://conda.io/\n.. _conda-forge: https://conda-forge.org/\n.. _nep29: https://numpy.org/neps/nep-0029-deprecation_policy.html\n\nUsage\n-----\n\nh5netcdf has two APIs, a new API and a legacy API. Both interfaces currently\nreproduce most of the features of the netCDF interface, including the ability\nto write NETCDF4 and NETCDF4_CLASSIC formatted files. Support for operations\nthat rename or delete existing objects is still missing, and patches would be\nvery welcome.\n\nNew API\n~~~~~~~\n\nThe new API supports direct hierarchical access of variables and groups. Its\ndesign is an adaptation of h5py to the netCDF data model. For example:\n\n.. code-block:: python\n\nNotes:\n\n- Automatic resizing of unlimited dimensions with array indexing is not available.\n- Dimensions need to be manually resized with ``Group.resize_dimension(dimension, size)``.\n- Arrays are returned padded with ``fillvalue`` (taken from underlying hdf5 dataset) up to\n  current size of variable's dimensions. The behaviour is equivalent to netCDF4-python's\n  ``Dataset.set_auto_mask(False)``.\n\nLegacy API\n~~~~~~~~~~\n\nThe legacy API is designed for compatibility with `netCDF4-python`_. To use it, import\n``h5netcdf.legacyapi``:\n\n.. _netCDF4-python: https://github.com/Unidata/netcdf4-python\n\n.. code-block:: python\n\nThe legacy API is designed to be easy to try-out for netCDF4-python users, but it is not an\nexact match. Here is an incomplete list of functionality we don't include:\n\n- Utility functions ``chartostring``, ``num2date``, etc., that are not directly necessary\n  for writing netCDF files.\n- h5netcdf variables do not support automatic masking or scaling (e.g., of values matching\n  the ``_FillValue`` attribute). We prefer to leave this functionality to client libraries\n  (e.g., `xarray`_), which can implement their exact desired scaling behavior. Nevertheless\n  arrays are returned padded with ``fillvalue`` (taken from underlying hdf5 dataset) up to\n  current size of variable's dimensions. The behaviour is equivalent to netCDF4-python's\n  ``Dataset.set_auto_mask(False)``.\n\n.. _invalid netcdf:\n\nInvalid netCDF files\n~~~~~~~~~~~~~~~~~~~~"}, {"name": "h5netcdf", "tags": ["data", "math", "web"], "summary": "netCDF4 via h5py", "text": "h5py implements some features that do not (yet) result in valid netCDF files:\n\n- Data types:\n- Arbitrary filters:\n\nBy default [#]_, h5netcdf will not allow writing files using any of these features,\nas files with such features are not readable by other netCDF tools.\n\nHowever, these are still valid HDF5 files. If you don't care about netCDF\ncompatibility, you can use these features by setting ``invalid_netcdf=True``\nwhen creating a file:\n\n.. code-block:: python\n\n# avoid the .nc extension for non-netcdf files\n  f = h5netcdf.File(\"mydata.h5\", invalid_netcdf=True)\n  ...\n\n# works with the legacy API, too, though compression options are not exposed\n  ds = h5netcdf.legacyapi.Dataset(\"mydata.h5\", invalid_netcdf=True)\n  ...\n\nIn such cases the `_NCProperties` attribute will not be saved to the file or be removed\nfrom an existing file. A warning will be issued if the file has `.nc`-extension.\n\n.. rubric:: Footnotes\n\n.. [#] h5netcdf we will raise ``h5netcdf.CompatibilityError``.\n\nDecoding variable length strings\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nh5py 3.0 introduced `new behavior`_ for handling variable length string.\nInstead of being automatically decoded with UTF-8 into NumPy arrays of ``str``,\nthey are required as arrays of ``bytes``.\n\nThe legacy API preserves the old behavior of h5py (which matches netCDF4),\nand automatically decodes strings.\n\nThe new API matches h5py behavior. Explicitly set ``decode_vlen_strings=True``\nin the ``h5netcdf.File`` constructor to opt-in to automatic decoding.\n\n.. _new behavior: https://docs.h5py.org/en/stable/strings.html\n\n.. _phony dims:\n\nDatasets with missing dimension scales\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nBy default [#]_ h5netcdf raises a ``ValueError`` if variables with no dimension\nscale associated with one of their axes are accessed.\nYou can set ``phony_dims='sort'`` when opening a file to let h5netcdf invent\nphony dimensions according to `netCDF`_ behaviour.\n\n.. code-block:: python\n\n# mimic netCDF-behaviour for non-netcdf files\n  f = h5netcdf.File(\"mydata.h5\", mode=\"r\", phony_dims=\"sort\")\n  ...\n\nNote, that this iterates once over the whole group-hierarchy. This has affects\non performance in case you rely on laziness of group access.\nYou can set ``phony_dims='access'`` instead to defer phony dimension creation\nto group access time. The created phony dimension naming will differ from\n`netCDF`_ behaviour.\n\n.. code-block:: python\n\nf = h5netcdf.File(\"mydata.h5\", mode=\"r\", phony_dims=\"access\")\n  ...\n\n.. rubric:: Footnotes\n\n.. [#] Keyword default setting ``phony_dims=None`` for backwards compatibility.\n\n.. _netCDF: https://docs.unidata.ucar.edu/netcdf-c/current/interoperability_hdf5.html\n\nTrack Order\n~~~~~~~~~~~\n\nAs of h5netcdf 1.1.0, if h5py 3.7.0 or greater is detected, the ``track_order``\nparameter is set to ``True`` enabling `order tracking`_ for newly created\nnetCDF4 files. This helps ensure that files created with the h5netcdf library\ncan be modified by the netCDF4-c and netCDF4-python implementation used in\nother software stacks. Since this change should be transparent to most users,\nit was made without deprecation.\n\nSince track_order is set at creation time, any dataset that was created with\n``track_order=False`` (h5netcdf version 1.0.2 and older except for 0.13.0) will\ncontinue to opened with order tracker disabled.\n\nThe following describes the behavior of h5netcdf with respect to order tracking\nfor a few key versions:"}, {"name": "h5netcdf", "tags": ["data", "math", "web"], "summary": "netCDF4 via h5py", "text": "- Version 0.12.0 and earlier, the ``track_order`` parameter`order was missing\n  and thus order tracking was implicitly set to ``False``.\n- Version 0.13.0 enabled order tracking by setting the parameter\n  ``track_order`` to ``True`` by default without deprecation.\n- Versions 0.13.1 to 1.0.2 set ``track_order`` to ``False`` due to a bug in a\n  core dependency of h5netcdf, h5py `upstream bug`_ which was resolved in h5py\n  3.7.0 with the help of the h5netcdf team.\n- In version 1.1.0, if h5py 3.7.0 or above is detected, the ``track_order``\n  parameter is set to ``True`` by default.\n\n.. _order tracking: https://docs.unidata.ucar.edu/netcdf-c/current/file_format_specifications.html#creation_order\n.. _upstream bug: https://github.com/h5netcdf/h5netcdf/issues/136\n.. _[*]: https://github.com/h5netcdf/h5netcdf/issues/128\n\n.. changelog\n\nChangelog\n---------\n\n`Changelog`_\n\n.. _Changelog: https://github.com/h5netcdf/h5netcdf/blob/main/CHANGELOG.rst\n\n.. license\n\nLicense\n-------\n\n`3-clause BSD`_\n\n.. _3-clause BSD: https://github.com/h5netcdf/h5netcdf/blob/main/LICENSE"}, {"name": "h5netcdf", "tags": ["data", "math", "web"], "summary": "netCDF4 via h5py", "text": "This library is used to read and write netCDF4 files directly via HDF5 format without relying on the Unidata netCDF library. With h5netcdf, developers can bypass the installation of additional binary dependencies, potentially improving performance for specific workflows by up to 4x faster execution times."}, {"name": "h5py", "tags": ["data", "math"], "summary": "Read and write HDF5 files from Python", "text": "The h5py package provides both a high- and low-level interface to the HDF5\nlibrary from Python. The low-level interface is intended to be a complete\nwrapping of the HDF5 API, while the high-level component supports  access to\nHDF5 files, datasets and groups using established Python and NumPy concepts.\n\nA strong emphasis on automatic conversion between Python (Numpy) datatypes and\ndata structures and their HDF5 equivalents vastly simplifies the process of\nreading and writing data from Python.\n\nWheels are provided for several popular platforms, with an included copy of\nthe HDF5 library (usually the latest version when h5py is released).\n\nYou can also `build h5py from source\n`_\nwith any HDF5 stable release from version 1.10.4 onwards, although naturally new\nHDF5 versions released after this version of h5py may not work.\nOdd-numbered minor versions of HDF5 (e.g. 1.13) are experimental, and may not\nbe supported."}, {"name": "h5py", "tags": ["data", "math"], "summary": "Read and write HDF5 files from Python", "text": "This library is used to read and write HDF5 files from Python with automatic conversion between data types, simplifying the process of accessing and storing complex data. It provides a high-level interface that allows developers to work with HDF5 files using familiar Python and NumPy concepts."}, {"name": "haversine", "tags": ["dev", "math"], "summary": "Calculate the distance between 2 points on Earth.", "text": "Haversine\n\nCalculate the distance (in various units) between two points on Earth using their latitude and longitude.\n\nInstallation\n\nUsage\n\nCalculate the distance between Lyon and Paris\n\nThe lat/lon values need to be provided in degrees of the ranges [-90,90] (lat) and [-180,180] (lon).\nIf values are outside their ranges, an error will be raised. This can be avoided by automatic normalization via the `normalize` parameter.\n\nThe `haversine.Unit` enum contains all supported units:\n\noutputs\n\nNote for radians and degrees\n\nThe radian and degrees returns the [great circle distance](https://en.wikipedia.org/wiki/Great-circle_distance) between two points on a sphere.\n\nNotes:\n\n- on a unit-sphere the angular distance in radians equals the distance between the two points on the sphere (definition of radians)\n- When using \"degree\", this angle is just converted from radians to degrees\n\nInverse Haversine Formula\n\nCalculates a point from a given vector (distance and direction) and start point.\nCurrently explicitly supports both cardinal (north, east, south, west) and intercardinal (northeast, southeast, southwest, northwest) directions.\nBut also allows for explicit angles expressed in Radians.\n\nExample: Finding arbitary point from Paris\n\nPerformance optimisation for distances between all points in two vectors\n\nYou will need to install [numpy](https://pypi.org/project/numpy/) in order to gain performance with vectors.\nFor optimal performance, you can turn off coordinate checking by adding `check=False` and install the optional packages [numba](https://pypi.org/project/numba/) and [icc_rt](https://pypi.org/project/icc_rt/).\n\nYou can then do this:\n\nIt is generally slower to use `haversine_vector` to get distance between two points, but can be really fast to compare distances between two vectors.\n\nCombine matrix\n\nYou can generate a matrix of all combinations between coordinates in different vectors by setting `comb` parameter as True.\n\nThe output array from the example above returns the following table:\n\nParis\n------\nLyon\nLondon\n\nBy definition, if you have a vector _a_ with _n_ elements, and a vector _b_ with _m_ elements. The result matrix _M_ would be $n x m$ and a element M\\[i,j\\] from the matrix would be the distance between the ith coordinate from vector _a_ and jth coordinate with vector _b_.\n\nContributing\n\nClone the project.\n\nInstall [pipenv](https://github.com/pypa/pipenv).\n\nRun `pipenv install --dev`\n\nLaunch test with `pipenv run pytest`"}, {"name": "haversine", "tags": ["dev", "math"], "summary": "Calculate the distance between 2 points on Earth.", "text": "This library is used to calculate the distance between two geographic locations on Earth using their latitude and longitude coordinates. With haversine, developers can easily determine distances between locations in various units."}, {"name": "hbutils", "tags": ["cli", "dev", "math", "ui", "web"], "summary": "Some useful functions and classes in Python infrastructure development.", "text": "hbutils\n\n(https://pypi.org/project/hbutils/)\n\n(https://codeclimate.com/github/HansBug/hbutils/maintainability)\n(https://codecov.io/gh/hansbug/hbutils)\n\n(https://github.com/hansbug/hbutils/actions?query=workflow%3A%22Docs+Deploy%22)\n(https://github.com/hansbug/hbutils/actions?query=workflow%3A%22Code+Test%22)\n\n(https://github.com/hansbug/hbutils/actions?query=workflow%3A%22Package+Release%22)\n\n(https://github.com/hansbug/hbutils/stargazers)\n(https://github.com/hansbug/hbutils/network)\n\n(https://github.com/hansbug/hbutils/issues)\n(https://github.com/hansbug/hbutils/pulls)\n(https://github.com/hansbug/hbutils/graphs/contributors)\n(https://github.com/HansBug/hbutils/blob/master/LICENSE)\n\n**hbutils** is a comprehensive collection of useful functions and classes designed to simplify and accelerate Python\ninfrastructure development. It provides a wide array of utilities covering algorithms, data structures, system\noperations, design patterns, and testing tools.\n\nInstallation\n\nYou can simply install it with the `pip` command line from the official PyPI site.\n\nFor more information about installation, you can refer to\nthe [Installation Guide](https://hbutils.readthedocs.io/en/latest/tutorials/installation/index.html).\n\nCompatibility\n\nThe library is designed to be cross-platform and supports a wide range of Python environments. It has been thoroughly\ntested on:\n\nOperating System\n:-----------------\n**Windows**\n**Ubuntu**\n**macOS**\n\nModules Overview\n\nThe project is structured into several top-level modules, each dedicated to a specific area of utility:\n\nModule\n:-------------------------\n**`hbutils.algorithm`**\n**`hbutils.binary`**\n**`hbutils.collection`**\n**`hbutils.color`**\n**`hbutils.concurrent`**\n**`hbutils.config`**\n**`hbutils.design`**\n**`hbutils.encoding`**\n**`hbutils.expression`**\n**`hbutils.file`**\n**`hbutils.logging`**\n**`hbutils.model`**\n**`hbutils.random`**\n**`hbutils.reflection`**\n**`hbutils.scale`**\n**`hbutils.string`**\n**`hbutils.system`**\n**`hbutils.testing`**\n\nFeatured Utilities\n\nHere are some representative examples showcasing the functionality of key modules.\n\n`hbutils.algorithm`\n\nThis module provides implementations for classic algorithms.\n\n`linear_map`\n\nCreates a callable piecewise linear function from a sequence of control points for custom interpolation.\n\n**Documentation:\n** [linear_map](https://hbutils.readthedocs.io/en/latest/api_doc/algorithm/linear.html#hbutils.algorithm.linear.linear_map)\n\n`topoids`\n\nPerforms topological sorting on a directed acyclic graph (DAG) represented by integer node IDs and edges.\n\n**Documentation:\n** [topoids](https://hbutils.readthedocs.io/en/latest/api_doc/algorithm/topological.html#hbutils.algorithm.topological.topoids)\n\n`hbutils.collection`\n\nUtilities for manipulating sequences and collections.\n\n`unique`\n\nRemoves duplicate elements from a sequence while preserving the original order.\n\n**Documentation:\n** [unique](https://hbutils.readthedocs.io/en/latest/api_doc/collection/sequence.html#hbutils.collection.sequence.unique)\n\n`group_by`\n\nDivides elements into groups based on a key function, with optional post-processing for each group.\n\n**Documentation:\n** [group_by](https://hbutils.readthedocs.io/en/latest/api_doc/collection/sequence.html#hbutils.collection.sequence.group_by)\n\n`hbutils.concurrent`\n\nUtilities for managing concurrent access to shared resources.\n\n`ReadWriteLock`\n\nA reader-writer lock implementation that allows multiple concurrent readers or a single exclusive writer, optimizing for\nread-heavy scenarios.\n\n**Documentation:\n** [ReadWriteLock](https://hbutils.readthedocs.io/en/latest/api_doc/concurrent/readwrite.html#hbutils.concurrent.readwrite.ReadWriteLock)\n\n`hbutils.design`\n\nImplementations of common design patterns.\n\n`SingletonMeta`\n\nA metaclass to enforce the traditional Singleton pattern, ensuring only one instance of a class exists.\n\n**Documentation:\n** [SingletonMeta](https://hbutils.readthedocs.io/en/latest/api_doc/design/singleton.html#hbutils.design.singleton.SingletonMeta)\n\n`ValueBasedSingletonMeta`\n\nA metaclass for creating singletons based on a specific initialization value.\n\n**Documentation:\n** [ValueBasedSingletonMeta](https://hbutils.readthedocs.io/en/latest/api_doc/design/singleton.html#hbutils.design.singleton.ValueBasedSingletonMeta)\n\n`hbutils.encoding`\n\nWrappers for cryptographic hash functions.\n\n`md5`, `sha256`, and `sha3`\n\nProvides simple functions for computing common cryptographic hashes of binary data.\n\n**Documentation:\n** [md5](https://hbutils.readthedocs.io/en/latest/api_doc/encoding/hash.html#hbutils.encoding.hash.md5), [sha256](https://hbutils.readthedocs.io/en/latest/api_doc/encoding/hash.html#hbutils.encoding.hash.sha256), [sha3](https://hbutils.readthedocs.io/en/latest/api_doc/encoding/hash.html#hbutils.encoding.hash.sha3)\n\n`hbutils.file`\n\nUtilities for managing file streams.\n\n`keep_cursor`\n\nA context manager that saves and restores the file stream's cursor position, ensuring operations within the block do not\naffect the external cursor state.\n\n**Documentation:\n** [keep_cursor](https://hbutils.readthedocs.io/en/latest/api_doc/file/stream.html#hbutils.file.stream.keep_cursor)\n\n`getsize`\n\nRetrieves the size of a seekable file stream in bytes, without changing the cursor position.\n\n**Documentation:\n** [getsize](https://hbutils.readthedocs.io/en/latest/api_doc/file/stream.html#hbutils.file.stream.getsize)\n\n`hbutils.logging`\n\nProvides enhanced logging capabilities.\n\n`ColoredFormatter`\n\nA logging formatter that applies colors to log messages based on their severity level and handles proper indentation for\nmulti-line messages.\n\n**Documentation:\n** [ColoredFormatter](https://hbutils.readthedocs.io/en/latest/api_doc/logging/format.html#hbutils.logging.format.ColoredFormatter)\n\n`hbutils.reflection`\n\nAdvanced function and object introspection and manipulation.\n\n`dynamic_call`\n\nA decorator that enables a function to be called with a flexible number of arguments, automatically filtering them based\non the function's signature.\n\n**Documentation:\n** [dynamic_call](https://hbutils.readthedocs.io/en/latest/api_doc/reflection/func.html#hbutils.reflection.func.dynamic_call)\n\n`frename`\n\nA decorator to easily rename a function by changing its `__name__` attribute.\n\n**Documentation:\n** [frename](https://hbutils.readthedocs.io/en/latest/api_doc/reflection/func.html#hbutils.reflection.func.frename)\n\n`hbutils.scale`\n\nHandling and formatting of scaled values.\n\n`size_to_bytes`\n\nConverts various memory size representations (int, float, string like \"3.54 GB\") into an integer value in bytes.\n\n**Documentation:\n** [size_to_bytes](https://hbutils.readthedocs.io/en/latest/api_doc/scale/size.html#hbutils.scale.size.size_to_bytes)\n\n`size_to_bytes_str`\n\nConverts a size value to a human-readable string with the most appropriate unit (e.g., KiB, MB, GB).\n\n**Documentation:\n** [size_to_bytes_str](https://hbutils.readthedocs.io/en/latest/api_doc/scale/size.html#hbutils.scale.size.size_to_bytes_str)\n\n`hbutils.string`\n\nSimple but powerful string processing utilities.\n\n`plural_word`\n\nFormats a word with its count, automatically using the correct singular or plural form.\n\n**Documentation:\n** [plural_word](https://hbutils.readthedocs.io/en/latest/api_doc/string/plural.html#hbutils.string.plural.plural_word)\n\n`plural_form`\n\nGets the plural form of a word, handling irregular plurals.\n\n**Documentation:\n** [plural_form](https://hbutils.readthedocs.io/en/latest/api_doc/string/plural.html#hbutils.string.plural.plural_form)\n\n`hbutils.system`\n\nUnix-like commands for filesystem operations.\n\n`copy`, `remove`, and `getsize`\n\nProvides powerful, glob-enabled utilities for file system manipulation, similar to Unix commands like `cp -rf`,\n`rm -rf`, and `du -sh`.\n\n**Documentation:\n** [copy](https://hbutils.readthedocs.io/en/latest/api_doc/system/filesystem.html#hbutils.system.filesystem.directory.copy), [remove](https://hbutils.readthedocs.io/en/latest/api_doc/system/filesystem.html#hbutils.system.filesystem.directory.remove), [getsize](https://hbutils.readthedocs.io/en/latest/api_doc/system/filesystem.html#hbutils.system.filesystem.directory.getsize)\n\n`hbutils.testing`\n\nUtilities for isolated and robust unit testing.\n\n`isolated_directory`\n\nA context manager that executes code within a temporary, isolated directory, with optional mapping of files/directories\nfrom the original location. This is ideal for tests that modify the file system.\n\n**Documentation:\n** [isolated_directory](https://hbutils.readthedocs.io/en/latest/api_doc/testing/isolated.html#hbutils.testing.isolated.directory.isolated_directory)\n\nContributing\n\nWe welcome contributions from the community! Please see our [CONTRIBUTING.md](CONTRIBUTING.md) for details on how to get\nstarted.\n\nLicense\n\nThis project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details."}, {"name": "hbutils", "tags": ["cli", "dev", "math", "ui", "web"], "summary": "Some useful functions and classes in Python infrastructure development.", "text": "This library is used to provide a set of reusable tools and utilities for Python developers, streamlining infrastructure development tasks and boosting productivity. With hbutils, developers can quickly integrate common functionality into their projects, saving time and effort in the development process."}, {"name": "hdbscan", "tags": ["math", "visualization", "web"], "summary": "Clustering based on density with variable density clusters", "text": ".. image:: https://img.shields.io/pypi/v/hdbscan.svg\n\n.. image:: https://img.shields.io/pypi/l/hdbscan.svg\n.. image:: https://travis-ci.org/scikit-learn-contrib/hdbscan.svg\n\n:target: https://codecov.io/gh/scikit-learn-contrib/hdbscan\n\n.. image:: http://joss.theoj.org/papers/10.21105/joss.00205/status.svg\n\n=======\nHDBSCAN\n=======\n\nHDBSCAN - Hierarchical Density-Based Spatial Clustering of Applications\nwith Noise. Performs DBSCAN over varying epsilon values and integrates \nthe result to find a clustering that gives the best stability over epsilon.\nThis allows HDBSCAN to find clusters of varying densities (unlike DBSCAN),\nand be more robust to parameter selection.\n\nIn practice this means that HDBSCAN returns a good clustering straight\naway with little or no parameter tuning -- and the primary parameter,\nminimum cluster size, is intuitive and easy to select.\n\nHDBSCAN is ideal for exploratory data analysis; it's a fast and robust\nalgorithm that you can trust to return meaningful clusters (if there\nare any).\n\nBased on the papers:\n\nDocumentation, including tutorials, are available on ReadTheDocs at http://hdbscan.readthedocs.io/en/latest/ .  \nNotebooks `comparing HDBSCAN to other clustering algorithms `_, explaining `how HDBSCAN works `_ and `comparing performance with other python clustering implementations `_ are available.\n\n------------------\nHow to use HDBSCAN\n------------------\n\nThe hdbscan package inherits from sklearn classes, and thus drops in neatly\nnext to other sklearn clusterers with an identical calling API. Similarly it\nsupports input in a variety of formats: an array (or pandas dataframe, or\nsparse matrix) of shape ``(num_samples x num_features)``; an array (or sparse matrix)\ngiving a distance matrix between samples.\n\n.. code:: python\n\n-----------\nPerformance\n-----------\n\nSignificant effort has been put into making the hdbscan implementation as fast as \npossible. It is `orders of magnitude faster than the reference implementation `_ in Java,\nand is currently faster than highly optimized single linkage implementations in C and C++.\n`version 0.7 performance can be seen in this notebook `_ .\nIn particular `performance on low dimensional data is better than sklearn's DBSCAN `_ ,\nand via support for caching with joblib, re-clustering with different parameters\ncan be almost free.\n\n------------------------\nAdditional functionality\n------------------------\n\nThe hdbscan package comes equipped with visualization tools to help you\nunderstand your clustering results. After fitting data the clusterer\nobject has attributes for:\n\n* The condensed cluster hierarchy\n* The robust single linkage cluster hierarchy\n* The reachability distance minimal spanning tree\n\nAll of which come equipped with methods for plotting and converting\nto Pandas or NetworkX for further analysis. See the notebook on\n`how HDBSCAN works `_ for examples and further details.\n\nThe clusterer objects also have an attribute providing cluster membership\nstrengths, resulting in optional soft clustering (and no further compute \nexpense). Finally each cluster also receives a persistence score giving\nthe stability of the cluster over the range of distance scales present\nin the data. This provides a measure of the relative strength of clusters.\n\n-----------------\nOutlier Detection\n-----------------"}, {"name": "hdbscan", "tags": ["math", "visualization", "web"], "summary": "Clustering based on density with variable density clusters", "text": "The HDBSCAN clusterer objects also support the GLOSH outlier detection algorithm. \nAfter fitting the clusterer to data the outlier scores can be accessed via the\n``outlier_scores_`` attribute. The result is a vector of score values, one for\neach data point that was fit. Higher scores represent more outlier like objects.\nSelecting outliers via upper quantiles is often a good approach.\n\nBased on the paper:\n\n---------------------\nRobust single linkage\n---------------------\n\nThe hdbscan package also provides support for the *robust single linkage*\nclustering algorithm of Chaudhuri and Dasgupta. As with the HDBSCAN \nimplementation this is a high performance version of the algorithm \noutperforming scipy's standard single linkage implementation. The\nrobust single linkage hierarchy is available as an attribute of\nthe robust single linkage clusterer, again with the ability to plot\nor export the hierarchy, and to extract flat clusterings at a given\ncut level and gamma value.\n\nExample usage:\n\n.. code:: python\n\nBased on the paper:\n\n----------------\nBranch detection\n----------------\n\nThe hdbscan package supports a branch-detection post-processing step \nby `Bot et al. `_. Cluster shapes,\nsuch as branching structures, can reveal interesting patterns \nthat are not expressed in density-based cluster hierarchies. The \nBranchDetector class mimics the HDBSCAN API and can be used to\ndetect branching hierarchies in clusters. It provides condensed \nbranch hierarchies, branch persistences, and branch memberships and \nsupports joblib's caching functionality. A notebook \n`demonstrating the BranchDetector is available `_.\n\nExample usage:\n\n.. code:: python\n\nBased on the paper:\n\n----------\nInstalling\n----------\n\nEasiest install, if you have Anaconda (thanks to conda-forge which is awesome!):\n\n.. code:: bash\n\nPyPI install, presuming you have an up to date pip:\n\n.. code:: bash\n\nBinary wheels for a number of platforms are available thanks to the work of\nRyan Helinski .\n\nIf pip is having difficulties pulling the dependencies then we'd suggest to first upgrade\npip to at least version 10 and try again:\n\n.. code:: bash\n\nOtherwise install the dependencies manually using anaconda followed by pulling hdbscan from pip:\n\n.. code:: bash\n\nFor a manual install of the latest code directly from GitHub:\n\n.. code:: bash\n\nAlternatively download the package, install requirements, and manually run the installer:\n\n.. code:: bash\n\n-----------------\nRunning the Tests\n-----------------\n\nThe package tests can be run after installation using the command:\n\n.. code:: bash\n\nor, if ``nose`` is installed but ``nosetests`` is not in your ``PATH`` variable:\n\n.. code:: bash\n\nIf one or more of the tests fail, please report a bug at https://github.com/scikit-learn-contrib/hdbscan/issues/new\n\n--------------\nPython Version\n--------------\n\nThe hdbscan library supports both Python 2 and Python 3. However we recommend Python 3 as the better option if it is available to you.\n----------------\nHelp and Support\n----------------\n\nFor simple issues you can consult the `FAQ `_ in the documentation.\nIf your issue is not suitably resolved there, please check the `issues `_ on github. Finally, if no solution is available there feel free to `open an issue `_ ; the authors will attempt to respond in a reasonably timely fashion.\n\n------------\nContributing\n------------"}, {"name": "hdbscan", "tags": ["math", "visualization", "web"], "summary": "Clustering based on density with variable density clusters", "text": "We welcome contributions in any form! Assistance with documentation, particularly expanding tutorials,\nis always welcome. To contribute please `fork the project `_ make your changes and submit a pull request. We will do our best to work through any issues with\nyou and get your code merged into the main branch.\n\n------\nCiting\n------\n\nIf you have used this codebase in a scientific publication and wish to cite it, please use the `Journal of Open Source Software article `_.\n\n.. code:: bibtex\n\nTo reference the high performance algorithm developed in this library please cite our paper in ICDMW 2017 proceedings.\n\n.. code:: bibtex\n\nIf you used the branch-detection functionality in this library please cite our `PeerJ paper `_:\n\n.. code:: bibtex\n\n---------\nLicensing\n---------\n\nThe hdbscan package is 3-clause BSD licensed. Enjoy."}, {"name": "hdbscan", "tags": ["math", "visualization", "web"], "summary": "Clustering based on density with variable density clusters", "text": "This library is used to perform hierarchical density-based spatial clustering, allowing for clusters of varying densities without requiring manual parameter tuning. This enables developers to easily identify stable clusterings in their data, reducing the need for manual optimization and improving overall model robustness."}, {"name": "hf-xet", "tags": ["data", "math", "ml", "web"], "summary": "Fast transfer of large files with the Hugging Face Hub.", "text": "Welcome\n\n`hf-xet` enables `huggingface_hub` to utilize xet storage for uploading and downloading to HF Hub. Xet storage provides chunk-based deduplication, efficient storage/retrieval with local disk caching, and backwards compatibility with Git LFS. This library is not meant to be used directly, and is instead intended to be used from [huggingface_hub](https://pypi.org/project/huggingface-hub).\n\nKey features\n\n\u267b **chunk-based deduplication implementation**: avoid transferring and storing chunks that are shared across binary files (models, datasets, etc).\n\n **Python bindings**: bindings for [huggingface_hub](https://github.com/huggingface/huggingface_hub/) package.\n\n\u2194 **network communications**: concurrent communication to HF Hub Xet backend services (CAS).\n\n **local disk caching**: chunk-based cache that sits alongside the existing [huggingface_hub disk cache](https://huggingface.co/docs/huggingface_hub/guides/manage-cache).\n\nInstallation\n\nInstall the `hf_xet` package with [pip](https://pypi.org/project/hf-xet/):\n\nQuick Start\n\n`hf_xet` is not intended to be run independently as it is expected to be used from `huggingface_hub`, so to get started with `huggingface_hub` check out the documentation [here](\"https://hf.co/docs/huggingface_hub\").\n\nContributions (feature requests, bugs, etc.) are encouraged & appreciated \ufe0f\n\nPlease join us in making hf-xet better. We value everyone's contributions. Code is not the only way to help. Answering questions, helping each other, improving documentation, filing issues all help immensely. If you are interested in contributing (please do!), check out the [contribution guide](https://github.com/huggingface/xet-core/blob/main/CONTRIBUTING.md) for this repository."}, {"name": "hf-xet", "tags": ["data", "math", "ml", "web"], "summary": "Fast transfer of large files with the Hugging Face Hub.", "text": "This library is used to enable fast transfer of large files with Hugging Face Hub by utilizing xet storage for efficient uploading and downloading, featuring deduplication and local disk caching. It provides a way for the huggingface_hub package to leverage these features, streamlining large file transfers."}, {"name": "hijridate", "tags": ["math", "ui", "web"], "summary": "Accurate Hijri-Gregorian dates converter based on Umm al-Qura calendar", "text": "HijriDate\n\n> formerly `hijri-converter`\n\nHijriDate is a Python package for converting between Hijri and Gregorian dates using the Umm al-Qura calendar. The package has been thoroughly verified and tested against original references to ensure its accuracy and reliability. It has an intuitive design, allows rich comparison and basic formatting of Hijri dates, and is optimized for performance.\n\nFeatures\n\n- Accurate and verified Hijri-Gregorian date conversion.\n- Optimized code performance compared to similar packages.\n- Intuitive, clean, and easy-to-use interface.\n- Most of the methods and formats are similar to those of standard library.\n- Multilingual representation of weekday names, months, and calendar era notations.\n- Easily extendable to support other natural languages.\n- Rich comparison between dates.\n- Validation of input dates.\n- Works on Python 3.8+ with zero dependencies.\n- Thoroughly tested with 100% test coverage.\n\nLimitations\n\n- The date range supported by converter is limited to the period from the beginning of 1343 AH (1 August 1924 CE) to the end of 1500 AH (16 November 2077 CE).\n- The conversion is not intended for religious purposes where sighting of the lunar crescent at the beginning of Hijri month is still preferred.\n\nInstallation\n\nTo install using `pip`, run:\n\nTo install using `conda`, run:\n\nBasic Usage\n\nDocumentation\n\nPlease refer to  for complete documentation on this package, which includes background information, benchmarking, usage examples, and API reference.\n\nContributing\n\nIf you're interested in contributing, please check out the [Contributing](https://github.com/dralshehri/hijridate/blob/main/CONTRIBUTING.md) guide for more information on how you can help!\n\nLicense\n\nThis project is licensed under the terms of the MIT license.\n\nAcknowledgements\n\n- [R.H. van Gent](http://www.staff.science.uu.nl/~gent0113) &mdash; inspiration, scientific guidance and resources.\n- [@AZalshehri7](https://github.com/AZalshehri7) &mdash; support in dates review and conversion accuracy verification.\n\nCitation\n\nIf you plan to cite this project in your academic publication, please refer to  for citation information."}, {"name": "hijridate", "tags": ["math", "ui", "web"], "summary": "Accurate Hijri-Gregorian dates converter based on Umm al-Qura calendar", "text": "This library is used to convert between Hijri and Gregorian dates with high accuracy and reliability, utilizing the Umm al-Qura calendar. It enables developers to seamlessly work with both date systems in their applications."}, {"name": "hist", "tags": ["math", "web"], "summary": "Hist classes and utilities", "text": "Hist\n\n[![PyPI version][pypi-version]][pypi-link]\n\n[![PyPI platforms][pypi-platforms]][pypi-link]\n\nHist is an analyst-friendly front-end for\n[boost-histogram](https://github.com/scikit-hep/boost-histogram), designed for\nPython 3.9+ (3.6-3.8 users get older versions). See [what's new](https://hist.readthedocs.io/en/latest/changelog.html).\n\nInstallation\n\nYou can install this library from [PyPI](https://pypi.org/project/hist/) with pip:\n\nIf you do not need the plotting features, you can skip the `[plot]` and/or\n`[fit]` extras. `[fit]` is not currently supported in WebAssembly.\n\nFeatures\n\nHist currently provides everything boost-histogram provides, and the following enhancements:\n\n- Hist augments axes with names:\n  - `name=` is a unique label describing each axis.\n  - `label=` is an optional string that is used in plotting (defaults to `name`\n  - Indexing, projection, and more support named axes.\n  - Experimental `NamedHist` is a `Hist` that disables most forms of positional access, forcing users to use only names.\n\n- The `Hist` class augments `bh.Histogram` with simpler construction:\n  - `flow=False` is a fast way to turn off flow for the axes on construction.\n  - Storages can be given by string.\n  - `storage=` can be omitted, strings and storages can be positional.\n  - `data=` can initialize a histogram with existing data.\n  - `Hist.from_columns` can be used to initialize with a DataFrame or dict.\n  - You can cast back and forth with boost-histogram (or any other extensions).\n\n- Hist support QuickConstruct, an import-free construction system that does not require extra imports:\n  - Use `Hist.new.().().()`.\n  - Axes names can be full (`Regular`) or short (`Reg`).\n  - Histogram arguments (like `data=`) can go in the storage.\n\n- Extended Histogram features:\n  - Direct support for `.name` and `.label`, like axes.\n  - `.density()` computes the density as an array.\n  - `.profile(remove_ax)` can convert a ND COUNT histogram into a (N-1)D MEAN histogram.\n  - `.sort(axis)` supports sorting a histogram by a categorical axis. Optionally takes a function to sort by.\n  - `.fill_flattened(...)` will flatten and fill, including support for AwkwardArray.\n  - `.integrate(...)`, which takes the opposite arguments as `.project`.\n\n- Hist implements UHI+; an extension to the UHI (Unified Histogram Indexing) system designed for import-free interactivity:\n  - Uses `j` suffix to switch to data coordinates in access or slices.\n  - Uses `j` suffix on slices to rebin.\n  - Strings can be used directly to index into string category axes.\n\n- Quick plotting routines encourage exploration:\n  - `.plot()` provides 1D and 2D plots (or use `plot1d()`, `plot2d()`)\n  - `.plot2d_full()` shows 1D projects around a 2D plot.\n  - `.plot_ratio(...)` make a ratio plot between the histogram and another histogram or callable.\n  - `.plot_pull(...)` performs a pull plot.\n  - `.plot_pie()` makes a pie plot.\n  - `.show()` provides a nice str printout using Histoprint.\n\n- Stacks: work with groups of histograms with identical axes\n  - Stacks can be created with `h.stack(axis)`, using index or name of an axis (`StrCategory` axes ideal).\n  - You can also create with `hist.stacks.Stack(h1, h2, ...)`, or use `from_iter` or `from_dict`.\n  - You can index a stack, and set an entry with a matching histogram.\n  - Stacks support `.plot()` and `.show()`, with names (plot labels default to original axes info).\n  - Stacks pass through `.project`, `*`, `+`, and `-`.\n\n- New modules\n  - `intervals` supports frequentist coverage intervals.\n\n- Notebook ready: Hist has gorgeous in-notebook representation.\n  - No dependencies required\n\nUsage\n\nDevelopment\n\nFrom a git checkout, either use [nox](https://nox.thea.codes), or run:\n\nSee [Contributing](https://hist.readthedocs.io/en/latest/contributing.html) guidelines for information on setting up a development environment.\n\nContributors\n\nWe would like to acknowledge the contributors that made this project possible ([emoji key](https://allcontributors.org/docs/en/emoji-key)):\n\n  \n  \n  \n  \n\nThis project follows the [all-contributors](https://github.com/all-contributors/all-contributors) specification.\n\nTalks\n\n---\n\nAcknowledgements\n\nThis library was primarily developed by Henry Schreiner and Nino Lau.\n\nSupport for this work was provided by the National Science Foundation cooperative agreement OAC-1836650 (IRIS-HEP) and OAC-1450377 (DIANA/HEP). Any opinions, findings, conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the National Science Foundation."}, {"name": "hist", "tags": ["math", "web"], "summary": "Hist classes and utilities", "text": "This library is used to provide a Python interface for creating and manipulating histogram data, leveraging the boost-histogram library for efficient computation. It offers analyst-friendly features like named axes and additional utilities for tasks such as plotting and fitting."}, {"name": "holoviews", "tags": ["math", "ui", "visualization", "web"], "summary": "A high-level plotting API for the PyData ecosystem built on HoloViews.", "text": "HoloViews\n\n**Stop plotting your data - annotate your data and let it visualize\nitself.**\n\n---\nDownloads\nBuild Status\nCoverage\nLatest dev release\nLatest release\nPython\nDocs\nBinder\nSupport\n\nHoloViews is an [open-source](https://github.com/holoviz/holoviews/blob/main/LICENSE.txt)\nPython library designed to make data analysis and visualization seamless\nand simple. With HoloViews, you can usually express what you want to do\nin very few lines of code, letting you focus on what you are trying to\nexplore and convey, not on the process of plotting.\n\nCheck out the [HoloViews web site](https://holoviews.org) for extensive examples and documentation.\n\nInstallation\n\nHoloViews works with [Python](https://github.com/holoviz/holoviews/actions/workflows/test.yaml)\non Linux, Windows, or Mac, and works seamlessly with [Jupyter Notebook and JupyterLab](https://jupyter.org).\n\nYou can install HoloViews either with `conda` or `pip`, for more information see the [install guide](https://holoviews.org/install.html).\n\nDeveloper Guide\n\nIf you want to help develop HoloViews, you can checkout the [developer guide](https://dev.holoviews.org/developer_guide/index.html),\nthis guide will help you get set-up. Making it easy to contribute.\n\nSupport & Feedback\n\nIf you find any bugs or have any feature suggestions please file a GitHub\n[issue](https://github.com/holoviz/holoviews/issues).\n\nIf you have any usage questions, please ask them on [HoloViz Discourse](https://discourse.holoviz.org/),\n\nFor general discussion, we have a [Discord channel](https://discord.gg/AXRHnJU6sP)."}, {"name": "holoviews", "tags": ["math", "ui", "visualization", "web"], "summary": "A high-level plotting API for the PyData ecosystem built on HoloViews.", "text": "This library is used to simplify data analysis and visualization by allowing developers to focus on the exploration and conveyance of insights rather than the plotting process itself. With HoloViews, developers can easily annotate their data to let it visualize itself in a seamless and simple manner."}, {"name": "huggingface-hub", "tags": ["data", "math", "ml"], "summary": "Client library to download and publish models, datasets and other repos on the huggingface.co hub", "text": "Welcome to the huggingface_hub library\n\nThe `huggingface_hub` library allows you to interact with the [Hugging Face Hub](https://huggingface.co/), a platform democratizing open-source Machine Learning for creators and collaborators. Discover pre-trained models and datasets for your projects or play with the thousands of machine learning apps hosted on the Hub. You can also create and share your own models, datasets and demos with the community. The `huggingface_hub` library provides a simple way to do all these things with Python.\n\nKey features\n\nInstallation\n\nInstall the `huggingface_hub` package with [pip](https://pypi.org/project/huggingface-hub/):\n\nIf you prefer, you can also install it with [conda](https://huggingface.co/docs/huggingface_hub/en/installation#install-with-conda).\n\nIn order to keep the package minimal by default, `huggingface_hub` comes with optional dependencies useful for some use cases. For example, if you want to use the MCP module, run:\n\nTo learn more installation and optional dependencies, check out the [installation guide](https://huggingface.co/docs/huggingface_hub/en/installation).\n\nQuick start\n\nDownload files\n\nDownload a single file\n\nOr an entire repository\n\nFiles will be downloaded in a local cache folder. More details in [this guide](https://huggingface.co/docs/huggingface_hub/en/guides/manage-cache).\n\nLogin\n\nThe Hugging Face Hub uses tokens to authenticate applications (see [docs](https://huggingface.co/docs/hub/security-tokens)). To log in your machine, run the following CLI:\n\nCreate a repository\n\nUpload files\n\nUpload a single file\n\nOr an entire folder\n\nFor details in the [upload guide](https://huggingface.co/docs/huggingface_hub/en/guides/upload).\n\nIntegrating to the Hub.\n\nWe're partnering with cool open source ML libraries to provide free model hosting and versioning. You can find the existing integrations [here](https://huggingface.co/docs/hub/libraries).\n\nThe advantages are:\n\n- Free model or dataset hosting for libraries and their users.\n- Built-in file versioning, even with very large files, thanks to a git-based approach.\n- In-browser widgets to play with the uploaded models.\n- Anyone can upload a new model for your library, they just need to add the corresponding tag for the model to be discoverable.\n- Fast downloads! We use Cloudfront (a CDN) to geo-replicate downloads so they're blazing fast from anywhere on the globe.\n- Usage stats and more features to come.\n\nIf you would like to integrate your library, feel free to open an issue to begin the discussion. We wrote a [step-by-step guide](https://huggingface.co/docs/hub/adding-a-library) with \ufe0f showing how to do this integration.\n\nContributions (feature requests, bugs, etc.) are super welcome \ufe0f\n\nEveryone is welcome to contribute, and we value everybody's contribution. Code is not the only way to help the community.\nAnswering questions, helping others, reaching out and improving the documentations are immensely valuable to the community.\nWe wrote a [contribution guide](https://github.com/huggingface/huggingface_hub/blob/main/CONTRIBUTING.md) to summarize\nhow to get started to contribute to this repository."}, {"name": "huggingface-hub", "tags": ["data", "math", "ml"], "summary": "Client library to download and publish models, datasets and other repos on the huggingface.co hub", "text": "This library is used to download and publish models, datasets, and other repositories on the Hugging Face Hub. It provides a simple way for developers to interact with the platform and share their own Machine Learning projects with the community."}, {"name": "humanfriendly", "tags": ["cli", "data", "math", "ui"], "summary": "Human friendly output for text interfaces using Python", "text": "humanfriendly: Human friendly input/output in Python\n====================================================\n\n:target: https://github.com/xolox/python-humanfriendly/actions\n\n:target: https://codecov.io/gh/xolox/python-humanfriendly\n\nThe functions and classes in the `humanfriendly` package can be used to make\ntext interfaces more user friendly. Some example features:\n\n- Parsing and formatting numbers, file sizes, pathnames and timespans in\n  simple, human friendly formats.\n\n- Easy to use timers for long running operations, with human friendly\n  formatting of the resulting timespans.\n\n- Prompting the user to select a choice from a list of options by typing the\n  option's number or a unique substring of the option.\n\n- Terminal interaction including text styling (`ANSI escape sequences`_), user\n  friendly rendering of usage messages and querying the terminal for its\n  size.\n\nThe `humanfriendly` package is currently tested on Python 2.7, 3.5+ and PyPy\n(2.7) on Linux and macOS. While the intention is to support Windows as well,\nyou may encounter some rough edges.\n\n.. contents::\n   :local:\n\nGetting started\n---------------\n\nIt's very simple to start using the `humanfriendly` package::\n\n>>> from humanfriendly import format_size, parse_size\n   >>> from humanfriendly.prompts import prompt_for_input\n   >>> user_input = prompt_for_input(\"Enter a readable file size: \")\n\n>>> num_bytes = parse_size(user_input)\n   >>> print(num_bytes)\n   16000000000\n   >>> print(\"You entered:\", format_size(num_bytes))\n   You entered: 16 GB\n   >>> print(\"You entered:\", format_size(num_bytes, binary=True))\n   You entered: 14.9 GiB\n\nTo get a demonstration of supported terminal text styles (based on\n`ANSI escape sequences`_) you can run the following command::\n\n$ humanfriendly --demo\n\nCommand line\n------------\n\n.. A DRY solution to avoid duplication of the `humanfriendly --help' text:\n..\n.. [[[cog\n.. from humanfriendly.usage import inject_usage\n.. inject_usage('humanfriendly.cli')\n.. ]]]\n\n**Usage:** `humanfriendly [OPTIONS]`\n\nHuman friendly input/output (text formatting) on the command\nline based on the Python package with the same name.\n\n**Supported options:**\n\n.. csv-table::\n   :header: Option, Description\n   :widths: 30, 70"}, {"name": "humanfriendly", "tags": ["cli", "data", "math", "ui"], "summary": "Human friendly output for text interfaces using Python", "text": "\"``-c``, ``--run-command``\",\"Execute an external command (given as the positional arguments) and render\n   a spinner and timer while the command is running. The exit status of the\n   command is propagated.\"\n   ``--format-table``,\"Read tabular data from standard input (each line is a row and each\n   whitespace separated field is a column), format the data as a table and\n   print the resulting table to standard output. See also the ``--delimiter``\n   option.\"\n   \"``-d``, ``--delimiter=VALUE``\",\"Change the delimiter used by ``--format-table`` to ``VALUE`` (a string). By default\n   all whitespace is treated as a delimiter.\"\n   \"``-l``, ``--format-length=LENGTH``\",\"Convert a length count (given as the integer or float ``LENGTH``) into a human\n   readable string and print that string to standard output.\"\n   \"``-n``, ``--format-number=VALUE``\",\"Format a number (given as the integer or floating point number ``VALUE``) with\n   thousands separators and two decimal places (if needed) and print the\n   formatted number to standard output.\"\n   \"``-s``, ``--format-size=BYTES``\",\"Convert a byte count (given as the integer ``BYTES``) into a human readable\n   string and print that string to standard output.\"\n   \"``-b``, ``--binary``\",\"Change the output of ``-s``, ``--format-size`` to use binary multiples of bytes\n   (base-2) instead of the default decimal multiples of bytes (base-10).\"\n   \"``-t``, ``--format-timespan=SECONDS``\",\"Convert a number of seconds (given as the floating point number ``SECONDS``)\n   into a human readable timespan and print that string to standard output.\"\n   ``--parse-length=VALUE``,\"Parse a human readable length (given as the string ``VALUE``) and print the\n   number of metres to standard output.\"\n   ``--parse-size=VALUE``,\"Parse a human readable data size (given as the string ``VALUE``) and print the\n   number of bytes to standard output.\"\n   ``--demo``,\"Demonstrate changing the style and color of the terminal font using ANSI\n   escape sequences.\"\n   \"``-h``, ``--help``\",Show this message and exit.\n\n.. [[[end]]]\n\nA note about size units\n-----------------------\n\nWhen I originally published the `humanfriendly` package I went with binary\nmultiples of bytes (powers of two). It was pointed out several times that this\nwas a poor choice (see issue `#4`_ and pull requests `#8`_ and `#9`_) and thus\nthe new default became decimal multiples of bytes (powers of ten):\n\n+------+---------------+---------------+\nUnit\n+------+---------------+---------------+\nKB\n+------+---------------+---------------+\nMB\n+------+---------------+---------------+\nGB\n+------+---------------+---------------+\nTB\n+------+---------------+---------------+\netc\n+------+---------------+---------------+\n\nThe option to use binary multiples of bytes remains by passing the keyword\nargument `binary=True` to the `format_size()`_ and `parse_size()`_ functions.\n\nWindows support\n---------------\n\nWindows 10 gained native support for ANSI escape sequences which means commands\nlike ``humanfriendly --demo`` should work out of the box (if your system is\nup-to-date enough). If this doesn't work then you can install the colorama_\npackage, it will be used automatically once installed.\n\nContact\n-------\n\nThe latest version of `humanfriendly` is available on PyPI_ and GitHub_. The\ndocumentation is hosted on `Read the Docs`_ and includes a changelog_. For bug\nreports please create an issue on GitHub_. If you have questions, suggestions,\netc. feel free to send me an e-mail at `peter@peterodding.com`_.\n\nLicense\n-------\n\nThis software is licensed under the `MIT license`_.\n\n\u00a9 2021 Peter Odding."}, {"name": "humanfriendly", "tags": ["cli", "data", "math", "ui"], "summary": "Human friendly output for text interfaces using Python", "text": ".. External references:\n.. _#4: https://github.com/xolox/python-humanfriendly/issues/4\n.. _#8: https://github.com/xolox/python-humanfriendly/pull/8\n.. _#9: https://github.com/xolox/python-humanfriendly/pull/9\n.. _ANSI escape sequences: https://en.wikipedia.org/wiki/ANSI_escape_code\n.. _changelog: https://humanfriendly.readthedocs.io/en/latest/changelog.html\n.. _colorama: https://pypi.org/project/colorama\n.. _format_size(): https://humanfriendly.readthedocs.io/en/latest/#humanfriendly.format_size\n.. _GitHub: https://github.com/xolox/python-humanfriendly\n.. _MIT license: https://en.wikipedia.org/wiki/MIT_License\n.. _parse_size(): https://humanfriendly.readthedocs.io/en/latest/#humanfriendly.parse_size\n.. _peter@peterodding.com: peter@peterodding.com\n.. _PyPI: https://pypi.org/project/humanfriendly\n.. _Read the Docs: https://humanfriendly.readthedocs.io"}, {"name": "humanfriendly", "tags": ["cli", "data", "math", "ui"], "summary": "Human friendly output for text interfaces using Python", "text": "This library is used to make text interfaces more user-friendly by providing features such as formatting numbers, timespans, and file sizes in human-readable formats. It also offers tools for easy terminal interaction, prompting users with lists of options, and displaying styled text."}, {"name": "ibis-framework", "tags": ["data", "math", "web"], "summary": "The portable Python dataframe library", "text": "Ibis\n\n(http://ibis-project.org)\n(https://ibis-project.zulipchat.com)\n(https://anaconda.org/conda-forge/ibis-framework)\n(https://pypi.org/project/ibis-framework)\n(https://github.com/ibis-project/ibis/actions/workflows/ibis-main.yml?query=branch%3Amain)\n(https://github.com/ibis-project/ibis/actions/workflows/ibis-backends.yml?query=branch%3Amain)\n(https://codecov.io/gh/ibis-project/ibis)\n\nWhat is Ibis?\n\nIbis is the portable Python dataframe library:\n\n- Fast local dataframes (via DuckDB by default)\n- Lazy dataframe expressions\n- Interactive mode for iterative data exploration\n- [Compose Python dataframe and SQL code](#python--sql-better-together)\n- Use the same dataframe API for [nearly 20 backends](#backends)\n- Iterate locally and deploy remotely by [changing a single line of code](#portability)\n\nSee the documentation on [\"Why Ibis?\"](https://ibis-project.org/why) to learn more.\n\nGetting started\n\nYou can `pip install` Ibis with a backend and example data:\n\n>  **Tip**\n>\n> See the [installation guide](https://ibis-project.org/install) for more installation options.\n\nThen use Ibis:\n\n>  **Tip**\n>\n> See the [getting started tutorial](https://ibis-project.org/tutorials/basics) for a full introduction to Ibis.\n\nPython + SQL: better together\n\nFor most backends, Ibis works by compiling its dataframe expressions into SQL:\n\nYou can mix SQL and Python code:\n\nThis allows you to combine the flexibility of Python with the scale and performance of modern SQL.\n\nBackends\n\nIbis supports nearly 20 backends:\n\nHow it works\n\nMost Python dataframes are tightly coupled to their execution engine. And many databases only support SQL, with no Python API. Ibis solves this problem by providing a common API for data manipulation in Python, and compiling that API into the backend\u2019s native language. This means you can learn a single API and use it across any supported backend (execution engine).\n\nIbis broadly supports two types of backend:\n\n1. SQL-generating backends\n2. DataFrame-generating backends\n\nPortability\n\nTo use different backends, you can set the backend Ibis uses:\n\nTypically, you'll create a connection object:\n\nAnd work with tables in that backend:\n\nYou can also read from common file formats like CSV or Apache Parquet:\n\nThis allows you to iterate locally and deploy remotely by changing a single line of code.\n\n>  **Tip**\n>\n> Check out [the blog on backend agnostic arrays](https://ibis-project.org/posts/backend-agnostic-arrays/) for one example using the same code across DuckDB and BigQuery.\n\nCommunity and contributing\n\nIbis is an open source project and welcomes contributions from anyone in the community.\n\nJoin our community by interacting on GitHub or chatting with us on [Zulip](https://ibis-project.zulipchat.com/).\n\nFor more information visit https://ibis-project.org/.\n\nGovernance\n\nThe Ibis project is an [independently governed](https://github.com/ibis-project/governance/blob/main/governance.md) open source community project to build and maintain the portable Python dataframe library. Ibis has contributors across a range of data companies and institutions."}, {"name": "ibis-framework", "tags": ["data", "math", "web"], "summary": "The portable Python dataframe library", "text": "This library is used to build high-performance, portable data applications with Python's familiar dataframe API across various data storage systems. It allows developers to seamlessly compose and execute local or remote data analysis tasks using a unified and expressive API."}, {"name": "ibm-watsonx-ai", "tags": ["math", "ml", "web"], "summary": "IBM watsonx.ai API Client", "text": "Welcome to `ibm-watsonx-ai`\n******************************************\n\n``ibm-watsonx-ai`` is a library that allows to work with watsonx.ai service on [IBM Cloud](https://console.bluemix.net/catalog/services/machine-learning) and\nIBM Cloud for Data. Train, test and deploy your models as APIs for application development, share with colleagues using this python library.\n\n[Package documentation](https://ibm.github.io/watsonx-ai-python-sdk)\n=========================================="}, {"name": "ibm-watsonx-ai", "tags": ["math", "ml", "web"], "summary": "IBM watsonx.ai API Client", "text": "This library is used to train, test, and deploy machine learning models as APIs on IBM Cloud and IBM Cloud for Data. With this library, developers can share their trained models with colleagues through a Python interface."}, {"name": "imbalanced-learn", "tags": ["data", "dev", "math", "ml", "web"], "summary": "Toolbox for imbalanced dataset in machine learning", "text": ".. -*- mode: rst -*-\n\n.. _scikit-learn: http://scikit-learn.org/stable/\n\n.. _scikit-learn-contrib: https://github.com/scikit-learn-contrib\n\nGitHubActions\n\n.. _GitHubActions: https://github.com/scikit-learn-contrib/imbalanced-learn/actions/workflows/tests.yml\n\n.. _Codecov: https://codecov.io/gh/scikit-learn-contrib/imbalanced-learn\n\n.. |CircleCI| image:: https://circleci.com/gh/scikit-learn-contrib/imbalanced-learn.svg?style=shield\n.. _CircleCI: https://circleci.com/gh/scikit-learn-contrib/imbalanced-learn/tree/master\n\n.. |PythonVersion| image:: https://img.shields.io/pypi/pyversions/imbalanced-learn.svg\n.. _PythonVersion: https://img.shields.io/pypi/pyversions/imbalanced-learn.svg\n\n.. _Black: :target: https://github.com/psf/black\n\n.. |PythonMinVersion| replace:: 3.10\n.. |NumPyMinVersion| replace:: 1.25.2\n.. |SciPyMinVersion| replace:: 1.11.4\n.. |ScikitLearnMinVersion| replace:: 1.4.2\n.. |MatplotlibMinVersion| replace:: 3.7.3\n.. |PandasMinVersion| replace:: 2.0.3\n.. |TensorflowMinVersion| replace:: 2.16.1\n.. |KerasMinVersion| replace:: 3.3.3\n.. |SeabornMinVersion| replace:: 0.12.2\n.. |PytestMinVersion| replace:: 7.2.2\n\nimbalanced-learn\n================\n\nimbalanced-learn is a python package offering a number of re-sampling techniques\ncommonly used in datasets showing strong between-class imbalance.\nIt is compatible with scikit-learn_ and is part of scikit-learn-contrib_\nprojects.\n\nDocumentation\n-------------\n\nInstallation documentation, API documentation, and examples can be found on the\ndocumentation_.\n\n.. _documentation: https://imbalanced-learn.org/stable/\n\nInstallation\n------------\n\nDependencies\n~~~~~~~~~~~~\n\n`imbalanced-learn` requires the following dependencies:\n\n- Python (>= |PythonMinVersion|)\n- NumPy (>= |NumPyMinVersion|)\n- SciPy (>= |SciPyMinVersion|)\n- Scikit-learn (>= |ScikitLearnMinVersion|)\n- Pytest (>= |PytestMinVersion|)\n\nAdditionally, `imbalanced-learn` requires the following optional dependencies:\n\n- Pandas (>= |PandasMinVersion|) for dealing with dataframes\n- Tensorflow (>= |TensorflowMinVersion|) for dealing with TensorFlow models\n- Keras (>= |KerasMinVersion|) for dealing with Keras models\n\nThe examples will requires the following additional dependencies:\n\n- Matplotlib (>= |MatplotlibMinVersion|)\n- Seaborn (>= |SeabornMinVersion|)\n\nInstallation\n~~~~~~~~~~~~\n\nFrom PyPi or conda-forge repositories\n.....................................\n\nimbalanced-learn is currently available on the PyPi's repositories and you can\ninstall it via `pip`::\n\nThe package is release also in Anaconda Cloud platform::\n\nFrom source available on GitHub\n...............................\n\nIf you prefer, you can clone it and run the setup.py file. Use the following\ncommands to get a copy from Github and install all dependencies::\n\n  git clone https://github.com/scikit-learn-contrib/imbalanced-learn.git\n  cd imbalanced-learn\n\nBe aware that you can install in developer mode with::\n\nIf you wish to make pull-requests on GitHub, we advise you to install\npre-commit::\n\n  pre-commit install\n\nTesting\n~~~~~~~\n\nAfter installation, you can use `pytest` to run the test suite::\n\n  make coverage\n\nDevelopment\n-----------\n\nThe development of this scikit-learn-contrib is in line with the one\nof the scikit-learn community. Therefore, you can refer to their\n`Development Guide\n`_.\n\nEndorsement of the Scientific Python Specification\n--------------------------------------------------\n\nWe endorse good practices from the Scientific Python Ecosystem Coordination (SPEC).\nThe full list of recommendations is available `here`_.\n\nSee below the list of recommendations that we endorse for the imbalanced-learn project.\n\nSPEC 0 \u2014 Minimum Supported Dependencies\n\n   :target: https://scientific-python.org/specs/spec-0000/\n\n.. _here: https://scientific-python.org/specs/\n\nAbout\n-----\n\nIf you use imbalanced-learn in a scientific publication, we would appreciate\ncitations to the following paper::\n\n  @article{JMLR:v18:16-365,\n  author  = {Guillaume  Lema{{\\^i}}tre and Fernando Nogueira and Christos K. Aridas},\n  title   = {Imbalanced-learn: A Python Toolbox to Tackle the Curse of Imbalanced Datasets in Machine Learning},\n  journal = {Journal of Machine Learning Research},\n  year    = {2017},\n  volume  = {18},\n  number  = {17},\n  pages   = {1-5},\n  url     = {http://jmlr.org/papers/v18/16-365}\n  }\n\nMost classification algorithms will only perform optimally when the number of\nsamples of each class is roughly the same. Highly skewed datasets, where the\nminority is heavily outnumbered by one or more classes, have proven to be a\nchallenge while at the same time becoming more and more common.\n\nOne way of addressing this issue is by re-sampling the dataset as to offset this\nimbalance with the hope of arriving at a more robust and fair decision boundary\nthan you would otherwise.\n\nYou can refer to the `imbalanced-learn`_ documentation to find details about\nthe implemented algorithms.\n\n.. _imbalanced-learn: https://imbalanced-learn.org/stable/user_guide.html"}, {"name": "imbalanced-learn", "tags": ["data", "dev", "math", "ml", "web"], "summary": "Toolbox for imbalanced dataset in machine learning", "text": "This library is used to handle and mitigate the challenges associated with imbalanced datasets in machine learning, enabling developers to build more accurate models when dealing with class imbalance problems. With this library, developers can employ various techniques such as oversampling, undersampling, and ensemble methods to improve classification performance on imbalanced data."}, {"name": "imgaug", "tags": ["math", "ml"], "summary": "Image augmentation library for deep neural networks", "text": "A library for image augmentation in machine learning experiments, particularly convolutional\nneural networks. Supports the augmentation of images, keypoints/landmarks, bounding boxes, heatmaps and segmentation\nmaps in a variety of different ways."}, {"name": "imgaug", "tags": ["math", "ml"], "summary": "Image augmentation library for deep neural networks", "text": "This library is used to augment images and various associated data (keypoints, bounding boxes, etc.) for use in deep neural network training, improving model robustness and performance. With imgaug, developers can increase the diversity of their training datasets through a range of image transformations and manipulations."}, {"name": "inference-cli", "tags": ["math", "ml", "web"], "summary": "With no prior knowledge of machine learning or device-specific deployment, you can deploy a computer vision model to a range of devices and environments using Roboflow Inference CLI.", "text": "[inference package](https://pypi.org/project/inference/) | [inference repo](https://github.com/roboflow/inference)\n\n  \n\n(https://pypistats.org/packages/inference-cli)\n(https://github.com/roboflow/inference/blob/main/LICENSE)\n\nRoboflow Inference CLI\n\nRoboflow Inference CLI offers a lightweight interface for running the Roboflow inference server locally or the Roboflow Hosted API.\n\nTo create custom inference server Docker images, go to the parent package, [Roboflow Inference](https://pypi.org/project/inference/).\n\n[Roboflow](https://roboflow.com) has everything you need to deploy a computer vision model to a range of devices and environments. Inference supports object detection, classification, and instance segmentation models, and running foundation models (CLIP and SAM).\n\n\u200d Examples\n\ninference server start\n\nStarts a local inference server. It optionally takes a port number (default is 9001) and will only start the docker container if there is not already a container running on that port.\n\nBefore you begin, ensure that you have Docker installed on your machine. Docker provides a containerized environment, \nallowing the Roboflow Inference Server to run in a consistent and isolated manner, regardless of the host system. If \nyou haven't installed Docker yet, you can get it from [Docker's official website](https://www.docker.com/get-started).\n\nThe CLI will automatically detect the device you are running on and pull the appropriate Docker image.\n\nParameter `--env-file` (or `-e`) is the optional path for .env file that will be loaded into inference server \nin case that values of internal parameters needs to be adjusted. Any value passed explicitly as command parameter\nis considered as more important and will shadow the value defined in `.env` file under the same target variable name.\n\ninference server status\n\nChecks the status of the local inference server.\n\ninference server stop\n\nStops the inference server.\n\ninference infer\n\nRuns inference on a single image. It takes a path to an image, a Roboflow project name, model version, and API key, and will return a JSON object with the model's predictions. You can also specify a host to run inference on our hosted inference server.\n\nLocal image\n\nHosted image\n\nHosted API inference\n\nSupported Devices\n\nRoboflow Inference CLI currently supports the following device targets:\n\n- x86 CPU\n- ARM64 CPU\n- NVIDIA GPU\n\nFor Jetson specific inference server images, check out the [Roboflow Inference](https://pypi.org/project/inference/) package, or pull the images directly following instructions in the official [Roboflow Inference documentation](https://inference.roboflow.com/quickstart/docker/#pull-from-docker-hub).\n\nlicense\n\nThe Roboflow Inference code is distributed under an [Apache 2.0 license](https://github.com/roboflow/inference/blob/master/LICENSE.core). The models supported by Roboflow Inference have their own licenses. View the licenses for supported models below.\n\nmodel\n:------------------------\n`inference/models/clip`\n`inference/models/gaze`\n`inference/models/sam`\n`inference/models/vit`\n`inference/models/yolact`\n`inference/models/yolov5`\n`inference/models/yolov7`\n`inference/models/yolov8`\n\nenterprise\n\nWith a Roboflow Inference Enterprise License, you can access additional Inference features, including:\n\n- Server cluster deployment\n- Active learning\n- YOLOv5 and YOLOv8 model sub-license\n\nTo learn more, [contact the Roboflow team](https://roboflow.com/sales).\n\ndocumentation\n\nVisit our [documentation](https://roboflow.github.io/inference) for usage examples and reference for Roboflow Inference.\n\nexplore more Roboflow open source projects\n\nProject\n:----------------------------------------------------------------\n[supervision](https://roboflow.com/supervision)\n[Autodistill](https://github.com/autodistill/autodistill)\n[Inference](https://github.com/roboflow/inference) (this project)\n[Notebooks](https://roboflow.com/notebooks)\n[Collect](https://github.com/roboflow/roboflow-collect)"}, {"name": "inference-cli", "tags": ["math", "ml", "web"], "summary": "With no prior knowledge of machine learning or device-specific deployment, you can deploy a computer vision model to a range of devices and environments using Roboflow Inference CLI.", "text": "This library is used to deploy computer vision models for tasks such as object detection, classification, and instance segmentation across various devices and environments with minimal setup and prior knowledge of machine learning. It provides a lightweight interface for running inference on local machines or leveraging the Roboflow Hosted API."}, {"name": "inference-gpu", "tags": ["data", "math", "ml", "ui", "web"], "summary": "With no prior knowledge of machine learning or device-specific deployment, you can deploy a computer vision model to a range of devices and environments using Roboflow Inference.", "text": "Make Any Camera an AI Camera\n\nInference turns any computer or edge device into a command center for your computer vision projects.\n\n* \ufe0f Self-host [your own fine-tuned models](https://inference.roboflow.com/quickstart/explore_models/)\n*  Access the latest and greatest foundation models (like [Florence-2](https://blog.roboflow.com/florence-2/), [CLIP](https://blog.roboflow.com/openai-clip/), and [SAM2](https://blog.roboflow.com/what-is-segment-anything-2/))\n*  Use [Workflows](https://inference.roboflow.com/workflows/about/) to track, count, time, measure, and visualize\n* \ufe0f Combine ML with traditional CV methods (like OCR, Barcode Reading, QR, and template matching)\n*  Monitor, record, and analyze predictions\n*  [Manage](https://inference.roboflow.com/workflows/video_processing/overview/) cameras and video streams\n*  Send notifications when events happen\n*  Connect with external systems and APIs\n*  [Extend](https://inference.roboflow.com/workflows/create_workflow_block/) with your own code and models\n*  Deploy production systems at scale\n\nSee [Example Workflows](https://inference.roboflow.com/workflows/gallery/) for common use-cases like detecting small objects with SAHI, multi-model consensus, active learning, reading license plates, blurring faces, background removal, and more.\n\n[Time In Zone Workflow Example](https://github.com/user-attachments/assets/743233d9-3460-442d-83f8-20e29e76b346)\n\nquickstart\n\n[Install Docker](https://docs.docker.com/engine/install/) (and\n[NVIDIA Container Toolkit](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html)\nfor GPU acceleration if you have a CUDA-enabled GPU). Then run\n\nThis will pull the proper image for your machine and start it in development mode.\n\nIn development mode, a Jupyter notebook server with a quickstart guide runs on\n[http://localhost:9001/notebook/start](http://localhost:9001/notebook/start). Dive in there for a whirlwind tour\nof your new Inference Server's functionality!\n\nNow you're ready to connect your camera streams and\n[start building & deploying Workflows in the UI](https://app.roboflow.com/workflows)\nor [interacting with your new server](https://inference.roboflow.com/workflows/create_and_run/)\nvia its API.\n\n\ufe0f build with Workflows\n\nA key component of Inference is [Workflows](https://roboflow.com/workflows), composable blocks of common functionality that give models a common interface to make chaining and experimentation easy.\n\nWith Workflows, you can:\n* Detect, classify, and segment objects in images using state-of-the-art models.\n* Use Large Multimodal Models (LMMs) to make determinations at any stage in a workflow.\n* Seamlessly swap out models for a given task.\n* Chain models together.\n* Track, count, time, measure, and visualize objects.\n* Add business logic and extend functionality to work with your external systems.\n\nWorkflows allow you to extend simple model predictions to build computer vision micro-services that fit into a larger application or fully self-contained visual agents that run on a video stream.\n\n[Learn more](https://roboflow.com/workflows), read [the Workflows docs](https://inference.roboflow.com/workflows/about/), or [start building](https://app.roboflow.com/workflows).\n\nconnecting via api\n\nOnce you've installed Inference, your machine is a fully-featured CV center.\nYou can use its API to run models and workflows on images and video streams.\nBy default, the server is running locally on\n[`localhost:9001`](http://localhost:9001).\n\nTo interface with your server via Python, use our SDK:\n\nThen run [an example model comparison Workflow](https://app.roboflow.com/workflows/embed/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ3b3JrZmxvd0lkIjoiSHhIODdZR0FGUWhaVmtOVWNEeVUiLCJ3b3Jrc3BhY2VJZCI6IlhySm9BRVFCQkFPc2ozMmpYZ0lPIiwidXNlcklkIjoiNXcyMFZ6UU9iVFhqSmhUanE2a2FkOXVicm0zMyIsImlhdCI6MTczNTIzNDA4Mn0.AA78pZnlivFs5pBPVX9cMigFAOIIMZk0dA4gxEF5tj4)\nlike this:\n\nIn other languages, use the server's REST API;\nyou can access the API docs for your server at\n[`/docs` (OpenAPI format)](http://localhost:9001/docs) or\n[`/redoc` (Redoc Format)](http://localhost:9001/redoc).\n\nCheck out [the inference_sdk docs](https://inference.roboflow.com/inference_helpers/inference_sdk/)\nto see what else you can do with your new server.\n\nconnect to video streams\n\nThe inference server is a video processing beast. You can set it up to run\nWorkflows on RTSP streams, webcam devices, and more. It will handle hardware\nacceleration, multiprocessing, video decoding and GPU batching to get the\nmost out of your hardware."}, {"name": "inference-gpu", "tags": ["data", "math", "ml", "ui", "web"], "summary": "With no prior knowledge of machine learning or device-specific deployment, you can deploy a computer vision model to a range of devices and environments using Roboflow Inference.", "text": "[This example workflow](https://app.roboflow.com/workflows/embed/eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ3b3JrZmxvd0lkIjoiNHMzSDAzcmtyU0JiSDhFMjEzZUUiLCJ3b3Jrc3BhY2VJZCI6IlhySm9BRVFCQkFPc2ozMmpYZ0lPIiwidXNlcklkIjoiNXcyMFZ6UU9iVFhqSmhUanE2a2FkOXVicm0zMyIsImlhdCI6MTczNTIzOTk3NX0.TYdmD5AS8tbpz8AxEr5xW-05LlegK61kq-5_OReIrwc?showGraph=true&hideToolbar=false)\nwill watch a stream for frames that\n[CLIP thinks](https://blog.roboflow.com/openai-clip/) match an\ninputted text prompt.\n\nPipeline outputs can be consumed via API for downstream processing or the\nWorkflow can be configured to call external services with Notification blocks\n(like [Email](https://inference.roboflow.com/workflows/blocks/email_notification/)\nor [Twilio](https://inference.roboflow.com/workflows/blocks/twilio_sms_notification/))\nor the [Webhook block](https://inference.roboflow.com/workflows/blocks/webhook_sink/).\nFor more info on video pipeline management, see the\n[Video Processing overview](https://inference.roboflow.com/workflows/video_processing/overview/).\n\nIf you have a Roboflow account & have linked an API key, you can also remotely\n[monitor and manage your running streams](https://app.roboflow.com/devices)\nvia the Roboflow UI.\n\nconnect to the cloud\n\nWithout an API Key, you can access a wide range of pre-trained and foundational models and run public Workflows.\n\nPass an optional [Roboflow API Key](https://app.roboflow.com/settings/api) to the `inference_sdk` or API to access additional features enhanced by Roboflow's Cloud\nplatform. When running with an API Key, usage is metered according to\nRoboflow's [pricing tiers](https://roboflow.com/pricing).\n\nOpen Access\n-------------------------\n[Pre-Trained Models](https://inference.roboflow.com/quickstart/aliases/#supported-pre-trained-models)\n[Foundation Models](https://inference.roboflow.com/foundation/about/)\n[Video Stream Management](https://inference.roboflow.com/workflows/video_processing/overview/)\n[Dynamic Python Blocks](https://inference.roboflow.com/workflows/custom_python_code_blocks/)\n[Public Workflows](https://inference.roboflow.com/workflows/about/)\n[Private Workflows](https://docs.roboflow.com/workflows/create-a-workflow)\n[Fine-Tuned Models](https://roboflow.com/train)\n[Universe Models](https://roboflow.com/universe)\n[Active Learning](https://inference.roboflow.com/workflows/blocks/roboflow_dataset_upload/)\n[Serverless Hosted API](https://docs.roboflow.com/deploy/hosted-api)\n[Dedicated Deployments](https://docs.roboflow.com/deploy/dedicated-deployments)\n[Commercial Model Licensing](https://roboflow.com/licensing)\n[Device Management](https://docs.roboflow.com/roboflow-enterprise)\n[Model Monitoring](https://docs.roboflow.com/deploy/model-monitoring)\n\n\ufe0f hosted compute\n\nIf you don't want to manage your own infrastructure for self-hosting, Roboflow offers a hosted Inference Server via [one-click Dedicated Deployments](https://docs.roboflow.com/deploy/dedicated-deployments) (CPU and GPU machines) billed hourly, or simple models and Workflows via our [serverless Hosted API](https://docs.roboflow.com/deploy/hosted-api) billed per API-call.\n\nWe offer a [generous free-tier](https://roboflow.com/pricing) to get started.\n\n\ufe0f run on-prem or self-hosted\n\nInference is designed to run on a wide range of hardware from beefy cloud servers to tiny edge devices. This lets you easily develop against your local machine or our cloud infrastructure and then seamlessly switch to another device for production deployment.\n\n`inference server start` attempts to automatically choose the optimal container to optimize performance on your machine (including with GPU acceleration via NVIDIA CUDA when available). Special installation notes and performance tips by device are listed below:\n\n* [Linux](https://inference.roboflow.com/install/linux/)\n* [Windows](https://inference.roboflow.com/install/windows/)\n* [Mac](https://inference.roboflow.com/install/mac/)\n* [NVIDIA Jetson](https://inference.roboflow.com/install/jetson/)\n* [Raspberry Pi](https://inference.roboflow.com/install/raspberry-pi/)\n* [Your Own Cloud](https://inference.roboflow.com/install/cloud/)\n* [Other Devices](https://inference.roboflow.com/install/other/)\n\n\u2b50\ufe0f New: Enterprise Hardware\n\nFor manufacturing and logistics use-cases Roboflow now offers [the NVIDIA Jetson-based Flowbox](https://roboflow.com/industries/manufacturing/box), a ruggedized CV center pre-configured with Inference and optimized for running in secure networks. It has integrated support for machine vision cameras like Basler and Lucid over GigE, supports interfacing with PLCs and HMIs via OPC or MQTT, enables enterprise device management through a DMZ, and comes with the support of our team of computer vision experts to ensure your project is a success.\n\ndocumentation\n\nVisit our [documentation](https://inference.roboflow.com) to explore comprehensive guides, detailed API references, and a wide array of tutorials designed to help you harness the full potential of the Inference package.\n\n\u00a9 license\n\nThe core of Inference is licensed under Apache 2.0.\n\nModels are subject to licensing which respects the underlying architecture. These licenses are listed in [`inference/models`](/inference/models). Paid Roboflow accounts include a commercial license for some models (see [roboflow.com/licensing](https://roboflow.com/licensing) for details)."}, {"name": "inference-gpu", "tags": ["data", "math", "ml", "ui", "web"], "summary": "With no prior knowledge of machine learning or device-specific deployment, you can deploy a computer vision model to a range of devices and environments using Roboflow Inference.", "text": "Cloud connected functionality (like our model and Workflows registries, dataset management, model monitoring, device management, and managed infrastructure) requires a Roboflow account and API key & is metered based on usage.\n\nEnterprise functionality is source-available in [`inference/enterprise`](/inference/enterprise/) under an [enterprise license](/inference/enterprise/LICENSE.txt) and usage in production requires an active Enterprise contract in good standing.\n\nSee the \"Self Hosting and Edge Deployment\" section of the [Roboflow Licensing](https://roboflow.com/licensing) documentation for more information on how Roboflow Inference is licensed.\n\ncontribution\n\nWe would love your input to improve Roboflow Inference! Please see our [contributing guide](https://github.com/roboflow/inference/blob/master/CONTRIBUTING.md) to get started. Thank you to all of our contributors!"}, {"name": "inference-gpu", "tags": ["data", "math", "ml", "ui", "web"], "summary": "With no prior knowledge of machine learning or device-specific deployment, you can deploy a computer vision model to a range of devices and environments using Roboflow Inference.", "text": "This library is used to deploy computer vision models on a range of devices and environments without requiring knowledge of machine learning or device-specific deployment. It enables users to turn any camera into an AI-powered camera by leveraging Roboflow Inference's capabilities, including self-hosting fine-tuned models and accessing latest foundation models."}, {"name": "inspect-ai", "tags": ["dev", "math", "ml"], "summary": "Framework for large language model evaluations", "text": "[](https://aisi.gov.uk/)\n\nWelcome to Inspect, a framework for large language model evaluations created by the [UK AI Security Institute](https://aisi.gov.uk/).\n\nInspect provides many built-in components, including facilities for prompt engineering, tool usage, multi-turn dialog, and model graded evaluations. Extensions to Inspect (e.g.\u00a0to support new elicitation and scoring techniques) can be provided by other Python packages.\n\nTo get started with Inspect, please see the documentation at .\n\nInspect also includes a collection of over 100 pre-built evaluations ready to run on any model (learn more at [Inspect Evals](https://ukgovernmentbeis.github.io/inspect_evals/))\n\n***\n\nTo work on development of Inspect, clone the repository and install with the `-e` flag and `[dev]` optional dependencies:\n\nOptionally install pre-commit hooks via\n\nRun linting, formatting, and tests via\n\nIf you use VS Code, you should be sure to have installed the recommended extensions (Python, Ruff, and MyPy). Note that you'll be prompted to install these when you open the project in VS Code.\n\n***\n\nTo work on the Inspect documentation, install the optional `[doc]` dependencies with the `-e` flag and build the docs:\n\nIf you intend to work on the docs iteratively, you'll want to install the Quarto extension in VS Code."}, {"name": "inspect-ai", "tags": ["dev", "math", "ml"], "summary": "Framework for large language model evaluations", "text": "This library is used to enable large language model evaluations through a framework that includes built-in components for prompt engineering, multi-turn dialog, and model graded evaluations. With this library, developers can easily run over 100 pre-built evaluation scripts on any model."}, {"name": "intervaltree", "tags": ["math", "ml", "web"], "summary": "Editable interval tree data structure for Python 2 and 3", "text": "intervaltree\n============\n\nA mutable, self-balancing interval tree for Python 2 and 3. Queries may be by point, by range overlap, or by range envelopment.\n\nThis library was designed to allow tagging text and time intervals, where the intervals include the lower bound but not the upper bound.\n\n**Version 3 changes!**\n\n* The `search(begin, end, strict)` method no longer exists. Instead, use one of these:\n* The `extend(items)` method no longer exists. Instead, use `update(items)`.\n* Methods like `merge_overlaps()` which took a `strict` argument consistently default to `strict=True`. Before, some methods defaulted to `True` and others to `False`.\n\nInstalling\n----------\n\nFeatures\n--------\n\n* Supports Python 2.7 and Python 3.5+ (Tested under 2.7, and 3.5 thru 3.14)\n* Initializing\n\n* Insertions\n\n* Deletions\n\n* Point queries\n\n* Overlap queries\n\n* Envelop queries\n\n* Membership queries\n\n* Iterable\n\n* Sizing\n\n* Set-like operations\n\n* Restructuring\n\n* Copying and typecasting\n\n* Pickle-friendly\n* Automatic AVL balancing\n\nExamples\n--------\n\n* Getting started\n\n* Adding intervals - any object works!\n\n* Query by point\n\n* Query by range\n\nFuture improvements\n-------------------\n\nSee the [issue tracker][] on GitHub.\n\nBased on\n--------\n\n* Eternally Confuzzled's [AVL tree][Confuzzled AVL tree]\n* Wikipedia's [Interval Tree][Wiki intervaltree]\n* Heavily modified from Tyler Kahn's [Interval Tree implementation in Python][Kahn intervaltree] ([GitHub project][Kahn intervaltree GH])\n* Incorporates contributions from:\n\nCopyright\n---------\n\n* [Chaim Leib Halbert][GH], 2013-2025\n* Modifications, [Konstantin Tretyakov][Konstantin intervaltree], 2014\n\nLicensed under the [Apache License, version 2.0][Apache].\n\nThe source code for this project is at https://github.com/chaimleib/intervaltree"}, {"name": "intervaltree", "tags": ["math", "ml", "web"], "summary": "Editable interval tree data structure for Python 2 and 3", "text": "This library is used to efficiently manage intervals of time or text in a mutable data structure that supports self-balancing queries by point, range overlap, or range envelopment. With intervaltree, developers can quickly and accurately identify overlapping intervals, allowing for efficient management of tagging text and time intervals."}, {"name": "jdcal", "tags": ["math"], "summary": "Julian dates from proleptic Gregorian and Julian calendars.", "text": "jdcal\n=====\n\n.. _TPM: http://www.sal.wisc.edu/~jwp/astro/tpm/tpm.html\n.. _Jeffrey W. Percival: http://www.sal.wisc.edu/~jwp/\n.. _IAU SOFA: http://www.iausofa.org/\n.. _pip: https://pypi.org/project/pip/\n.. _easy_install: https://setuptools.readthedocs.io/en/latest/easy_install.html\n\n.. image:: https://travis-ci.org/phn/jdcal.svg?branch=master\n\nThis module contains functions for converting between Julian dates and\ncalendar dates.\n\nA function for converting Gregorian calendar dates to Julian dates, and\nanother function for converting Julian calendar dates to Julian dates\nare defined. Two functions for the reverse calculations are also\ndefined.\n\nDifferent regions of the world switched to Gregorian calendar from\nJulian calendar on different dates. Having separate functions for Julian\nand Gregorian calendars allow maximum flexibility in choosing the\nrelevant calendar.\n\nJulian dates are stored in two floating point numbers (double).  Julian\ndates, and Modified Julian dates, are large numbers. If only one number\nis used, then the precision of the time stored is limited. Using two\nnumbers, time can be split in a manner that will allow maximum\nprecision. For example, the first number could be the Julian date for\nthe beginning of a day and the second number could be the fractional\nday. Calculations that need the latter part can now work with maximum\nprecision.\n\nAll the above functions are \"proleptic\". This means that they work for\ndates on which the concerned calendar is not valid. For example,\nGregorian calendar was not used prior to around October 1582.\n\nA function to test if a given Gregorian calendar year is a leap year is\nalso defined.\n\nZero point of Modified Julian Date (MJD) and the MJD of 2000/1/1\n12:00:00 are also given as module level constants.\n\nExamples\n--------\n\nSome examples are given below. For more information see\n\nGregorian calendar:\n\n.. code-block:: python\n\nJulian calendar:\n\n.. code-block:: python\n\nGregorian leap year:\n\n.. code-block:: python\n\nJD for zero point of MJD, and MJD for JD2000.0:\n\n.. code-block:: python\n\nInstallation\n------------\n\nThe module can be installed using `pip`_ or `easy_install`_::\n\n  $ pip install jdcal\n\nor,\n\n::\n\n  $ easy_install jdcal\n\nTests are in ``test_jdcal.py``.\n\nCredits\n--------\n\n1. A good amount of the code is based on the excellent `TPM`_ C library\n   by `Jeffrey W. Percival`_.\n2. The inspiration to split Julian dates into two numbers came from the\n   `IAU SOFA`_ C library. No code or algorithm from the SOFA library is\n   used in `jdcal`.\n\nLicense\n-------\n\nReleased under BSD; see LICENSE.txt.\n\nFor comments and suggestions, email to user `prasanthhn` in the `gmail.com`\ndomain."}, {"name": "jdcal", "tags": ["math"], "summary": "Julian dates from proleptic Gregorian and Julian calendars.", "text": "This library is used to convert between Julian and Gregorian calendar dates, allowing developers to work with dates in both calendars seamlessly. It provides a convenient way to perform date conversions, enabling the creation of applications that can handle diverse calendaring systems and their varying adoption dates worldwide."}, {"name": "joblib", "tags": ["dev", "math"], "summary": "Lightweight pipelining with Python functions", "text": "PyPi\n\n   :alt: Joblib version\n\n   :target: https://github.com/joblib/joblib/actions/workflows/test.yml?query=branch%3Amain\n   :alt: CI status\n\n   :target: https://codecov.io/gh/joblib/joblib\n   :alt: Codecov coverage\n\nThe homepage of joblib with user documentation is located on:\n\nGetting the latest code\n=======================\n\nTo get the latest code using git, simply type::\n\nIf you don't have git installed, you can download a zip\nof the latest code: https://github.com/joblib/joblib/archive/refs/heads/main.zip\n\nInstalling\n==========\n\nYou can use `pip` to install joblib from any directory::\n\nor install it in editable mode from the source directory::\n\nDependencies\n============\n\n- Joblib has no mandatory dependencies besides Python (supported versions are\n  3.9+).\n- Joblib has an optional dependency on Numpy (at least version 1.6.1) for array\n  manipulation.\n- Joblib includes its own vendored copy of\n  `loky `_ for process management.\n- Joblib can efficiently dump and load numpy arrays but does not require numpy\n  to be installed.\n- Joblib has an optional dependency on\n  `python-lz4 `_ as a faster alternative to\n  zlib and gzip for compressed serialization.\n- Joblib has an optional dependency on psutil to mitigate memory leaks in\n  parallel worker processes.\n- Some examples require external dependencies such as pandas. See the\n  instructions in the `Building the docs`_ section for details.\n\nWorkflow to contribute\n======================\n\nTo contribute to joblib, first create an account on `github\n`_. Once this is done, fork the `joblib repository\n`_ to have your own repository,\nclone it using ``git clone``. Make your changes in a branch of your clone, push\nthem to your github account, test them locally, and when you are happy with\nthem, send a pull request to the main repository.\n\nYou can use `pre-commit `_ to run code style checks\nbefore each commit::\n\npre-commit checks can be disabled for a single commit with::\n\nRunning the test suite\n======================\n\nTo run the test suite, you need the pytest (version >= 3) and coverage modules.\nRun the test suite using::\n\nfrom the root of the project.\n\nBuilding the docs\n=================\n\nTo build the docs you need to have sphinx (>=1.4) and some dependencies\ninstalled::\n\nThe docs can then be built with the following command::\n\nThe html docs are located in the ``doc/_build/html`` directory.\n\nMaking a source tarball\n=======================\n\nTo create a source tarball, eg for packaging or distributing, run the\nfollowing command::\n\nThe tarball will be created in the `dist` directory. This command will create\nthe resulting tarball that can be installed with no extra dependencies than the\nPython standard library.\n\nMaking a release and uploading it to PyPI\n=========================================\n\nThis command is only run by project manager, to make a release, and\nupload in to PyPI::\n\nNote that the documentation should automatically get updated at each git\npush. If that is not the case, try building th doc locally and resolve\nany doc build error (in particular when running the examples).\n\nUpdating the changelog\n======================\n\nChanges are listed in the CHANGES.rst file. They must be manually updated\nbut, the following git command may be used to generate the lines::"}, {"name": "joblib", "tags": ["dev", "math"], "summary": "Lightweight pipelining with Python functions", "text": "This library is used to simplify and parallelize complex computations with Python functions, allowing developers to efficiently process large datasets in a scalable manner. With joblib, developers can create robust pipelines that leverage multi-core CPUs and distribute tasks across processes for optimal performance."}, {"name": "jplephem", "tags": ["cli", "math", "web"], "summary": "Use a JPL ephemeris to predict planet positions.", "text": "Cite as: `Astrophysics Source Code Library, record ascl:1112.014\n`_\n\nThis package can load and use a Jet Propulsion Laboratory (JPL)\nephemeris for predicting the position and velocity of a planet or other\nSolar System body.  It currently supports binary SPK files (extension\n``.bsp``) like `those distributed by the Jet Propulsion Laboratory\n`_ that are:\n\n* **Type 2** \u2014 positions stored as Chebyshev polynomials, with velocity\n  derived by computing their derivative.\n\n* **Type 3** \u2014 positions and velocities both stored explicitly as\n  Chebyshev polynomials.\n\n* **Type 9** \u2014 a series of discrete positions and velocities, with\n  separate timestamps that do not need to be equally spaced.  Currently\n  there is only support for linear interpolation: for Type\u00a09 ephemerides\n  of polynomial degree\u00a01, not of any higher degrees.\n\nNote that even if an ephemeris isn\u2019t one of the above types, you can\nstill use ``jplephem`` to read its text comment and list the segments\ninside, using the subcommands ``comment`` and ``daf`` described below.\n\nInstallation\n------------\n\nThe only third-party package that ``jplephem`` depends on is `NumPy\n`_, which ``pip`` will automatically attempt to\ninstall alongside ``pyephem`` when you run::\n\nIf you see NumPy compilation errors, then try downloading and installing\nNumPy directly from `its web site `_ or simply\nuse a distribution of Python with science tools already installed, like\n`Anaconda `_.\n\nNote that ``jplephem`` offers only the logic necessary to produce plain\nthree-dimensional vectors.  Most programmers interested in astronomy\nwill want to look at `Skyfield `_\ninstead, which uses ``jplephem`` but converts the numbers into more\ntraditional measurements like right ascension and declination.\n\nMost users will use ``jplephem`` with the Satellite Planet Kernel (SPK)\nfiles that the NAIF facility at NASA JPL offers for use with their own\nSPICE toolkit.  They have collected their most useful kernels beneath\nthe directory:\n\nTo learn more about SPK files, the official `SPK Required Reading\n`_\ndocument is available from the NAIF facility\u2019s web site under the NASA\nJPL domain.\n\nCommand Line Tool\n-----------------\n\nIf you have downloaded a ``.bsp`` file, you can run ``jplephem`` from\nthe command line to display the data inside of it::\n\nYou can also take a large ephemeris and produce a smaller excerpt by\nlimiting the range of dates that it covers::\n\nThe comment text of the output ephemeris is copied verbatim from the\ninput ephemeris, with the addition of a few lines of text at the top\nthat identify the output file as a mere excerpt, and record the dates\nthe user asked for.\n\nYou will get an error if your starting year is negative, because Unix\ncommands expect a list of options when they see a dash.  The fix is to\nprovide a special argument ``--`` which says \u201cI\u2019m done passing options,\neven if the next argument stars with a dash\u201d::"}, {"name": "jplephem", "tags": ["cli", "math", "web"], "summary": "Use a JPL ephemeris to predict planet positions.", "text": "You can also filter by the integer codes for the targets you need.\nUnrecognized targets will not raise an error, to let you apply a master\nlist of targets to a whole series of SPK files that might or might not\neach have all of the targets::\n\nIf the input ephemeris is a URL, then ``jplephem`` will try to save\nbandwidth by fetching only the blocks of the remote file that are\nnecessary to cover the dates you have specified.  For example, the\nJupiter satellite ephemeris ``jup310.bsp`` is famously large, weighing\nin a nearly a gigabyte.  But if all you need are Jupiter's satellites\nfor a few months, you can download considerably less data::\n\nIn this case only about one-thousandth of the ephemeris's data needed to\nbe downloaded.\n\nGetting Started With DE421\n--------------------------\n\nThe DE421 ephemeris is a useful starting point.  It weighs in at 17\u00a0MB,\nbut provides predictions over the years 1900\u20132050:\n\nAfter the kernel has downloaded, you can use ``jplephem`` to load this\nSPK file and learn about the segments it offers:\n\n>>> from jplephem.spk import SPK\n>>> kernel = SPK.open('de421.bsp')\n>>> print(kernel)\nFile type DAF/SPK and format LTL-IEEE with 15 segments:\n1899-07-29..2053-10-09  Type 2  Solar System Barycenter (0) -> Mercury Barycenter (1)\n1899-07-29..2053-10-09  Type 2  Solar System Barycenter (0) -> Venus Barycenter (2)\n1899-07-29..2053-10-09  Type 2  Solar System Barycenter (0) -> Earth Barycenter (3)\n1899-07-29..2053-10-09  Type 2  Solar System Barycenter (0) -> Mars Barycenter (4)\n1899-07-29..2053-10-09  Type 2  Solar System Barycenter (0) -> Jupiter Barycenter (5)\n1899-07-29..2053-10-09  Type 2  Solar System Barycenter (0) -> Saturn Barycenter (6)\n1899-07-29..2053-10-09  Type 2  Solar System Barycenter (0) -> Uranus Barycenter (7)\n1899-07-29..2053-10-09  Type 2  Solar System Barycenter (0) -> Neptune Barycenter (8)\n1899-07-29..2053-10-09  Type 2  Solar System Barycenter (0) -> Pluto Barycenter (9)\n1899-07-29..2053-10-09  Type 2  Solar System Barycenter (0) -> Sun (10)\n1899-07-29..2053-10-09  Type 2  Earth Barycenter (3) -> Moon (301)\n1899-07-29..2053-10-09  Type 2  Earth Barycenter (3) -> Earth (399)\n1899-07-29..2053-10-09  Type 2  Mercury Barycenter (1) -> Mercury (199)\n1899-07-29..2053-10-09  Type 2  Venus Barycenter (2) -> Venus (299)\n1899-07-29..2053-10-09  Type 2  Mars Barycenter (4) -> Mars (499)\n\nSince the next few examples involve vector output, let\u2019s tell NumPy to\nmake vector output attractive.\n\n>>> import numpy as np\n>>> np.set_printoptions(precision=3)\n\nEach segment of the file lets you predict the position of one body with\nrespect to another for a given Julian date.  A small routine is provided\nto convert Gregorian calendar dates to Julian dates:\n\n>>> from jplephem.calendar import compute_julian_date\n>>> compute_julian_date(2015, 2, 8)\n2457061.5\n\nHere is how to compute the coordinates of Mars (target\u00a04) relative to\nthe Solar System barycenter (target\u00a00) at midnight 2015 February\u00a08 TDB\n(Barycentric Dynamical Time), using the Julian date we just computed:\n\n>>> position = kernel[0,4].compute(2457061.5)\n>>> print(position)\n[2.057e+08 4.251e+07 1.394e+07]\n\nBy contrast, it takes three steps to learn the position of Mars with\nrespect to the Earth: from Mars to the Solar System barycenter, to the\nEarth-Moon barycenter (3), and finally to Earth itself (399)."}, {"name": "jplephem", "tags": ["cli", "math", "web"], "summary": "Use a JPL ephemeris to predict planet positions.", "text": ">>> position = kernel[0,4].compute(2457061.5)\n>>> position -= kernel[0,3].compute(2457061.5)\n>>> position -= kernel[3,399].compute(2457061.5)\n>>> print(position)\n[ 3.161e+08 -4.679e+07 -2.476e+07]\n\nYou can see that the output of this ephemeris DE421 is in kilometers.\nIf you use another ephemeris, check its documentation to be sure of the\nunits that it employs.\n\nIf you supply the date as a NumPy array, then each component that is\nreturned will itself be a vector as long as your date:\n\n>>> jd = np.array([2457061.5, 2457062.5, 2457063.5, 2457064.5])\n>>> position = kernel[0,4].compute(jd)\n>>> print(position)\n[[2.057e+08 2.053e+08 2.049e+08 2.045e+08]\n [4.251e+07 4.453e+07 4.654e+07 4.855e+07]\n [1.394e+07 1.487e+07 1.581e+07 1.674e+07]]\n\nSome ephemerides include velocity inline by returning a 6-vector instead\nof a 3-vector.  For an ephemeris that does not, you can ask for the\nChebyshev polynomial to be differentiated to produce a velocity, which\nis delivered as a second return value:\n\n>>> position, velocity = kernel[0,4].compute_and_differentiate(2457061.5)\n>>> print(position)\n[2.057e+08 4.251e+07 1.394e+07]\n>>> print(velocity)\n[-363896.059 2019662.996  936169.773]\n\nThe velocity will by default be distance traveled per day, in whatever\nunits for distance the ephemeris happens to use.  To get a velocity per\nsecond, simply divide by the number of seconds in a day:\n\n>>> velocity_per_second = velocity / 86400.0\n>>> print(velocity_per_second)\n[-4.212 23.376 10.835]\n\nDetails of the API\n------------------\n\nHere are a few details for people ready to go beyond the high-level API\nprovided above and read through the code to learn more.\n\n* Instead of reading an entire ephemeris into memory, ``jplephem``\n  memory-maps the underlying file so that the operating system can\n  efficiently page into RAM only the data that your code is using.\n\n* Once the metadata has been parsed from the binary SPK file, the\n  polynomial coefficients themselves are loaded by building a NumPy\n  array object that has access to the raw binary file contents.\n  Happily, NumPy already knows how to interpret a packed array of\n  double-precision floats.  You can learn about the underlying DAF\n  \u201cDouble Precision Array File\u201d format, in case you ever need to open\n  other such array files in Python, through the ``DAF`` class in the\n  module ``jplephem.daf``.\n\n* An SPK file is made of segments.  When you first create an ``SPK``\n  kernel object ``k``, it examines the file and creates a list of\n  ``Segment`` objects that it keeps in a list under an attribute named\n  ``k.segments`` which you are free to examine in your own code by\n  looping over it.\n\n* There is more information about each segment beyond the one-line\n  summary that you get when you print out the SPK file, which you can\n  see by asking the segment to print itself verbosely:\n\n>>> segment = kernel[3,399]\n  >>> print(segment.describe())\n  1899-07-29..2053-10-09  Type 2  Earth Barycenter (3) -> Earth (399)\n\n* Each ``Segment`` loaded from the kernel has a number of attributes\n  that are loaded from the SPK file:"}, {"name": "jplephem", "tags": ["cli", "math", "web"], "summary": "Use a JPL ephemeris to predict planet positions.", "text": ">>> from jplephem.spk import BaseSegment\n  >>> help(BaseSegment)\n  Help on class BaseSegment in module jplephem.spk:\n  ...\nsegment.source - official ephemeris name, like 'DE-0430LE-0430'\nsegment.start_second - initial epoch, as seconds from J2000\nsegment.end_second - final epoch, as seconds from J2000\nsegment.start_jd - start_second, converted to a Julian Date\nsegment.end_jd - end_second, converted to a Julian Date\nsegment.center - integer center identifier\nsegment.target - integer target identifier\nsegment.frame - integer frame identifier\nsegment.data_type - integer data type identifier\nsegment.start_i - index where segment starts\nsegment.end_i - index where segment ends\n  ...\n\n* If you want to access the raw coefficients, use the segment\n  ``load_array()`` method.  It returns two floats and a NumPy array:\n\n>>> initial_epoch, interval_length, coefficients = segment.load_array()\n  >>> print(coefficients.shape)\n  (3, 14080, 13)\n\n* The square-bracket lookup mechanism ``kernel[3,399]`` is a\n  non-standard convenience that returns only the last matching segment\n  in the file.  While the SPK standard does say that the last segment\n  takes precedence, it also says that earlier segments for a particular\n  center-target pair should be fallen back upon for dates that the last\n  segment does not cover.  So, if you ever tackle a complicated kernel,\n  you will need to implement fallback rules that send some dates to the\n  final segment for a given center and target, but that send other dates\n  to earlier segments that are qualified to cover them.\n\n* If you are accounting for light travel time and require repeated\n  computation of the position, but then need the velocity at the end,\n  and want to avoid repeating the expensive position calculation, then\n  try out the ``segment.generate()`` method - it will let you ask for\n  the position, and then only proceed to the velocity once you are sure\n  that the light-time error is now small enough.\n\nHigh-Precision Dates\n--------------------\n\nSince all modern Julian dates are numbers larger than 2.4 million, a\nstandard 64-bit Python or NumPy float necessarily leaves only a limited\nnumber of bits available for the fractional part.  *Technical Note\n2011-02* from the United States Naval Observatory's Astronomical\nApplications Department suggests that the `precision possible with a\n64-bit floating point Julian date is around 20.1\u00a0\u00b5s\n`_.\n\nIf you need to supply times and receive back planetary positions with\ngreater precision than 20.1\u00a0\u00b5s, then you have two options.\n\nFirst, you can supply times using the special ``float96`` NumPy type,\nwhich is also aliased to the name ``longfloat``.  If you provide either\na ``float96`` scalar or a ``float96`` array as your ``tdb`` parameter to\nany ``jplephem`` routine, you should get back a high-precision result.\n\nSecond, you can split your date or dates into two pieces, and supply\nthem as a pair of arguments two ``tdb`` and ``tdb2``.  One popular\napproach for how to split your date is to use the ``tdb`` float for the\ninteger Julian date, and ``tdb2`` for the fraction that specifies the\ntime of day.  Nearly all ``jplephem`` routines accept this optional\n``tdb2`` argument if you wish to provide it, thanks to the work of\nMarten van Kerkwijk!\n\nSupport for Binary PCKs\n-----------------------"}, {"name": "jplephem", "tags": ["cli", "math", "web"], "summary": "Use a JPL ephemeris to predict planet positions.", "text": "You can also load and produce rotation matrices from a binary PCK file.\nIts segments are available through the ``segments`` attributes of the\nreturned object.\n\n>>> from jplephem.pck import PCK\n>>> p = PCK.open('moon_pa_de421_1900-2050.bpc')\n>>> p.segments[0].body\n31006\n>>> p.segments[0].frame\n1\n>>> p.segments[0].data_type\n2\n\nGiven a solary system barycenter Julian date, the segment will return\nthe three angles necessary to build a rotation matrix: right ascension\nof the pole, declination of the pole, and cumulative rotation of the\nbody\u2019s axis.  Typically these will all be in radians.\n\n>>> tdb = 2454540.34103\n>>> print(p.segments[0].compute(tdb, 0.0, False))\n[3.928e-02 3.878e-01 3.253e+03]\n\nYou can ask for velocity as well.\n\n>>> r, v = p.segments[0].compute(tdb, 0.0, True)\n>>> print(r)\n[3.928e-02 3.878e-01 3.253e+03]\n>>> print(v)\n[6.707e-09 4.838e-10 2.655e-06]\n\nClosing an ephemeris\n--------------------\n\nTo release all open files and memory maps associated with an ephemeris,\ncall its ``close()`` method.\n\n>>> kernel.close()\n>>> p.close()\n\nReporting issues\n----------------\n\nYou can report any issues, bugs, or problems at the GitHub repository:\n\nChangelog\n---------\n\n**2025 June 22 \u2014 Version 2.23**\n\n* An ephemeris created with the ``excerpt`` command, instead of simply\n  copying verbatim the comments area of the original ephemeris, now adds\n  text declaring \u201cThis is an ephemeris excerpt created by jplephem\u201d and\n  recording the dates that the user asked for.\n\n* The segments of an excerpt ephemeris now advertise exactly the start\n  date and end date that the user asked for, even if the underlying\n  polynomials cover a wider range of dates.\n\n* A new ``-v`` (\u201cverbose\u201d) command-line option to the ``spk``\n  sub-command prints not only each segment\u2019s descriptor, but the\n  dimensions and date range of its underlying polynomial array.\n\n**2024 April 24 \u2014 Version 2.22**\n\n* When printed, segments now print their start and end dates using the\n  Gregorian calendar instead of printing raw Julian dates.\n\n* A small ``compute_julian_date`` routine is now provided for converting\n  calendar dates into Julian dates.\n\n* Fixed the text of the ``ValueError`` that is raised when the PCK\n  segment ``compute()`` method is given an out-of-range date; it was\n  reporting incorrectly large numbers for the Julian date range, because\n  a PCK counts time using seconds before or after J2000, not years.\n\n**2023 December 1 \u2014 Version 2.21**\n\n* Tweaked an import to avoid a fatal exception under Python\u00a02, in case\n  anyone is still using it.\n\n**2023 November 13 \u2014 Version 2.20**\n\n* Each segment is now protected by a lock, in case two threads\n  simultaneously trigger the code that performs the initial load of the\n  segment\u2019s data; the symptom was a rare exception ``ValueError: cannot\n  reshape array``.\n\n**2023 September 6 \u2014 Version 2.19**\n\n* Fixed a bug in the ``excerpt`` command that was causing it to truncate\n  its output when the input ephemeris had more than about two dozen\n  segments.  The command\u2019s output should now include all matching\n  segments from even a very large ephemeris.\n\n* Fixed the ``excerpt`` command so the calendar dates specified on the\n  command line produce Julian dates ending with the fraction ``.5``,\n  which makes excerpt endpoints more exact."}, {"name": "jplephem", "tags": ["cli", "math", "web"], "summary": "Use a JPL ephemeris to predict planet positions.", "text": "**2022 September 28 \u2014 Version 2.18**\n\n* Added support for big-endian processors, and created a GitHub Actions\n  CI build that includes both a big- and a little-endian architecture.\n\n**2021 December 31 \u2014 Version 2.17**\n\n* Fixed an ``AttributeError`` in the ``excerpt`` command.\n\n**2021 July 3 \u2014 Version 2.16**\n\n* Fixed a ``ValueError`` raised in the ``excerpt`` command when an\n  ephemeris segment needs to be entirely skipped because it has no\n  overlap with the user-specified range of dates.\n\n* Added a ``__version__`` constant to the package\u2019s top level.\n\n**2020 September 2 \u2014 Version 2.15**\n\n* The ``excerpt`` subcommand now accepts a ``--targets`` option to save\n  space by copying only matching segments into the output SPK file.\n\n* The Julian day fraction ``tdb2`` is handled even more carefully than\n  before, providing a smoother delta between successive positions when\n  the difference between successive times is down around\u00a00.1\u00a0\u00b5s.\n\n**2020 March 26 \u2014 Version 2.14**\n\n* Fall back to plain file I/O on platforms that support ``fileno()`` but\n  that don\u2019t support ``mmap()``, like the `Pyodide platform\n  `_.\n\n**2020 February 22 \u2014 Version 2.13**\n\n* The exception raised when a segment is given a Julian date outside the\n  segment\u2019s date range is now an instance of the ``ValueError`` subclass\n  ``OutOfRangeError`` that reminds the caller of the range of dates\n  supported by the SPK segment, and carries an array attribute\n  indicating which input dates were at fault.\n\n**2019 December 13 \u2014 Version 2.12**\n\n* Replaced use of NumPy ``flip()`` with a reverse slice ``[::-1]`` after\n  discovering the function was a recent addition that some user installs\n  of NumPy do not support.\n\n**2019 December 13 \u2014 Version 2.11**\n\n* Reverse the order in which Chebyshev polynomials are computed to\n  slightly increase speed, to simplify the code, and in one case\n  (comparing PCK output to NASA) to gain a partial digit of extra\n  precision.\n\n**2019 December 11 \u2014 Version 2.10**\n\n* Document and release support for ``.bcp`` binary PCK kernel files\n  through the new ``jplephem.pck`` module.\n\n**2019 January 3 \u2014 Version 2.9**\n\n* Added the ``load_array()`` method to the segment class.\n\n**2018 July 22 \u2014 Version 2.8**\n\n* Switched to a making a single memory map of the entire file, to avoid\n  running out of file descriptors when users load an ephemeris with\n  hundreds of segments.\n\n**2018 February 11 \u2014 Version 2.7**\n\n* Expanded the command line tool, most notably with the ability to fetch\n  over HTTP only those sections of a large ephemeris that cover a\n  specific range of dates, producing a smaller ``.bsp`` file.\n\n**2016 December 19 \u2014 Version 2.6**\n\n* Fixed the ability to invoke the module from the command line with\n  ``python -m jplephem``, and added a test to keep it fixed.\n\n**2015 November 9 \u2014 Version 2.5**\n\n* Move ``fileno()`` call out of the ``DAF`` constructor to support\n  fetching at least summary information from ``StringIO`` objects.\n\n**2015 November 1 \u2014 Version 2.4**\n\n* Add Windows compatibility by switching ``mmap()`` from using\n  ``PAGESIZE`` to ``ALLOCATIONGRANULARITY``."}, {"name": "jplephem", "tags": ["cli", "math", "web"], "summary": "Use a JPL ephemeris to predict planet positions.", "text": "* Avoid a new NumPy deprecation warning by being careful to use only\n  integers in the NumPy ``shape`` tuple.\n\n* Add names \"TDB\" and \"TT\" to the names database for DE430.\n\n**2015 August 16 \u2014 Version 2.3**\n\n* Added auto-detection and support for old NAIF/DAF kernels like\n  ``de405.bsp`` to the main ``DAF`` class itself, instead of requiring\n  the awkward use of an entirely different alternative class.\n\n**2015 August 5 \u2014 Version 2.2**\n\n* You can now invoke ``jplephem`` from the command line.\n\n* Fixes an exception that was raised for SPK segments with a coefficient\n  count of only 2, like the DE421 and DE430 segments that provide the\n  offset of Mercury from the Mercury barycenter.\n\n* Supports old NAIF/DAF kernels like ``de405.bsp``.\n\n* The ``SPK()`` constructor is now simpler, taking a ``DAF`` object\n  instead of an open file.  This is considered an internal API change \u2014\n  the public API is the constructor ``SPK.open()``.\n\n**2015 February 24 \u2014 Version 2.1**\n\n* Switched from mapping an entire SPK file into memory at once to\n  memory-mapping each segment separately on demand.\n\n**2015 February 8 \u2014 Version 2.0**\n\n* Added support for SPICE SPK kernel files downloaded directly from\n  NASA, and designated old Python-packaged ephemerides as \u201clegacy.\u201d\n\n**2013 November 26 \u2014 Version 1.2**\n\n* Helge Eichhorn fixed the default for the ``position_and_velocity()``\n  argument ``tdb2`` so it defaults to zero days instead of 2.0 days.\n  Tests were added to prevent any future regression.\n\n**2013 July 10 \u2014 Version 1.1**\n\n* Deprecates the old ``compute()`` method in favor of separate\n  ``position()`` and ``position_and_velocity()`` methods.\n\n* Supports computing position and velocity in two separate phases by\n  saving a \u201cbundle\u201d of coefficients returned by ``compute_bundle()``.\n\n* From Marten van Kerkwijk: a second ``tdb2`` time argument, for users\n  who want to build higher precision dates out of two 64-bit floats.\n\n**2013 January 18 \u2014 Version 1.0**\n\n* Initial release\n\nReferences\n----------\n\nThe Jet Propulsion Laboratory's \u201cSolar System Dynamics\u201d page introduces\nthe various options for doing solar system position computations:\n\nEquivalent FORTRAN code for using the ephemerides be found at the same\nFTP site: ftp://ssd.jpl.nasa.gov/pub/eph/planets/fortran/"}, {"name": "jplephem", "tags": ["cli", "math", "web"], "summary": "Use a JPL ephemeris to predict planet positions.", "text": "This library is used to load and use a JPL ephemeris for predicting the position and velocity of planets or other Solar System bodies from binary SPK files. It supports various types of ephemeris files, including Type 2, 3, and 9, allowing developers to accurately calculate celestial body positions and velocities."}, {"name": "jpype1", "tags": ["dev", "math", "web"], "summary": "A Python to Java bridge", "text": ".. image:: doc/logo_small.png\n   :alt: JPype logo\n   :align: center\n\nJPype\n=====\n   \nimplementation\n\nJPype is a Python module to provide full access to Java from \nwithin Python. It allows Python to make use of Java only libraries,\nexploring and visualization of Java structures, development and testing\nof Java libraries, scientific computing, and much more.  By gaining \nthe best of both worlds using Python for rapid prototyping and Java\nfor strong typed production code, JPype provides a powerful environment\nfor engineering and code development.  \n\nThis is achieved not through re-implementing Python, as\nJython has done, but rather through interfacing at the native\nlevel in both virtual machines. This shared memory based \napproach achieves decent computing performance, while providing the\naccess to the entirety of CPython and Java libraries.\n\n:Code: `GitHub\n `_\n:Issue tracker: `GitHub Issues\n `_\n:Discussions: `GitHub Discussions\n `_\n:Documentation: `Python Docs`_\n:License: `Apache 2 License`_\n:Build status:  |TestsCI|_ |Docs|_\n:Quality status:  |Codecov|_ |lgtm_python|_ |lgtm_java|_ |lgtm_cpp|_\n:Version: |PypiVersion|_ |Conda|_\n\nThe work on this project began on `Sourceforge `__.\nLLNL-CODE- 812311\n\n.. |alerts| image:: https://img.shields.io/lgtm/alerts/g/jpype-project/jpype.svg?logo=lgtm&logoWidth=18\n.. _alerts: https://lgtm.com/projects/g/jpype-project/jpype/alerts/\n.. |lgtm_python| image:: https://img.shields.io/lgtm/grade/python/g/jpype-project/jpype.svg?logo=lgtm&logoWidth=18&label=python\n.. _lgtm_python: https://lgtm.com/projects/g/jpype-project/jpype/context:python\n.. |lgtm_java| image:: https://img.shields.io/lgtm/grade/java/g/jpype-project/jpype.svg?logo=lgtm&logoWidth=18&label=java\n.. _lgtm_java: https://lgtm.com/projects/g/jpype-project/jpype/context:java\n.. |lgtm_cpp| image:: https://img.shields.io/lgtm/grade/cpp/g/jpype-project/jpype.svg?logo=lgtm&logoWidth=18&label=C++\n.. _lgtm_cpp: https://lgtm.com/projects/g/jpype-project/jpype/context:cpp\n.. |PypiVersion| image:: https://img.shields.io/pypi/v/Jpype1.svg\n\n.. |Conda| image:: https://img.shields.io/conda/v/conda-forge/jpype1.svg\n.. _Conda: https://anaconda.org/conda-forge/jpype1\n.. |TestsCI| image:: https://dev.azure.com/jpype-project/jpype/_apis/build/status/jpype-project.jpype?branchName=master\n.. _TestsCI: https://dev.azure.com/jpype-project/jpype/_build/latest?definitionId=1&branchName=master\n.. |Docs| image:: https://img.shields.io/readthedocs/jpype.svg\n.. _Docs: http://jpype.readthedocs.org/en/latest/\n\n.. _Codecov: https://codecov.io/gh/jpype-project/jpype\n.. |implementation| image:: https://img.shields.io/pypi/implementation/jpype1.svg\n.. |pyversions| image:: https://img.shields.io/pypi/pyversions/jpype1.svg\n\n.. |platform| image:: https://img.shields.io/conda/pn/conda-forge/jpype1.svg\n.. |license| image:: https://img.shields.io/github/license/jpype-project/jpype.svg\n.. _Apache 2 License: https://github.com/jpype-project/jpype/blob/master/LICENSE\n.. _Python Docs: http://jpype.readthedocs.org/en/latest/\n\nSPDX-License-Identifier: Apache-2.0"}, {"name": "jpype1", "tags": ["dev", "math", "web"], "summary": "A Python to Java bridge", "text": "This library is used to provide full access to Java from within Python, allowing developers to leverage Java libraries and structures directly from their Python code. With JPype, developers can combine the rapid prototyping capabilities of Python with the strong typing and production-ready features of Java."}, {"name": "julius", "tags": ["dev", "math", "web"], "summary": "Nice DSP sweets: resampling, FFT Convolutions. All with PyTorch, differentiable and with CUDA support.", "text": "Julius, fast PyTorch based DSP for audio and 1D signals\n\nJulius contains different Digital Signal Processing algorithms implemented\nwith PyTorch, so that they are differentiable and available on CUDA.\nNote that all the modules implemented here can be used with TorchScript.\n\nFor now, I have implemented:\n\nAlong that, you might found useful utilities in:\n\n- [julius.core](https://adefossez.github.io/julius/julius/core.html): DSP related functions.\n- [julius.utils](https://adefossez.github.io/julius/julius/utils.html): Generic utilities.\n\nNews\n\n- 19/09/2022: __`julius` 0.2.7 released:__: fixed ONNX compat (thanks @iver56). I know I missed the 0.2.6 one...\n- 28/07/2021: __`julius` 0.2.5 released:__: support for setting a custom output length when resampling.\n- 22/06/2021: __`julius` 0.2.4 released:__: adding highpass and band passfilters.\n  Extra linting and type checking of the code. New `unfold` implemention, up to\n  x6 faster FFT convolutions and more efficient memory usage.\n- 26/01/2021: __`julius` 0.2.2 released:__ fixing normalization of filters in lowpass and resample to avoid very low frequencies to be leaked.\n  Switch from zero padding to replicate padding (uses first/last value instead of 0) to avoid discontinuities with strong artifacts.\n- 20/01/2021: `julius` implementation of resampling is now officially part of Torchaudio.\n\nInstallation\n\n`julius` requires python 3.6. To install:\n\nUsage\n\nSee the [Julius documentation][docs] for the usage of Julius. Hereafter you will find a few examples\nto get you quickly started:\n\nAlgorithms\n\nResample\n\nThis is an implementation of the [sinc resample algorithm][resample] by Julius O. Smith.\nIt is the same algorithm than the one used in [resampy][resampy] but to run efficiently on GPU it\nis limited to fractional changes of the sample rate. It will be fast if the old and new sample rate\nare small after dividing them by their GCD. For instance going from a sample rate of 2000 to 3000 (2, 3 after removing the GCD)\nwill be extremely fast, while going from 20001 to 30001 will not.\nJulius resampling is faster than resampy even on CPU, and when running on GPU it makes resampling a completely negligible part of your pipeline\n(except of course for weird cases like going from a sample rate of 20001 to 30001).\n\nFFTConv1d\n\nComputing convolutions with very large kernels (>= 128) and a stride of 1 can be much faster\nusing FFT. This implements the same API as `torch.nn.Conv1d` and `torch.nn.functional.conv1d`\nbut with a FFT backend. Dilation and groups are not supported.\nFFTConv will be faster on CPU even for relatively small tensors (a few dozen channels, kernel size\nof 128). On CUDA, due to the higher parallelism, regular convolution can be faster in many cases,\nbut for kernel sizes above 128, for a large number of channels or batch size, FFTConv1d\nwill eventually be faster (basically when you no longer have idle cores that can hide\nthe true complexity of the operation).\n\nLowPass\n\nClassical Finite Impulse Reponse windowed sinc lowpass filter. It will use FFT convolutions automatically\nif the filter size is large enough. This is the basic block from which you can build\nhigh pass and band pass filters (see `julius.filters`).\n\nBands\n\nDecomposition of a signal over frequency bands in the waveform domain. This can be useful for\ninstance to perform parametric EQ (see [Usage](#usage) above).\n\nBenchmarks\n\nYou can find speed tests (and comparisons to reference implementations) on the\n[benchmark][bench]. The CPU benchmarks are run on a Mac Book Pro 2020, with a 2.4 GHz\n8-core intel CPU i9. The GPUs benchmark are run on Nvidia V100 with 16GB of memory.\nWe also compare the validity of our implementations, as compared to reference ones like `resampy`\nor `torch.nn.Conv1d`.\n\nRunning tests\n\nClone this repository, then\n\nTo run the benchmarks:\n\nLicense\n\n`julius` is released under the MIT license.\n\nThanks\n\nThis package is named in the honor of\n[Julius O. Smith](https://ccrma.stanford.edu/~jos/),\nwhose books and website were a gold mine of information for me to learn about DSP. Go checkout his website if you want\nto learn more about DSP.\n\n[bench]:  ./bench.md"}, {"name": "julius", "tags": ["dev", "math", "web"], "summary": "Nice DSP sweets: resampling, FFT Convolutions. All with PyTorch, differentiable and with CUDA support.", "text": "This library is used to accelerate and differentiable digital signal processing tasks, such as resampling and FFT Convolutions, in PyTorch with CUDA support. Developers can leverage this library to perform various DSP operations, including those related to audio and 1D signals, within their PyTorch projects."}, {"name": "keras-tuner", "tags": ["math", "web"], "summary": "A Hyperparameter Tuning Library for Keras", "text": "KerasTuner\n\n(https://github.com/keras-team/keras-tuner/actions?query=workflow%3ATests+branch%3Amaster)\n(https://codecov.io/gh/keras-team/keras-tuner)\n\nKerasTuner is an easy-to-use, scalable hyperparameter optimization framework\nthat solves the pain points of hyperparameter search. Easily configure your\nsearch space with a define-by-run syntax, then leverage one of the available\nsearch algorithms to find the best hyperparameter values for your models.\nKerasTuner comes with Bayesian Optimization, Hyperband, and Random Search algorithms\nbuilt-in, and is also designed to be easy for researchers to extend in order to\nexperiment with new search algorithms.\n\nOfficial Website: [https://keras.io/keras_tuner/](https://keras.io/keras_tuner/)\n\nQuick links\n\n* [Getting started with KerasTuner](https://keras.io/guides/keras_tuner/getting_started)\n* [KerasTuner developer guides](https://keras.io/guides/keras_tuner/)\n* [KerasTuner API reference](https://keras.io/api/keras_tuner/)\n\nInstallation\n\nKerasTuner requires **Python 3.8+** and **TensorFlow 2.0+**.\n\nInstall the latest release:\n\nYou can also check out other versions in our\n[GitHub repository](https://github.com/keras-team/keras-tuner).\n\nQuick introduction\n\nImport KerasTuner and TensorFlow:\n\nWrite a function that creates and returns a Keras model.\nUse the `hp` argument to define the hyperparameters during model creation.\n\nInitialize a tuner (here, `RandomSearch`).\nWe use `objective` to specify the objective to select the best models,\nand we use `max_trials` to specify the number of different models to try.\n\nStart the search and get the best model:\n\nTo learn more about KerasTuner, check out [this starter guide](https://keras.io/guides/keras_tuner/getting_started/).\n\nContributing Guide\n\nPlease refer to the [CONTRIBUTING.md](https://github.com/keras-team/keras-tuner/blob/master/CONTRIBUTING.md) for the contributing guide.\n\nThank all the contributors!\n\n(https://github.com/keras-team/keras-tuner/graphs/contributors)\n\nCommunity\n\nAsk your questions on our [GitHub Discussions](https://github.com/keras-team/keras-tuner/discussions).\n\nCiting KerasTuner\n\nIf KerasTuner helps your research, we appreciate your citations.\nHere is the BibTeX entry:"}, {"name": "keras-tuner", "tags": ["math", "web"], "summary": "A Hyperparameter Tuning Library for Keras", "text": "This library is used to optimize hyperparameters for Keras models by configuring a search space and leveraging built-in search algorithms such as Bayesian Optimization, Hyperband, or Random Search. With KerasTuner, developers can easily fine-tune their models to achieve better performance without manual experimentation."}, {"name": "kmodes", "tags": ["math"], "summary": "Python implementations of the k-modes and k-prototypes clustering algorithms for clustering categorical data.", "text": ".. image:: https://img.shields.io/pypi/v/kmodes.svg\n\n   :alt: Codacy\n\n.. image:: https://img.shields.io/pypi/dm/kmodes.svg\n.. image:: https://img.shields.io/pypi/pyversions/kmodes.svg\n.. image:: https://img.shields.io/pypi/l/kmodes.svg\n\nkmodes\n======\n\nDescription\n-----------\n\nPython implementations of the k-modes and k-prototypes clustering\nalgorithms. Relies on numpy for a lot of the heavy lifting.\n\nk-modes is used for clustering categorical variables. It defines clusters\nbased on the number of matching categories between data points. (This is\nin contrast to the more well-known k-means algorithm, which clusters\nnumerical data based on Euclidean distance.) The k-prototypes algorithm\ncombines k-modes and k-means and is able to cluster mixed numerical /\ncategorical data.\n\nImplemented are:\n\n- k-modes [HUANG97]_ [HUANG98]_\n- k-modes with initialization based on density [CAO09]_\n- k-prototypes [HUANG97]_\n\nThe code is modeled after the clustering algorithms in :code:`scikit-learn`\nand has the same familiar interface.\n\nI would love to have more people play around with this and give me\nfeedback on my implementation. If you come across any issues in running or\ninstalling kmodes,\n`please submit a bug report `_.\n\nEnjoy!\n\nInstallation\n------------\n\n`kmodes` can be installed using `pip`:\n\n.. code:: bash\n\nTo upgrade to the latest version (recommended), run it like this:\n\n.. code:: bash\n\n`kmodes` can also conveniently be installed with `conda` from the `conda-forge` channel:\n\n.. code:: bash\n\nAlternatively, you can build the latest development version from source:\n\n.. code:: bash\n\nUsage\n-----\n.. code:: python\n\nThe examples directory showcases simple use cases of both k-modes\n('soybean.py') and k-prototypes ('stocks.py').\n\nParallel execution\n------------------\n\nThe k-modes and k-prototypes implementations both offer support for\nmultiprocessing via the \n`joblib library `_,\nsimilar to e.g.\u00a0scikit-learn's implementation of k-means, using the\n:code:`n_jobs` parameter. It generally does not make sense to set more jobs\nthan there are processor cores available on your system.\n\nThis potentially speeds up any execution with more than one initialization try,\n:code:`n_init > 1`, which may be helpful to reduce the execution time for\nlarger problems. Note that it depends on your problem whether multiprocessing\nactually helps, so be sure to try that out first. You can check out the\nexamples for some benchmarks.\n\nFAQ\n---\n\n**Q: I'm seeing errors such as \"TypeError: '<' not supported between instances of 'str' and 'float'\"\nwhen using the kprototypes algorithm.**\n\nA: One or more of your numerical feature columns have string values in them. Make sure that all \ncolumns have consistent data types.\n\n----\n\n**Q: How does k-protypes know which of my features are numerical and which are categorical?**\n\nA: You tell it which column indices are categorical using the :code:`categorical` argument. All others are assumed numerical. E.g., :code:`clusters = KPrototypes().fit_predict(X, categorical=[1, 2])`\n\n----\n\n**Q: I'm getting the following error, what gives? \"ModuleNotFoundError: No module named 'kmodes.kmodes'; 'kmodes' is not a package\".**\n\nA: Make sure your working file is not called 'kmodes.py', because it might overrule the :code:`kmodes` package.\n\n----\n\n**Q: I'm getting the following error: \"ValueError: Clustering algorithm could not initialize. Consider assigning the initial clusters manually.\"**\n\nA: This is a feature, not a bug. :code:`kmodes` is telling you that it can't make sense of the data you are presenting it. At least, not with the parameters you are setting the algorithm with. It is up to you, the data scientist, to figure out why. Some hints to possible solutions:\n\n- Run with fewer clusters as the data might not support a large number of clusters\n- Explore and visualize your data, checking for weird distributions, outliers, etc.\n- Clean and normalize the data\n- Increase the ratio of rows to columns\n\n----\n\n**Q: I'm getting the following error: \"ValueError: Input contains NaN, infinity, or a value too large for dtype('float64').\"**\n\nA: Following scikit-learn, the k-modes algorithm does not accept :code:`np.NaN` \nvalues in the :code:`X` matrix. Users are suggested to fill in the missing \ndata in a way that makes sense for the problem at hand.\n\n----\n\n**Q: How would like your library to be cited?**\n\nA: Something along these lines would do nicely:\n\n.. code-block::\n\n  @Misc{devos2015,\n  }\n\nReferences\n----------\n\n.. [HUANG97] Huang, Z.: Clustering large data sets with mixed numeric and\n   categorical values, Proceedings of the First Pacific Asia Knowledge\n   Discovery and Data Mining Conference, Singapore, pp. 21-34, 1997.\n\n.. [HUANG98] Huang, Z.: Extensions to the k-modes algorithm for clustering\n   large data sets with categorical values, Data Mining and Knowledge\n   Discovery 2(3), pp. 283-304, 1998.\n\n.. [CAO09] Cao, F., Liang, J, Bai, L.: A new initialization method for\n   categorical data clustering, Expert Systems with Applications 36(7),\n   pp. 10223-10228., 2009."}, {"name": "kmodes", "tags": ["math"], "summary": "Python implementations of the k-modes and k-prototypes clustering algorithms for clustering categorical data.", "text": "This library is used to perform clustering on categorical data with the k-modes algorithm or handle mixed numerical/categorical data with the k-prototypes algorithm. It enables developers to efficiently group similar records in datasets that contain non-numerical attributes."}, {"name": "kneed", "tags": ["data", "math", "visualization", "web"], "summary": "Knee-point detection in Python", "text": "kneed\n Knee-point detection in Python\n\nThis repository is an attempt to implement the kneedle algorithm, published [here](https://www1.icsi.berkeley.edu/~barath/papers/kneedle-simplex11.pdf). Given a set of `x` and `y` values, `kneed` will return the knee point of the function. The knee point is the point of maximum curvature.\n\nTable of contents\n- [Installation](#installation)\n- [Usage](#usage)\n  - [Input Data](#input-data)\n  - [Find Knee](#find-knee)\n  - [Visualize](#visualize)\n- [Documentation](#documentation)\n- [Interactive](#interactive)\n- [Contributing](#contributing)\n- [Citation](#citation)\n\nInstallation  \n`kneed` has been tested with Python 3.7, 3.8, 3.9, and 3.10.\n\n**anaconda**\n\n**pip**\n\n**Clone from GitHub**\n\nUsage\nThese steps introduce how to use `kneed` by reproducing Figure 2 from the manuscript.\n\nInput Data\nThe `DataGenerator` class is only included as a utility to generate sample datasets. \n>  Note: `x` and `y` must be equal length arrays.\n\nFind Knee  \nThe knee (or elbow) point is calculated simply by instantiating the `KneeLocator` class with `x`, `y` and the appropriate `curve` and `direction`.  \nHere, `kneedle.knee` and/or `kneedle.elbow` store the point of maximum curvature.\n\nThe knee point returned is a value along the `x` axis. The `y` value at the knee can be identified:\n\nVisualize\nThe `KneeLocator` class also has two plotting functions for quick visualizations.\n**Note that all (x, y) are transformed for the normalized plots**\n\nDocumentation\nDocumentation of the parameters and a full API reference can be found [here](https://kneed.readthedocs.io/).\n\nInteractive\nAn interactive streamlit app was developed to help users explore the effect of tuning the parameters.\nThere are two sites where you can test out kneed by copy-pasting your own data:\n1. https://share.streamlit.io/arvkevi/ikneed/main/ikneed.py\n2. https://ikneed.herokuapp.com/\n\nYou can also run your own version -- head over to the [source code for ikneed](https://github.com/arvkevi/ikneed).\n\nContributing\n\nContributions are welcome, please refer to [CONTRIBUTING](https://github.com/arvkevi/kneed/blob/main/CONTRIBUTING.md) \nto learn more about how to contribute.\n\nCitation\n\nFinding a \u201cKneedle\u201d in a Haystack:\nDetecting Knee Points in System Behavior\nVille Satopa\n\u2020\n, Jeannie Albrecht\u2020\n, David Irwin\u2021\n, and Barath Raghavan\u00a7\n\u2020Williams College, Williamstown, MA\n\u2021University of Massachusetts Amherst, Amherst, MA\n\u00a7\nInternational Computer Science Institute, Berkeley, CA"}, {"name": "kneed", "tags": ["data", "math", "visualization", "web"], "summary": "Knee-point detection in Python", "text": "This library is used to detect the knee point of a function given its x and y values, which represents the point of maximum curvature. By using kneed, developers can automatically identify key points in data where the relationship between variables changes most significantly."}, {"name": "kornia-rs", "tags": ["dev", "math", "ml", "ui", "web"], "summary": "Low level implementations for computer vision in Rust", "text": "kornia-rs: low level computer vision library in Rust\n\nEnglish | [\u7b80\u4f53\u4e2d\u6587](README.zh-CN.md)\n\n(https://docs.rs/kornia)\n(LICENSE)\n(https://discord.gg/HfnywwpBnD)\n\nThe `kornia` crate is a low level library for Computer Vision written in [Rust](https://www.rust-lang.org/) \n\nUse the library to perform image I/O, visualization and other low level operations in your machine learning and data-science projects in a thread-safe and efficient way.\n\nTable of Contents\n\n- [Getting Started](#getting-started)\n- [Features](#features)\n- [Installation](#\ufe0f-installation)\n- [Examples](#examples-image-processing)\n- [Python Usage](#python-usage)\n- [Development](#-development)\n- [Contributing](#-contributing)\n- [Citation](#citation)\n\nGetting Started\n\nQuick Example\n\nThe following example demonstrates how to read and display image information:\n\nFeatures\n\n-  The library is primarily written in [Rust](https://www.rust-lang.org/).\n-  Multi-threaded and efficient image I/O, image processing and advanced computer vision operators.\n-  Efficient Tensor and Image API for deep learning and scientific computing.\n-  Python bindings are created with [PyO3/Maturin](https://github.com/PyO3/maturin).\n-  We package with support for Linux [amd64/arm64], macOS and Windows.\n- Supported Python versions are 3.7/3.8/3.9/3.10/3.11/3.12/3.13, including the free-threaded build.\n\nSupported image formats\n\n- Read images from AVIF, BMP, DDS, Farbeld, GIF, HDR, ICO, JPEG (libjpeg-turbo), OpenEXR, PNG, PNM, TGA, TIFF, WebP.\n\nImage processing\n\n- Convert images to grayscale, resize, crop, rotate, flip, pad, normalize, denormalize, and other image processing operations.\n\nVideo processing\n\n- Capture video frames from a camera and video writers.\n\n\ufe0f Installation\n\nRust\n\nAdd the following to your `Cargo.toml`:\n\nAlternatively, you can use each sub-crate separately:\n\nPython\n\nA subset of the full rust API is exposed. See the [kornia documentation](https://kornia.readthedocs.io/en/stable/) for more detail about the API for python functions and objects exposed by the `kornia-rs` Python module.\n\nThe `kornia-rs` library is thread-safe for use under the free-threaded Python build.\n\nSystem Dependencies (Optional)\n\nDepending on the features you want to use, you might need to install the following dependencies in your system:\n\nv4l (Video4Linux camera support)\n\nturbojpeg\n\ngstreamer\n\n**Note:** Check the [gstreamer installation guide](https://docs.rs/gstreamer/latest/gstreamer/#installation) for more details.\n\nExamples: Image Processing\n\nThe following example shows how to read an image, convert it to grayscale and resize it. The image is then logged to a [`rerun`](https://github.com/rerun-io/rerun) recording stream for visualization.\n\nFor more examples and use cases, check out the [`examples`](https://github.com/kornia/kornia-rs/tree/main/examples) directory, which includes:\n- Image processing operations (resize, rotate, normalize, filters)\n- Video capture and processing\n- AprilTag detection\n- Feature detection (FAST)\n- Visual language models (VLM) integration\n- And more...\n\nPython Usage\n\nReading Images\n\nLoad an image, which is converted directly to a numpy array to ease the integration with other libraries.\n\nWriting Images\n\nWrite an image to disk:\n\nEncoding and Decoding\n\nEncode or decode image streams using the `turbojpeg` backend:\n\nImage Resizing\n\nResize an image using the `kornia-rs` backend with SIMD acceleration:\n\n\u200d Development\n\nPrerequisites\n\nBefore you begin, ensure you have `rust` and `python3` installed on your system.\n\nSetting Up Your Development Environment\n\n1. **Install Rust** using rustup:\n   \n\n2. **Install [`uv`](https://docs.astral.sh/uv/)** to manage Python dependencies:\n   \n\n3. **Install [`just`](https://github.com/casey/just)** command runner for managing development tasks:\n   \n\n4. **Clone the repository** to your local directory:\n\nAvailable Commands\n\nYou can check all available development commands by running `just` in the root directory of the project:\n\nDevelopment Container\n\nThis project includes a development container configuration for a consistent development environment across different machines.\n\n**Using the Dev Container:**\n\n1. Install the `Remote - Containers` extension in Visual Studio Code\n2. Open the project folder in VS Code\n3. Press `F1` and select `Remote-Containers: Reopen in Container`\n4. VS Code will build and open the project in the containerized environment\n\nThe devcontainer includes all necessary dependencies and tools for building and testing `kornia-rs`.\n\nRust Development\n\nCompile the project and run all tests:\n\nTo run specific tests:\n\nTo run clippy linting:\n\nPython Development\n\nBuild Python wheels using `maturin`:\n\nRun Python tests:\n\nContributing\n\nWe welcome contributions! Please read [CONTRIBUTING.md](CONTRIBUTING.md) for:\n- Coding standards and style guidelines\n- Development workflow\n- How to run local checks before submitting PRs\n\nCommunity\n\nThis is a child project of [Kornia](https://github.com/kornia/kornia).\n\nCitation\n\nIf you use kornia-rs in your research, please cite:"}, {"name": "kornia-rs", "tags": ["dev", "math", "ml", "ui", "web"], "summary": "Low level implementations for computer vision in Rust", "text": "This library is used to perform image I/O, visualization, and other low-level operations in machine learning and data-science projects. It provides a thread-safe and efficient way to handle computer vision tasks written in Rust."}, {"name": "kornia", "tags": ["math", "ml", "web"], "summary": "Open Source Differentiable Computer Vision Library for PyTorch", "text": "Key Components\n1. **Differentiable Image Processing**\n  Kornia provides a comprehensive suite of image processing operators, all differentiable and ready to integrate into deep learning pipelines.\n2. **Advanced Augmentations**\nPerform powerful data augmentation with Kornia\u2019s built-in functions, ideal for training AI models with complex augmentation pipelines.\n3. **AI Models**\nLeverage pre-trained AI models optimized for a variety of vision tasks, all within the Kornia ecosystem.\n\nSee here for some of the methods that we support! (>500 ops in total !)\n\n**Category**\n----------------------------\n**Image Processing**\n**Augmentation**\n**Feature Detection**\n**Geometry**\n**Deep Learning Layers**\n**Photometric Functions**\n**Filtering**\n**Color**\n**Stereo Vision**\n**Image Registration**\n**Pose Estimation**\n**Optical Flow**\n**3D Vision**\n**Image Denoising**\n**Edge Detection**\n**Transformations**\n**Loss Functions**\n**Morphological Operations**\n\nSponsorship\n\nKornia is an open-source project that is developed and maintained by volunteers. Whether you're using it for research or commercial purposes, consider sponsoring or collaborating with us. Your support will help ensure Kornia's growth and ongoing innovation. Reach out to us today and be a part of shaping the future of this exciting initiative!\n\nInstallation\n\n(https://pypi.org/project/kornia)\n(https://pytorch.org/get-started/locally/)\n\nFrom pip\n\n  \n\n  Other installation options\n\nFrom source with editable mode\n\nFor development with UV (Recommended)\n\nFor development, Kornia uses [uv](https://github.com/astral-sh/uv) for fast Python package management and virtual environment creation. The project includes a `uv.lock` file for reproducible dependency management.\n\n  \n\nThis will set up a complete development environment with all dependencies using the lock file for reproducibility. For more details on dependency management and lock file usage, see [CONTRIBUTING.md](CONTRIBUTING.md).\n\nFrom Github url (latest version)\n\nQuick Start\n\nKornia is not just another computer vision library \u2014 it's your gateway to effortless Computer Vision and AI.\n\nGet started with Kornia image transformation and augmentation!\n\nFind out Kornia ONNX models with ONNXSequential!\n\nMulti-framework support\n\nYou can now use Kornia with [TensorFlow](https://www.tensorflow.org/), [JAX](https://jax.readthedocs.io/en/latest/index.html), and [NumPy](https://numpy.org/). See [Multi-Framework Support](docs/source/get-started/multi-framework-support.rst) for more details.\n\n  Powered by\n\nCall For Contributors\n\nAre you passionate about computer vision, AI, and open-source development? Join us in shaping the future of Kornia! We are actively seeking contributors to help expand and enhance our library, making it even more powerful, accessible, and versatile. Whether you're an experienced developer or just starting, there's a place for you in our community.\n\nAccessible AI Models\n\nWe are excited to announce our latest advancement: a new initiative designed to seamlessly integrate lightweight AI models into Kornia.\nWe aim to run any models as smooth as big models such as StableDiffusion, to support them well in many perspectives.\nWe have already included a selection of lightweight AI models like [YuNet (Face Detection)](), [Loftr (Feature Matching)](), and [SAM (Segmentation)](). Now, we're looking for contributors to help us:\n\n- Expand the Model Selection: Import decent models into our library. If you are a researcher, Kornia is an excellent place for you to promote your model!\n- Model Optimization: Work on optimizing models to reduce their computational footprint while maintaining accuracy and performance. You may start from offering ONNX support!\n- Model Documentation: Create detailed guides and examples to help users get the most out of these models in their projects.\n\nDocumentation And Tutorial Optimization\n\nKornia's foundation lies in its extensive collection of classic computer vision operators, providing robust tools for image processing, feature extraction, and geometric transformations. We continuously seek for contributors to help us improve our documentation and present nice tutorials to our users.\n\nCite\n\nIf you are using kornia in your research-related documents, it is recommended that you cite the paper. See more in [CITATION](./CITATION.md).\n\nContributing\n\nWe appreciate all contributions. If you are planning to contribute back bug-fixes, please do so without any further discussion. If you plan to contribute new features, utility functions or extensions, please first open an issue and discuss the feature with us. Please, consider reading the [CONTRIBUTING](./CONTRIBUTING.md) notes. The participation in this open source project is subject to [Code of Conduct](./CODE_OF_CONDUCT.md).\n\nCommunity\n\n  \n\nMade with [contrib.rocks](https://contrib.rocks).\n\nLicense\n\nKornia is released under the Apache 2.0 license. See the [LICENSE](./LICENSE) file for more information."}, {"name": "kornia", "tags": ["math", "ml", "web"], "summary": "Open Source Differentiable Computer Vision Library for PyTorch", "text": "This library is used to develop and train AI models for various vision tasks by providing a comprehensive set of differentiable image processing operators and advanced augmentations. With Kornia, developers can leverage pre-trained AI models and build complex data augmentation pipelines to improve the performance of their computer vision applications."}, {"name": "lancedb", "tags": ["dev", "math"], "summary": "lancedb", "text": "LanceDB\n\nA Python library for [LanceDB](https://github.com/lancedb/lancedb).\n\nInstallation\n\nPreview Releases\n\nStable releases are created about every 2 weeks. For the latest features and bug fixes, you can install the preview release. These releases receive the same level of testing as stable releases, but are not guaranteed to be available for more than 6 months after they are released. Once your application is stable, we recommend switching to stable releases.\n\nUsage\n\nBasic Example\n\nDevelopment\n\nSee [CONTRIBUTING.md](./CONTRIBUTING.md) for information on how to contribute to LanceDB."}, {"name": "lancedb", "tags": ["dev", "math"], "summary": "lancedb", "text": "This library is used to provide a Python implementation of the LanceDB database, allowing developers to create and manage data in an efficient and scalable way. With lancedb, developers can build high-performance applications that require fast and reliable data storage and retrieval capabilities."}, {"name": "lap", "tags": ["math", "ml", "web"], "summary": "Linear Assignment Problem solver (LAPJV/LAPMOD).", "text": "lap: Linear Assignment Problem Solver\n\n[`lap`](https://github.com/gatagat/lap) is a [linear assignment problem](https://en.wikipedia.org/wiki/Assignment_problem) solver using Jonker-Volgenant algorithm for dense LAPJV\u00b9 or sparse LAPMOD\u00b2 matrices. Both algorithms are implemented from scratch based solely on the papers\u00b9\u02d2\u00b2 and the public domain Pascal implementation provided by A. Volgenant\u00b3. The LAPMOD implementation seems to be faster than the LAPJV implementation for matrices with a side of more than ~5000 and with less than 50% finite coefficients.\n\n\u00b9 R. Jonker and A. Volgenant, \"A Shortest Augmenting Path Algorithm for Dense and Sparse Linear Assignment Problems\", Computing 38, 325-340 (1987) \n\u00b2 A. Volgenant, \"Linear and Semi-Assignment Problems: A Core Oriented Approach\", Computer Ops Res. 23, 917-932 (1996) \n\u00b3 http://www.assignmentproblems.com/LAPJV.htm | [[archive.org](https://web.archive.org/web/20220221010749/http://www.assignmentproblems.com/LAPJV.htm)]\n\nInstallation\n\nInstall from [PyPI](https://pypi.org/project/lap/):\n\n(https://pepy.tech/project/lap)\n(https://pepy.tech/project/lap)\n\n**Pre-built Wheels** \n:---:\nPython 3.7\nPython 3.8\nPython 3.9-3.13 \u00b9\n\n\u00b9 v0.5.10 supports numpy v2.x for Python 3.9-3.13. \ud83c\udd95 \n\u00b2 Windows ARM64 is experimental.\n\nOther options\n\nInstall from GitHub repo (requires C++ compiler):\n\nBuild and install (requires C++ compiler):\n\nUsage\n\nMore details\n\n`cost, x, y = lap.lapjv(C)`\n\nThe function `lapjv(C)` returns the assignment cost `cost` and two arrays `x` and `y`. If cost matrix `C` has shape NxM, then `x` is a size-N array specifying to which column each row is assigned, and `y` is a size-M array specifying to which row each column is assigned. For example, an output of `x = [1, 0]` indicates that row 0 is assigned to column 1 and row 1 is assigned to column 0. Similarly, an output of `x = [2, 1, 0]` indicates that row 0 is assigned to column 2, row 1 is assigned to column 1, and row 2 is assigned to column 0.\n\nNote that this function *does not* return the assignment matrix (as done by scipy's [`linear_sum_assignment`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.linear_sum_assignment.html) and lapsolver's [`solve dense`](https://github.com/cheind/py-lapsolver)). The assignment matrix can be constructed from `x` as follows:\n\nEquivalently, we could construct the assignment matrix from `y`:\n\nFinally, note that the outputs are redundant: we can construct `x` from `y`, and vise versa:\n\nLicense\n\nReleased under the 2-clause BSD license, see [LICENSE](./LICENSE).\n\nCopyright (C) 2012-2024, Tomas Kazmar\n\nContributors (in alphabetic order):\n- Benjamin Eysenbach\n- L\u00e9o Duret\n- Raphael Reme\n- Ratha Siv\n- Robert Wen\n- Steven\n- Tom White\n- Tomas Kazmar\n- Wok"}, {"name": "lap", "tags": ["math", "ml", "web"], "summary": "Linear Assignment Problem solver (LAPJV/LAPMOD).", "text": "This library is used to solve the linear assignment problem efficiently, providing optimized solutions for dense and sparse matrices using the Jonker-Volgenant algorithm. Developers can leverage this library to quickly find optimal assignments between two sets of variables with minimal computational overhead."}, {"name": "latex2mathml", "tags": ["math"], "summary": "Pure Python library for LaTeX to MathML conversion", "text": "latex2mathml\n\nPure Python library for LaTeX to MathML conversion\n\nInstallation\n\nUsage\n\nPython\n\nCommand-line\n\nReferences\n\nLaTeX\n\nMathML\n\n- http://www.xmlmind.com/tutorials/MathML/\n\nAuthor\n\n- [Ronie Martinez](mailto:ronmarti18@gmail.com)\n\nContributors \n\nThanks goes to these wonderful people ([emoji key](https://allcontributors.org/docs/en/emoji-key)):\n\n  \n  \n\nThis project follows the [all-contributors](https://github.com/all-contributors/all-contributors) specification. Contributions of any kind welcome!"}, {"name": "latex2mathml", "tags": ["math"], "summary": "Pure Python library for LaTeX to MathML conversion", "text": "This library is used to convert LaTeX mathematical expressions into MathML format, enabling developers to display and render math equations in web applications and documents. Developers can use this library to seamlessly integrate LaTeX formulas with other technologies that support MathML, such as HTML5, CSS3, and JavaScript libraries like MathJax."}, {"name": "leather", "tags": ["math", "visualization"], "summary": "Python charting for 80% of humans.", "text": ".. image:: https://img.shields.io/pypi/dw/leather.svg\n\n.. image:: https://img.shields.io/pypi/v/leather.svg\n\n.. image:: https://img.shields.io/pypi/l/leather.svg\n\n.. image:: https://img.shields.io/pypi/pyversions/leather.svg\n\nLeather is the Python charting library for those who need charts *now* and don't care if they're perfect.\n\nLeather isn't picky. It's rough. It gets dirty. It looks sexy just hanging on the back of a chair. Leather doesn't need your accessories. Leather is how Snake Plissken would make charts.\n\nGet it?\n\nImportant links:\n\n* Documentation:    https://leather.rtfd.io\n* Repository:       https://github.com/wireservice/leather\n* Issues:           https://github.com/wireservice/leather/issues"}, {"name": "leather", "tags": ["math", "visualization"], "summary": "Python charting for 80% of humans.", "text": "This library is used to quickly create charts for non-technical users who prioritize speed over perfection. It provides a straightforward and easy-to-use charting solution that allows developers to rapidly generate visualizations without requiring extensive customization or polish."}, {"name": "lhotse", "tags": ["data", "math", "ml", "web"], "summary": "Data preparation for speech processing models training.", "text": "Lhotse\n\nLhotse is a Python library aiming to make multimodal (speech, audio, video, image, text) data preparation flexible and accessible to a wider community.\nAlongside [k2](https://github.com/k2-fsa/k2), it is a part of the next generation [Kaldi](https://github.com/kaldi-asr/kaldi) speech processing library.\n\nTutorial presentations and materials\n\nAbout\n\nMain goals (updated for 2025)\n\n- Scale to multimodal data pipelines including audio, text, image, and video modalities.\n- Provide state-of-the-art dataloading algorithms such as dataset blending and efficient on-the-fly bucketing.\n- Handle data randomization (or de-duplication) for distributed multi-node training.\n- Attract a wider community to multimodal processing tasks with a **Python-centric design**.\n- Provide **standard data preparation recipes** for commonly used corpora.\n- Flexible data preparation for model training with the notion of **audio/video cuts**.\n- Support for efficient sequential I/O data formats such as Lhotse Shar (similar to webdataset).\n\nTutorials\n\nWe offer the following tutorials available in `examples` directory:\n\nExamples of use\n\nCheck out the following links to see how Lhotse is being put to use:\n- [Icefall recipes](https://github.com/k2-fsa/icefall): where k2 and Lhotse meet.\n- Minimal ESPnet+Lhotse example: (https://colab.research.google.com/drive/1HKSYPsWx_HoCdrnLpaPdYj5zwlPsM3NH)\n\nMain ideas\n\nLike Kaldi, Lhotse provides standard data preparation recipes, but extends that with a seamless PyTorch integration\nthrough task-specific Dataset classes. The data and meta-data are represented in human-readable text manifests and\nexposed to the user through convenient Python classes.\n\nLhotse introduces the notion of audio cuts, designed to ease the training data construction with operations such as\nmixing, truncation and padding that are performed on-the-fly to minimize the amount of storage required. Data\naugmentation and feature extraction are supported both in pre-computed mode, with highly-compressed feature matrices\nstored on disk, and on-the-fly mode that computes the transformations upon request. Additionally, Lhotse introduces\nfeature-space cut mixing to make the best of both worlds.\n\nInstallation\n\nLhotse supports Python version 3.7 and later.\n\nPip\n\nLhotse is available on PyPI:\n\nTo install the latest, unreleased version, do:\n\nDevelopment installation\n\nFor development installation, you can fork/clone the GitHub repo and install with pip:\n\nThis is an editable installation (`-e` option), meaning that your changes to the source code are automatically\nreflected when importing lhotse (no re-install needed). The `[dev]` part means you're installing extra dependencies\nthat are used to run tests, build documentation or launch jupyter notebooks.\n\nEnvironment variables"}, {"name": "lhotse", "tags": ["data", "math", "ml", "web"], "summary": "Data preparation for speech processing models training.", "text": "Lhotse uses several environment variables to customize it's behavior. They are as follows:\n- `LHOTSE_REQUIRE_TORCHAUDIO` - when it's set and not any of `1|True|true|yes`, we'll not check for torchaudio being installed and remove it from the requirements. It will disable many functionalities of Lhotse but the basic capabilities will remain (including reading audio with `soundfile`).\n- `LHOTSE_AUDIO_DURATION_MISMATCH_TOLERANCE` - used when we load audio from a file and receive a different number of samples than declared in `Recording.num_samples`. This is sometimes necessary because different codecs (or even different versions of the same codec) may use different padding when decoding compressed audio. Typically values up to 0.1, or even 0.3 (second) are still reasonable, and anything beyond that indicates a serious issue.\n- `LHOTSE_AUDIO_BACKEND` - may be set to any of the values returned from CLI `lhotse list-audio-backends` to override the default behavior of trial-and-error and always use a specific audio backend.\n- `LHOTSE_RESAMPLING_BACKEND` - may be set to any of the value returned from CLI `lhotse list-resampling-backends` to override the default behaviour.\n- `LHOTSE_AUDIO_LOADING_EXCEPTION_VERBOSE` - when set to `1` we'll emit full exception stack traces when every available audio backend fails to load a given file (they might be very large).\n- `LHOTSE_DILL_ENABLED` - when it's set to `1|True|true|yes`, we will enable `dill`-based serialization of `CutSet` and `Sampler` across processes (it's disabled by default even when `dill` is installed).\n- `LHOTSE_LEGACY_OPUS_LOADING` - (`=1`) reverts to a legacy OPUS loading mechanism that triggered a new ffmpeg subprocess for each OPUS file.\n- `LHOTSE_PREPARING_RELEASE` - used internally by developers when releasing a new version of Lhotse.\n- `TORCHAUDIO_USE_BACKEND_DISPATCHER` - when set to `1` and torchaudio version is below 2.1, we'll enable the experimental ffmpeg backend of torchaudio.\n- `AIS_ENDPOINT` is read by AIStore client to determine AIStore endpoint URL. Required for AIStore dataloading.\n- `RANK`, `WORLD_SIZE`, `WORKER`, and `NUM_WORKERS` are internally used to inform Lhotse Shar dataloading subprocesses.\n- `READTHEDOCS` is internally used for documentation builds.\n- `LHOTSE_MSC_OVERRIDE_PROTOCOLS` - when set, it will override your input protocols before feeding to MSCIOBackend.  Useful when you don't want to change your existing url format but want to use MSCIOBackend.  For example, if you have `s3://s3-bucket/path/to/my/object` and `gs://gs-bucket/path/to/my/object`, you can set `LHOTSE_MSC_OVERRIDE_PROTOCOLS=s3,gs` to override the urls to `msc://s3-bucket/path/to/my/object` and `msc://gs-bucket/path/to/my/object`.\n- `LHOTSE_MSC_PROFILE` - when set, it will override the your bucket name before feeding to MSCIOBackend.  Useful when your msc profile is not the same as your bucket name.  For example, if you have `s3://s3-bucket/path/to/my/object`, you can set `LHOTSE_MSC_OVERRIDE_PROTOCOLS=s3` and `LHOTSE_MSC_PROFILE=msc-s3-profile` to override the url to `msc://msc-s3-profile/path/to/my/object`.\n- `LHOTSE_MSC_BACKEND_FORCED` - when set to `True`, forces Lhotse to use MSCIOBackend for all URLs. Use with caution as functionality may break if MSC does not support the provided URL format.\n\nOptional dependencies"}, {"name": "lhotse", "tags": ["data", "math", "ml", "web"], "summary": "Data preparation for speech processing models training.", "text": "**Other pip packages.** You can leverage optional features of Lhotse by installing the relevant supporting package:\n- `torchaudio` used to be a core dependency in Lhotse, but is now optional. Refer to [official PyTorch documentation for installation](https://pytorch.org/get-started/locally/).\n- `pip install lhotse[kaldi]` for a maximal feature set related to Kaldi compatibility. It includes libraries such as `kaldi_native_io` (a more efficient variant of `kaldi_io`) and `kaldifeat` that port some of Kaldi functionality into Python.\n- `pip install lhotse[orjson]` for up to 50% faster reading of JSONL manifests.\n- `pip install lhotse[webdataset]`. We support \"compiling\" your data into WebDataset tarball format for more effective IO. You can still interact with the data as if it was a regular lazy CutSet. To learn more, check out the following tutorial: (https://colab.research.google.com/github/lhotse-speech/lhotse/blob/master/examples/02-webdataset-integration.ipynb)\n- `pip install h5py` if you want to extract speech features and store them as HDF5 arrays.\n- `pip install dill`. When `dill` is installed, we'll use it to pickle CutSet that uses a lambda function in calls such as `.map` or `.filter`. This is helpful in PyTorch DataLoader with `num_jobs>0`. Without `dill`, depending on your environment, you'll see an exception or a hanging script.\n- `pip install aistore` to read manifests, tar fles, and other data from AIStore using AIStore-supported URLs (set `AIS_ENDPOINT` environment variable to activate it). See [AIStore documentation](https://aiatscale.org) for more details.\n- `pip install smart_open` to read and write manifests and data in any location supported by `smart_open` (e.g. cloud, http).\n- `pip install opensmile` for feature extraction using the OpenSmile toolkit's Python wrapper.\n- `pip install multi-storage-client` for read and write manifests and data in different storage backends. See [multi-storage-client](https://github.com/NVIDIA/multi-storage-client) for more details.\n\n**sph2pipe.** For reading older LDC SPHERE (.sph) audio files that are compressed with codecs unsupported by ffmpeg and sox, please run:\n\nIt will download it to `~/.lhotse/tools`, compile it, and auto-register in `PATH`. The program should be automatically detected and used by Lhotse.\n\nExamples\n\nWe have example recipes showing how to prepare data and load it in Python as a PyTorch `Dataset`.\nThey are located in the `examples` directory.\n\nA short snippet to show how Lhotse can make audio data preparation quick and easy:\n\nThe `VadDataset` will yield a batch with pairs of feature and supervision tensors such as the following - the speech\nstarts roughly at the first second (100 frames):"}, {"name": "lhotse", "tags": ["data", "math", "ml", "web"], "summary": "Data preparation for speech processing models training.", "text": "This library is used to prepare multimodal data, including speech, audio, video, image, and text, for training models in a flexible and accessible way. With Lhotse, developers can scale their data pipelines to handle large datasets and implement state-of-the-art dataloading algorithms with ease."}, {"name": "libcudf-cu12", "tags": ["data", "math", "web"], "summary": "cuDF - GPU Dataframe (C++)", "text": "&nbsp;cuDF - A GPU-accelerated DataFrame library for tabular data processing\n\ncuDF (pronounced \"KOO-dee-eff\") is an [Apache 2.0 licensed](LICENSE), GPU-accelerated DataFrame library\nfor tabular data processing. The cuDF library is one part of the [RAPIDS](https://rapids.ai/) GPU\nAccelerated Data Science suite of libraries.\n\nAbout\n\ncuDF is composed of multiple libraries including:\n\n* [libcudf](https://docs.rapids.ai/api/cudf/stable/libcudf_docs/): A CUDA C++ library with [Apache Arrow](https://arrow.apache.org/) compliant\ndata structures and fundamental algorithms for tabular data.\n* [pylibcudf](https://docs.rapids.ai/api/cudf/stable/pylibcudf/): A Python library providing [Cython](https://cython.org/) bindings for libcudf.\n* [cudf](https://docs.rapids.ai/api/cudf/stable/user_guide/): A Python library providing\n* [cudf-polars](https://docs.rapids.ai/api/cudf/stable/cudf_polars/): A Python library providing a GPU engine for [Polars](https://pola.rs/)\n* [dask-cudf](https://docs.rapids.ai/api/dask-cudf/stable/): A Python library providing a GPU backend for [Dask](https://www.dask.org/) DataFrames\n\nNotable projects that use cuDF include:\n\n* [Spark RAPIDS](https://github.com/NVIDIA/spark-rapids): A GPU accelerator plugin for [Apache Spark](https://spark.apache.org/)\n* [Velox-cuDF](https://github.com/facebookincubator/velox/blob/main/velox/experimental/cudf/README.md): A [Velox](https://velox-lib.io/)\nextension module to execute Velox plans on the GPU\n* [Sirius](https://www.sirius-db.com/): A GPU-native SQL engine providing extensions for libraries like [DuckDB](https://duckdb.org/)\n\nInstallation\n\nSystem Requirements\n\nOperating System, GPU driver, and supported CUDA version information can be found at the [RAPIDS Installation Guide](https://docs.rapids.ai/install/#system-req)\n\npip\n\nA stable release of each cudf library is available on PyPI. You will need to match the major version number of your installed CUDA version with a `-cu##` suffix when installing from PyPI.\n\nA development version of each library is available as a nightly release by including the `-i https://pypi.anaconda.org/rapidsai-wheels-nightly/simple` index.\n\nconda\n\nA stable release of each cudf library is available to be installed with the conda package manager by specifying the `-c rapidsai` channel.\n\nA development version of each library is available as a nightly release by specifying the `-c rapidsai-nightly` channel instead.\n\nsource\n\nTo install cuDF from source, please follow [the contribution guide](CONTRIBUTING.md#setting-up-your-build-environment) detailing\nhow to setup the build environment.\n\nExamples\n\nThe following examples showcase reading a parquet file, dropping missing rows with a null value,\nand performing a groupby aggregation on the data.\n\ncudf\n\n`import cudf` and the APIs are largely similar to pandas.\n\ncudf.pandas\n\nWith a Python file containing pandas code:\n\nUse cudf.pandas by invoking `python` with `-m cudf.pandas`\n\nIf running the pandas code in an interactive Jupyter environment, call `%load_ext cudf.pandas` before\nimporting pandas.\n\ncudf-polars\n\nUsing Polars' [lazy API](https://docs.pola.rs/user-guide/lazy/), call `collect` with `engine=\"gpu\"` to run\nthe operation on the GPU\n\nQuestions and Discussion\n\nFor bug reports or feature requests, please [file an issue](https://github.com/rapidsai/cudf/issues/new/choose) on the GitHub issue tracker.\n\nFor questions or discussion about cuDF and GPU data processing, feel free to post in the [RAPIDS Slack](https://rapids.ai/slack-invite) workspace.\n\nContributing\n\ncuDF is open to contributions from the community! Please see our [guide for contributing to cuDF](CONTRIBUTING.md) for more information."}, {"name": "libcudf-cu12", "tags": ["data", "math", "web"], "summary": "cuDF - GPU Dataframe (C++)", "text": "This library is used to accelerate tabular data processing on GPUs, enabling developers to efficiently perform complex computations on large datasets. With cuDF, developers can leverage the power of parallel computing to speed up data manipulation and analysis tasks."}, {"name": "libcuml-cu12", "tags": ["data", "math", "ml", "ui", "web"], "summary": "cuML - RAPIDS ML Algorithms (C++)", "text": "&nbsp;cuML - GPU Machine Learning Algorithms\n\ncuML is a suite of libraries that implement machine learning algorithms and mathematical primitives functions that share compatible APIs with other [RAPIDS](https://rapids.ai/) projects.\n\ncuML enables data scientists, researchers, and software engineers to run\ntraditional tabular ML tasks on GPUs without going into the details of CUDA\nprogramming. In most cases, cuML's Python API matches the API from\n[scikit-learn](https://scikit-learn.org).\n\nFor large datasets, these GPU-based implementations can complete 10-50x faster\nthan their CPU equivalents. For details on performance, see the [cuML Benchmarks\nNotebook](https://github.com/rapidsai/cuml/tree/release/25.12/notebooks/tools).\n\nAs an example, the following Python snippet loads input and computes DBSCAN clusters, all on GPU, using cuDF:\n\nOutput:\n\ncuML also features multi-GPU and multi-node-multi-GPU operation, using [Dask](https://www.dask.org), for a\ngrowing list of algorithms. The following Python snippet reads input from a CSV file and performs\na NearestNeighbors query across a cluster of Dask workers, using multiple GPUs on a single node:\n\nInitialize a `LocalCUDACluster` configured with [UCXX](https://github.com/rapidsai/ucxx) for fast transport of CUDA arrays\n\nLoad data and perform `k-Nearest Neighbors` search. `cuml.dask` estimators also support `Dask.Array` as input:\n\nFor additional examples, browse our complete [API\ndocumentation](https://docs.rapids.ai/api/cuml/stable/), or check out our\nexample [walkthrough\nnotebooks](https://github.com/rapidsai/cuml/tree/release/25.12/notebooks). Finally, you\ncan find complete end-to-end examples in the [notebooks-contrib\nrepo](https://github.com/rapidsai/notebooks-contrib).\n\nSupported Algorithms\nCategory\n---\n**Clustering**\nHierarchical Density-Based Spatial Clustering of Applications with Noise (HDBSCAN)\nK-Means\nSingle-Linkage Agglomerative Clustering\nSpectral Clustering\n**Dimensionality Reduction**\nIncremental PCA\nTruncated Singular Value Decomposition (tSVD)\nUniform Manifold Approximation and Projection (UMAP)\nRandom Projection\nt-Distributed Stochastic Neighbor Embedding (TSNE)\nSpectral Embedding\n**Linear Models for Regression or Classification**\nLinear Regression with Lasso or Ridge Regularization\nElasticNet Regression\nLARS Regression\nLogistic Regression\nNaive Bayes\nStochastic Gradient Descent (SGD), Coordinate Descent (CD), and Quasi-Newton (QN) (including L-BFGS and OWL-QN) solvers for linear models\n**Nonlinear Models for Regression or Classification**\nRandom Forest (RF) Regression\nInference for decision tree-based models\nK-Nearest Neighbors (KNN) Classification\nK-Nearest Neighbors (KNN) Regression\nSupport Vector Machine Classifier (SVC)\nEpsilon-Support Vector Regression (SVR)\n**Preprocessing**\n**Time Series**\nAuto-regressive Integrated Moving Average (ARIMA)\n**Model Explanation**\nSHAP Permutation Explainer\n**Execution device interoperability**\n**Other**\n\n---\n\nInstallation\n\nSee [the RAPIDS Release Selector](https://docs.rapids.ai/install#selector) for\nthe command line to install either nightly or official release cuML packages\nvia conda, pip, or Docker.\n\nBuild/Install from Source\nSee the build [guide](BUILD.md).\n\nScikit-learn Compatibility\n\ncuML is compatible with scikit-learn version 1.4 or higher.\n\nContributing\n\nPlease see our [guide for contributing to cuML](CONTRIBUTING.md).\n\nReferences\n\nThe RAPIDS team has a number of blogs with deeper technical dives and examples. [You can find them here on Medium.](https://medium.com/rapids-ai/tagged/machine-learning)\n\nFor additional details on the technologies behind cuML, as well as a broader overview of the Python Machine Learning landscape, see [_Machine Learning in Python: Main developments and technology trends in data science, machine learning, and artificial intelligence_ (2020)](https://arxiv.org/abs/2002.04803) by Sebastian Raschka, Joshua Patterson, and Corey Nolet.\n\nPlease consider citing this when using cuML in a project. You can use the citation BibTeX:\n\nContact\n\nFind out more details on the [RAPIDS site](https://rapids.ai/community.html)\n\nOpen GPU Data Science\n\nThe RAPIDS suite of open source software libraries aim to enable execution of end-to-end data science and analytics pipelines entirely on GPUs. It relies on NVIDIA\u00ae CUDA\u00ae primitives for low-level compute optimization, but exposing that GPU parallelism and high-bandwidth memory speed through user-friendly Python interfaces."}, {"name": "libcuml-cu12", "tags": ["data", "math", "ml", "ui", "web"], "summary": "cuML - RAPIDS ML Algorithms (C++)", "text": "This library is used to accelerate traditional tabular machine learning tasks on GPUs using a compatible API with scikit-learn. With cuML, developers can run these tasks up to 50x faster than their CPU equivalents for large datasets."}, {"name": "lifelines", "tags": ["math", "web"], "summary": "Survival analysis in Python, including Kaplan Meier, Nelson Aalen and regression", "text": "Documentation and intro to survival analysis\n\nIf you are new to survival analysis, wondering why it is useful, or are interested in *lifelines* examples, API, and syntax, please read the [Documentation and Tutorials page](http://lifelines.readthedocs.org/en/latest/index.html)\n\nContact\n\nDevelopment\n\nSee our [Contributing](https://github.com/CamDavidsonPilon/lifelines/blob/master/.github/CONTRIBUTING.md) guidelines."}, {"name": "lifelines", "tags": ["math", "web"], "summary": "Survival analysis in Python, including Kaplan Meier, Nelson Aalen and regression", "text": "This library is used to perform advanced statistical analysis of time-to-event data, allowing developers to easily model and visualize survival distributions using methods such as Kaplan Meier and Nelson Aalen. With lifelines, developers can build sophisticated models that account for regression effects on survival times."}, {"name": "lightning", "tags": ["dev", "math", "ml"], "summary": "The Deep Learning framework to train, deploy, and ship AI products Lightning fast.", "text": "Looking for GPUs?\nOver 340,000 developers use [Lightning Cloud](https://lightning.ai/?utm_source=ptl_readme&utm_medium=referral&utm_campaign=ptl_readme) - purpose-built for PyTorch and PyTorch Lightning.\n\nWhy PyTorch Lightning?   \n\nTraining models in plain PyTorch is tedious and error-prone - you have to manually handle things like backprop, mixed precision, multi-GPU, and distributed training, often rewriting code for every new project. PyTorch Lightning organizes PyTorch code to automate those complexities so you can focus on your model and data, while keeping full control and scaling from CPU to multi-node without changing your core code. But if you want control of those things, you can still opt into [expert-level control](#lightning-fabric-expert-control).   \n\nFun analogy: If PyTorch is Javascript, PyTorch Lightning is ReactJS or NextJS.\n\nLightning has 2 core packages\n\n[PyTorch Lightning: Train and deploy PyTorch at scale](#why-pytorch-lightning).\n\n[Lightning Fabric: Expert control](#lightning-fabric-expert-control).\n\nLightning gives you granular control over how much abstraction you want to add over PyTorch.\n\n&nbsp;\n\nQuick start\nInstall Lightning:\n\nPyTorch Lightning example\nDefine the training workflow. Here's a toy example ([explore real examples](https://lightning.ai/lightning-ai/studios?view=public&section=featured&query=pytorch+lightning&utm_source=ptl_readme&utm_medium=referral&utm_campaign=ptl_readme)):\n\nRun the model on your terminal\n\n&nbsp;\n\nConvert from PyTorch to PyTorch Lightning\n\nPyTorch Lightning is just organized PyTorch - Lightning disentangles PyTorch code to decouple the science from the engineering.\n\n&nbsp;\n\n----\n\nExamples\nExplore various types of training possible with PyTorch Lightning. Pretrain and finetune ANY kind of model to perform ANY task like classification, segmentation, summarization and more:    \n\nTask\n------\n\n______________________________________________________________________\n\nAdvanced features\n\nLightning has over [40+ advanced features](https://lightning.ai/docs/pytorch/stable/common/trainer.html?utm_source=ptl_readme&utm_medium=referral&utm_campaign=ptl_readme#trainer-flags)\ndesigned for professional AI research at scale.\n\nHere are some examples:\n\n  \n\n  Train on 1000s of GPUs without code changes\n\n  Train on other accelerators like TPUs without code changes\n\n  16-bit precision\n\n  Experiment managers\n\nEarly Stopping\n\n  Checkpointing\n\n  Export to torchscript (JIT) (production use)\n\n  Export to ONNX (production use)\n\n______________________________________________________________________\n\nAdvantages over unstructured PyTorch\n\n- Models become hardware agnostic\n- Code is clear to read because engineering code is abstracted away\n- Easier to reproduce\n- Make fewer mistakes because lightning handles the tricky engineering\n- Keeps all the flexibility (LightningModules are still PyTorch modules), but removes a ton of boilerplate\n- Lightning has dozens of integrations with popular machine learning tools.\n- [Tested rigorously with every new PR](https://github.com/Lightning-AI/lightning/tree/master/tests). We test every combination of PyTorch and Python supported versions, every OS, multi GPUs and even TPUs.\n- Minimal running speed overhead (about 300 ms per epoch compared with pure PyTorch).\n\n______________________________________________________________________\n\n______________________________________________________________________\n\n&nbsp;\n&nbsp;\n\nLightning Fabric: Expert control\n\nRun on any device at any scale with expert-level control over PyTorch training loop and scaling strategy. You can even write your own Trainer.\n\nFabric is designed for the most complex models like foundation model scaling, LLMs, diffusion, transformers, reinforcement learning, active learning. Of any size.\n\nWhat to change\nResulting Fabric Code (copy me!)\n\nKey features\n\n  Easily switch from running on CPU to GPU (Apple Silicon, CUDA, \u2026), TPU, multi-GPU or even multi-node training\n\n  Use state-of-the-art distributed training strategies (DDP, FSDP, DeepSpeed) and mixed precision out of the box\n\n  All the device logic boilerplate is handled for you\n\n  Build your own custom Trainer using Fabric primitives for training checkpointing, logging, and more\n\nYou can find a more extensive example in our [examples](examples/fabric/build_your_own_trainer)\n\n______________________________________________________________________\n\n______________________________________________________________________\n\n&nbsp;\n&nbsp;\n\nExamples\n\nSelf-supervised Learning\n\nConvolutional Architectures\n\n- [GPT-2](https://lightning-bolts.readthedocs.io/en/stable/models/convolutional.html#gpt-2)\n- [UNet](https://lightning-bolts.readthedocs.io/en/stable/models/convolutional.html#unet)\n\nReinforcement Learning\n\nGANs\n\n- [Basic GAN](https://lightning-bolts.readthedocs.io/en/stable/models/gans.html#basic-gan)\n- [DCGAN](https://lightning-bolts.readthedocs.io/en/stable/models/gans.html#dcgan)\n\nClassic ML\n\n- [Logistic Regression](https://lightning-bolts.readthedocs.io/en/stable/models/classic_ml.html#logistic-regression)\n- [Linear Regression](https://lightning-bolts.readthedocs.io/en/stable/models/classic_ml.html#linear-regression)\n\n&nbsp;\n&nbsp;\n\nContinuous Integration\n\nLightning is rigorously tested across multiple CPUs, GPUs and TPUs and against major Python and PyTorch versions.\n\n\\*Codecov is > 90%+ but build delays may show less\n\n  Current build statuses\n\nSystem / PyTorch ver.\n:--------------------------------:\nLinux py3.9 \\[GPUs\\]\nLinux (multiple Python versions)\nOSX (multiple Python versions)\nWindows (multiple Python versions)\n\n&nbsp;\n&nbsp;\n\nCommunity\n\nThe lightning community is maintained by\n\n- [10+ core contributors](https://lightning.ai/docs/pytorch/latest/community/governance.html) who are all a mix of professional engineers, Research Scientists, and Ph.D. students from top AI labs.\n- 800+ community contributors.\n\nWant to help us build Lightning and reduce boilerplate for thousands of researchers? [Learn how to make your first contribution here](https://lightning.ai/docs/pytorch/stable/generated/CONTRIBUTING.html?utm_source=ptl_readme&utm_medium=referral&utm_campaign=ptl_readme)\n\nLightning is also part of the [PyTorch ecosystem](https://pytorch.org/ecosystem/) which requires projects to have solid testing, documentation and support.\n\nAsking for help\n\nIf you have any questions please:\n\n1. [Read the docs](https://lightning.ai/docs?utm_source=ptl_readme&utm_medium=referral&utm_campaign=ptl_readme).\n1. [Search through existing Discussions](https://github.com/Lightning-AI/lightning/discussions), or [add a new question](https://github.com/Lightning-AI/lightning/discussions/new)\n1. [Join our discord](https://discord.com/invite/tfXFetEZxv)."}, {"name": "lightning", "tags": ["dev", "math", "ml"], "summary": "The Deep Learning framework to train, deploy, and ship AI products Lightning fast.", "text": "This library is used to simplify the process of building and deploying AI products by automating complexities such as backpropagation, mixed precision training, and distributed training. It enables developers to focus on model development and data while scaling from CPU to multi-node without changing their core code."}, {"name": "lingua-language-detector", "tags": ["dev", "math", "ml", "web"], "summary": "An accurate natural language detection library, suitable for short text and mixed-language text", "text": "1. What does this library do?\n\nIts task is simple: It tells you which language some text is written in.\nThis is very useful as a preprocessing step for linguistic data\nin natural language processing applications such as text classification and\nspell checking. Other use cases, for instance, might include routing e-mails\nto the right geographically located customer service department, based on the\ne-mails' languages.\n\n2. Why does this library exist?\n\nLanguage detection is often done as part of large machine learning frameworks\nor natural language processing applications. In cases where you don't need\nthe full-fledged functionality of those systems or don't want to learn the\nropes of those, a small flexible library comes in handy.\n\nPython is widely used in natural language processing, so there are a couple\nof comprehensive open source libraries for this task, such as Google's\n[*CLD 2*](https://github.com/CLD2Owners/cld2) and\n[*CLD 3*](https://github.com/google/cld3),\n[*Langid*](https://github.com/saffsd/langid.py),\n[*Simplemma*](https://github.com/adbar/simplemma) and\n[*Langdetect*](https://github.com/Mimino666/langdetect).\nUnfortunately, except for the last one they have two major drawbacks:\n\n1. Detection only works with quite lengthy text fragments. For very short\n   text snippets such as Twitter messages, they do not provide adequate results.\n2. The more languages take part in the decision process, the less accurate are\n   the detection results.\n\n*Lingua* aims at eliminating these problems. She nearly does not need any\nconfiguration and yields pretty accurate results on both long and short text,\neven on single words and phrases. She draws on both rule-based and statistical\nNaive Bayes methods but does not use neural networks or any dictionaries of words. \nShe does not need a connection to any external API or service either. \nOnce the library has been downloaded, it can be used completely offline.\n\n3. A short history of this library\n\nThis library started as a pure Python implementation. Python's quick prototyping\ncapabilities made an important contribution to its improvements. Unfortunately,\nthere was always a tradeoff between performance and memory consumption. At first,\n*Lingua's* language models were stored in dictionaries during runtime. This led\nto quick performance at the cost of large memory consumption (more than 3 GB).\nBecause of that, the language models were then stored in NumPy arrays instead of\ndictionaries. Memory consumption reduced to approximately 800 MB but CPU\nperformance dropped significantly. Both approaches were not satisfying.\n\nStarting from version 2.0.0, the pure Python implementation was replaced with\ncompiled Python bindings to the native\n[Rust implementation](https://github.com/pemistahl/lingua-rs) of *Lingua*.\nThis decision has led to both quick performance and a small memory\nfootprint of less than 1 GB. The pure Python implementation is still available\nin a [separate branch](https://github.com/pemistahl/lingua-py/tree/pure-python-impl)\nin this repository and will be kept up-to-date in subsequent 1.* releases.\nThere are environments that do not support native Python extensions such as\n[Juno](https://juno.sh/), so a pure Python implementation is still useful.\nBoth 1.* and 2.* versions will remain available on the Python package index (PyPI).\n\n4. Which languages are supported?"}, {"name": "lingua-language-detector", "tags": ["dev", "math", "ml", "web"], "summary": "An accurate natural language detection library, suitable for short text and mixed-language text", "text": "Compared to other language detection libraries, *Lingua's* focus is on\n*quality over quantity*, that is, getting detection right for a small set of\nlanguages first before adding new ones. Currently, the following 75 languages\nare supported:\n\n- A\n- B\n- C\n- D\n- E\n- F\n- G\n- H\n- I\n- J\n- K\n- L\n- M\n- N\n- P\n- R\n- S\n- T\n- U\n- V\n- W\n- X\n- Y\n- Z\n\n5. How accurate is it?\n\n*Lingua* is able to report accuracy statistics for some bundled test data\navailable for each supported language. The test data for each language is split\ninto three parts:\n\n1. a list of single words with a minimum length of 5 characters\n2. a list of word pairs with a minimum length of 10 characters\n3. a list of complete grammatical sentences of various lengths\n\nBoth the language models and the test data have been created from separate\ndocuments of the [Wortschatz corpora](https://wortschatz.uni-leipzig.de)\noffered by Leipzig University, Germany. Data crawled from various news websites\nhave been used for training, each corpus comprising one million sentences.\nFor testing, corpora made of arbitrarily chosen websites have been used, each\ncomprising ten thousand sentences. From each test corpus, a random unsorted\nsubset of 1000 single words, 1000 word pairs and 1000 sentences has been\nextracted, respectively.\n\nGiven the generated test data, I have compared the detection results of\n*Lingua*, *Langdetect*, *Langid*, *Simplemma*, *CLD 2* and *CLD 3*\nrunning over the data of *Lingua's* supported 75 languages. Languages that are\nnot supported by the other detectors are simply ignored for them during the\ndetection process.\n\nEach of the following sections contains three plots. The bar plot shows the detailed accuracy\nresults for each supported language. The box plots illustrate the distributions of the\naccuracy values for each classifier. The boxes themselves represent the areas which the\nmiddle 50 % of data lie within. Within the colored boxes, the horizontal lines mark the\nmedian of the distributions.\n\n5.1 Single word detection\n\n5.2 Word pair detection\n\n5.3 Sentence detection\n\n5.4 Average detection\n\n5.5 Mean, median and standard deviation\n\nThe tables found [here](https://github.com/pemistahl/lingua-py/tree/main/tables)\nshow detailed statistics for each language and classifier including mean, median and standard deviation.\n\n6. How fast is it?\n\nThe accuracy reporter script measures the time each language detector needs\nto classify 3000 input texts for each of the supported 75 languages. The results\nbelow have been produced on an iMac 3.6 Ghz 8-Core Intel Core i9 with 40 GB RAM.\n\nLingua in [multi-threaded mode](https://github.com/pemistahl/lingua-py#117-single-threaded-versus-multi-threaded-language-detection)\nis one of the fastest algorithms in this comparison. CLD 2 and 3\nare similarly fast as they have been implemented in C or C++. Pure Python libraries\nsuch as Simplemma, Langid or Langdetect a significantly slower.\n\nDetector\n----------------------------------------------\nLingua (low accuracy mode, multi-threaded)\nLingua (high accuracy mode, multi-threaded)\nCLD 2\nCLD 3\nLingua (low accuracy mode, single-threaded)\nLingua (high accuracy mode, single-threaded)\nSimplemma\nLangid\nLangdetect"}, {"name": "lingua-language-detector", "tags": ["dev", "math", "ml", "web"], "summary": "An accurate natural language detection library, suitable for short text and mixed-language text", "text": "7. Why is it better than other libraries?\n\nEvery language detector uses a probabilistic\n[n-gram](https://en.wikipedia.org/wiki/N-gram) model trained on the character\ndistribution in some training corpus. Most libraries only use n-grams of size 3\n(trigrams) which is satisfactory for detecting the language of longer text\nfragments consisting of multiple sentences. For short phrases or single words,\nhowever, trigrams are not enough. The shorter the input text is, the less\nn-grams are available. The probabilities estimated from such few n-grams are not\nreliable. This is why *Lingua* makes use of n-grams of sizes 1 up to 5 which\nresults in much more accurate prediction of the correct language.\n\nA second important difference is that *Lingua* does not only use such a\nstatistical model, but also a rule-based engine. This engine first determines\nthe alphabet of the input text and searches for characters which are unique\nin one or more languages. If exactly one language can be reliably chosen this\nway, the statistical model is not necessary anymore. In any case, the\nrule-based engine filters out languages that do not satisfy the conditions of\nthe input text. Only then, in a second step, the probabilistic n-gram model is\ntaken into consideration. This makes sense because loading less language models\nmeans less memory consumption and better runtime performance.\n\nIn general, it is always a good idea to restrict the set of languages to be\nconsidered in the classification process using the respective api methods.\nIf you know beforehand that certain languages are never to occur in an input\ntext, do not let those take part in the classification process. The filtering\nmechanism of the rule-based engine is quite good, however, filtering based on\nyour own knowledge of the input text is always preferable.\n\n8. Test report generation\n\nIf you want to reproduce the accuracy results above, you can generate the test\nreports yourself for all classifiers and languages by installing\n[Poetry](https://python-poetry.org) and executing:\n\nAccuracy reports for only a subset of classifiers and / or languages can be created by\npassing command line arguments:\n\nFor each detector and language, a test report file is then written into\n[`/accuracy-reports`](https://github.com/pemistahl/lingua-py/tree/main/accuracy-reports).\nAs an example, here is the current output of the *Lingua* German report:\n\n9. How to add it to your project?\n\n*Lingua* is available in the [Python Package Index](https://pypi.org/project/lingua-language-detector)\nand can be installed with:\n\n10. How to build?\n\n*Lingua* requires Python >= 3.10.\nFirst create a virtualenv and install the Python wheel for your platform with `pip`.\n\nIn the scripts directory, there are Python scripts for writing accuracy reports,\ndrawing plots and writing accuracy values in an HTML table. The dependencies\nfor these scripts are managed by [Poetry](https://python-poetry.org) which\nyou need to install if you have not done so yet. In order to install the script\ndependencies in your virtualenv, run\n\nThe project makes uses of type annotations which allow for static type checking with\n[Mypy](http://mypy-lang.org). Run the following commands for checking the types:\n\nThe Python source code is formatted with [Black](https://github.com/psf/black):\n\n11. How to use?"}, {"name": "lingua-language-detector", "tags": ["dev", "math", "ml", "web"], "summary": "An accurate natural language detection library, suitable for short text and mixed-language text", "text": "11.1 Basic usage\n\nThe entire library is thread-safe, i.e. you can use a single `LanguageDetector` instance and\nits methods in multiple threads. Multiple instances of `LanguageDetector` share thread-safe\naccess to the language models, so every language model is loaded into memory just once, no\nmatter how many instances of `LanguageDetector` have been created.\n\n11.2 Minimum relative distance\n\nBy default, *Lingua* returns the most likely language for a given input text.\nHowever, there are certain words that are spelled the same in more than one\nlanguage. The word *prologue*, for instance, is both a valid English and French\nword. *Lingua* would output either English or French which might be wrong in\nthe given context. For cases like that, it is possible to specify a minimum\nrelative distance that the logarithmized and summed up probabilities for\neach possible language have to satisfy. It can be stated in the following way:\n\nBe aware that the distance between the language probabilities is dependent on\nthe length of the input text. The longer the input text, the larger the\ndistance between the languages. So if you want to classify very short text\nphrases, do not set the minimum relative distance too high. Otherwise, `None`\nwill be returned most of the time as in the example above. This is the return\nvalue for cases where language detection is not reliably possible.\n\n11.3 Confidence values\n\nKnowing about the most likely language is nice but how reliable is the computed\nlikelihood? And how less likely are the other examined languages in comparison\nto the most likely one? These questions can be answered as well:\n\nIn the example above, a list is returned containing those languages which the\ncalling instance of LanguageDetector has been built from, sorted by\ntheir confidence value in descending order. Each value is a probability between\n0.0 and 1.0. The probabilities of all languages will sum to 1.0.\nIf the language is unambiguously identified by the rule engine, the value 1.0\nwill always be returned for this language. The other languages will receive a\nvalue of 0.0.\n\nThere is also a method for returning the confidence value for one specific\nlanguage only:\n\nThe value that this method computes is a number between 0.0 and 1.0. If the\nlanguage is unambiguously identified by the rule engine, the value 1.0 will\nalways be returned. If the given language is not supported by this detector\ninstance, the value 0.0 will always be returned.\n\n11.4 Eager loading versus lazy loading\n\nBy default, *Lingua* uses lazy-loading to load only those language models on\ndemand which are considered relevant by the rule-based filter engine. For web\nservices, for instance, it is rather beneficial to preload all language models\ninto memory to avoid unexpected latency while waiting for the service response.\nIf you want to enable the eager-loading mode, you can do it like this:\n\nMultiple instances of `LanguageDetector` share the same language models in\nmemory which are accessed asynchronously by the instances.\n\n11.5 Low accuracy mode versus high accuracy mode"}, {"name": "lingua-language-detector", "tags": ["dev", "math", "ml", "web"], "summary": "An accurate natural language detection library, suitable for short text and mixed-language text", "text": "*Lingua's* high detection accuracy comes at the cost of being noticeably slower\nthan other language detectors. The large language models also consume significant\namounts of memory. These requirements might not be feasible for systems running low\non resources. If you want to classify mostly long texts or need to save resources,\nyou can enable a *low accuracy mode* that loads only a small subset of the language\nmodels into memory:\n\nThe downside of this approach is that detection accuracy for short texts consisting\nof less than 120 characters will drop significantly. However, detection accuracy for\ntexts which are longer than 120 characters will remain mostly unaffected.\n\nIn high accuracy mode (the default), the language detector consumes approximately\n1 GB of memory if all language models are loaded. In low accuracy mode, memory\nconsumption is reduced to approximately 100 MB.\n\nAn alternative for a smaller memory footprint and faster performance is to reduce the set\nof languages when building the language detector. In most cases, it is not advisable to\nbuild the detector from all supported languages. When you have knowledge about\nthe texts you want to classify you can almost always rule out certain languages as impossible\nor unlikely to occur.\n\n11.6 Single-language mode\n\nIf you build a `LanguageDetector` from one language only it will operate in single-language mode.\nThis means the detector will try to find out whether a given text has been written in the given language or not.\nIf not, then `None` will be returned, otherwise the given language. In single-language mode, the detector decides based on a set of unique and most common n-grams which\nhave been collected beforehand for every supported language.\n\n11.7 Detection of multiple languages in mixed-language texts\n\nIn contrast to most other language detectors, *Lingua* is able to detect multiple languages\nin mixed-language texts. This feature can yield quite reasonable results but it is still\nin an experimental state and therefore the detection result is highly dependent on the input\ntext. It works best in high-accuracy mode with multiple long words for each language.\nThe shorter the phrases and their words are, the less accurate are the results. Reducing the\nset of languages when building the language detector can also improve accuracy for this task\nif the languages occurring in the text are equal to the languages supported by the respective\nlanguage detector instance.\n\nIn the example above, a list of\n[`DetectionResult`](https://github.com/pemistahl/lingua-py/blob/pure-python-impl/lingua/detector.py#L148)\nis returned. Each entry in the list describes a contiguous single-language text section,\nproviding start and end indices of the respective substring.\n\n11.8 Single-threaded versus multi-threaded language detection\n\nThe `LanguageDetector` methods explained above all operate in a single thread.\nIf you want to classify a very large set of texts, you will probably want to\nuse all available CPU cores efficiently in multiple threads for maximum performance.\n\nEvery single-threaded method has a multi-threaded equivalent that accepts a list of texts\nand returns a list of results.\n\nSingle-threaded\n--------------------------------------\n`detect_language_of`\n`detect_multiple_languages_of`\n`compute_language_confidence_values`\n`compute_language_confidence`\n\n11.9 Methods to build the LanguageDetector"}, {"name": "lingua-language-detector", "tags": ["dev", "math", "ml", "web"], "summary": "An accurate natural language detection library, suitable for short text and mixed-language text", "text": "There might be classification tasks where you know beforehand that your\nlanguage data is definitely not written in Latin, for instance. The detection\naccuracy can become better in such cases if you exclude certain languages from\nthe decision process or just explicitly include relevant languages:\n\n11.10 Differences to native Python enums\n\nAs version >= 2.0 has been implemented in Rust with Python bindings implemented\nwith [PyO3](https://pyo3.rs), there are some limitations with regard to enums.\n[PyO3 does not yet support meta classes](https://github.com/PyO3/pyo3/issues/906), \nthat's why Lingua's enums do not exactly behave like native Python enums.\n\nIf you want to iterate through all members of the `Language` enum, for instance, \nyou can do it like this:\n\nPyO3 enums are not subscriptable. If you want to get an enum member dynamically,\nyou can do it like this:\n\n12. What's next for version 2.2.0?\n\nTake a look at the [planned issues](https://github.com/pemistahl/lingua-py/milestone/10)."}, {"name": "lingua-language-detector", "tags": ["dev", "math", "ml", "web"], "summary": "An accurate natural language detection library, suitable for short text and mixed-language text", "text": "This library is used to detect the languages present in short text and mixed-language text with high accuracy, enabling developers to preprocess linguistic data for various applications such as text classification and spell checking. This library serves as a convenient alternative to large machine learning frameworks or natural language processing systems, allowing developers to quickly and easily perform language detection tasks."}, {"name": "livekit-agents", "tags": ["math", "ml"], "summary": "A powerful framework for building realtime voice AI agents", "text": "LiveKit Agents for Python\n\nRealtime framework for production-grade multimodal and voice AI agents.\n\nSee [https://docs.livekit.io/agents/](https://docs.livekit.io/agents/) for quickstarts, documentation, and examples."}, {"name": "livekit-agents", "tags": ["math", "ml"], "summary": "A powerful framework for building realtime voice AI agents", "text": "This library is used to build real-time voice AI agents with multimodal capabilities for production-grade applications. Developers can utilize it to create complex conversational interfaces that integrate audio, video, and text inputs seamlessly."}, {"name": "livekit-plugins-deepgram", "tags": ["math", "ml", "web"], "summary": "Agent Framework plugin for services using Deepgram's API.", "text": "Deepgram plugin for LiveKit Agents\n\nSupport for [Deepgram](https://deepgram.com/)'s voice AI services in LiveKit Agents.\n\nMore information is available in the docs for the [STT](https://docs.livekit.io/agents/integrations/stt/deepgram/) and [TTS](https://docs.livekit.io/agents/integrations/tts/deepgram/) integrations.\n\nInstallation\n\nPre-requisites\n\nYou'll need an API key from DeepGram. It can be set as an environment variable: `DEEPGRAM_API_KEY`"}, {"name": "livekit-plugins-deepgram", "tags": ["math", "ml", "web"], "summary": "Agent Framework plugin for services using Deepgram's API.", "text": "This library is used to integrate Deepgram's voice AI services into LiveKit Agents, enabling text-to-speech and speech-to-text capabilities. With this plugin, developers can leverage Deepgram's API within their LiveKit applications for advanced audio processing and analysis functionalities."}, {"name": "livekit-plugins-openai", "tags": ["math", "ml", "web"], "summary": "Agent Framework plugin for services from OpenAI", "text": "OpenAI plugin for LiveKit Agents\n\nSupport for OpenAI Realtime API, LLM, TTS, and STT APIs.\n\nAlso includes support for a large number of OpenAI-compatible APIs including [Azure OpenAI](https://docs.livekit.io/agents/integrations/llm/azure-openai/), [Cerebras](https://docs.livekit.io/agents/integrations/cerebras/), [Fireworks](https://docs.livekit.io/agents/integrations/llm/fireworks/), [Perplexity](https://docs.livekit.io/agents/integrations/llm/perplexity/), [Telnyx](https://docs.livekit.io/agents/integrations/llm/telnyx/), [xAI](https://docs.livekit.io/agents/integrations/llm/xai/), [Ollama](https://docs.livekit.io/agents/integrations/llm/ollama/), and [DeepSeek](https://docs.livekit.io/agents/integrations/llm/deepseek/).\n\nSee [https://docs.livekit.io/agents/integrations/openai/](https://docs.livekit.io/agents/integrations/openai/) for more information.\n\nInstallation\n\nPre-requisites\n\nYou'll need an API key from OpenAI. It can be set as an environment variable: `OPENAI_API_KEY`"}, {"name": "livekit-plugins-openai", "tags": ["math", "ml", "web"], "summary": "Agent Framework plugin for services from OpenAI", "text": "This library is used to integrate services from OpenAI with LiveKit Agents, providing support for various APIs such as Realtime API, LLM, TTS, and STT. This integration enables developers to leverage a large number of OpenAI-compatible APIs in their applications."}, {"name": "livekit-plugins-silero", "tags": ["math", "ml"], "summary": "Agent Framework Plugin for Silero", "text": "Silero VAD plugin for LiveKit Agents\n\nSupport for VAD-based turn detection.\n\nSee [https://docs.livekit.io/agents/build/turns/vad/](https://docs.livekit.io/agents/build/turns/vad/) for more information.\n\nInstallation\n\nThis plugin contains model files that would need to be downloaded prior to use."}, {"name": "livekit-plugins-silero", "tags": ["math", "ml"], "summary": "Agent Framework Plugin for Silero", "text": "This library is used to enable voice activity detection (VAD) based turn detection in LiveKit Agents, allowing developers to accurately identify and manage turns in conversations. With this library, developers can build more efficient and accurate turn-taking logic into their applications, improving overall user experience."}, {"name": "livekit-plugins-turn-detector", "tags": ["math", "ml", "web"], "summary": "End of utterance detection for LiveKit Agents", "text": "Turn detector plugin for LiveKit Agents\n\nThis plugin introduces end-of-turn detection for LiveKit Agents using a custom open-weight model to determine when a user has finished speaking.\n\nTraditional voice agents use VAD (voice activity detection) for end-of-turn detection. However, VAD models lack language understanding, often causing false positives where the agent interrupts the user before they finish speaking.\n\nBy leveraging a language model specifically trained for this task, this plugin offers a more accurate and robust method for detecting end-of-turns.\n\nSee [https://docs.livekit.io/agents/build/turns/turn-detector/](https://docs.livekit.io/agents/build/turns/turn-detector/) for more information.\n\nInstallation\n\nUsage\n\nMultilingual model\n\nWe've trained a multilingual model that supports the following languages: `English, French, Spanish, German, Italian, Portuguese, Dutch, Chinese, Japanese, Korean, Indonesian, Russian, Turkish`\n\nThe multilingual model requires ~400MB of RAM and completes inferences in ~25ms.\n\nUsage with RealtimeModel\n\nThe turn detector can be used even with speech-to-speech models such as OpenAI's Realtime API. You'll need to provide a separate STT to ensure our model has access to the text content.\n\nRunning your agent\n\nThis plugin requires model files. Before starting your agent for the first time, or when building Docker images for deployment, run the following command to download the model files:\n\nDownloaded model files\n\nModel files are downloaded to and loaded from the location specified by the `HF_HUB_CACHE` environment variable. If not set, this defaults to `$HF_HOME/hub` (typically `~/.cache/huggingface/hub`).\n\nFor offline deployment, download the model files first while connected to the internet, then copy the cache directory to your deployment environment.\n\nModel system requirements\n\nThe end-of-turn model is optimized to run on CPUs with modest system requirements. It is designed to run on the same server hosting your agents.\n\nThe model requires <500MB of RAM and runs within a shared inference server, supporting multiple concurrent sessions.\n\nLicense\n\nThe plugin source code is licensed under the Apache-2.0 license.\n\nThe end-of-turn model is licensed under the [LiveKit Model License](https://huggingface.co/livekit/turn-detector/blob/main/LICENSE)."}, {"name": "livekit-plugins-turn-detector", "tags": ["math", "ml", "web"], "summary": "End of utterance detection for LiveKit Agents", "text": "This library is used to provide end-of-turn detection for LiveKit Agents with improved accuracy and robustness, leveraging a custom language model specifically trained for this task. This library helps ensure that voice agents interrupt users only when they have truly finished speaking, reducing false positives."}, {"name": "livekit", "tags": ["math", "ml", "web"], "summary": "Python Real-time SDK for LiveKit", "text": "LiveKit SDK for Python\n\nPython SDK to integrate LiveKit's real-time video, audio, and data capabilities into your Python applications using WebRTC. Designed for use with [LiveKit Agents](https://github.com/livekit/agents) to build powerful voice AI apps.\n\nSee https://docs.livekit.io/ for more information."}, {"name": "livekit", "tags": ["math", "ml", "web"], "summary": "Python Real-time SDK for LiveKit", "text": "This library is used to integrate real-time video, audio, and data capabilities into Python applications using WebRTC, enabling developers to build powerful voice AI apps. With LiveKit SDK for Python, developers can seamlessly integrate live video and audio streams into their applications."}, {"name": "llama-index-core", "tags": ["math", "ml"], "summary": "Interface between LLMs and your data", "text": "LlamaIndex Core\n\nThe core python package to the LlamaIndex library. Core classes and abstractions\nrepresent the foundational building blocks for LLM applications, most notably,\nRAG. Such building blocks include abstractions for LLMs, Vector Stores, Embeddings,\nStorage, Callables and several others.\n\nWe've designed the core library so that it can be easily extended through subclasses.\nBuilding LLM applications with LlamaIndex thus involves building with LlamaIndex\ncore as well as with the LlamaIndex [integrations](https://github.com/run-llama/llama_index/tree/main/llama-index-integrations) needed for your application."}, {"name": "llama-index-core", "tags": ["math", "ml"], "summary": "Interface between LLMs and your data", "text": "This library is used to provide a core interface between Large Language Models (LLMs) and various data storage and processing systems, enabling the development of LLM applications. It serves as the foundational building block for RAG and other LLM applications, offering extensible classes and abstractions for tasks such as embedding, storage, and model interactions."}, {"name": "lxml", "tags": ["web"], "summary": "Powerful and Pythonic XML processing library combining libxml2/libxslt with the ElementTree API.", "text": "lxml is a Pythonic, mature binding for the libxml2 and libxslt libraries.\nIt provides safe and convenient access to these libraries using the\nElementTree API.\n\nIt extends the ElementTree API significantly to offer support for XPath,\nRelaxNG, XML Schema, XSLT, C14N and much more.\n\nTo contact the project, go to the `project home page `_\nor see our bug tracker at https://launchpad.net/lxml\n\nIn case you want to use the current in-development version of lxml,\nyou can get it from the github repository at\nbuild the sources, see the build instructions on the project home page.\n\nAfter an official release of a new stable series, bug fixes may become available at\nRunning ``pip install https://github.com/lxml/lxml/archive/refs/heads/lxml-6.0.tar.gz``\nwill install the unreleased branch state as soon as a maintenance branch has been established.\nNote that this requires Cython to be installed at an appropriate version for the build.\n\n6.0.2 (2025-09-21)\n==================\n\nBugs fixed\n----------\n\n* LP#2125278: Compilation with libxml2 2.15.0 failed.\n  Original patch by Xi Ruoyao.\n\n* Setting ``decompress=True`` in the parser had no effect in libxml2 2.15.\n\n* Binary wheels on Linux and macOS use the library version libxml2 2.14.6.\n  See https://gitlab.gnome.org/GNOME/libxml2/-/releases/v2.14.6\n\n* Test failures in libxml2 2.15.0 were fixed.\n\nOther changes\n-------------\n\n* Binary wheels for Py3.9-3.11 on the ``riscv64`` architecture were added.\n\n* Error constants were updated to match libxml2 2.15.0.\n\n* Built using Cython 3.1.4."}, {"name": "lxml", "tags": ["web"], "summary": "Powerful and Pythonic XML processing library combining libxml2/libxslt with the ElementTree API.", "text": "This library is used to efficiently and safely process XML documents with features like XPath, RelaxNG validation, and XSLT transformations. Developers can leverage lxml to perform complex XML operations in a Pythonic way, extending the ElementTree API for robust XML parsing and manipulation."}, {"name": "mapclassify", "tags": ["math", "visualization"], "summary": "Classification Schemes for Choropleth Maps.", "text": "`mapclassify` implements a family of classification schemes for choropleth maps.\nIts focus is on the determination of the number of classes, and the assignment\nof observations to those classes. It is intended for use with upstream mapping\nand geovisualization packages (see `geopandas`_ and `geoplot`_)\nthat handle the rendering of the maps.\n\nFor further theoretical background see \"`Choropleth Mapping`_\" in Rey, S.J., D. Arribas-Bel, and L.J. Wolf (2020) \"Geographic Data Science with PySAL and the PyData Stack\u201d.\n\n.. _geopandas: https://geopandas.org/mapping.html\n.. _geoplot: https://residentmario.github.io/geoplot/user_guide/Customizing_Plots.html\n.. _Choropleth Mapping: https://geographicdata.science/book/notebooks/05_choropleth.html"}, {"name": "mapclassify", "tags": ["math", "visualization"], "summary": "Classification Schemes for Choropleth Maps.", "text": "This library is used to determine the optimal number of classes and assign observations to those classes for choropleth maps, enabling effective data visualization. This functionality is designed for use in geovisualization packages such as `geopandas` and `geoplot`."}, {"name": "marisa-trie", "tags": ["math", "web"], "summary": "Static memory-efficient and fast Trie-like structures for Python.", "text": "MARISA Trie\n===========\n\nPyPI Version\nPyPI Status\nPyPI Python Versions\nGithub Build Status\n\n.. tip::\n\nPatreon\n\nStatic memory-efficient Trie-like structures for Python (3.8+)\nbased on `marisa-trie`_ C++ library.\n\nString data in a MARISA-trie may take up to 50x-100x less memory than\nin a standard Python dict; the raw lookup speed is comparable; trie also\nprovides fast advanced methods like prefix search.\n\n.. note::\n\n.. _marisa-trie: https://github.com/s-yata/marisa-trie\n\nInstallation\n============\n\n::\n\nUsage\n=====\n\nSee `tutorial`_ and `API`_ for details.\n\n.. _tutorial: https://marisa-trie.readthedocs.io/en/latest/tutorial.html\n.. _API: https://marisa-trie.readthedocs.io/en/latest/api.html\n\nCurrent limitations\n===================\n\n* The library is not tested with mingw32 compiler;\n* ``.prefixes()`` method of ``BytesTrie`` and ``RecordTrie`` is quite slow\n  and doesn't have iterator counterpart;\n* ``read()`` and ``write()`` methods don't work with file-like objects\n  (they work only with real files; pickling works fine for file-like objects);\n* there are ``keys()`` and ``items()`` methods but no ``values()`` method.\n\nLicense\n=======\n\nWrapper code is licensed under MIT License.\n\nBundled `marisa-trie`_ C++ library is dual-licensed under\nLGPL or BSD 2-clause license.\n\n.. |PyPI Version| image:: https://img.shields.io/pypi/v/marisa-trie.svg\n   :target: https://pypi.python.org/pypi/marisa-trie/\n.. |PyPI Status| image:: https://img.shields.io/pypi/status/marisa-trie.svg\n   :target: https://pypi.python.org/pypi/marisa-trie/\n.. |PyPI Python Versions| image:: https://img.shields.io/pypi/pyversions/marisa-trie.svg\n   :target: https://pypi.python.org/pypi/marisa-trie/\n\n:target: https://github.com/pytries/marisa-trie/actions/workflows/tests.yml\n\n:target: https://www.patreon.com/mschoentgen\n\nCHANGES\n=======\n\n1.3.1 (2025-08-26)\n------------------\n\n* Set the Cython language level to \"3\" (#126)\n\n1.3.0 (2025-08-16)\n------------------\n\n* Updated ``libmarisa-trie`` to the latest version (0.2.7) (#116).\n* Dropped Python 3.7, 3.8 support (#112, #120).\n* Added Python 3.13 support (#112).\n* Rebuild Cython wrapper with Cython 3.1.3 (#119, [4d564de](4d564de332191c3fe33aa2240cae2494c597bba2)).\n* Moved static project metadata to ``pyproject.toml`` (#120).\n* Updated metadata license to include the bundled one from marisa-trie as well (#120).\n* Add Cython as build dependency (#122).\n\n1.2.1 (2024-10-12)\n------------------\n\n* Publish Python 3.13 wheels (only CPython ones, PyPy ones are skipped until https://github.com/pypa/distutils/issues/283 is fixed).\n* Rebuild Cython wrapper with Cython 3.0.11.\n\n1.2.0 (2024-06-05)\n------------------\n\n* Added Python 3.13 support (#105).\n* Rebuild Cython wrapper with Cython 3.0.10 (#105).\n\n1.1.1 (2024-05-06)\n------------------\n\n* Publish Linux aarch64 wheels (#101).\n\n1.1.0 (2023-10-06)\n------------------\n\n* Added Python 3.12 support.\n\n1.0.0 (2023-09-03)\n------------------\n\n* Dropped Python 2.7, 3.4, 3.5, 3.6 support.\n* Added ``Trie.map()`` (#90).\n* Rebuilt Cython wrapper with Cython 3.0.2.\n* Fixed benchmark documentation typos (#89).\n\n0.8.0 (2023-03-25)\n------------------\n\n* Add ``Trie.iter_prefixes_with_ids()`` method to return ``(prefix, id)`` pairs (#83).\n* Rebuild Cython wrapper with Cython 0.29.33 (#88).\n\n0.7.8 (2022-10-25)\n------------------\n\n* Added Python 3.11 support.\n* Rebuild Cython wrapper with Cython 0.29.32.\n\n0.7.7 (2021-08-04)\n------------------\n\n* Restored Python 2.7 support.\n* Fixed README image references not working on Windows.\n\n0.7.6 (2021-07-28)\n------------------\n\n* Wheels are now published for all platforms.\n* Fixed ``ResourceWarning: unclosed file`` in ``setup.py``.\n* Run ``black`` on the entire source code.\n* Moved the QA/CI to GitHub.\n* Rebuild Cython wrapper with Cython 0.29.24.\n* Updated ``libmarisa-trie`` to the latest version (0.2.6).\n* Fixed failing tests and usage of deprecated methods.\n* Expanded supported Python version (2.7, 3.4 - 3.10).\n\n0.7.5 (2018-04-10)\n------------------\n\n* Removed redundant ``DeprecationWarning`` messages in ``Trie.save`` and\n  ``Trie.load``.\n* Dropped support for Python 2.6.\n* Rebuild Cython wrapper with Cython 0.28.1."}, {"name": "marisa-trie", "tags": ["math", "web"], "summary": "Static memory-efficient and fast Trie-like structures for Python.", "text": "0.7.4 (2017-03-27)\n------------------\n\n* Fixed packaging issue, ``MANIFEST.in`` was not updated after ``libmarisa-trie``\n  became a submodule.\n\n0.7.3 (2017-02-14)\n------------------\n\n* Added ``BinaryTrie`` for storing arbitrary sequences of bytes, e.g. IP\n  addresses (thanks Tomasz Melcer);\n* Deprecated ``Trie.has_keys_with_prefix`` which can be trivially implemented in\n  terms of ``Trie.iterkeys``;\n* Deprecated ``Trie.read`` and ``Trie.write`` which onlywork for \"real\" files\n  and duplicate the functionality of ``load`` and ``save``. See issue #31 on\n  GitHub;\n* Updated ``libmarisa-trie`` to the latest version. Yay, 64-bit Windows support.\n* Rebuilt Cython wrapper with Cython 0.25.2.\n\n0.7.2 (2015-04-21)\n------------------\n\n* packaging issue is fixed.\n\n0.7.1 (2015-04-21)\n------------------\n\n* setup.py is switched to setuptools;\n* a tiny speedup;\n* wrapper is rebuilt with Cython 0.22.\n\n0.7 (2014-12-15)\n----------------\n\n* ``trie1 == trie2`` and ``trie1 != trie2`` now work (thanks Sergei Lebedev);\n* ``for key in trie:`` is fixed (thanks Sergei Lebedev);\n* wrapper is rebuilt with Cython 0.21.1 (thanks Sergei Lebedev);\n* https://bitbucket.org/kmike/marisa-trie repo is no longer supported.\n\n0.6 (2014-02-22)\n----------------\n\n* New ``Trie`` methods: ``__getitem__``, ``get``, ``items``, ``iteritems``.\n  ``trie[u'key']`` is now the same as ``trie.key_id(u'key')``.\n* small optimization for ``BytesTrie.get``.\n* wrapper is rebuilt with Cython 0.20.1.\n\n0.5.3 (2014-02-08)\n------------------\n\n* small ``Trie.restore_key`` optimization (it should work 5-15% faster)\n\n0.5.2 (2014-02-08)\n------------------\n\n* fix ``Trie.restore_key`` method - it was reading past declared string length;\n* rebuild wrapper with Cython 0.20.\n\n0.5.1 (2013-10-03)\n------------------\n\n* ``has_keys_with_prefix(prefix)`` method (thanks\n  `Matt Hickford `_)\n\n0.5 (2013-05-07)\n----------------\n\n* ``BytesTrie.iterkeys``, ``BytesTrie.iteritems``,\n  ``RecordTrie.iterkeys`` and ``RecordTrie.iteritems`` methods;\n* wrapper is rebuilt with Cython 0.19;\n* ``value_separator`` parameter for ``BytesTrie`` and ``RecordTrie``.\n\n0.4 (2013-02-28)\n----------------\n\n* improved trie building: ``weights`` optional parameter;\n* improved trie building: unnecessary input sorting is removed;\n* wrapper is rebuilt with Cython 0.18;\n* bundled marisa-trie C++ library is updated to svn r133.\n\n0.3.8 (2013-01-03)\n------------------\n\n* Rebuild wrapper with Cython pre-0.18;\n* update benchmarks.\n\n0.3.7 (2012-09-21)\n------------------\n\n* Update bundled marisa-trie C++ library (this may fix more mingw issues);\n* Python 3.3 support is back.\n\n0.3.6 (2012-09-05)\n------------------\n\n* much faster (3x-7x) ``.items()`` and ``.keys()`` methods for all tries;\n  faster (up to 3x) ``.prefixes()`` method for ``Trie``.\n\n0.3.5 (2012-08-30)\n------------------\n\n* Pickling of RecordTrie is fixed (thanks lazarou for the report);\n* error messages should become more useful.\n\n0.3.4 (2012-08-29)\n------------------\n\n* Issues with mingw32 should be resolved (thanks Susumu Yata).\n\n0.3.3 (2012-08-27)\n------------------\n\n* ``.get(key, default=None)`` method for ``BytesTrie`` and ``RecordTrie``;\n* small README improvements.\n\n0.3.2 (2012-08-26)\n------------------\n\n* Small code cleanup;\n* ``load``, ``read`` and ``mmap`` methods returns 'self';\n* I can't run tests (via tox) under Python 3.3 so it is\n  removed from supported versions for now.\n\n0.3.1 (2012-08-23)\n------------------\n\n* ``.prefixes()`` support for RecordTrie and BytesTrie.\n\n0.3 (2012-08-23)\n----------------\n\n* RecordTrie and BytesTrie are introduced;\n* IntTrie class is removed (probably temporary?);\n* dumps/loads methods are renamed to tobytes/frombytes;\n* benchmark & tests improvements;\n* support for MARISA-trie config options is added.\n\n0.2 (2012-08-19)\n------------------\n\n* Pickling/unpickling support;\n* dumps/loads methods;\n* python 3.3 workaround;\n* improved tests;\n* benchmarks.\n\n0.1 (2012-08-17)\n----------------\n\nInitial release."}, {"name": "marisa-trie", "tags": ["math", "web"], "summary": "Static memory-efficient and fast Trie-like structures for Python.", "text": "This library is used to provide fast and memory-efficient Trie-like data structures for Python, allowing developers to efficiently store and retrieve large amounts of string data while minimizing memory usage. With MARISA Trie, developers can achieve significant memory savings (up to 50x-100x less) compared to traditional Python dictionaries, along with comparable lookup speeds and advanced methods like prefix search."}, {"name": "matplotlib", "tags": ["math", "visualization"], "summary": "Python plotting package", "text": "Install\n\nSee the [install\ndocumentation](https://matplotlib.org/stable/users/installing/index.html),\nwhich is generated from `/doc/install/index.rst`\n\nContribute\n\nYou've discovered a bug or something else you want to change \u2014 excellent!\n\nYou've worked out a way to fix it \u2014 even better!\n\nYou want to tell us about it \u2014 best of all!\n\nStart at the [contributing\nguide](https://matplotlib.org/devdocs/devel/contribute.html)!\n\nContact\n\n[Discourse](https://discourse.matplotlib.org/) is the discussion forum\nfor general questions and discussions and our recommended starting\npoint.\n\nOur active mailing lists (which are mirrored on Discourse) are:\n\n[Gitter](https://gitter.im/matplotlib/matplotlib) is for coordinating\ndevelopment and asking questions directly related to contributing to\nmatplotlib.\n\nCiting Matplotlib\n\nIf Matplotlib contributes to a project that leads to publication, please\nacknowledge this by citing Matplotlib.\n\n[A ready-made citation\nentry](https://matplotlib.org/stable/users/project/citing.html) is\navailable."}, {"name": "matplotlib", "tags": ["math", "visualization"], "summary": "Python plotting package", "text": "This library is used to create high-quality 2D and 3D plots with various customizable options. It provides a wide range of visualization tools, allowing developers to effectively communicate complex data insights through interactive and static graphs."}, {"name": "mdanalysis", "tags": ["dev", "math", "web"], "summary": "An object-oriented toolkit to analyze molecular dynamics trajectories.", "text": "numfocus\n\nbuild\n\ndocs\n\npypi\n\nMDAnalysis_ is a Python library for the analysis of computer simulations of many-body systems at the molecular scale, spanning use cases from interactions of drugs with proteins to novel materials. It is widely used in the scientific community and is written by scientists for scientists. \n\nIt works with a wide range of popular simulation packages including GROMACS, Amber, NAMD, CHARMM, DL_POLY, HOOMD, LAMMPS and many others \u2014 see the lists of supported `trajectory formats`_ and `topology formats`_.\nMDAnalysis also includes widely used analysis algorithms in the `MDAnalysis.analysis`_ module.\n\n.. _numfocus-fiscal-sponsor-attribution:\n\nThe MDAnalysis project uses an `open governance model`_ and is fiscally sponsored by `NumFOCUS`_. Consider making \na `tax-deductible donation`_ to help the project pay for developer time, professional services, travel, workshops, and a variety of other needs.\n\n.. image:: https://www.mdanalysis.org/public/images/numfocus-sponsored-small.png\n   :alt: NumFOCUS (Fiscally Sponsored Project)\n   :target: https://numfocus.org/project/mdanalysis\n   :align: center\n\nThis project is bound by a `Code of Conduct`_.\n\npowered_by_MDA\n\nExample analysis script\n=======================\n\n.. code:: python\n\n   import MDAnalysis as mda\n\n   # Load simulation results with a single line\n   u = mda.Universe('topol.tpr','traj.trr')\n\n   # Select atoms\n   ag = u.select_atoms('name OH')\n\n   # Atom data made available as Numpy arrays\n   ag.positions\n   ag.velocities\n   ag.forces\n\n   # Iterate through trajectories\n   for ts in u.trajectory:\n\nDocumentation\n=============\n\n**New users** should read the `Quickstart Guide`_ and might want to\nlook at our videos_, in which core developers explain various aspects\nof MDAnalysis.\n\n**All users** should read the `User Guide`_.\n\n**Developers** may also want to refer to the `MDAnalysis API docs`_.\n\nA growing number of `tutorials`_ are available that explain how to\nconduct RMSD calculations, structural alignment, distance and contact\nanalysis, and many more.\n\nInstallation and availability\n=============================\n\nThe latest release can be **installed via pip or conda** as\ndescribed in the `Installation Quick Start`_.\n\n**Source code** is hosted in a git repository at\nGNU Lesser General Public License, version 3 or any later version (LGPLv3+).\nIndividual source code components are provided under the\nGNU Lesser General Public License, version 2.1 or any later version (LGPLv2.1+).\nPlease see the file LICENSE_ for more information.\n\nContributing\n============\n\nPlease report **bugs** or **enhancement requests** through the `Issue\nTracker`_. Questions can also be asked on `GitHub Discussions`_.\n\nIf you are a **new developer** who would like to start contributing to\nMDAnalysis get in touch on `GitHub Discussions`_. To set up a\ndevelopment environment and run the test suite read the `developer\nguide`_.\n\nCitation\n========\n\nWhen using MDAnalysis in published work, please cite the following\ntwo papers:\n\n*   R\\. J. Gowers, M. Linke, J. Barnoud, T. J. E. Reddy,\n*   N. Michaud-Agrawal, E. J. Denning, T. B. Woolf,\n\nFor citations of included algorithms and sub-modules please see the references_.\n\n.. _NumFOCUS: https://numfocus.org/\n.. _open governance model: https://www.mdanalysis.org/pages/governance/\n.. _tax-deductible donation: https://numfocus.org/donate-to-mdanalysis\n.. _`Code of Conduct`: https://www.mdanalysis.org/conduct/\n.. _trajectory formats: https://docs.mdanalysis.org/documentation_pages/coordinates/init.html#id1\n.. _topology formats: https://docs.mdanalysis.org/documentation_pages/topology/init.html#supported-topology-formats\n.. _MDAnalysis: https://www.mdanalysis.org\n.. _LICENSE:\n.. _`Installation Quick Start`:\n.. _`MDAnalysis.analysis`: https://docs.mdanalysis.org/documentation_pages/analysis_modules.html\n.. _`tutorials`: https://userguide.mdanalysis.org/examples/README.html\n.. _`videos`: https://www.mdanalysis.org/pages/learning_MDAnalysis/#videos\n.. _`Quickstart Guide`:\n.. _`User Guide`: https://userguide.mdanalysis.org\n.. _`MDAnalysis API docs`:\n.. _`Issue Tracker`: https://github.com/mdanalysis/mdanalysis/issues\n.. _`GitHub Discussions`:\n.. _`developer guide`:\n.. _`10.1002/jcc.21787`: https://dx.doi.org/10.1002/jcc.21787\n.. _`10.25080/Majora-629e541a-00e`: https://doi.org/10.25080/Majora-629e541a-00e\n.. _references: https://docs.mdanalysis.org/documentation_pages/references.html\n.. _Embedding code: https://www.mdanalysis.org/pages/citations/#powered-by-mdanalysis\n\n   :alt: Documentation (latest release)\n   :target: https://docs.mdanalysis.org\n\n   :alt: Documentation (development version)\n   :target: https://docs.mdanalysis.org/dev\n\n   :alt: Powered by NumFOCUS\n   :target: https://www.numfocus.org/\n\n   :alt: Github Actions Build Status\n   :target: https://github.com/MDAnalysis/mdanalysis/actions/workflows/gh-ci.yaml\n\n   :alt: Github Actions Cron Job Status\n   :target: https://github.com/MDAnalysis/mdanalysis/actions/workflows/gh-ci-cron.yaml\n\n.. |cirruscron| image:: https://img.shields.io/cirrus/github/MDAnalysis/mdanalysis/develop\n   :alt: Cirrus CI - Cron job status\n   :target: https://cirrus-ci.com/github/MDAnalysis/mdanalysis/develop\n\n   :alt: Github Actions Linters Status\n   :target: https://github.com/MDAnalysis/mdanalysis/actions/workflows/linters.yaml\n\n   :alt: Coverage Status\n   :target: https://codecov.io/gh/MDAnalysis/mdanalysis\n\n.. |pypi| image:: https://img.shields.io/pypi/v/MDAnalysis\n   :alt: PyPI Version\n   :target: https://pypi.org/project/MDAnalysis/\n\n   :alt: Anaconda\n   :target: https://anaconda.org/conda-forge/mdanalysis\n   \n\n   :alt: ASV Benchmarks\n   :target:  https://www.mdanalysis.org/benchmarks/\n\n   :alt: Powered by MDAnalysis\n   :target: https://www.mdanalysis.org\n\n.. |discussions| image:: https://img.shields.io/github/discussions/MDAnalysis/MDAnalysis\n   :alt: GitHub Discussions\n   :target: https://github.com/MDAnalysis/mdanalysis/discussions"}, {"name": "mdanalysis", "tags": ["dev", "math", "web"], "summary": "An object-oriented toolkit to analyze molecular dynamics trajectories.", "text": "This library is used to analyze molecular dynamics trajectories from various simulation packages, enabling scientists to extract insights into complex many-body systems. With MDAnalysis, developers can implement a range of analysis algorithms for tasks such as drug-protein interactions and novel materials research."}, {"name": "measurement", "tags": ["math", "web"], "summary": "Easily use and manipulate unit-aware measurements in Python.", "text": ".. image:: https://travis-ci.org/coddingtonbear/python-measurement.svg?branch=master\n   :target: https://travis-ci.org/coddingtonbear/python-measurement\n\nEasily use and manipulate unit-aware measurement objects in Python.\n\n`django.contrib.gis.measure `_\nhas these wonderful 'Distance' objects that can be used not only for storing a\nunit-aware distance measurement, but also for converting between different\nunits and adding/subtracting these objects from one another.\n\nThis module not only provides those Distance and Area measurement\nobjects, but also other measurements including:\n\n- Energy\n- Speed\n- Temperature\n- Time\n- Volume\n- Weight\n\nExample:\n\n.. code-block:: python\n\n   >>> from measurement.measures import Weight\n   >>> weight_1 = Weight(lb=125)\n   >>> weight_2 = Weight(kg=40)\n   >>> added_together = weight_1 + weight_2\n   >>> added_together\n   Weight(lb=213.184976807)\n   >>> added_together.kg  # Maybe I actually need this value in kg?\n   96.699\n\n.. warning::\n   Measurements are stored internally by converting them to a\n   floating-point number of a (generally) reasonable SI unit.  Given that \n   floating-point numbers are very slightly lossy, you should be aware of\n   any inaccuracies that this might cause.\n\n   TLDR: Do not use this in\n   `navigation algorithms guiding probes into the atmosphere of extraterrestrial worlds `_.\n\n- Documentation for python-measurement is available an\n  `ReadTheDocs `_.\n- Please post issues on\n  `Github `_.\n- Test status available on\n  `Travis-CI `_.\n\n.. image:: https://d2weczhvl823v0.cloudfront.net/coddingtonbear/python-measurement/trend.png\n\n   :target: https://bitdeli.com/free"}, {"name": "measurement", "tags": ["math", "web"], "summary": "Easily use and manipulate unit-aware measurements in Python.", "text": "This library is used to create and manipulate unit-aware measurements in various physical units, including distance, area, energy, speed, temperature, time, volume, and weight. With this library, developers can easily convert between different measurement units and perform arithmetic operations on these values."}, {"name": "mediapipe", "tags": ["math", "ml", "ui", "web"], "summary": "MediaPipe is the simplest way for researchers and developers to build world-class ML solutions and applications for mobile, edge, cloud and the web.", "text": "Get started\n\nYou can get started with MediaPipe Solutions by by checking out any of the\ndeveloper guides for\n[vision](https://developers.google.com/mediapipe/solutions/vision/object_detector),\n[text](https://developers.google.com/mediapipe/solutions/text/text_classifier),\nand\n[audio](https://developers.google.com/mediapipe/solutions/audio/audio_classifier)\ntasks. If you need help setting up a development environment for use with\nMediaPipe Tasks, check out the setup guides for\n[Android](https://developers.google.com/mediapipe/solutions/setup_android), [web\napps](https://developers.google.com/mediapipe/solutions/setup_web), and\n[Python](https://developers.google.com/mediapipe/solutions/setup_python).\n\nSolutions\n\nMediaPipe Solutions provides a suite of libraries and tools for you to quickly\napply artificial intelligence (AI) and machine learning (ML) techniques in your\napplications. You can plug these solutions into your applications immediately,\ncustomize them to your needs, and use them across multiple development\nplatforms. MediaPipe Solutions is part of the MediaPipe [open source\nproject](https://github.com/google/mediapipe), so you can further customize the\nsolutions code to meet your application needs.\n\nThese libraries and resources provide the core functionality for each MediaPipe\nSolution:\n\n*   **MediaPipe Tasks**: Cross-platform APIs and libraries for deploying\n*   **MediaPipe models**: Pre-trained, ready-to-run models for use with each\n\nThese tools let you customize and evaluate solutions:\n\n*   **MediaPipe Model Maker**: Customize models for solutions with your data.\n*   **MediaPipe Studio**: Visualize, evaluate, and benchmark solutions in your\n\nLegacy solutions\n\nWe have ended support for [these MediaPipe Legacy Solutions](https://developers.google.com/mediapipe/solutions/guide#legacy)\nas of March 1, 2023. All other MediaPipe Legacy Solutions will be upgraded to\na new MediaPipe Solution. See the [Solutions guide](https://developers.google.com/mediapipe/solutions/guide#legacy)\nfor details. The [code repository](https://github.com/google/mediapipe/tree/master/mediapipe)\nand prebuilt binaries for all MediaPipe Legacy Solutions will continue to be\nprovided on an as-is basis.\n\nFor more on the legacy solutions, see the [documentation](https://github.com/google/mediapipe/tree/master/docs/solutions).\n\nFramework\n\nTo start using MediaPipe Framework, [install MediaPipe\nFramework](https://developers.google.com/mediapipe/framework/getting_started/install)\nand start building example applications in C++, Android, and iOS.\n\n[MediaPipe Framework](https://developers.google.com/mediapipe/framework) is the\nlow-level component used to build efficient on-device machine learning\npipelines, similar to the premade MediaPipe Solutions.\n\nBefore using MediaPipe Framework, familiarize yourself with the following key\n[Framework\nconcepts](https://developers.google.com/mediapipe/framework/framework_concepts/overview.md):\n\n*   [Packets](https://developers.google.com/mediapipe/framework/framework_concepts/packets.md)\n*   [Graphs](https://developers.google.com/mediapipe/framework/framework_concepts/graphs.md)\n*   [Calculators](https://developers.google.com/mediapipe/framework/framework_concepts/calculators.md)\n\nCommunity\n\n*   [Slack community](https://mediapipe.page.link/joinslack) for MediaPipe\n*   [Discuss](https://groups.google.com/forum/#!forum/mediapipe) - General\n*   [Awesome MediaPipe](https://mediapipe.page.link/awesome-mediapipe) - A\n\nContributing\n\nWe welcome contributions. Please follow these\n[guidelines](https://github.com/google/mediapipe/blob/master/CONTRIBUTING.md).\n\nWe use GitHub issues for tracking requests and bugs. Please post questions to\nthe MediaPipe Stack Overflow with a `mediapipe` tag.\n\nResources\n\nPublications\n\n*   [Bringing artworks to life with AR](https://developers.googleblog.com/2021/07/bringing-artworks-to-life-with-ar.html)\n*   [Prosthesis control via Mirru App using MediaPipe hand tracking](https://developers.googleblog.com/2021/05/control-your-mirru-prosthesis-with-mediapipe-hand-tracking.html)\n*   [SignAll SDK: Sign language interface using MediaPipe is now available for\n*   [MediaPipe Holistic - Simultaneous Face, Hand and Pose Prediction, on\n*   [Background Features in Google Meet, Powered by Web ML](https://ai.googleblog.com/2020/10/background-features-in-google-meet.html)\n*   [MediaPipe 3D Face Transform](https://developers.googleblog.com/2020/09/mediapipe-3d-face-transform.html)\n*   [Instant Motion Tracking With MediaPipe](https://developers.googleblog.com/2020/08/instant-motion-tracking-with-mediapipe.html)\n*   [BlazePose - On-device Real-time Body Pose Tracking](https://ai.googleblog.com/2020/08/on-device-real-time-body-pose-tracking.html)\n*   [MediaPipe Iris: Real-time Eye Tracking and Depth Estimation](https://ai.googleblog.com/2020/08/mediapipe-iris-real-time-iris-tracking.html)\n*   [MediaPipe KNIFT: Template-based feature matching](https://developers.googleblog.com/2020/04/mediapipe-knift-template-based-feature-matching.html)\n*   [Alfred Camera: Smart camera features using MediaPipe](https://developers.googleblog.com/2020/03/alfred-camera-smart-camera-features-using-mediapipe.html)\n*   [Real-Time 3D Object Detection on Mobile Devices with MediaPipe](https://ai.googleblog.com/2020/03/real-time-3d-object-detection-on-mobile.html)\n*   [AutoFlip: An Open Source Framework for Intelligent Video Reframing](https://ai.googleblog.com/2020/02/autoflip-open-source-framework-for.html)\n*   [MediaPipe on the Web](https://developers.googleblog.com/2020/01/mediapipe-on-web.html)\n*   [Object Detection and Tracking using MediaPipe](https://developers.googleblog.com/2019/12/object-detection-and-tracking-using-mediapipe.html)\n*   [On-Device, Real-Time Hand Tracking with MediaPipe](https://ai.googleblog.com/2019/08/on-device-real-time-hand-tracking-with.html)\n*   [MediaPipe: A Framework for Building Perception Pipelines](https://arxiv.org/abs/1906.08172)\n\nVideos\n\n*   [YouTube Channel](https://www.youtube.com/c/MediaPipe)"}, {"name": "mediapipe", "tags": ["math", "ml", "ui", "web"], "summary": "MediaPipe is the simplest way for researchers and developers to build world-class ML solutions and applications for mobile, edge, cloud and the web.", "text": "This library is used to build world-class ML solutions and applications across various platforms, including mobile, edge, cloud, and the web. Developers can use MediaPipe Solutions to implement tasks such as object detection, text classification, and audio classification with ease."}, {"name": "mercantile", "tags": ["cli", "math", "web"], "summary": "Web mercator XYZ tile utilities", "text": "==========\nMercantile\n==========\n\n.. image:: https://travis-ci.com/mapbox/mercantile.svg\n   :target: https://travis-ci.com/mapbox/mercantile\n   :alt: Build Status\n\n   :target: https://coveralls.io/github/mapbox/mercantile?branch=master\n   :alt: Coverage Status\n\n   :alt: Documentation Status\n\nSpherical mercator coordinate and tile utilities\n\nDocumentation: http://mercantile.readthedocs.io/en/latest/\n\nThe mercantile module provides ``ul(xtile, ytile, zoom)`` and ``bounds(xtile,\nytile, zoom)`` functions that respectively return the upper left corner and\nbounding longitudes and latitudes for XYZ tiles, a ``xy(lng, lat)`` function\nthat returns spherical mercator x and y coordinates, a ``tile(lng, lat,\nzoom)`` function that returns the tile containing a given point, and\nquadkey conversion functions ``quadkey(xtile, ytile, zoom)`` and\n``quadkey_to_tile(quadkey)`` for translating between quadkey and tile\ncoordinates.\n\n.. code-block:: pycon\n\nAlso in mercantile are functions to traverse the tile stack.\n\n.. code-block:: pycon\n\nNamed tuples are used to represent tiles, coordinates, and bounding boxes.\n\nMercantile CLI\n==============\n\nMercantile's command line interface, named \"mercantile\", has commands for \ngetting the shapes of Web Mercator tiles as GeoJSON and getting the tiles\nthat intersect with a GeoJSON bounding box. \n\n.. code-block:: console\n\n\t$ mercantile --help\n\tUsage: mercantile [OPTIONS] COMMAND [ARGS]...\n\n\t  Command line interface for the Mercantile Python package.\n\n\tOptions:\n\t  -v, --verbose  Increase verbosity.\n\t  -q, --quiet    Decrease verbosity.\n\t  --version      Show the version and exit.\n\t  --help         Show this message and exit.\n\n\tCommands:\n\t  bounding-tile  Print the bounding tile of a lng/lat point, bounding box, or\n\t\t\t\t\t GeoJSON objects.\n\t  children       Print the children of the tile.\n\t  neighbors      Print the neighbors of the tile.\n\t  parent         Print the parent tile.\n\t  quadkey        Convert to/from quadkeys.\n\t  shapes         Print the shapes of tiles as GeoJSON.\n\t  tiles          Print tiles that overlap or contain a lng/lat point, bounding\n\t\t\t\t\t box, or GeoJSON objects.\n\nSee Also\n========\n\n`supermercado `__ is another python lib\nwith added tile logic functionality (union tile shapes, find edge tiles, and\nfind tile intersections for complex geometries).\n\n`node-sphericalmercator `__\nprovides many of the same features for Node.\n\n`tilebelt `__ has some of the GeoJSON\nfeatures as mercantile and a few more (tile parents, quadkey).\n\n`morecantile `__ is like mercantile,\nbut with support for other TileMatrixSet grids."}, {"name": "mercantile", "tags": ["cli", "math", "web"], "summary": "Web mercator XYZ tile utilities", "text": "This library is used to convert between longitude/latitude coordinates and their corresponding XYZ tiles in a Mercator projection. It provides utilities for determining the boundaries of tiles, converting between spherical Mercator coordinates and tile coordinates, and more."}, {"name": "mizani", "tags": ["math", "visualization"], "summary": "Scales for Python", "text": "Mizani\n\n(https://pypi.python.org/pypi/mizani)\n(https://pypi.python.org/pypi/mizani)\n(https://github.com/has2k1/mizani/actions?query=branch%3Amain+workflow%3A%22build%22)\n(https://mizani.readthedocs.io/en/latest/)\n(https://codecov.io/github/has2k1/mizani?branch=main)\n\nMizani is a scales package for graphics. It is written in Python and is\nbased on Hadley Wickham's [Scales](https://github.com/r-lib/scales).\nSee the [documentation](https://mizani.readthedocs.io/en/latest/)\nfor how to use it in a graphics system.\n\nInstallation\n\nOfficial Release\n\nDevelopment version"}, {"name": "mizani", "tags": ["math", "visualization"], "summary": "Scales for Python", "text": "This library is used to create scalable numerical and date-time scales for Python graphics, providing a robust and consistent way to format data on plots. This library helps developers build customizable and aesthetically pleasing visualizations by transforming raw data into readable formats."}, {"name": "ml-collections", "tags": ["dev", "math", "ml"], "summary": "ML Collections is a library of Python collections designed for ML usecases.", "text": "ML Collections\n\nML Collections is a library of Python Collections designed for ML use cases.\n\n(https://github.com/google/ml_collections/actions/workflows/pytest_and_autopublish.yml)\n\nConfigDict\n\nThe two classes called `ConfigDict` and `FrozenConfigDict` are \"dict-like\" data\nstructures with dot access to nested elements. Together, they are supposed to be\nused as a main way of expressing configurations of experiments and models.\n\nThis document describes example usage of `ConfigDict`, `FrozenConfigDict`,\n`FieldReference`.\n\nFeatures\n\n*   Dot-based access to fields.\n*   Locking mechanism to prevent spelling mistakes.\n*   Lazy computation.\n*   FrozenConfigDict() class which is immutable and hashable.\n*   Type safety.\n*   \"Did you mean\" functionality.\n*   Human readable printing (with valid references and cycles), using valid YAML\n*   Fields can be passed as keyword arguments using the `**` operator.\n*   There is one exception to the strong type-safety of the ConfigDict: `int`\n\nBasic Usage\n\nFrozenConfigDict\n\nA `FrozenConfigDict`is an immutable, hashable type of `ConfigDict`:\n\nFieldReferences and placeholders\n\nA `FieldReference` is useful for having multiple fields use the same value. It\ncan also be used for [lazy computation](#lazy-computation).\n\nYou can use `placeholder()` as a shortcut to create a `FieldReference` (field)\nwith a `None` default value. This is useful if a program uses optional\nconfiguration fields.\n\nNote that the indirection provided by `FieldReference`s will be lost if accessed\nthrough a `ConfigDict`.\n\nLazy computation\n\nUsing a `FieldReference` in a standard operation (addition, subtraction,\nmultiplication, etc...) will return another `FieldReference` that points to the\noriginal's value. You can use `FieldReference.get()` to execute the operations\nand get the reference's computed value, and `FieldReference.set()` to change the\noriginal reference's value.\n\nIf a `FieldReference` has `None` as its original value, or any operation has an\nargument of `None`, then the lazy computation will evaluate to `None`.\n\nWe can also use fields in a `ConfigDict` in lazy computation. In this case a\nfield will only be lazily evaluated if `ConfigDict.get_ref()` is used to get it.\n\nChanging lazily computed values\n\nLazily computed values in a ConfigDict can be overridden in the same way as\nregular values. The reference to the `FieldReference` used for the lazy\ncomputation will be lost and all computations downstream in the reference graph\nwill use the new value.\n\nCycles\n\nYou cannot create cycles using references. Fortunately\n[the only way](#changing-lazily-computed-values) to create a cycle is by\nassigning a computed field to one that *is not* the result of computation. This\nis forbidden:\n\nOne-way references\n\nOne gotcha with `get_ref` is that it creates a bi-directional dependency when no operations are performed on the value.\n\nThis can be avoided by using `get_oneway_ref` instead of `get_ref`.\n\nAdvanced usage\n\nHere are some more advanced examples showing lazy computation with different\noperators and data types.\n\nEquality checking\n\nYou can use `==` and `.eq_as_configdict()` to check equality among `ConfigDict`\nand `FrozenConfigDict` objects.\n\nEquality checking with lazy computation\n\nEquality checks see if the computed values are the same. Equality is satisfied\nif two sets of computations are different as long as they result in the same\nvalue.\n\nLocking and copying\n\nHere is an example with `lock()` and `deepcopy()`:\n\nOutput:\n\nDictionary attributes and initialization\n\nMore Examples"}, {"name": "ml-collections", "tags": ["dev", "math", "ml"], "summary": "ML Collections is a library of Python collections designed for ML usecases.", "text": "For more examples, take a look at\n[`ml_collections/config_dict/examples/`](https://github.com/google/ml_collections/tree/master/ml_collections/config_dict/examples)\n\nFor examples and gotchas specifically about initializing a ConfigDict, see\n[`ml_collections/config_dict/examples/config_dict_initialization.py`](https://github.com/google/ml_collections/blob/master/ml_collections/config_dict/examples/config_dict_initialization.py).\n\nConfig Flags\n\nThis library adds flag definitions to `absl.flags` to handle config files. It\ndoes not wrap `absl.flags` so if using any standard flag definitions alongside\nconfig file flags, users must also import `absl.flags`.\n\nCurrently, this module adds two new flag types, namely `DEFINE_config_file`\nwhich accepts a path to a Python file that generates a configuration, and\n`DEFINE_config_dict` which accepts a configuration directly. Configurations are\ndict-like structures (see [ConfigDict](#configdict)) whose nested elements\ncan be overridden using special command-line flags. See the examples below\nfor more details.\n\nUsage\n\nUse `ml_collections.config_flags` alongside `absl.flags`. For\nexample:\n\n`script.py`:\n\n`config.py`:\n\nWarning: If you are using a pickle-based distributed programming framework such\nas [Launchpad](https://github.com/deepmind/launchpad#readme), be aware of\nlimitations on the structure of this script that are [described below]\n(#config_files_and_pickling).\n\nNow, after running:\n\nwe get:\n\nUsage of `DEFINE_config_dict` is similar to `DEFINE_config_file`, the main\ndifference is the configuration is defined in `script.py` instead of in a\nseparate file.\n\n`script.py`:\n\n`config_file` flags are compatible with the command-line flag syntax. All the\nfollowing options are supported for non-boolean values in configurations:\n\n*   `-(-)config.field=value`\n*   `-(-)config.field value`\n\nOptions for boolean values are slightly different:\n\n*   `-(-)config.boolean_field`: set boolean value to True.\n*   `-(-)noconfig.boolean_field`: set boolean value to False.\n*   `-(-)config.boolean_field=value`: `value` is `true`, `false`, `True` or\n\nNote that `-(-)config.boolean_field value` is not supported.\n\nParameterising the get_config() function\n\nIt's sometimes useful to be able to pass parameters into `get_config`, and\nchange what is returned based on this configuration. One example is if you are\ngrid searching over parameters which have a different hierarchical structure -\nthe flag needs to be present in the resulting ConfigDict. It would be possible\nto include the union of all possible leaf values in your ConfigDict,\nbut this produces a confusing config result as you have to remember which\nparameters will actually have an effect and which won't.\n\nA better system is to pass some configuration, indicating which structure of\nConfigDict should be returned. An example is the following config file:\n\nThe value of `config_string` will be anything that is to the right of the first\ncolon in the config file path, if one exists. If no colon exists, no value is\npassed to `get_config` (producing a TypeError if `get_config` expects a value).\n\nThe above example can be run like:\n\nor like:\n\nAdditional features\n\n*   Loads any valid python script which defines `get_config()` function\n*   Automatic locking of the loaded object, if the loaded object defines a\n*   Supports command-line overriding of arbitrarily nested values in dict-like\n*   Overriding is type safe.\n*   Overriding of a `tuple` can be done by passing in the `tuple` value as a\n*   The overriding `tuple` object can be of a different length and have\n\nConfig Files and Pickling {#config_files_and_pickling}\n\nThis is likely to be troublesome:\n\nThis is not:\n\nExplanation\n\nA config file is a Python module but it is not imported through Python's usual\nmodule-importing mechanism."}, {"name": "ml-collections", "tags": ["dev", "math", "ml"], "summary": "ML Collections is a library of Python collections designed for ML usecases.", "text": "Meanwhile, serialization libraries such as [`cloudpickle`](\npickling every type to which it refers, on the assumption that types defined\nat module scope can later be reconstructed simply by re-importing the modules\nin which they are defined.\n\nThat assumption does not hold for a type that is defined at module scope in a\nconfig file, because the config file can't be imported the usual way. The\nsymptom of this will be an `ImportError` when unpickling an object.\n\nThe treatment is to move types from module scope into `get_config()` so that\nthey will be serialized along with the values that have those types.\n\nAuthors\n*   Sergio G\u00f3mez Colmenarejo - sergomez@google.com\n*   Wojciech Marian Czarnecki - lejlot@google.com\n*   Nicholas Watters\n*   Mohit Reddy - mohitreddy@google.com"}, {"name": "ml-collections", "tags": ["dev", "math", "ml"], "summary": "ML Collections is a library of Python collections designed for ML usecases.", "text": "This library is used to provide a set of Python collections with features like dot-based access, locking, lazy computation, and type safety for ML use cases. It enables developers to create and manage configurations for experiments and models in a structured and efficient manner."}, {"name": "mlflow-skinny", "tags": ["cli", "data", "math", "ml"], "summary": "MLflow is an open source platform for the complete machine learning lifecycle", "text": "Installation\n\nTo install the MLflow Python package, run the following command:\n\nCore Components\n\nMLflow is **the only platform that provides a unified solution for all your AI/ML needs**, including LLMs, Agents, Deep Learning, and traditional machine learning.\n\nFor LLM / GenAI Developers\n\nFor Data Scientists\n\nHosting MLflow Anywhere\n\n  \n\nYou can run MLflow in many different environments, including local machines, on-premise servers, and cloud infrastructure.\n\nTrusted by thousands of organizations, MLflow is now offered as a managed service by most major cloud providers:\n\nFor hosting MLflow on your own infrastructure, please refer to [this guidance](https://mlflow.org/docs/latest/ml/tracking/#tracking-setup).\n\n\ufe0f Supported Programming Languages\n\nIntegrations\n\nMLflow is natively integrated with many popular machine learning frameworks and GenAI libraries.\n\nUsage Examples\n\nTracing (Observability) ([Doc](https://mlflow.org/docs/latest/llms/tracing/index.html))\n\nMLflow Tracing provides LLM observability for various GenAI libraries such as OpenAI, LangChain, LlamaIndex, DSPy, AutoGen, and more. To enable auto-tracing, call `mlflow.xyz.autolog()` before running your models. Refer to the documentation for customization and manual instrumentation.\n\nThen navigate to the \"Traces\" tab in the MLflow UI to find the trace records OpenAI query.\n\nEvaluating LLMs, Prompts, and Agents ([Doc](https://mlflow.org/docs/latest/genai/eval-monitor/index.html))\n\nThe following example runs automatic evaluation for question-answering tasks with several built-in metrics.\n\nNavigate to the \"Evaluations\" tab in the MLflow UI to find the evaluation results.\n\nTracking Model Training ([Doc](https://mlflow.org/docs/latest/ml/tracking/))\n\nThe following examples trains a simple regression model with scikit-learn, while enabling MLflow's [autologging](https://mlflow.org/docs/latest/tracking/autolog.html) feature for experiment tracking.\n\nOnce the above code finishes, run the following command in a separate terminal and access the MLflow UI via the printed URL. An MLflow **Run** should be automatically created, which tracks the training dataset, hyper parameters, performance metrics, the trained model, dependencies, and even more.\n\nSupport\n\n- For help or questions about MLflow usage (e.g. \"how do I do X?\") visit the [documentation](https://mlflow.org/docs/latest).\n- In the documentation, you can ask the question to our AI-powered chat bot. Click on the **\"Ask AI\"** button at the right bottom.\n- Join the [virtual events](https://lu.ma/mlflow?k=c) like office hours and meetups.\n- To report a bug, file a documentation issue, or submit a feature request, please [open a GitHub issue](https://github.com/mlflow/mlflow/issues/new/choose).\n- For release announcements and other discussions, please subscribe to our mailing list (mlflow-users@googlegroups.com)\n  or join us on [Slack](https://mlflow.org/slack).\n\nContributing\n\nWe happily welcome contributions to MLflow!\n\n- Submit [bug reports](https://github.com/mlflow/mlflow/issues/new?template=bug_report_template.yaml) and [feature requests](https://github.com/mlflow/mlflow/issues/new?template=feature_request_template.yaml)\n- Contribute for [good-first-issues](https://github.com/mlflow/mlflow/issues?q=is%3Aissue+is%3Aopen+label%3A%22good+first+issue%22) and [help-wanted](https://github.com/mlflow/mlflow/issues?q=is%3Aissue+is%3Aopen+label%3A%22help+wanted%22)\n- Writing about MLflow and sharing your experience\n\nPlease see our [contribution guide](CONTRIBUTING.md) to learn more about contributing to MLflow.\n\n\u2b50\ufe0f Star History\n\n\ufe0f Citation\n\nIf you use MLflow in your research, please cite it using the \"Cite this repository\" button at the top of the [GitHub repository page](https://github.com/mlflow/mlflow), which will provide you with citation formats including APA and BibTeX.\n\nCore Members\n\nMLflow is currently maintained by the following core members with significant contributions from hundreds of exceptionally talented community members."}, {"name": "mlflow-skinny", "tags": ["cli", "data", "math", "ml"], "summary": "MLflow is an open source platform for the complete machine learning lifecycle", "text": "This library is used to enable a unified machine learning lifecycle across various AI/ML needs, including LLMs, Agents, Deep Learning, and traditional machine learning. It allows developers to manage their entire ML workflow in a single platform, regardless of the environment or infrastructure they choose."}, {"name": "mlflow-tracing", "tags": ["math", "ml", "web"], "summary": "MLflow Tracing SDK is an open-source, lightweight Python package that only includes the minimum set of dependencies and functionality to instrument your code/models/agents with MLflow Tracing.", "text": "MLflow Tracing: An Open-Source SDK for Observability and Monitoring GenAI Applications\n\n(https://mlflow.org/docs/latest/index.html)\n(https://github.com/mlflow/mlflow/blob/master/LICENSE.txt)\n(https://mlflow.org/community/#slack)\n(https://twitter.com/MLflow)\n\nMLflow Tracing (`mlflow-tracing`) is an open-source, lightweight Python package that only includes the minimum set of dependencies and functionality\nto instrument your code/models/agents with [MLflow Tracing Feature](https://mlflow.org/docs/latest/tracing). It is designed to be a perfect fit for production environments where you want:\n\n- **\u26a1\ufe0f Faster Deployment**: The package size and dependencies are significantly smaller than the full MLflow package, allowing for faster deployment times in dynamic environments such as Docker containers, serverless functions, and cloud-based applications.\n- ** Simplified Dependency Management**: A smaller set of dependencies means less work keeping up with dependency updates, security patches, and breaking changes from upstream libraries.\n- ** Portability**: With the less number of dependencies, MLflow Tracing can be easily deployed across different environments and platforms, without worrying about compatibility issues.\n- ** Fewer Security Risks**: Each dependency potentially introduces security vulnerabilities. By reducing the number of dependencies, MLflow Tracing minimizes the attack surface and reduces the risk of security breaches.\n\nFeatures\n\n- Other tracing APIs such as `mlflow.set_trace_tag`, `mlflow.search_traces`, etc.\n\nChoose Backend\n\nThe MLflow Trace package is designed to work with a remote hosted MLflow server as a backend. This allows you to log your traces to a central location, making it easier to manage and analyze your traces. There are several different options for hosting your MLflow server, including:\n\nGetting Started\n\nInstallation\n\nTo install the MLflow Python package, run the following command:\n\nTo install from the source code, run the following command:\n\n> **NOTE:** It is **not** recommended to co-install this package with the full MLflow package together, as it may cause version mismatches issues.\n\nConnect to the MLflow Server\n\nTo connect to your MLflow server to log your traces, set the `MLFLOW_TRACKING_URI` environment variable or use the `mlflow.set_tracking_uri` function:\n\nStart Logging Traces\n\nDocumentation\n\nOfficial documentation for MLflow Tracing can be found at [here](https://mlflow.org/docs/latest/tracing).\n\nFeatures _Not_ Included\n\nThe following MLflow features are not included in this package.\n\n- MLflow tracking server and UI.\n- MLflow's other tracking capabilities such as Runs, Model Registry, Projects, etc.\n- Evaluate models/agents and log evaluation results.\n\nTo leverage the full feature set of MLflow, install the full package by running `pip install mlflow`."}, {"name": "mlflow-tracing", "tags": ["math", "ml", "web"], "summary": "MLflow Tracing SDK is an open-source, lightweight Python package that only includes the minimum set of dependencies and functionality to instrument your code/models/agents with MLflow Tracing.", "text": "This library is used to instrument your code/models/agents with MLflow Tracing features, enabling observability and monitoring of GenAI applications. By doing so, it facilitates faster deployment of production environments through reduced package size and dependencies."}, {"name": "mlflow", "tags": ["cli", "data", "math", "ml"], "summary": "MLflow is an open source platform for the complete machine learning lifecycle", "text": "Installation\n\nTo install the MLflow Python package, run the following command:\n\nCore Components\n\nMLflow is **the only platform that provides a unified solution for all your AI/ML needs**, including LLMs, Agents, Deep Learning, and traditional machine learning.\n\nFor LLM / GenAI Developers\n\nFor Data Scientists\n\nHosting MLflow Anywhere\n\n  \n\nYou can run MLflow in many different environments, including local machines, on-premise servers, and cloud infrastructure.\n\nTrusted by thousands of organizations, MLflow is now offered as a managed service by most major cloud providers:\n\nFor hosting MLflow on your own infrastructure, please refer to [this guidance](https://mlflow.org/docs/latest/ml/tracking/#tracking-setup).\n\n\ufe0f Supported Programming Languages\n\nIntegrations\n\nMLflow is natively integrated with many popular machine learning frameworks and GenAI libraries.\n\nUsage Examples\n\nTracing (Observability) ([Doc](https://mlflow.org/docs/latest/llms/tracing/index.html))\n\nMLflow Tracing provides LLM observability for various GenAI libraries such as OpenAI, LangChain, LlamaIndex, DSPy, AutoGen, and more. To enable auto-tracing, call `mlflow.xyz.autolog()` before running your models. Refer to the documentation for customization and manual instrumentation.\n\nThen navigate to the \"Traces\" tab in the MLflow UI to find the trace records OpenAI query.\n\nEvaluating LLMs, Prompts, and Agents ([Doc](https://mlflow.org/docs/latest/genai/eval-monitor/index.html))\n\nThe following example runs automatic evaluation for question-answering tasks with several built-in metrics.\n\nNavigate to the \"Evaluations\" tab in the MLflow UI to find the evaluation results.\n\nTracking Model Training ([Doc](https://mlflow.org/docs/latest/ml/tracking/))\n\nThe following examples trains a simple regression model with scikit-learn, while enabling MLflow's [autologging](https://mlflow.org/docs/latest/tracking/autolog.html) feature for experiment tracking.\n\nOnce the above code finishes, run the following command in a separate terminal and access the MLflow UI via the printed URL. An MLflow **Run** should be automatically created, which tracks the training dataset, hyper parameters, performance metrics, the trained model, dependencies, and even more.\n\nSupport\n\n- For help or questions about MLflow usage (e.g. \"how do I do X?\") visit the [documentation](https://mlflow.org/docs/latest).\n- In the documentation, you can ask the question to our AI-powered chat bot. Click on the **\"Ask AI\"** button at the right bottom.\n- Join the [virtual events](https://lu.ma/mlflow?k=c) like office hours and meetups.\n- To report a bug, file a documentation issue, or submit a feature request, please [open a GitHub issue](https://github.com/mlflow/mlflow/issues/new/choose).\n- For release announcements and other discussions, please subscribe to our mailing list (mlflow-users@googlegroups.com)\n  or join us on [Slack](https://mlflow.org/slack).\n\nContributing\n\nWe happily welcome contributions to MLflow!\n\n- Submit [bug reports](https://github.com/mlflow/mlflow/issues/new?template=bug_report_template.yaml) and [feature requests](https://github.com/mlflow/mlflow/issues/new?template=feature_request_template.yaml)\n- Contribute for [good-first-issues](https://github.com/mlflow/mlflow/issues?q=is%3Aissue+is%3Aopen+label%3A%22good+first+issue%22) and [help-wanted](https://github.com/mlflow/mlflow/issues?q=is%3Aissue+is%3Aopen+label%3A%22help+wanted%22)\n- Writing about MLflow and sharing your experience\n\nPlease see our [contribution guide](CONTRIBUTING.md) to learn more about contributing to MLflow.\n\n\u2b50\ufe0f Star History\n\n\ufe0f Citation\n\nIf you use MLflow in your research, please cite it using the \"Cite this repository\" button at the top of the [GitHub repository page](https://github.com/mlflow/mlflow), which will provide you with citation formats including APA and BibTeX.\n\nCore Members\n\nMLflow is currently maintained by the following core members with significant contributions from hundreds of exceptionally talented community members."}, {"name": "mlflow", "tags": ["cli", "data", "math", "ml"], "summary": "MLflow is an open source platform for the complete machine learning lifecycle", "text": "This library is used to provide a unified solution for the complete machine learning lifecycle, including tasks such as LLMs, Agents, Deep Learning, and traditional machine learning. With MLflow, developers can run their AI/ML projects anywhere, from local machines to cloud infrastructure, making it easy to deploy and manage models in various environments."}, {"name": "mlxtend", "tags": ["math", "ml"], "summary": "Machine Learning Library Extensions", "text": "Links\n\nInstalling mlxtend\n\nPyPI\n\nTo install mlxtend, just execute  \n\nAlternatively, you could download the package manually from the Python Package Index [https://pypi.python.org/pypi/mlxtend](https://pypi.python.org/pypi/mlxtend), unzip it, navigate into the package, and use the command:\n\nConda\nIf you use conda, to install mlxtend just execute\n\nDev Version\n\nThe mlxtend version on PyPI may always be one step behind; you can install the latest development version from the GitHub repository by executing\n\nOr, you can fork the GitHub repository from https://github.com/rasbt/mlxtend and install mlxtend from your local drive via\n\nExamples\n\n---\n\nIf you use mlxtend as part of your workflow in a scientific publication, please consider citing the mlxtend repository with the following DOI:\n\n- Raschka, Sebastian (2018) MLxtend: Providing machine learning and data science utilities and extensions to Python's scientific computing stack.\nJ Open Source Softw 3(24).\n\n---\n\nLicense\n\n- This project is released under a permissive new BSD open source license ([LICENSE-BSD3.txt](https://github.com/rasbt/mlxtend/blob/master/LICENSE-BSD3.txt)) and commercially usable. There is no warranty; not even for merchantability or fitness for a particular purpose.\n- In addition, you may use, copy, modify and redistribute all artistic creative works (figures and images) included in this distribution under the directory\naccording to the terms and conditions of the Creative Commons Attribution 4.0 International License.  See the file [LICENSE-CC-BY.txt](https://github.com/rasbt/mlxtend/blob/master/LICENSE-CC-BY.txt) for details. (Computer-generated graphics such as the plots produced by matplotlib fall under the BSD license mentioned above).\n\nContact\n\nThe best way to ask questions is via the [GitHub Discussions channel](https://github.com/rasbt/mlxtend/discussions). In case you encounter usage bugs, please don't hesitate to use the [GitHub's issue tracker](https://github.com/rasbt/mlxtend/issues) directly."}, {"name": "mlxtend", "tags": ["math", "ml"], "summary": "Machine Learning Library Extensions", "text": "This library is used to extend existing machine learning functionalities and provide additional tools for data analysis. It offers various utilities and algorithms to enhance the capabilities of other popular machine learning libraries."}, {"name": "mpi4py", "tags": ["math", "ui", "web"], "summary": "Python bindings for MPI", "text": "==============\nMPI for Python\n==============\n\nThis package provides Python bindings for the *Message Passing\nInterface* (MPI_) standard. It is implemented on top of the MPI\nspecification and exposes an API which grounds on the standard MPI-2\nC++ bindings.\n\n.. _MPI: https://www.mpi-forum.org\n\nFeatures\n========\n\nThis package supports:\n\n* Convenient communication of any *picklable* Python object\n\n+ point-to-point (send & receive)\n  + collective (broadcast, scatter & gather, reductions)\n\n* Fast communication of Python object exposing the *Python buffer\n  interface* (NumPy arrays, builtin bytes/string/array objects)\n\n+ point-to-point (blocking/nonblocking/persistent send & receive)\n  + collective (broadcast, block/vector scatter & gather, reductions)\n\n* Process groups and communication domains\n\n+ creation of new intra/inter communicators\n  + creation/query of Cartesian & graph topologies\n\n* Parallel input/output:\n\n+ read & write\n  + blocking/nonblocking & collective/noncollective\n  + individual/shared file pointers & explicit offset\n\n* Dynamic process management\n\n+ spawn & spawn multiple\n  + accept/connect\n  + name publishing & lookup\n\n* One-sided operations\n\n+ remote memory access (put, get, accumulate)\n  + passive target synchronization (start/complete & post/wait)\n  + active target synchronization (lock & unlock)\n\nInstall\n=======\n\n**Wheel** packages\n------------------\n\nThe mpi4py project builds and publishes binary wheels able to run in a\nvariety of:\n\n* operating systems: *Linux*, *macOS*, *Windows*;\n* processor architectures: *AMD64*, *ARM64*;\n* MPI implementations: *MPICH*, *Open MPI*,\n  *MVAPICH*, *Intel MPI*, *HPE Cray MPICH*, *Microsoft MPI*;\n* Python implementations: *CPython*, *PyPy*.\n\n.. _MPICH:          https://mpich.org\n.. _Open MPI:       https://open-mpi.org\n.. _MVAPICH:        https://mvapich.cse.ohio-state.edu\n.. _HPE Cray MPICH: https://cpe.ext.hpe.com/docs/latest/mpt/mpich/\n.. _NVIDIA HPC-X:   https://developer.nvidia.com/networking/hpc-x\n.. _Intel MPI:      https://software.intel.com/intel-mpi-library\n.. _Microsoft MPI:  https://learn.microsoft.com/message-passing-interface/microsoft-mpi\n\nThese mpi4py wheels are distributed via the Python Package Index\n(`PyPI `_) and can be installed\nwith Python package managers like `pip`_:\n\n.. code:: sh\n\npython -m pip install mpi4py\n\n.. _pip:   https://pip.pypa.io\n\nThe mpi4py wheels can be installed in standard Python virtual\nenvironments. The MPI runtime can be provided by other wheels\ninstalled in the same virtual environment.\n\n.. tip::\n\nIntel publishes production-grade `Intel MPI wheels\n   `_ for Linux (x86_64) and Windows (AMD64).\n   mpi4py and MPI wheels can be installed side by side to get a\n   ready-to-use Python+MPI environment:\n\n.. code:: sh\n\n.. _impi-rt-wheels: https://pypi.org/project/impi-rt/#files\n\n.. tip::\n\nThe mpi4py project publishes `MPICH wheels `_ and\n   `Open MPI wheels `_ for Linux\n   (x86_64/aarch64) and macOS (arm64/x86_64).\n   mpi4py and MPI wheels can be installed side by side to get a\n   ready-to-use Python+MPI environment:\n\n.. code:: sh\n\n.. _mpich-wheels:   https://pypi.org/project/mpich/#files\n   .. _openmpi-wheels: https://pypi.org/project/openmpi/#files\n\n.. warning::\n\nThe mpi4py wheels can also be installed (with `pip`_) in `conda`_\nenvironments and they should work out of the box, without any special\ntweak to environment variables, for any of the MPI packages provided\nby `conda-forge`_.\n\nExternally-provided MPI implementations may come from a system package\nmanager, sysadmin-maintained builds accessible via module files, or\ncustomized user builds. Such usage is supported and encouraged.\nHowever, there are a few platform-specific considerations to take into\naccount.\n\nLinux\n^^^^^\n\nThe Linux (x86_64/aarch64) wheels require one of\n\n* `MPICH`_ or any other ABI-compatible derivative,\n  like `MVAPICH`_, `Intel MPI`_, `HPE Cray MPICH`_\n\n* `Open MPI`_ or any other ABI-compatible derivative,\n  like `NVIDIA HPC-X`_"}, {"name": "mpi4py", "tags": ["math", "ui", "web"], "summary": "Python bindings for MPI", "text": "Users may need to set the ``LD_LIBRARY_PATH`` environment variable\nsuch that the dynamic linker is able to find at runtime the MPI shared\nlibrary file (``libmpi.so.*``).\n\nFedora/RHEL\n~~~~~~~~~~~\n\nOn Fedora/RHEL systems, both MPICH and Open MPI are available for\ninstallation. There is no default or preferred MPI implementation.\nInstead, users must select their favorite MPI implementation by\nloading the proper MPI module.\n\n.. code:: sh\n\nmodule load mpi/mpich-$(arch)    # for MPICH\n   module load mpi/openmpi-$(arch)  # for Open MPI\n\nAfter loading the requested MPI module, the ``LD_LIBRARY_PATH``\nenvironment variable should be properly setup.\n\nDebian/Ubuntu\n~~~~~~~~~~~~~\n\nOn Debian/Ubuntu systems, Open MPI is the default MPI implementation\nand most of the MPI-based applications and libraries provided by the\ndistribution depend on Open MPI. Nonetheless, MPICH is also\navailable to users for installation.\n\nIn Ubuntu 22.04 and older, due to legacy reasons, the MPICH ABI is\nslightly broken: the MPI shared library file is named\n``libmpich.so.12`` instead of ``libmpi.so.12`` as required by the\n`MPICH ABI Compatibility Initiative `_.\n\nUsers without ``sudo`` access can workaround this issue creating a\nsymbolic link anywhere in their home directory and appending to\n``LD_LIBRARY_PATH``.\n\n.. code:: sh\n\nmkdir -p ~/.local/lib\n   libdir=/usr/lib/$(arch)-linux-gnu\n   ln -s $libdir/libmpich.so.12 ~/.local/lib/libmpi.so.12\n   export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:~/.local/lib\n\nA system-wide fix for all users requires ``sudo`` access:\n\n.. code:: sh\n\nlibdir=/usr/lib/$(arch)-linux-gnu\n   sudo ln -sr $libdir/libmpi{ch,}.so.12\n\nHPE Cray OS\n~~~~~~~~~~~\n\nOn HPE Cray systems, users must load the ``cray-mpich-abi`` module.\nFor further details, refer to `man intro_mpi `_.\n\n.. _cray-mpt-mpichabi: https://cpe.ext.hpe.com/docs/latest/mpt/mpich/intro_mpi.html#using-mpich-abi-compatibility\n\nmacOS\n^^^^^\n\nThe macOS (arm64/x86_64) wheels require\n\n* `MPICH`_ or `Open MPI`_ installed (either manually or via a package\n  manager) in the standard system prefix ``/usr/local``\n\n* `MPICH`_ or `Open MPI`_ installed via `Homebrew`_ in the default\n  prefix ``/opt/homebrew``\n\n* `MPICH`_ or `Open MPI`_ installed via `MacPorts`_ in the default\n  prefix ``/opt/local``\n\n.. _Homebrew: https://brew.sh/\n.. _MacPorts: https://www.macports.org/\n\nWindows\n^^^^^^^\n\nThe Windows (AMD64) wheels require one of\n\n* `Intel MPI`_\n\n* `Microsoft MPI`_\n\nUser may need to set the ``I_MPI_ROOT`` or ``MSMPI_BIN`` environment\nvariables such that the MPI dynamic link library (DLL) (``impi.dll``\nor ``msmpi.dll``) can be found at runtime.\n\nIntel MPI is under active development and supports recent versions of\nthe MPI standard. Intel MPI can be installed with ``pip`` (see the\n`impi-rt`_ package on PyPI), being therefore straightforward to get it\nup and running within a Python environment. Intel MPI can also be\ninstalled system-wide as part of the Intel oneAPI HPC Toolkit for\nWindows or via standalone online/offline installers.\n\n.. _impi-rt: https://pypi.org/project/impi-rt/\n\n**Conda** packages\n------------------\n\nThe `conda-forge`_ community provides ready-to-use binary packages\nfrom an ever growing collection of software libraries built around the\nmulti-platform *conda* package manager. Four MPI implementations are\navailable on conda-forge: Open MPI (Linux and macOS), MPICH (Linux and\nmacOS), Intel MPI (Linux and Windows), and Microsoft MPI (Windows).\nYou can install mpi4py and your preferred MPI implementation using the\n`conda`_ package manager:\n\n* to use MPICH do:\n\n.. code:: sh\n\n* to use Open MPI do:\n\n.. code:: sh\n\n* to use Intel MPI do:\n\n.. code:: sh"}, {"name": "mpi4py", "tags": ["math", "ui", "web"], "summary": "Python bindings for MPI", "text": "* to use Microsoft MPI do:\n\n.. code:: sh\n\nMPICH and many of its derivatives are ABI-compatible. You can provide\nthe package specification ``mpich=X.Y.*=external_*`` (where ``X`` and\n``Y`` are the major and minor version numbers) to request the conda\npackage manager to use system-provided MPICH (or derivative)\nlibraries. Similarly, you can provide the package specification\n``openmpi=X.Y.*=external_*`` to use system-provided Open MPI\nlibraries.\n\nThe ``openmpi`` package on conda-forge has built-in CUDA support, but\nit is disabled by default. To enable it, follow the instruction\noutlined during ``conda install``. Additionally, UCX support is also\navailable once the ``ucx`` package is installed.\n\n.. warning::\n\nThe MPI conda-forge packages are built with special focus on\n   compatibility. The MPICH and Open MPI packages are built in a\n   constrained environment with relatively dated OS images. Therefore,\n   they may lack support for high-performance features like\n   cross-memory attach (XPMEM/CMA). In production scenarios, it is\n   recommended to use external (either custom-built or system-provided)\n   MPI installations. See the relevant conda-forge documentation about\n   `using external MPI libraries `_ .\n\n.. _conda: https://docs.conda.io\n.. _conda-forge: https://conda-forge.org/\n.. _cf-mpi-docs: https://conda-forge.org/docs/user/tipsandtricks/#using-external-message-passing-interface-mpi-libraries\n\nSystem packages\n---------------\n\nmpi4py is readily available through system package managers of most\nLinux distributions and the most popular community package managers\nfor macOS.\n\n.. _sys-pkg-linux:\n\nLinux\n^^^^^\n\nOn **Fedora Linux** systems (as well as **RHEL** and their derivatives\nusing the EPEL software repository), you can install binary packages\nwith the system package manager:\n\n* using ``dnf`` and the ``mpich`` package:\n\n.. code:: sh\n\n* using ``dnf`` and the ``openmpi`` package:\n\n.. code:: sh\n\nPlease remember to load the correct MPI module for your chosen MPI\nimplementation:\n\n* for the ``mpich`` package do:\n\n.. code:: sh\n\n* for the ``openmpi`` package do:\n\n.. code:: sh\n\nOn **Ubuntu Linux** and **Debian Linux** systems, binary packages are\navailable for installation using the system package manager:\n\n.. code:: sh\n\nsudo apt install python3-mpi4py\n\nOn **Arch Linux** systems, binary packages are available for\ninstallation using the system package manager:\n\n.. code:: sh\n\nsudo pacman -S python-mpi4py\n\n.. _sys-pkg-macos:\n\nmacOS\n^^^^^\n\nmacOS users can install mpi4py using the `Homebrew`_ package\nmanager:\n\n.. code:: sh\n\nNote that the Homebrew mpi4py package uses Open MPI. Alternatively,\ninstall the ``mpich`` package and next install mpi4py from sources\nusing ``pip``.\n\nAlternatively, mpi4py can be installed from `MacPorts`_:\n\n.. code:: sh\n\nsudo port install py-mpi4py\n\nBuilding from sources\n---------------------\n\nInstalling mpi4py from pre-built binary wheels, conda packages, or\nsystem packages is not always desired or appropriate. For example, the\nmpi4py wheels published on PyPI may not be interoperable with\nnon-mainstream, vendor-specific MPI implementations; or a system\nmpi4py package may be built with a alternative, non-default MPI\nimplementation. In such scenarios, mpi4py can still be installed from\nits source distribution (sdist) using ``pip``:\n\n.. code:: sh\n\npython -m pip install --no-binary=mpi4py mpi4py\n\nYou can also install the in-development version with:\n\n.. code:: sh\n\npython -m pip install git+https://github.com/mpi4py/mpi4py\n\nor:\n\n.. code:: sh\n\npython -m pip install https://github.com/mpi4py/mpi4py/tarball/master\n\n.. note::"}, {"name": "mpi4py", "tags": ["math", "ui", "web"], "summary": "Python bindings for MPI", "text": "Installing mpi4py from its source distribution (available on PyPI)\n   or Git source code repository (available on GitHub) requires a C\n   compiler and a working MPI implementation with development headers\n   and libraries.\n\n.. warning::\n\n``pip`` keeps previously built wheel files in its cache for future\n   reuse. If you want to reinstall the ``mpi4py`` package from its source\n   distribution using a different or updated MPI implementation, you have\n   to either first remove the cached wheel file:\n\n.. code:: sh\n\nor ask ``pip`` to disable the cache:\n\n.. code:: sh\n\nCitation\n========\n\nIf MPI for Python been significant to a project that leads to an\nacademic publication, please acknowledge that fact by citing the\nproject.\n\n* M. Rogowski, S. Aseeri, D. Keyes, and L. Dalcin,\n  *mpi4py.futures: MPI-Based Asynchronous Task Execution for Python*,\n  IEEE Transactions on Parallel and Distributed Systems, 34(2):611-622, 2023.\n\n* L. Dalcin and Y.-L. L. Fang,\n  *mpi4py: Status Update After 12 Years of Development*,\n  Computing in Science & Engineering, 23(4):47-54, 2021.\n\n* L. Dalcin, P. Kler, R. Paz, and A. Cosimo,\n  *Parallel Distributed Computing using Python*,\n  Advances in Water Resources, 34(9):1124-1139, 2011.\n\n* L. Dalcin, R. Paz, M. Storti, and J. D'Elia,\n  *MPI for Python: performance improvements and MPI-2 extensions*,\n  Journal of Parallel and Distributed Computing, 68(5):655-662, 2008.\n\n* L. Dalcin, R. Paz, and M. Storti,\n  *MPI for Python*,\n  Journal of Parallel and Distributed Computing, 65(9):1108-1115, 2005."}, {"name": "mpi4py", "tags": ["math", "ui", "web"], "summary": "Python bindings for MPI", "text": "This library is used to enable high-performance parallel computing in Python by providing a standardized interface for inter-process communication and data exchange. It allows developers to easily implement scalable and efficient parallel algorithms using the Message Passing Interface (MPI) standard."}, {"name": "mplfinance", "tags": ["math", "visualization", "web"], "summary": "Utilities for the visualization, and visual analysis, of financial data", "text": "mplfinance\nmatplotlib utilities for the visualization, and visual analysis, of financial data\n\nInstallation\n\n- mplfinance requires [matplotlib](https://pypi.org/project/matplotlib/) and [pandas](https://pypi.org/project/pandas/)\n\n---\n\n**&roarr; [Latest Release Information](https://github.com/matplotlib/mplfinance/releases) &loarr;**\n\n&roarr; **[Older Release Information](https://github.com/matplotlib/mplfinance/blob/master/RELEASE_NOTES.md)**\n---\n\nContents and Tutorials\n\n---\n\nThe New API\n\nThis repository, `matplotlib/mplfinance`, contains a new **matplotlib finance** API that makes it easier to create financial plots.  It interfaces nicely with **Pandas** DataFrames.\n\n*More importantly, **the new API automatically does the extra matplotlib work that the user previously had to do \"manually\" with the old API.***   (The old API is still available within this package; see below).\n\nThe conventional way to import the new API is as follows:\n\nThe most common usage is then to call\n\nwhere `data` is a `Pandas DataFrame` object containing Open, High, Low and Close data, with a Pandas `DatetimeIndex`.\n\nDetails on how to call the new API can be found below under **[Basic Usage](https://github.com/matplotlib/mplfinance#usage)**, as well as in the jupyter notebooks in the **[examples](https://github.com/matplotlib/mplfinance/blob/master/examples/)** folder.\n\nI am very interested to hear from you regarding what you think of the new `mplfinance`, plus any suggestions you may have for improvement.  You can reach me at **dgoldfarb.github@gmail.com**  or, if you prefer, provide feedback or a ask question on our **[issues page.](https://github.com/matplotlib/mplfinance/issues/new/choose)**\n\n---\n\nBasic Usage\nStart with a Pandas DataFrame containing OHLC data.  For example,\n\n...\n\nAfter importing mplfinance, plotting OHLC data is as simple as calling `mpf.plot()` on the dataframe\n\nThe default plot type, as you can see above, is `'ohlc'`.  Other plot types can be specified with the keyword argument `type`, for example, `type='candle'`, `type='line'`, `type='renko'`, or `type='pnf'`\n\n---\n\nWe can also plot moving averages with the `mav` keyword\n- use a scalar for a single moving average \n- use a tuple or list of integers for multiple moving averages\n\n---\nWe can also display `Volume`\n\nNotice, in the above chart, there are no gaps along the x-coordinate, even though there are days on which there was no trading.  ***Non-trading days are simply not shown*** (since there are no prices for those days).\n\n- However, sometimes people like to see these gaps, so that they can tell, with a quick glance, where the weekends and holidays fall."}, {"name": "mplfinance", "tags": ["math", "visualization", "web"], "summary": "Utilities for the visualization, and visual analysis, of financial data", "text": "- Non-trading days can be displayed with the **`show_nontrading`** keyword.\n  - Note that for these purposes **non-trading** intervals are those that ***are not represented in the data at all***.  (There are simply no rows for those dates or datetimes).  This is because, when data is retrieved from an exchange or other market data source, that data typically will *not* include rows for non-trading days (weekends and holidays for example).  Thus ...\n  - **`show_nontrading=True`** will display all dates (all time intervals) between the first time stamp and the last time stamp in the data (regardless of whether rows exist for those dates or datetimes).\n  - **`show_nontrading=False`** (the default value) will show ***only*** dates (or datetimes) that have actual rows in the data.  (This means that if there are rows in your DataFrame that exist but contain only **`NaN`** values, these rows *will still appear* on the plot even if **`show_nontrading=False`**)\n- For example, in the chart below, you can easily see weekends, as well as a gap at Thursday, November 28th for the U.S. Thanksgiving holiday.\n\n---\n\nWe can also plot intraday data:\n\n...\n\nThe above dataframe contains Open,High,Low,Close data at 1 minute intervals for the S&P 500 stock index for November 5, 6, 7 and 8, 2019.  Let's look at the last hour of trading on November 6th, with a 7 minute and 12 minute moving average.\n\nThe \"time-interpretation\" of the `mav` integers depends on the frequency of the data, because the mav integers are the *number of data points* used in the Moving Average (not the number of days or minutes, etc).  Notice above that for intraday data the x-axis automatically displays TIME *instead of* date.  Below we see that if the intraday data spans into two (or more) trading days the x-axis automatically displays *BOTH* TIME and DATE\n\n---\nIn the plot below, we see what an intraday plot looks like when we **display non-trading time periods** with **`show_nontrading=True`** for intraday data spanning into two or more days.\n\n---\nBelow: 4 days of intraday data with `show_nontrading=True`\n\n---\nBelow: the same 4 days of intraday data with `show_nontrading` defaulted to `False`.\n\n---\nBelow: Daily data spanning across a year boundary automatically adds the *YEAR* to the DATE format\n\n...\n\nFor more examples of using mplfinance, please see the jupyter notebooks in the **[`examples`](https://github.com/matplotlib/mplfinance/blob/master/examples/)** directory.\n\n---\n\nSome History\nMy name is Daniel Goldfarb.  In November 2019, I became the maintainer of `matplotlib/mpl-finance`.  That module is being deprecated in favor of the current `matplotlib/mplfinance`.  The old `mpl-finance` consisted of code extracted from the deprecated `matplotlib.finance` module along with a few examples of usage.  It has been mostly un-maintained for the past three years."}, {"name": "mplfinance", "tags": ["math", "visualization", "web"], "summary": "Utilities for the visualization, and visual analysis, of financial data", "text": "It is my intention to archive the `matplotlib/mpl-finance` repository soon, and direct everyone to `matplotlib/mplfinance`.  The main reason for the rename is to avoid confusion with the hyphen and the underscore: As it was, `mpl-finance` was *installed with the hyphen, but imported with an underscore `mpl_finance`.*  Going forward it will be a simple matter of both installing and importing `mplfinance`.\n\n---\n\nOld API availability\n\n**With this new ` mplfinance ` package installed, in addition to the new API, users can still access the old API**. The old API may be removed someday, but for the foreseeable future we will keep it ... at least until we are very confident that users of the old API can accomplish the same things with the new API.\n\nTo access the old API with the new ` mplfinance ` package installed, change the old import statements\n\n**from:**\n\n**to:**\n\nwhere `` indicates the method you want to import, for example:"}, {"name": "mplfinance", "tags": ["math", "visualization", "web"], "summary": "Utilities for the visualization, and visual analysis, of financial data", "text": "This library is used to simplify the creation of financial plots and visual analysis through a streamlined API, interfacing seamlessly with Pandas DataFrames. It automates complex tasks, reducing the effort required for matplotlib configuration."}, {"name": "mplhep", "tags": ["math", "web"], "summary": "Matplotlib styles for HEP", "text": "Installation\n\nGetting Started\nA tutorial given at PyHEP 2020 is available as a binder [here](https://github.com/andrzejnovak/2020-07-17-pyhep2020-mplhep)\nor you can watch the recording [here](https://www.youtube.com/watch?v=gUziXqCGe0o).\n\nDocumentation can be found at [mplhep.readthedocs.io](https://mplhep.readthedocs.io).\n\nStyling\n\nOr use `matplotlib` API directly\n\n**If the default styles are not what you need, please open an issue.**\n\nDefault experiment labels are also available.\n\nYou can use `loc={0..5}` to control the label positioning.\n\nPlotting\n\n1D Histograms\n\n2D Histograms\n\nMore Information\n\nSave all labels at once\n- `hep.savelabels('test.png')` will produces 4 variation on experiment label\n  - \"\" -> \"test.png\"\n  - \"Preliminary\" -> \"test_pas.png\"\n  - \"Supplementary\" -> \"test_supp.png\"\n  - \"Work in Progress\" -> \"test_wip.png\"\n- Options can also be specified manually\n  - `hep.savelabels('test', labels=[\"FOO\", \"BAR\"], suffixes=[\"foo.pdf\", \"bar\"])` will produce\n- Other components of `.label()` will remain unchanged.\n\nOther styles:\n- `hep.style.use(\"fira\")` - use Fira Sans\n- `hep.style.use(\"firamath\")` - use Fira Math\n\nStyles can be chained:\n- e.g. `hep.style.use([\"CMS\", \"fira\", \"firamath\"])`\n- reappearing `rcParams` get overwritten silently\n\nStyles can be modified on the fly\n- Since styles are dictionaries and they can be chained/overwritten they can be easily modified on the fly. e.g.\n\nStyling with LaTeX\n- `hep.style.use(\"CMSTex\")` - Use LaTeX to produce all text labels\n- Requires having the full tex-live distro\n- True Helvetica\n- Use sansmath as the math font\n- Takes longer and not always better\n- In general more possibilities, but a bit more difficult to get everything working properly\n\nNotes\n\nConsistency \\& Fonts\nAs it is ROOT does not come with any fonts and therefore relies on using system fonts. Therefore the font in a figure can be dependent on whether it was produced on OSX or PC. The default sans-serif font used is Helvetica, but it only comes with OSX, in Windows this will silently fallback to Arial.\n\nLicense\nBoth Helvetica and Arial are proprietary, which as far as fonts go means you can use it to create any text/graphics once you have the license, but you cannot redistribute the font files as part of other software. That means we cannot just package Helvetica with this to make sure everyone has the same font in plots.\n\nLuckily for fonts it seems only the software is copyrighted, not the actual shapes, which means there are quite a few open alternatives with similar look. The most closely resembling Helvetica being Tex Gyre Heros\n\nTex Gyre Heros\n\nYou can compare yourself if the differences are meanigful below.\n\n  \n\nThey are Tex Gyre Heros, Helvetica and Arial respectively.\n\nMath Fonts\n- Math fonts are a separate set from regular fonts due to the amount of special characters\n- It's not trivial to make sure you get a matching math font to your regular font\n- Most math-fonts are serif fonts, but this is not ideal if one wants to use sans-serif font for normal text like Helvetica or Arial\n- The number of sans-serif math-fonts is very limited\n \t- The number of **open** sans-serif math-fonts is **extremely** limited\n \t- Basically there's two, Fira Sans and GFS Neohellenic Math, of which I like Fira Sans better\n \t- https://tex.stackexchange.com/questions/374250/are-there-opentype-sans-math-fonts-under-development\n\nFor consistent styling Fira Sans is included as well.\n\nDefault Fira Sans\n\nMath font extension\n\nWhat doesn't work\n\nContext styles and fonts\n\n- This syntax would be ideal, however, it doesn't work properly for fonts and there are no plans by mpl devs to fix this behaviour https://github.com/matplotlib/matplotlib/issues/11673\n\nFor now one has to set the style globally:\n\nUse in publications\n\nUpdating list of citations and use cases of `mplhep` in publications:\n\n- And many others by now..."}, {"name": "mplhep", "tags": ["math", "web"], "summary": "Matplotlib styles for HEP", "text": "This library is used to provide customized Matplotlib styles and plotting capabilities for High Energy Physics (HEP) applications, offering a set of default experiment labels that can be easily customized. With mplhep, developers can create publication-quality plots with HEP-specific styling and labeling options."}, {"name": "mpmath", "tags": ["dev", "math", "ui", "web"], "summary": "Python library for arbitrary-precision floating-point arithmetic", "text": "mpmath\n======\n\n.. |pypi version| image:: https://img.shields.io/pypi/v/mpmath.svg\n   :target: https://pypi.python.org/pypi/mpmath\n\n:target: https://github.com/fredrik-johansson/mpmath/actions?workflow=test\n\n:target: https://codecov.io/gh/fredrik-johansson/mpmath\n\nA Python library for arbitrary-precision floating-point arithmetic.\n\nWebsite: http://mpmath.org/\nMain author: Fredrik Johansson\n\nMpmath is free software released under the New BSD License (see the\nLICENSE file for details)\n\n0. History and credits\n----------------------\n\nThe following people (among others) have contributed major patches\nor new features to mpmath:\n\n* Pearu Peterson \n* Mario Pernici \n* Ondrej Certik \n* Vinzent Steinberg \n* Nimish Telang \n* Mike Taschuk \n* Case Van Horsen \n* Jorn Baayen \n* Chris Smith \n* Juan Arias de Reyna \n* Ioannis Tziakos \n* Aaron Meurer \n* Stefan Krastanov \n* Ken Allen \n* Timo Hartmann \n* Sergey B Kirpichev \n* Kris Kuhlman \n* Paul Masson \n* Michael Kagalenko \n* Jonathan Warner \n* Max Gaukler \n* Guillermo Navas-Palencia \n* Nike Dattani\n\nNumerous other people have contributed by reporting bugs,\nrequesting new features, or suggesting improvements to the\ndocumentation.\n\nFor a detailed changelog, including individual contributions,\nsee the CHANGES file.\n\nFredrik's work on mpmath during summer 2008 was sponsored by Google\nas part of the Google Summer of Code program.\n\nFredrik's work on mpmath during summer 2009 was sponsored by the\nAmerican Institute of Mathematics under the support of the National Science\nFoundation Grant No. 0757627 (FRG: L-functions and Modular Forms).\n\nAny opinions, findings, and conclusions or recommendations expressed in this\nmaterial are those of the author(s) and do not necessarily reflect the\nviews of the sponsors.\n\nCredit also goes to:\n\n* The authors of the GMP library and the Python wrapper\n  gmpy, enabling mpmath to become much faster at\n  high precision\n* The authors of MPFR, pari/gp, MPFUN, and other arbitrary-\n  precision libraries, whose documentation has been helpful\n  for implementing many of the algorithms in mpmath\n* Wikipedia contributors; Abramowitz & Stegun; Gradshteyn & Ryzhik;\n  Wolfram Research for MathWorld and the Wolfram Functions site.\n  These are the main references used for special functions\n  implementations.\n* George Brandl for developing the Sphinx documentation tool\n  used to build mpmath's documentation\n\nRelease history:"}, {"name": "mpmath", "tags": ["dev", "math", "ui", "web"], "summary": "Python library for arbitrary-precision floating-point arithmetic", "text": "* Version 1.3.0 released on March 7, 2023\n* Version 1.2.0 released on February 1, 2021\n* Version 1.1.0 released on December 11, 2018\n* Version 1.0.0 released on September 27, 2017\n* Version 0.19 released on June 10, 2014\n* Version 0.18 released on December 31, 2013\n* Version 0.17 released on February 1, 2011\n* Version 0.16 released on September 24, 2010\n* Version 0.15 released on June 6, 2010\n* Version 0.14 released on February 5, 2010\n* Version 0.13 released on August 13, 2009\n* Version 0.12 released on June 9, 2009\n* Version 0.11 released on January 26, 2009\n* Version 0.10 released on October 15, 2008\n* Version 0.9 released on August 23, 2008\n* Version 0.8 released on April 20, 2008\n* Version 0.7 released on March 12, 2008\n* Version 0.6 released on January 13, 2008\n* Version 0.5 released on November 24, 2007\n* Version 0.4 released on November 3, 2007\n* Version 0.3 released on October 5, 2007\n* Version 0.2 released on October 2, 2007\n* Version 0.1 released on September 27, 2007\n\n1. Download & installation\n--------------------------\n\nMpmath requires Python 2.7 or 3.5 (or later versions). It has been tested\nwith CPython 2.7, 3.5 through 3.7 and for PyPy.\n\nThe latest release of mpmath can be downloaded from the mpmath\nwebsite and from https://github.com/fredrik-johansson/mpmath/releases\n\nIt should also be available in the Python Package Index at\n\nTo install latest release of Mpmath with pip, simply run\n\n``pip install mpmath``\n\nOr unpack the mpmath archive and run\n\n``python setup.py install``\n\nMpmath can also be installed using\n\n``python -m easy_install mpmath``\n\nThe latest development code is available from\n\nSee the main documentation for more detailed instructions.\n\n2. Running tests\n----------------\n\nThe unit tests in mpmath/tests/ can be run via the script\nruntests.py, but it is recommended to run them with py.test\n(https://pytest.org/), especially\nto generate more useful reports in case there are failures.\n\nYou may also want to check out the demo scripts in the demo\ndirectory.\n\nThe master branch is automatically tested by Travis CI.\n\n3. Documentation\n----------------\n\nDocumentation in reStructuredText format is available in the\ndoc directory included with the source package. These files\nare human-readable, but can be compiled to prettier HTML using\nthe build.py script (requires Sphinx, http://sphinx.pocoo.org/).\n\nSee setup.txt in the documentation for more information.\n\nThe most recent documentation is also available in HTML format:\n\n4. Known problems\n-----------------\n\nMpmath is a work in progress. Major issues include:\n\n* Some functions may return incorrect values when given extremely\n  large arguments or arguments very close to singularities.\n\n* Directed rounding works for arithmetic operations. It is implemented\n  heuristically for other operations, and their results may be off by one\n  or two units in the last place (even if otherwise accurate).\n\n* Some IEEE 754 features are not available. Inifinities and NaN are\n  partially supported; denormal rounding is currently not available\n  at all."}, {"name": "mpmath", "tags": ["dev", "math", "ui", "web"], "summary": "Python library for arbitrary-precision floating-point arithmetic", "text": "* The interface for switching precision and rounding is not finalized.\n  The current method is not threadsafe.\n\n5. Help and bug reports\n-----------------------\n\nGeneral questions and comments can be sent to the mpmath mailinglist,\nmpmath@googlegroups.com\n\nYou can also report bugs and send patches to the mpmath issue tracker,"}, {"name": "mpmath", "tags": ["dev", "math", "ui", "web"], "summary": "Python library for arbitrary-precision floating-point arithmetic", "text": "This library is used to perform high-precision numerical computations, allowing developers to handle floating-point arithmetic with arbitrary precision. This enables the creation of accurate and reliable mathematical models in applications that require precise calculations, such as scientific simulations or financial modeling."}, {"name": "mrcfile", "tags": ["math", "ui", "web"], "summary": "MRC file I/O library", "text": "mrcfile.py\n==========\n\nbuild-status\n\n.. |build-status| image:: https://app.travis-ci.com/ccpem/mrcfile.svg?branch=master\n\n.. |python-versions| image:: https://img.shields.io/pypi/pyversions/mrcfile.svg\n\n.. |pypi-version| image:: https://img.shields.io/pypi/v/mrcfile.svg\n\n.. |conda-forge-version| image:: https://img.shields.io/conda/vn/conda-forge/mrcfile.svg\n\n.. start_of_main_text\n\n``mrcfile`` is a Python implementation of the `MRC2014 file format`_, which\nis used in structural biology to store image and volume data.\n\nIt allows MRC files to be created and opened easily using a very simple API,\nwhich exposes the file's header and data as `numpy`_ arrays. The code runs in\nPython 2 and 3 and is fully unit-tested.\n\n.. _MRC2014 file format: http://www.ccpem.ac.uk/mrc_format/mrc2014.php\n.. _numpy: http://www.numpy.org/\n\nThis library aims to allow users and developers to read and write\nstandard-compliant MRC files in Python as easily as possible, and with no\ndependencies on any compiled libraries except `numpy`_. You can use it\ninteractively to inspect files, correct headers and so on, or in scripts and\nlarger software packages to provide basic MRC file I/O functions.\n\nKey Features\n------------\n\n* Clean, simple API for access to MRC files\n* Easy to install and use\n* Validation of files according to the MRC2014 format\n* Seamless support for gzip and bzip2 files\n* Memory-mapped file option for fast random access to very large files\n* Asynchronous opening option for background loading of multiple files\n* Runs in Python 2 & 3, on Linux, Mac OS X and Windows\n\nInstallation\n------------\n\nThe ``mrcfile`` library is available from the `Python package index`_::\n\nOr from `conda-forge`_::\n\nIt is also included in the ``ccpem-python`` environment in the  `CCP-EM`_\nsoftware suite.\n\n.. _CCP-EM: http://www.ccpem.ac.uk\n\nThe source code (including the full test suite) can be found `on GitHub`_.\n\n.. _Python package index: https://pypi.org/project/mrcfile\n.. _conda-forge: https://anaconda.org/conda-forge/mrcfile\n.. _on GitHub: https://github.com/ccpem/mrcfile\n\nBasic usage\n-----------\n\nThe easiest way to open a file is with the `mrcfile.open`_ and `mrcfile.new`_\nfunctions. These return an `MrcFile`_ object which represents an MRC file on\ndisk.\n\n.. _mrcfile.open: http://mrcfile.readthedocs.io/en/latest/source/mrcfile.html#mrcfile.open\n.. _mrcfile.new: http://mrcfile.readthedocs.io/en/latest/source/mrcfile.html#mrcfile.new\n.. _MrcFile: http://mrcfile.readthedocs.io/en/latest/usage_guide.html#using-mrcfile-objects\n\nTo open an MRC file and read a slice of data::\n\nTo create a new file with a 2D data array, and change some values::\n\nThe data will be saved to disk when the file is closed, either automatically at\nthe end of the ``with`` block (like a normal Python file object) or manually by\ncalling ``close()``. You can also call ``flush()`` to write any changes to disk\nand keep the file open.\n\nTo validate an MRC file::\n\nDocumentation\n-------------\n\nFull documentation is available on `Read the Docs`_.\n\n.. _Read the Docs: http://mrcfile.readthedocs.org\n\nCiting mrcfile\n--------------\n\nIf you find ``mrcfile`` useful in your work, please cite:\n\nBurnley T, Palmer C & Winn M (2017) Recent developments in the CCP-EM\nsoftware suite. *Acta Cryst.* D\\ **73**:469--477.\n`doi: 10.1107/S2059798317007859`_\n\n.. _`doi: 10.1107/S2059798317007859`: https://doi.org/10.1107/S2059798317007859\n\nContributing\n------------\n\nPlease use the `GitHub issue tracker`_ for bug reports and feature requests, or\n`email CCP-EM`_.\n\n.. _GitHub issue tracker: https://github.com/ccpem/mrcfile/issues\n.. _email CCP-EM: ccpem@stfc.ac.uk\n\nCode contributions are also welcome, please submit pull requests to the\n`GitHub repository`_.\n\n.. _GitHub repository: https://github.com/ccpem/mrcfile\n\nTo run the test suite, go to the top-level project directory (which contains\nthe ``mrcfile`` and ``tests`` packages) and run ``python -m unittest tests``.\n(Or, if you have `tox`_ installed, run ``tox``.)\n\n.. _tox: http://tox.readthedocs.org\n\nLicence\n-------\n\nThe project is released under the BSD licence."}, {"name": "mrcfile", "tags": ["math", "ui", "web"], "summary": "MRC file I/O library", "text": "This library is used to easily create, open, and manipulate MRC files in Python, allowing developers to store and work with image and volume data in structural biology applications. With mrcfile, developers can access the file's header and data as numpy arrays, simplifying the process of working with these files."}, {"name": "msgpack-numpy", "tags": ["dev", "math"], "summary": "Numpy data serialization using msgpack", "text": "Package Description\n-------------------\nThis package provides encoding and decoding routines that enable the\nserialization and deserialization of numerical and array data types provided by \n[numpy](http://www.numpy.org/) using the highly efficient\n[msgpack](http://msgpack.org/) format. Serialization of Python's\nnative complex data types is also supported.\n\n(https://pypi.python.org/pypi/msgpack-numpy)\n(https://travis-ci.org/lebedov/msgpack-numpy)\n\nInstallation\n------------\nmsgpack-numpy requires msgpack-python and numpy. If you \nhave [pip](http://www.pip-installer.org/) installed on your\nsystem, run\n\nto install the package and all dependencies. You can also download \nthe source tarball, unpack it, and run\n\nfrom within the source directory.\n\nUsage\n-----\nThe easiest way to use msgpack-numpy is to call its monkey patching\nfunction after importing the Python msgpack package:\n\nThis will automatically force all msgpack serialization and deserialization\nroutines (and other packages that use them) to become numpy-aware. \nOf course, one can also manually pass the encoder and \ndecoder provided by msgpack-numpy to the msgpack routines:\n\nmsgpack-numpy will try to use the binary (fast) extension in msgpack by default.  \nIf msgpack was not compiled with Cython (or if the ``MSGPACK_PUREPYTHON`` \nvariable is set), it will fall back to using the slower pure Python msgpack \nimplementation.\n\nNotes\n-----\nThe primary design goal of msgpack-numpy is ensuring preservation of numerical\ndata types during msgpack serialization and deserialization. Inclusion of type\ninformation in the serialized data necessarily incurs some storage overhead; if\npreservation of type information is not needed, one may be able to avoid some\nof this overhead by writing a custom encoder/decoder pair that produces more\nefficient serializations for those specific use cases. \n\nNumpy arrays with a dtype of 'O' are serialized/deserialized using pickle as \na fallback solution to enable msgpack-numpy to handle\nsuch arrays. As the additional overhead of pickle serialization negates one\nof the reasons to use msgpack, it may be advisable to either write a custom\nencoder/decoder to handle the specific use case efficiently or else not bother\nusing msgpack-numpy.\n\nNote that numpy arrays deserialized by msgpack-numpy are read-only and must be copied \nif they are to be modified.\n\nDevelopment\n-----------\nThe latest source code can be obtained from [GitHub](https://github.com/lebedov/msgpack-numpy/).\n\nmsgpack-numpy maintains compatibility with python versions 2.7 and 3.5+.\n\nInstall [`tox`](https://tox.readthedocs.io/en/latest/) to support testing\nacross multiple python versions in your development environment. If you\nuse [`conda`](https://docs.conda.io/en/latest/) to install `python` use\n[`tox-conda`](https://github.com/tox-dev/tox-conda) to automatically manage\ntesting across all supported python versions.\n\nExecute tests across supported python versions:\n\nAuthors\n-------\nSee the included [AUTHORS.md](https://github.com/lebedov/msgpack-numpy/blob/master/AUTHORS.md) file for \nmore information.\n\nLicense\n-------\nThis software is licensed under the [BSD License](http://www.opensource.org/licenses/bsd-license).\nSee the included [LICENSE.md](https://github.com/lebedov/msgpack-numpy/blob/master/LICENSE.md) file for \nmore information."}, {"name": "msgpack-numpy", "tags": ["dev", "math"], "summary": "Numpy data serialization using msgpack", "text": "This library is used to efficiently serialize and deserialize numerical and array data types provided by numpy using the msgpack format. It supports serialization of Python's native complex data types for added flexibility."}, {"name": "mujoco", "tags": ["math", "web"], "summary": "MuJoCo Physics Simulator", "text": "MuJoCo Python Bindings\n\nThis package is the canonical Python bindings for the\n[MuJoCo physics engine](https://github.com/google-deepmind/mujoco).\nThese bindings are developed and maintained by Google DeepMind, and is kept\nup-to-date with the latest developments in MuJoCo itself.\n\nThe `mujoco` package provides direct access to raw MuJoCo C API functions,\nstructs, constants, and enumerations. Structs are provided as Python classes,\nwith Pythonic initialization and deletion semantics.\n\nIt is not the aim of this package to provide fully fledged\nscene/environment/game authoring API, as there are already a number of existing\npackages that do this well. However, this package does provide a number of\nlower-level components outside of MuJoCo itself that are likely to be useful to\nmost users who access MuJoCo through Python. For example, the `egl`, `glfw`, and\n`osmesa` subpackages contain utilities for setting up OpenGL rendering contexts.\n\nInstallation\n\nThe recommended way to install this package is via [PyPI](https://pypi.org/project/mujoco/):\n\nA copy of the MuJoCo library is provided as part of the package and does **not**\nneed to be downloaded or installed separately.\n\nSource\n\n**IMPORTANT:** Building from source is only necessary if you are modifying the\nPython bindings (or are trying to run on exceptionally old Linux systems).\nIf that's not the case, then we recommend installing the prebuilt binaries from\nPyPI.\n\nIf you need to build the Python bindings from source, please consult\n[the documentation](https://mujoco.readthedocs.io/en/latest/python.html#building-from-source).\n\nUsage\n\nOnce installed, the package can be imported via `import mujoco`. Please consult\nour [documentation](https://mujoco.readthedocs.io/en/stable/python.html) for\nfurther detail on the package's API.\n\nWe recommend going through the tutorial notebook which covers the basics of\nMuJoCo using Python:\n(https://colab.research.google.com/github/google-deepmind/mujoco/blob/main/python/tutorial.ipynb)\n\nVersioning\n\nThe `major.minor.micro` portion of the version number matches the version of\nMuJoCo that the bindings provide. Optionally, if we release updates to the\nPython bindings themselves that target the same version of MuJoCo, a `.postN`\nsuffix is added, for example `2.1.2.post2` represents the second update to the\nbindings for MuJoCo 2.1.2.\n\nLicense and Disclaimer\n\nCopyright 2022 DeepMind Technologies Limited\n\nMuJoCo and its Python bindings are licensed under the Apache License,\nVersion 2.0. You may obtain a copy of the License at\n\nThis is not an officially supported Google product.\n\nThird-party licenses\n\nPre-built [wheels](https://packaging.python.org/en/latest/specifications/\nbinary-distribution-format/#binary-distribution-format) contain binaries that\nincorporate third-party open source software, which are redistributed under the\nfollowing licenses.\n\nAbseil C++ Common Libraries\n\ncereal\n\nCollisions\n\nEigen\n\nWe build with the `EIGEN_MPL2_ONLY` preprocessor macro set.\n\n{fmt}\n\nGLAD\n\nGLFW\n\nlibccd\n\nLLVM runtime libraries (`compiler_rt`, `libc++`, `libc++abi`, `libunwind`)\n\nLodePNG\n\nMarchingCubeCpp\n\nOpenGL Mathematics\n\npybind11\n\nQhull\n\nTinyOBJLoader\n\nTinyXML2\n\nTriangleMeshDistance"}, {"name": "mujoco", "tags": ["math", "web"], "summary": "MuJoCo Physics Simulator", "text": "This library is used to directly access and interface with the MuJoCo physics engine from Python applications, enabling control over its complex simulations and models. This allows developers to leverage MuJoCo's advanced physics capabilities in their own projects."}, {"name": "murmurhash", "tags": ["math"], "summary": "Cython bindings for MurmurHash", "text": "Cython bindings for MurmurHash2\n\n(https://github.com/explosion/murmurhash/actions/workflows/tests.yml)\n(https://pypi.python.org/pypi/murmurhash)\n(https://anaconda.org/conda-forge/murmurhash)\n(https://github.com/explosion/wheelwright/releases)"}, {"name": "murmurhash", "tags": ["math"], "summary": "Cython bindings for MurmurHash", "text": "This library is used to efficiently compute hash values using the MurmurHash2 algorithm in Python applications. It provides a Cython interface for generating non-cryptographic, fast hash functions that can be used for data indexing and caching purposes."}, {"name": "mxnet", "tags": ["math", "ml"], "summary": "Apache MXNet is an ultra-scalable deep learning framework. This version uses openblas and MKLDNN.", "text": "Apache MXNet (Incubating) Python Package\n========================================\n[Apache MXNet](http://beta.mxnet.io) is a deep learning framework designed for both *efficiency* and *flexibility*.\nIt allows you to mix the flavours of deep learning programs together to maximize the efficiency and your productivity.\n\nFor feature requests on the PyPI package, suggestions, and issue reports, create an issue by clicking [here](https://github.com/apache/incubator-mxnet/issues/new).\nPrerequisites\n-------------\nThis package supports Linux, Mac OSX, and Windows platforms. You may also want to check:\n\nTo use this package on Linux you need the `libquadmath.so.0` shared library. On\nDebian based systems, including Ubuntu, run `sudo apt install libquadmath0` to\ninstall the shared library. On RHEL based systems, including CentOS, run `sudo\na GPL library and MXNet part of the Apache Software Foundation, MXNet must not\nredistribute `libquadmath.so.0` as part of the Pypi package and users must\nmanually install it.\n\nTo install for other platforms (e.g. Windows, Raspberry Pi/ARM) or other versions, check [Installing MXNet](https://mxnet.apache.org/versions/master) for instructions on building from source.\n\nInstallation\n------------\nTo install, use:"}, {"name": "mxnet", "tags": ["math", "ml"], "summary": "Apache MXNet is an ultra-scalable deep learning framework. This version uses openblas and MKLDNN.", "text": "This library is used to develop ultra-scalable deep learning models that can be efficiently trained and deployed on various platforms, combining flexibility with high performance. It allows developers to mix different flavors of deep learning programs together to maximize efficiency and productivity."}, {"name": "mygene", "tags": ["math", "web"], "summary": "Python Client for MyGene.Info services.", "text": ".. image:: https://img.shields.io/pypi/dm/mygene.svg\n\n.. image:: https://img.shields.io/pypi/pyversions/mygene.svg\n\n.. image:: https://img.shields.io/pypi/format/mygene.svg\n\n.. image:: https://img.shields.io/pypi/status/mygene.svg\n\nIntro\n=====\n\nMyGene.Info_ provides simple-to-use REST web services to query/retrieve gene annotation data.\nIt's designed with simplicity and performance emphasized. ``mygene``, is an easy-to-use Python\nwrapper to access MyGene.Info_ services.\n\n.. _MyGene.Info: http://mygene.info\n.. _biothings_client: https://pypi.org/project/biothings-client/\n.. _mygene: https://pypi.org/project/mygene/\n\nSince v3.1.0, mygene_ Python package has become a thin wrapper of underlying biothings_client_ package,\na universal Python client for all `BioThings APIs `_, including MyGene.info_.\nThe installation of mygene_ will install biothings_client_ automatically. The following code snippets\nare essentially equivalent:\n\n* Continue using mygene_ package\n\n* Use biothings_client_ package directly\n\nAfter that, the use of ``mg`` instance is exactly the same, e.g. the usage examples below.\n\nRequirements\n============\n\nOptional dependencies\n======================\n\nInstallation\n=============\n\nVersion history\n===============\n\nTutorial\n=========\n\n* `ID mapping using mygene module in Python `_\n\nDocumentation\n=============\n\nUsage\n=====\n\n.. code-block:: python\n\nContact\n========\nDrop us any question or feedback:"}, {"name": "mygene", "tags": ["math", "web"], "summary": "Python Client for MyGene.Info services.", "text": "This library is used to access and retrieve gene annotation data from MyGene.Info services with ease using Python. It provides a simple-to-use interface to query/retrieve gene information through REST web services, making it an ideal tool for bioinformatics applications."}, {"name": "natsort", "tags": ["dev", "math", "ui", "web"], "summary": "Simple yet flexible natural sorting in Python.", "text": "natsort\n=======\n\n.. image:: https://img.shields.io/pypi/v/natsort.svg\n\n.. image:: https://img.shields.io/pypi/pyversions/natsort.svg\n\n.. image:: https://img.shields.io/pypi/l/natsort.svg\n\n.. image:: https://img.shields.io/pypi/dw/natsort.svg\n\nSimple yet flexible natural sorting in Python.\n\n**NOTE**: Please see the `Dropped Deprecated APIs`_ section for changes.\n\nQuick Description\n-----------------\n\nWhen you try to sort a list of strings that contain numbers, the normal python\nsort algorithm sorts lexicographically, so you might not get the results that\nyou expect:\n\n.. code-block:: pycon\n\nNotice that it has the order ('1', '10', '2') - this is because the list is\nbeing sorted in lexicographical order, which sorts numbers like you would\nletters (i.e. 'b', 'ba', 'c').\n\n`natsort`_ provides a function `natsorted()`_ that helps sort lists\n\"naturally\" (\"naturally\" is rather ill-defined, but in general it means\nsorting based on meaning and not computer code point).\nUsing `natsorted()`_ is simple:\n\n.. code-block:: pycon\n\n`natsorted()`_ identifies numbers anywhere in a string and sorts them\nnaturally. Below are some other things you can do with `natsort`_\n(also see the `Examples and Recipes`_ for a quick start guide, or the\n`API`_ for complete details).\n\n**Note**: `natsorted()`_ is designed to be a drop-in replacement for the\nbuilt-in `sorted()`_ function. Like `sorted()`_, `natsorted()`_\n`does not sort in-place`. To sort a list and assign the output to the same\nvariable, you must explicitly assign the output to a variable:\n\n.. code-block:: pycon\n\nPlease see `Generating a Reusable Sorting Key and Sorting In-Place`_ for\nan alternate way to sort in-place naturally.\n\nQuick Examples\n--------------\n\n- `Sorting Versions`_\n- `Sort Paths Like My File Browser (e.g. Windows Explorer on Windows)`_\n- `Sorting by Real Numbers (i.e. Signed Floats)`_\n- `Locale-Aware Sorting (or \"Human Sorting\")`_\n- `Further Customizing Natsort`_\n- `Sorting Mixed Types`_\n- `Handling Bytes`_\n- `Generating a Reusable Sorting Key and Sorting In-Place`_\n- `Other Useful Things`_\n\nSorting Versions\n++++++++++++++++\n\n`natsort`_ does not actually *comprehend* version numbers.\nIt just so happens that the most common versioning schemes are designed to\nwork with standard natural sorting techniques; these schemes include\n``MAJOR.MINOR``, ``MAJOR.MINOR.PATCH``, ``YEAR.MONTH.DAY``. If your data\nconforms to a scheme like this, then it will work out-of-the-box with\n`natsorted()`_ (as of `natsort`_ version >= 4.0.0):\n\n.. code-block:: pycon\n\nIf you need to versions that use a more complicated scheme, please see\n`these version sorting examples`_.\n\nSort Paths Like My File Browser (e.g. Windows Explorer on Windows)\n++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n\nPrior to `natsort`_ version 7.1.0, it was a common request to be able to\nsort paths like Windows Explorer. As of `natsort`_ 7.1.0, the function\n`os_sorted()`_ has been added to provide users the ability to sort\nin the order that their file browser might sort (e.g Windows Explorer on\nWindows, Finder on MacOS, Dolphin/Nautilus/Thunar/etc. on Linux).\n\n.. code-block:: python\n\nOutput will be different depending on the operating system you are on."}, {"name": "natsort", "tags": ["dev", "math", "ui", "web"], "summary": "Simple yet flexible natural sorting in Python.", "text": "For users **not** on Windows (e.g. MacOS/Linux) it is **strongly** recommended\nto also install `PyICU`_, which will help\n`natsort`_ give results that match most file browsers. If this is not installed,\nit will fall back on Python's built-in `locale`_ module and will give good\nresults for most input, but will give poor results for special characters.\n\nSorting by Real Numbers (i.e. Signed Floats)\n++++++++++++++++++++++++++++++++++++++++++++\n\nThis is useful in scientific data analysis (and was the default behavior\nof `natsorted()`_ for `natsort`_ version >> from natsort import realsorted, ns\n\nLocale-Aware Sorting (or \"Human Sorting\")\n+++++++++++++++++++++++++++++++++++++++++\n\nThis is where the non-numeric characters are also ordered based on their\nmeaning, not on their ordinal value, and a locale-dependent thousands\nseparator and decimal separator is accounted for in the number.\nThis can be achieved with the `humansorted()`_ function:\n\n.. code-block:: pycon\n\nYou may find you need to explicitly set the locale to get this to work\n(as shown in the example). Please see `locale issues`_ and the\n`Optional Dependencies`_ section below before using the `humansorted()`_ function.\n\nFurther Customizing Natsort\n+++++++++++++++++++++++++++\n\nIf you need to combine multiple algorithm modifiers (such as ``ns.REAL``,\n``ns.LOCALE``, and ``ns.IGNORECASE``), you can combine the options using the\nbitwise OR operator (``|``). For example,\n\n.. code-block:: pycon\n\nAll of the available customizations can be found in the documentation for\n`the ns enum`_.\n\nYou can also add your own custom transformation functions with the ``key``\nargument. These can be used with ``alg`` if you wish.\n\n.. code-block:: pycon\n\nSorting Mixed Types\n+++++++++++++++++++\n\nYou can mix and match `int`_, `float`_, and `str`_ types when you sort:\n\n.. code-block:: pycon\n\nHandling Bytes\n++++++++++++++\n\n`natsort`_ does not officially support the `bytes`_ type, but\nconvenience functions are provided that help you decode to `str`_ first:\n\n.. code-block:: pycon\n\nGenerating a Reusable Sorting Key and Sorting In-Place\n++++++++++++++++++++++++++++++++++++++++++++++++++++++\n\nUnder the hood, `natsorted()`_ works by generating a custom sorting\nkey using `natsort_keygen()`_ and then passes that to the built-in\n`sorted()`_. You can use the `natsort_keygen()`_ function yourself to\ngenerate a custom sorting key to sort in-place using the `list.sort()`_\nmethod.\n\n.. code-block:: pycon\n\nAll of the algorithm customizations mentioned in the\n`Further Customizing Natsort`_ section can also be applied to\n`natsort_keygen()`_ through the *alg* keyword option.\n\nOther Useful Things\n+++++++++++++++++++\n\n- recursively descend into lists of lists\n - automatic unicode normalization of input data\n - `controlling the case-sensitivity`_\n - `sorting file paths correctly`_\n - `allow custom sorting keys`_\n - `accounting for units`_\n\nFAQ\n---\n\nHow do I debug `natsorted()`_?\n\n`natsort`_ gave me results I didn't expect, and it's a terrible library!\n\nHow *does* `natsort`_ work?\n\nShell script\n------------\n\n`natsort`_ comes with a shell script called `natsort`_, or can also be called\nfrom the command line with ``python -m natsort``.  Check out the\n`shell script wiki documentation`_ for more details.\n\nRequirements\n------------\n\n`natsort`_ requires Python 3.7 or greater.\n\nOptional Dependencies\n---------------------\n\nfastnumbers\n+++++++++++"}, {"name": "natsort", "tags": ["dev", "math", "ui", "web"], "summary": "Simple yet flexible natural sorting in Python.", "text": "The most efficient sorting can occur if you install the\n`fastnumbers`_ package\n(version >=2.0.0); it helps with the string to number conversions.\n`natsort`_ will still run (efficiently) without the package, but if you need\nto squeeze out that extra juice it is recommended you include this as a\ndependency. `natsort`_ will not require (or check) that\n`fastnumbers`_ is installed at installation.\n\nPyICU\n+++++\n\nIt is recommended that you install `PyICU`_ if you wish to sort in a\nlocale-dependent manner, see this page on `locale issues`_ for an explanation why.\n\nInstallation\n------------\n\nUse ``pip``!\n\n.. code-block:: console\n\nIf you want to install the `Optional Dependencies`_, you can use the\n`\"extras\" notation`_ at installation time to install those dependencies as\nwell - use ``fast`` for `fastnumbers`_ and ``icu`` for `PyICU`_.\n\n.. code-block:: console\n\nHow to Run Tests\n----------------\n\nPlease note that `natsort`_ is NOT set-up to support ``python setup.py test``.\n\nThe recommended way to run tests is with `tox`_. After installing ``tox``,\nrunning tests is as simple as executing the following in the `natsort`_ directory:\n\n.. code-block:: console\n\n``tox`` will create virtual a virtual environment for your tests and install\nall the needed testing requirements for you.  You can specify a particular\npython version with the ``-e`` flag, e.g. ``tox -e py36``. Static analysis\nis done with ``tox -e flake8``. You can see all available testing environments\nwith ``tox --listenvs``.\n\nHow to Build Documentation\n--------------------------\n\nIf you want to build the documentation for `natsort`_, it is recommended to\nuse ``tox``:\n\n.. code-block:: console\n\nThis will place the documentation in ``build/sphinx/html``.\n\nDropped Deprecated APIs\n-----------------------\n\nIn `natsort`_ version 6.0.0, the following APIs and functions were removed\n\n- ``number_type`` keyword argument (deprecated since 3.4.0)\n - ``signed`` keyword argument (deprecated since 3.4.0)\n - ``exp`` keyword argument (deprecated since 3.4.0)\n - ``as_path`` keyword argument (deprecated since 3.4.0)\n - ``py3_safe`` keyword argument (deprecated since 3.4.0)\n - ``ns.TYPESAFE`` (deprecated since version 5.0.0)\n - ``ns.DIGIT`` (deprecated since version 5.0.0)\n - ``ns.VERSION`` (deprecated since version 5.0.0)\n - ``versorted()`` (discouraged since version 4.0.0,\n   officially deprecated since version 5.5.0)\n - ``index_versorted()`` (discouraged since version 4.0.0,\n   officially deprecated since version 5.5.0)\n\nIn general, if you want to determine if you are using deprecated APIs you\ncan run your code with the following flag\n\n.. code-block:: console\n\nBy default `DeprecationWarnings`_ are not shown, but this will cause them\nto be shown. Alternatively, you can just set the environment variable\n``PYTHONWARNINGS`` to \"default::DeprecationWarning\" and then run your code.\n\nAuthor\n------\n\nSeth M. Morton\n\nHistory\n-------\n\nPlease visit the changelog `on GitHub`_."}, {"name": "natsort", "tags": ["dev", "math", "ui", "web"], "summary": "Simple yet flexible natural sorting in Python.", "text": ".. _natsort: https://natsort.readthedocs.io/en/stable/index.html\n.. _natsorted(): https://natsort.readthedocs.io/en/stable/api.html#natsort.natsorted\n.. _natsort_keygen(): https://natsort.readthedocs.io/en/stable/api.html#natsort.natsort_keygen\n.. _realsorted(): https://natsort.readthedocs.io/en/stable/api.html#natsort.realsorted\n.. _humansorted(): https://natsort.readthedocs.io/en/stable/api.html#natsort.humansorted\n.. _os_sorted(): https://natsort.readthedocs.io/en/stable/api.html#natsort.os_sorted\n.. _the ns enum: https://natsort.readthedocs.io/en/stable/api.html#natsort.ns\n.. _fastnumbers: https://github.com/SethMMorton/fastnumbers\n.. _sorted(): https://docs.python.org/3/library/functions.html#sorted\n.. _list.sort(): https://docs.python.org/3/library/stdtypes.html#list.sort\n.. _key function: https://docs.python.org/3/howto/sorting.html#key-functions\n.. _locale: https://docs.python.org/3/library/locale.html\n.. _int: https://docs.python.org/3/library/functions.html#int\n.. _float: https://docs.python.org/3/library/functions.html#float\n.. _str: https://docs.python.org/3/library/stdtypes.html#str\n.. _bytes: https://docs.python.org/3/library/stdtypes.html#bytes\n.. _list: https://docs.python.org/3/library/stdtypes.html#list\n.. _tuple: https://docs.python.org/3/library/stdtypes.html#tuple\n.. _TypeError: https://docs.python.org/3/library/exceptions.html#TypeError\n.. _DeprecationWarnings: https://docs.python.org/3/library/exceptions.html#DeprecationWarning\n.. _\"extras\" notation: https://packaging.python.org/tutorials/installing-packages/#installing-setuptools-extras\n.. _PyICU: https://pypi.org/project/PyICU\n.. _tox: https://tox.readthedocs.io/en/latest/\n.. _Examples and Recipes: https://github.com/SethMMorton/natsort/wiki/Examples-and-Recipes\n.. _How Does Natsort Work?: https://github.com/SethMMorton/natsort/wiki/How-Does-Natsort-Work%3F\n.. _API: https://natsort.readthedocs.io/en/stable/api.html\n.. _on GitHub: https://github.com/SethMMorton/natsort/blob/main/CHANGELOG.md\n.. _file an issue: https://github.com/SethMMorton/natsort/issues/new\n.. _look at this issue describing how to debug: https://github.com/SethMMorton/natsort/issues/13#issuecomment-50422375\n.. _controlling the case-sensitivity: https://github.com/SethMMorton/natsort/wiki/Examples-and-Recipes#controlling-case-when-sorting\n.. _sorting file paths correctly: https://github.com/SethMMorton/natsort/wiki/Examples-and-Recipes#sort-os-generated-paths\n.. _allow custom sorting keys: https://github.com/SethMMorton/natsort/wiki/Examples-and-Recipes#using-a-custom-sorting-key\n.. _accounting for units: https://github.com/SethMMorton/natsort/wiki/Examples-and-Recipes#accounting-for-units-when-sorting\n.. _these version sorting examples: https://github.com/SethMMorton/natsort/wiki/Examples-and-Recipes#sorting-more-expressive-versioning-schemes\n.. _locale issues: https://github.com/SethMMorton/natsort/wiki/Possible-Issues-with-natsort.humansorted-or-ns.LOCALE\n.. _shell script wiki documentation: https://github.com/SethMMorton/natsort/wiki/Shell-Script"}, {"name": "natsort", "tags": ["dev", "math", "ui", "web"], "summary": "Simple yet flexible natural sorting in Python.", "text": "This library is used to sort lists of strings containing numbers in a natural way, ensuring that numbers are sorted correctly regardless of their formatting. This allows developers to easily achieve the desired sorting order when working with data that contains numerical values in various formats."}, {"name": "nemo-toolkit", "tags": ["data", "math", "ml", "ui"], "summary": "NeMo - a toolkit for Conversational AI", "text": "**NVIDIA NeMo Framework**\n\nLatest News\n\nPivot notice: This repo will pivot to focus on speech models only\n\nPretrain and finetune :hugs:Hugging Face models via AutoModel\n\n- AutoModelForCausalLM in the Text Generation category\n- AutoModelForImageTextToText in the Image-Text-to-Text category\n\nMore Details in Blog: Run Hugging Face Models Instantly with Day-0 Support from NVIDIA NeMo Framework. Future releases will enable support for more model families such as Video Generation models.(2025-05-19)\n\nTraining on Blackwell using Nemo\n\nTraining Performance on GPU Tuning Guide\n\nNew Models Support\n\nNeMo Framework 2.0\n\nNew Cosmos World Foundation Models Support\n\nLarge Language Models and Multimodal Models\n\nSpeech Recognition\n\nIntroduction\n\nNVIDIA NeMo Framework is a scalable and cloud-native generative AI\nframework built for researchers and PyTorch developers working on Large\nLanguage Models (LLMs), Multimodal Models (MMs), Automatic Speech\nRecognition (ASR), Text to Speech (TTS), and Computer Vision (CV)\ndomains. It is designed to help you efficiently create, customize, and\ndeploy new generative AI models by leveraging existing code and\npre-trained model checkpoints.\n\nFor technical documentation, please see the [NeMo Framework User\nGuide](https://docs.nvidia.com/nemo-framework/user-guide/latest/playbooks/index.html).\n\nWhat's New in NeMo 2.0\n\nNVIDIA NeMo 2.0 introduces several significant improvements over its predecessor, NeMo 1.0, enhancing flexibility, performance, and scalability.\n\n- **Python-Based Configuration** - NeMo 2.0 transitions from YAML files to a Python-based configuration, providing more flexibility and control. This shift makes it easier to extend and customize configurations programmatically.\n\n- **Modular Abstractions** - By adopting PyTorch Lightning\u2019s modular abstractions, NeMo 2.0 simplifies adaptation and experimentation. This modular approach allows developers to more easily modify and experiment with different components of their models.\n\n- **Scalability** - NeMo 2.0 seamlessly scaling large-scale experiments across thousands of GPUs using [NeMo-Run](https://github.com/NVIDIA/NeMo-Run), a powerful tool designed to streamline the configuration, execution, and management of machine learning experiments across computing environments.\n\nOverall, these enhancements make NeMo 2.0 a powerful, scalable, and user-friendly framework for AI model development.\n\n> [!IMPORTANT]  \n> NeMo 2.0 is currently supported by the LLM (large language model) and VLM (vision language model) collections.\n\nGet Started with NeMo 2.0\n\nGet Started with Cosmos\n\nNeMo Curator and NeMo Framework support video curation and post-training of the Cosmos World Foundation Models, which are open and available on [NGC](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/cosmos/collections/cosmos) and [Hugging Face](https://huggingface.co/collections/nvidia/cosmos-6751e884dc10e013a0a0d8e6). For more information on video datasets, refer to [NeMo Curator](https://developer.nvidia.com/nemo-curator). To post-train World Foundation Models using the NeMo Framework for your custom physical AI tasks, see the [Cosmos Diffusion models](https://github.com/NVIDIA/Cosmos/blob/main/cosmos1/models/diffusion/nemo/post_training/README.md) and the [Cosmos Autoregressive models](https://github.com/NVIDIA/Cosmos/blob/main/cosmos1/models/autoregressive/nemo/post_training/README.md).\n\nLLMs and MMs Training, Alignment, and Customization\n\nAll NeMo models are trained with\n[Lightning](https://github.com/Lightning-AI/lightning). Training is\nautomatically scalable to 1000s of GPUs. You can check the performance benchmarks using the\nlatest NeMo Framework container [here](https://docs.nvidia.com/nemo-framework/user-guide/latest/performance/performance_summary.html).\n\nWhen applicable, NeMo models leverage cutting-edge distributed training\ntechniques, incorporating [parallelism\nstrategies](https://docs.nvidia.com/nemo-framework/user-guide/latest/modeloverview.html)\nto enable efficient training of very large models. These techniques\ninclude Tensor Parallelism (TP), Pipeline Parallelism (PP), Fully\nSharded Data Parallelism (FSDP), Mixture-of-Experts (MoE), and Mixed\nPrecision Training with BFloat16 and FP8, as well as others."}, {"name": "nemo-toolkit", "tags": ["data", "math", "ml", "ui"], "summary": "NeMo - a toolkit for Conversational AI", "text": "NeMo Transformer-based LLMs and MMs utilize [NVIDIA Transformer\nEngine](https://github.com/NVIDIA/TransformerEngine) for FP8 training on\nNVIDIA Hopper GPUs, while leveraging [NVIDIA Megatron\nCore](https://github.com/NVIDIA/Megatron-LM/tree/main/megatron/core) for\nscaling Transformer model training.\n\nNeMo LLMs can be aligned with state-of-the-art methods such as SteerLM,\nDirect Preference Optimization (DPO), and Reinforcement Learning from\nHuman Feedback (RLHF). See [NVIDIA NeMo\nAligner](https://github.com/NVIDIA/NeMo-Aligner) for more information.\n\nIn addition to supervised fine-tuning (SFT), NeMo also supports the\nlatest parameter efficient fine-tuning (PEFT) techniques such as LoRA,\nP-Tuning, Adapters, and IA3. Refer to the [NeMo Framework User\nGuide](https://docs.nvidia.com/nemo-framework/user-guide/latest/sft_peft/index.html)\nfor the full list of supported models and techniques.\n\nLLMs and MMs Deployment and Optimization\n\nNeMo LLMs and MMs can be deployed and optimized with [NVIDIA NeMo\nMicroservices](https://developer.nvidia.com/nemo-microservices-early-access).\n\nSpeech AI\n\nNeMo ASR and TTS models can be optimized for inference and deployed for\nproduction use cases with [NVIDIA Riva](https://developer.nvidia.com/riva).\n\nNeMo Framework Launcher\n\n> [!IMPORTANT]  \n> NeMo Framework Launcher is compatible with NeMo version 1.0 only. [NeMo-Run](https://github.com/NVIDIA/NeMo-Run) is recommended for launching experiments using NeMo 2.0.\n\n[NeMo Framework\nLauncher](https://github.com/NVIDIA/NeMo-Megatron-Launcher) is a\ncloud-native tool that streamlines the NeMo Framework experience. It is\nused for launching end-to-end NeMo Framework training jobs on CSPs and\nSlurm clusters.\n\nThe NeMo Framework Launcher includes extensive recipes, scripts,\nutilities, and documentation for training NeMo LLMs. It also includes\nthe NeMo Framework [Autoconfigurator](https://github.com/NVIDIA/NeMo-Megatron-Launcher#53-using-autoconfigurator-to-find-the-optimal-configuration),\nwhich is designed to find the optimal model parallel configuration for\ntraining on a specific cluster.\n\nTo get started quickly with the NeMo Framework Launcher, please see the\n[NeMo Framework\nPlaybooks](https://docs.nvidia.com/nemo-framework/user-guide/latest/playbooks/index.html).\nThe NeMo Framework Launcher does not currently support ASR and TTS\ntraining, but it will soon.\n\nGet Started with NeMo Framework\n\nGetting started with NeMo Framework is easy. State-of-the-art pretrained\nNeMo models are freely available on [Hugging Face\nHub](https://huggingface.co/models?library=nemo&sort=downloads&search=nvidia)\nand [NVIDIA\nNGC](https://catalog.ngc.nvidia.com/models?query=nemo&orderBy=weightPopularDESC).\nThese models can be used to generate text or images, transcribe audio,\nand synthesize speech in just a few lines of code.\n\nWe have extensive\n[tutorials](https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/stable/starthere/tutorials.html)\nthat can be run on [Google Colab](https://colab.research.google.com) or\nwith our [NGC NeMo Framework\nContainer](https://catalog.ngc.nvidia.com/orgs/nvidia/containers/nemo).\nWe also have\n[playbooks](https://docs.nvidia.com/nemo-framework/user-guide/latest/playbooks/index.html)\nfor users who want to train NeMo models with the NeMo Framework\nLauncher.\n\nFor advanced users who want to train NeMo models from scratch or\nfine-tune existing NeMo models, we have a full suite of [example\nscripts](https://github.com/NVIDIA/NeMo/tree/main/examples) that support\nmulti-GPU/multi-node training.\n\nKey Features\n\n- [Large Language Models](nemo/collections/nlp/README.md)\n- [Multimodal](nemo/collections/multimodal/README.md)\n- [Automatic Speech Recognition](nemo/collections/asr/README.md)\n- [Text to Speech](nemo/collections/tts/README.md)\n- [Computer Vision](nemo/collections/vision/README.md)\n\nRequirements\n\n- Python 3.10 or above\n- Pytorch 2.5 or above\n- NVIDIA GPU (if you intend to do model training)\n\nDeveloper Documentation\n\nVersion\n-------\nLatest\nStable\n\nInstall NeMo Framework\n\nThe NeMo Framework can be installed in a variety of ways, depending on\nyour needs. Depending on the domain, you may find one of the following\ninstallation methods more suitable."}, {"name": "nemo-toolkit", "tags": ["data", "math", "ml", "ui"], "summary": "NeMo - a toolkit for Conversational AI", "text": "- [Conda / Pip](#conda--pip): Install NeMo-Framework with native Pip into a virtual environment.\n  - Used to explore NeMo on any supported platform.\n  - This is the recommended method for ASR and TTS domains.\n  - Limited feature-completeness for other domains.\n- [NGC PyTorch container](#ngc-pytorch-container): Install NeMo-Framework from source with feature-completeness into a highly optimized container.\n  - For users that want to install from source in a highly optimized container.\n- [NGC NeMo container](#ngc-nemo-container): Ready-to-go solution of NeMo-Framework\n  - For users that seek highest performance.\n  - Contains all dependencies installed and tested for performance and convergence.\n\nSupport matrix\n\nNeMo-Framework provides tiers of support based on OS / Platform and mode of installation. Please refer the following overview of support levels:\n\n- Fully supported: Max performance and feature-completeness.\n- Limited supported: Used to explore NeMo.\n- No support yet: In development.\n- Deprecated: Support has reached end of life.\n\nPlease refer to the following table for current support levels:\n\nOS / Platform\n----------------------------\n`linux` - `amd64/x84_64`\n`linux` - `arm64`\n`darwin` - `amd64/x64_64`\n`darwin` - `arm64`\n`windows` - `amd64/x64_64`\n`windows` - `arm64`\n\nConda / Pip\n\nInstall NeMo in a fresh Conda environment:\n\nPick the right version\n\nNeMo-Framework publishes pre-built wheels with each release.\nTo install nemo_toolkit from such a wheel, use the following installation method:\n\nIf a more specific version is desired, we recommend a Pip-VCS install. From [NVIDIA/NeMo](github.com/NVIDIA/NeMo), fetch the commit, branch, or tag that you would like to install.  \nTo install nemo_toolkit from this Git reference `$REF`, use the following installation method:\n\nInstall a specific Domain\n\nTo install a specific domain of NeMo, you must first install the\nnemo_toolkit using the instructions listed above. Then, you run the\nfollowing domain-specific commands:\n\nNGC PyTorch container\n\n**NOTE: The following steps are supported beginning with 24.04 (NeMo-Toolkit 2.3.0)**\n\nWe recommended that you start with a base NVIDIA PyTorch container:\nnvcr.io/nvidia/pytorch:25.01-py3.\n\nIf starting with a base NVIDIA PyTorch container, you must first launch\nthe container:\n\nFrom [NVIDIA/NeMo](github.com/NVIDIA/NeMo), fetch the commit/branch/tag that you want to install.  \nTo install nemo_toolkit including all of its dependencies from this Git reference `$REF`, use the following installation method:\n\nNGC NeMo container\n\nNeMo containers are launched concurrently with NeMo version updates.\nNeMo Framework now supports LLMs, MMs, ASR, and TTS in a single\nconsolidated Docker container. You can find additional information about\nreleased containers on the [NeMo releases\npage](https://github.com/NVIDIA/NeMo/releases).\n\nTo use a pre-built container, run the following code:\n\nFuture Work\n\nThe NeMo Framework Launcher does not currently support ASR and TTS\ntraining, but it will soon.\n\nDiscussions Board\n\nFAQ can be found on the NeMo [Discussions\nboard](https://github.com/NVIDIA/NeMo/discussions). You are welcome to\nask questions or start discussions on the board.\n\nContribute to NeMo\n\nWe welcome community contributions! Please refer to\n[CONTRIBUTING.md](https://github.com/NVIDIA/NeMo/blob/stable/CONTRIBUTING.md)\nfor the process.\n\nPublications\n\nWe provide an ever-growing list of\n[publications](https://nvidia.github.io/NeMo/publications/) that utilize\nthe NeMo Framework.\n\nTo contribute an article to the collection, please submit a pull request\nto the `gh-pages-src` branch of this repository. For detailed\ninformation, please consult the README located at the [gh-pages-src\nbranch](https://github.com/NVIDIA/NeMo/tree/gh-pages-src#readme).\n\nBlogs"}, {"name": "nemo-toolkit", "tags": ["data", "math", "ml", "ui"], "summary": "NeMo - a toolkit for Conversational AI", "text": "Large Language Models and Multimodal Models\n\nLicenses\n\nNeMo is licensed under the [Apache License 2.0](https://github.com/NVIDIA/NeMo?tab=Apache-2.0-1-ov-file)."}, {"name": "nemo-toolkit", "tags": ["data", "math", "ml", "ui"], "summary": "NeMo - a toolkit for Conversational AI", "text": "This library is used to pretrain and fine-tune Hugging Face models for various conversational AI tasks, including text generation and image-text-to-text conversion. With NVIDIA NeMo Framework, developers can run these models instantly with day-0 support, enabling faster development of conversational AI applications."}, {"name": "neptune-api", "tags": ["math", "ml", "web"], "summary": "A client library for accessing Neptune API", "text": "neptune-api\nA client library for accessing Neptune API\n\nUsage\nFirst, create a client:\n\nIf the endpoints you're going to hit require authentication, use `AuthenticatedClient` instead:\n\nNow call your endpoint and use your models:\n\nOr do the same thing with an async version:\n\nBy default, when you're calling an HTTPS API it will attempt to verify that SSL is working correctly. Using certificate verification is highly recommended most of the time, but sometimes you may need to authenticate to a server (especially an internal server) using a custom certificate bundle.\n\nYou can also disable certificate validation altogether, but beware that **this is a security risk**.\n\nThere are more settings on the generated `Client` class which let you control more runtime behavior, check out the docstring on that class for more info.\n\nThings to know:\n1. Every path/method combo becomes a Python module with four functions:\n\n1. All path/query params, and bodies become method arguments.\n1. If your endpoint had any tags on it, the first tag will be used as a module name for the function (my_tag above)\n1. Any endpoint which did not have a tag will be in `neptune_api.api.default`\n\nUpdate OpenAPI spec\n\nRun the following command to regenerate the OpenAPI client:"}, {"name": "neptune-api", "tags": ["math", "ml", "web"], "summary": "A client library for accessing Neptune API", "text": "This library is used to create clients for accessing Neptune API endpoints, providing authenticated or unauthenticated access to data. Developers can use it to build applications that interact with Neptune API securely and efficiently, leveraging its async and synchronous calling capabilities."}, {"name": "networkx", "tags": ["math"], "summary": "Python package for creating and manipulating graphs and networks", "text": "NetworkX\n========\n\n.. image::\n\n.. image::\n\n.. image::\n\n.. image::\n\n.. image::\n\n.. image::\n\nNetworkX is a Python package for the creation, manipulation,\nand study of the structure, dynamics, and functions\nof complex networks.\n\nSimple example\n--------------\n\nFind the shortest path between two nodes in an undirected graph:\n\n.. code:: pycon\n\nInstall\n-------\n\nInstall the latest released version of NetworkX:\n\n.. code:: shell\n\nInstall with all optional dependencies:\n\n.. code:: shell\n\nFor additional details,\nplease see the `installation guide `_.\n\nBugs\n----\n\nPlease report any bugs that you find `here `_.\nOr, even better, fork the repository on `GitHub `_\nand create a pull request (PR). We welcome all changes, big or small, and we\nwill help you make the PR if you are new to `git` (just ask on the issue and/or\nsee the `contributor guide `_).\n\nLicense\n-------\n\nReleased under the `3-clause BSD license `_::"}, {"name": "networkx", "tags": ["math"], "summary": "Python package for creating and manipulating graphs and networks", "text": "This library is used to create, manipulate, and study complex networks with features such as finding shortest paths between nodes. This allows developers to efficiently analyze and visualize network structures and behaviors in various applications, including graph theory research, data analysis, and network optimization."}, {"name": "nibabel", "tags": ["dev", "math", "web"], "summary": "Access a multitude of neuroimaging data formats", "text": ".. -*- rest -*-\n.. vim:syntax=rst\n\n.. Use raw location to ensure image shows up on PyPI\n.. image:: https://raw.githubusercontent.com/nipy/nibabel/master/doc/pics/logo.png\n   :target: https://nipy.org/nibabel\n   :alt: NiBabel logo\n\n.. list-table::\n   :widths: 20 80\n   :header-rows: 0\n\n   * - Code\n\n   * - Tests\n\n   * - PyPI\n\n   * - Packages\n\n   * - License & DOI\n\n.. Following contents should be copied from LONG_DESCRIPTION in nibabel/info.py\n\nRead and write access to common neuroimaging file formats, including:\nANALYZE_ (plain, SPM99, SPM2 and later), GIFTI_, NIfTI1_, NIfTI2_, `CIFTI-2`_,\nMINC1_, MINC2_, `AFNI BRIK/HEAD`_, ECAT_ and Philips PAR/REC.\nIn addition, NiBabel also supports FreeSurfer_'s MGH_, geometry, annotation and\nmorphometry files, and provides some limited support for DICOM_.\n\nNiBabel's API gives full or selective access to header information (metadata),\nand image data is made available via NumPy arrays. For more information, see\nNiBabel's `documentation site`_ and `API reference`_.\n\n.. _API reference: https://nipy.org/nibabel/api.html\n.. _AFNI BRIK/HEAD: https://afni.nimh.nih.gov/pub/dist/src/README.attributes\n.. _ANALYZE: http://www.grahamwideman.com/gw/brain/analyze/formatdoc.htm\n.. _CIFTI-2: https://www.nitrc.org/projects/cifti/\n.. _DICOM: http://medical.nema.org/\n.. _documentation site: http://nipy.org/nibabel\n.. _ECAT: http://xmedcon.sourceforge.net/Docs/Ecat\n.. _Freesurfer: https://surfer.nmr.mgh.harvard.edu\n.. _GIFTI: https://www.nitrc.org/projects/gifti\n.. _MGH: https://surfer.nmr.mgh.harvard.edu/fswiki/FsTutorial/MghFormat\n.. _MINC1:\n.. _MINC2:\n.. _NIfTI1: http://nifti.nimh.nih.gov/nifti-1/\n.. _NIfTI2: http://nifti.nimh.nih.gov/nifti-2/\n\nInstallation\n============\n\nTo install NiBabel's `current release`_ with ``pip``, run::\n\nTo install the latest development version, run::\n\nWhen working on NiBabel itself, it may be useful to install in \"editable\" mode::\n\n   git clone https://github.com/nipy/nibabel.git\n\nFor more information on previous releases, see the `release archive`_ or\n`development changelog`_.\n\n.. _current release: https://pypi.python.org/pypi/NiBabel\n.. _release archive: https://github.com/nipy/NiBabel/releases\n.. _development changelog: https://nipy.org/nibabel/changelog.html\n\nTesting\n=======\n\nDuring development, we recommend using tox_ to run nibabel tests::\n\nTo test an installed version of nibabel, install the test dependencies\nand run pytest_::\n\nFor more information, consult the `developer guidelines`_.\n\n.. _tox: https://tox.wiki\n.. _pytest: https://docs.pytest.org\n.. _developer guidelines: https://nipy.org/nibabel/devel/devguide.html\n\nMailing List\n============\n\nPlease send any questions or suggestions to the `neuroimaging mailing list\n`_.\n\nLicense\n=======\n\nNiBabel is licensed under the terms of the `MIT license\n`__.\nSome code included with NiBabel is licensed under the `BSD license`_.\nFor more information, please see the COPYING_ file.\n\n.. _BSD license: https://opensource.org/licenses/BSD-3-Clause\n.. _COPYING: https://github.com/nipy/nibabel/blob/master/COPYING\n\nCitation\n========\n\n.. _Digital Object Identifier: https://en.wikipedia.org/wiki/Digital_object_identifier\n.. _zenodo: https://zenodo.org"}, {"name": "nibabel", "tags": ["dev", "math", "web"], "summary": "Access a multitude of neuroimaging data formats", "text": "This library is used to read and write a variety of neuroimaging data formats, including NIfTI, ANALYZE, GIFTI, MINC, and more. With this library, developers can access and manipulate numerous types of neuroimaging files with ease."}, {"name": "numdifftools", "tags": ["math", "web"], "summary": "Solves automatic numerical differentiation problems in one or more variables.", "text": "============\nnumdifftools\n============\n\nThe numdifftools library is a suite of tools written in `_Python `_\nto solve automatic numerical differentiation problems in one or more variables.\nFinite differences are used in an adaptive manner, coupled with a Richardson\nextrapolation methodology to provide a maximally accurate result.\nThe user can configure many options like; changing the order of the method or\nthe extrapolation, even allowing the user to specify whether complex-step,\ncentral, forward or backward differences are used.\n\nThe methods provided are:\n\n- **Derivative**: Compute the derivatives of order 1 through 10 on any scalar function.\n\n- **directionaldiff**: Compute directional derivative of a function of n variables\n\n- **Gradient**: Compute the gradient vector of a scalar function of one or more variables.\n\n- **Jacobian**: Compute the Jacobian matrix of a vector valued function of one or more variables.\n\n- **Hessian**: Compute the Hessian matrix of all 2nd partial derivatives of a scalar function of one or more variables.\n\n- **Hessdiag**: Compute only the diagonal elements of the Hessian matrix\n\nAll of these methods also produce error estimates on the result.\n\nNumdifftools also provide an easy to use interface to derivatives calculated\nwith in `_AlgoPy `_. Algopy stands for Algorithmic\nDifferentiation in Python.\nThe purpose of AlgoPy is the evaluation of higher-order derivatives in the\n`forward` and `reverse` mode of Algorithmic Differentiation (AD) of functions\nthat are implemented as Python programs.\n\nGetting Started\n===============\n\nVisualize high order derivatives of the tanh function\n\n.. image:: https://raw.githubusercontent.com/pbrod/numdifftools/master/examples/fun.png\n\nCompute 1'st and 2'nd derivative of exp(x), at x == 1::\n\nNonlinear least squares::\n\nCompute gradient of sum(x**2)::\n\nCompute the same with the easy to use interface to AlgoPy::\n\nNonlinear least squares::\n\nCompute gradient of sum(x**2)::\n\nSee also\n--------\nscipy.misc.derivative\n\nDocumentation and code\n======================\n\nNumdifftools works on Python 2.7+ and Python 3.0+.\n\nOfficial releases available at: http://pypi.python.org/pypi/numdifftools |pkg_img|\n\nOfficial documentation available at: http://numdifftools.readthedocs.io/en/latest/ |docs_img|\n\nBleeding edge: https://github.com/pbrod/numdifftools.\n\nInstallation\n============\n\nIf you have pip installed, then simply type:\n\nto get the lastest stable version. Using pip also has the advantage that all\nrequirements are automatically installed.\n\nUnit tests\n==========\nTo test if the toolbox is working paste the following in an interactive\npython session::\n\n   import numdifftools as nd\n   nd.test('--doctest-modules', '--disable-warnings')\n\nAcknowledgement\n===============\nThe `numdifftools package `_ for\n`Python `_ was written by Per A. Brodtkorb\nbased on the adaptive numerical differentiation toolbox written in\n`Matlab `_  by John D'Errico [DErrico06]_.\n\nLater the package was extended with some of the functionality\nfound in the statsmodels.tools.numdiff module written by Josef Perktold\n[JPerktold14]_ which is based on [Rid09]_.\nThe implementation of bicomplex numbers is based on the matlab implementation\ndescribed in the project report of [Ver14]_ which is based on [GLD12].\nFor completeness the [For98]_  method for computing the weights and points in general\nfinite difference formulas as well as the [For81]_ method for cumputing the\ntaylor coefficients of complex analytic function using FFT, was added.\n\nReferences\n===========\n\n.. [JPerktold14] Perktold, J (2014), numdiff package\n\n.. [Ver14] Adriaen Verheyleweghen, (2014)\n\n.. [GLD12] Gregory Lantoine, R.P. Russell, and T. Dargent (2012)\n\n.. [MELEV12] M.E. Luna-Elizarraras, M. Shapiro, D.C. Struppa1, A. Vajiac (2012),\n\n.. [Lan10] Gregory Lantoine (2010),\n\n.. [Rid09] Ridout, M.S. (2009)\n\n.. [DErrico06] D'Errico, J. R.  (2006),\n\n.. [KLLK05] K.-L. Lai, J.L. Crassidis, Y. Cheng, J. Kim (2005),\n\n.. [For98] B. Fornberg (1998)\n\n.. [For81] Fornberg, B. (1981).\n\n.. [JML69] Lyness, J. M., Moler, C. B. (1969).\n\n.. [JML66] Lyness, J. M., Moler, C. B. (1966).\n\n.. [NAG] *NAG Library*. NAG Fortran Library Document: D04AAF"}, {"name": "numdifftools", "tags": ["math", "web"], "summary": "Solves automatic numerical differentiation problems in one or more variables.", "text": "This library is used to compute derivatives, directional derivatives, and gradients of scalar functions in one or more variables with high accuracy using adaptive finite differences. Developers can leverage numdifftools for automatic numerical differentiation tasks, allowing them to focus on higher-level analysis without manually implementing differentiation methods."}, {"name": "numpy-quaternion", "tags": ["math", "web"], "summary": "Add a quaternion dtype to NumPy", "text": "Quaternions in numpy\n\nThis Python module adds a quaternion dtype to NumPy.\n\nThe code was originally based on [code by Martin\nLing](https://github.com/martinling/numpy_quaternion) (which he wrote\nwith help from Mark Wiebe), but was rewritten with ideas from\n[rational](https://github.com/numpy/numpy-dtypes/tree/master/npytypes/rational)\nto work with newer python versions (and to fix a few bugs), and\n*greatly* expands the applications of quaternions.\n\nSee also the pure-python package\n[quaternionic](https://github.com/moble/quaternionic).\n\nQuickstart\n\nor\n\nOptionally add `--user` after `install` in the second command if\nyou're not using a python environment \u2014 though you should start.\n\nInstallation\n\nAssuming you use `conda` to manage your python installation (which is\ncurrently the preferred choice for science and engineering with\npython), you can install this package simply as\n\nIf you prefer to use `pip`, you can instead do\n\n(See [here](https://snarky.ca/why-you-should-use-python-m-pip/) for a\nveteran python core contributor's explanation of why you should always\nuse `python -m pip` instead of just `pip` or `pip3`.)  The `--upgrade\n--force-reinstall` options are not always necessary, but will ensure\nthat pip will update numpy if it has to.\n\nIf you refuse to use `conda`, you might want to install inside your\nhome directory without root privileges.  (Conda does this by default\nanyway.)  This is done by adding `--user` to the above command:\n\nNote that pip will attempt to compile the code \u2014 which requires a\nworking `C` compiler.\n\nFinally, there's also the fully manual option of just downloading the\ncode, changing to the code directory, and running\n\nThis should work regardless of the installation method, as long as you\nhave a compiler hanging around.\n\nBasic usage\n\nThe full documentation can be found on [Read the\nDocs](https://quaternion.readthedocs.io/), and most functions have\ndocstrings that should explain the relevant points.  The following are\nmostly for the purposes of example.\n\nNote that this package represents a quaternion as a scalar, followed\nby the `x` component of the vector part, followed by `y`, followed by\n`z`.  These components can be accessed directly:\n\nHowever, this only works on an individual `quaternion`; for arrays it\nis better to use \"vectorized\" operations like `as_float_array`.\n\nThe following ufuncs are implemented (which means they run fast on\nnumpy arrays):\n\nQuaternion components are stored as double-precision floating point\nnumbers \u2014 `float`s, in python language, or `float64` in more precise\nnumpy language.  Numpy arrays with `dtype=quaternion` can be accessed\nas arrays of doubles without any (slow, memory-consuming) copying of\ndata; rather, a `view` of the exact same memory space can be created\nwithin a microsecond, regardless of the shape or size of the\nquaternion array.\n\nComparison operations follow the same lexicographic ordering as\ntuples.\n\nThe unary tests isnan and isinf return true if they would return true\nfor any individual component; isfinite returns true if it would return\ntrue for all components.\n\nReal types may be cast to quaternions, giving quaternions with zero\nfor all three imaginary components. Complex types may also be cast to\nquaternions, with their single imaginary component becoming the first\nimaginary component of the quaternion. Quaternions may not be cast to\nreal or complex types."}, {"name": "numpy-quaternion", "tags": ["math", "web"], "summary": "Add a quaternion dtype to NumPy", "text": "Several array-conversion functions are also included.  For example, to\nconvert an Nx4 array of floats to an N-dimensional array of\nquaternions, use `as_quat_array`:\n\n[Note that quaternions are printed with full precision, unlike floats,\nwhich is why you see extra digits above.  But the actual data is\nidentical in the two cases.]  To convert an N-dimensional array of\nquaternions to an Nx4 array of floats, use `as_float_array`:\n\nIt is also possible to convert a quaternion to or from a 3x3 array of\nfloats representing a rotation matrix, or an array of N quaternions to\nor from an Nx3x3 array of floats representing N rotation matrices,\nusing `as_rotation_matrix` and `from_rotation_matrix`.  Similar\nconversions are possible for rotation vectors using\n`as_rotation_vector` and `from_rotation_vector`, and for spherical\ncoordinates using `as_spherical_coords` and `from_spherical_coords`.\nFinally, it is possible to derive the Euler angles from a quaternion\nusing `as_euler_angles`, or create a quaternion from Euler angles\nusing `from_euler_angles` \u2014\u00a0though be aware that Euler angles are\nbasically the worst things\never.[1](#1-euler-angles-are-awful) Before you complain\nabout those functions using something other than your favorite\nconventions, please read [this\npage](https://github.com/moble/quaternion/wiki/Euler-angles-are-horrible).\n\nDependencies\n\nWith the standard installation methods, hopefully you won't need to\nworry about dependencies directly.  But in case you do, here's what\nyou need to know.\n\nThe basic requirements for this code are reasonably current versions\nof `python` and `numpy`.  In particular, `python` versions 3.10\nthrough 3.13 are routinely tested.  Because of its crucial dependence\non `numpy`, this package can only support versions of `python` that\nare directly supported by `numpy` \u2014 which limits support to releases\nfrom the past few years.  Old versions of `python` will work with\n*older* versions of this package, which are still available from PyPI\nand conda-forge.  Some older versions of `python` may still work with\nnewer versions of this package, but your mileage may vary.\n\nHowever, certain advanced functions in this package (including\n`squad`, `mean_rotor_in_intrinsic_metric`,\n`integrate_angular_velocity`, and related functions) require\n[`scipy`](http://scipy.org/) and can automatically use\n[`numba`](http://numba.pydata.org/).  `Scipy` is a standard python\npackage for scientific computation, and implements interfaces to C and\nFortran codes for optimization (among other things) need for finding\nmean and optimal rotors.  `Numba` uses [LLVM](http://llvm.org/) to\ncompile python code to machine code, accelerating many numerical\nfunctions by factors of anywhere from 2 to 2000.  It is *possible* to\nrun all the code without `numba`, but these particular functions can\nbe anywhere from 4 to 400 times slower without it.\n\nBoth `scipy` and `numba` can be installed with `pip` or `conda`.\nHowever, because `conda` is specifically geared toward scientific\npython, it is generally more robust for these more complicated\npackages.  In fact, the main\n[`anaconda`](https://www.anaconda.com/products/individual) package\ncomes with both `numba` and `scipy`.  If you prefer the smaller\ndownload size of [`miniconda`](http://conda.pydata.org/miniconda.html)\n(which comes with minimal extras), you'll also have to run this\ncommand:\n\nBug reports and feature requests"}, {"name": "numpy-quaternion", "tags": ["math", "web"], "summary": "Add a quaternion dtype to NumPy", "text": "Bug reports and feature requests are entirely welcome (with [very few\nexceptions](https://github.com/moble/quaternion/wiki/Euler-angles-are-horrible#opening-issues-and-pull-requests)).\nThe best way to do this is to open an [issue on this code's github\npage](https://github.com/moble/quaternion/issues).  For bug reports,\nplease try to include a minimal working example demonstrating the\nproblem.\n\n[Pull requests](https://help.github.com/articles/using-pull-requests/)\nare also entirely welcome, of course, if you have an idea where the\ncode is going wrong, or have an idea for a new feature that you know\nhow to implement.\n\nThis code is routinely tested on recent versions of both python (3.8\nthough 3.11) and numpy (>=1.13).  But the test coverage is not\nnecessarily as complete as it could be, so bugs may certainly be\npresent, especially in the higher-level functions like\n`mean_rotor_...`.\n\nAcknowledgments\n\nThis code is, of course, hosted on github.  Because it is an\nopen-source project, the hosting is free, and all the wonderful\nfeatures of github are available, including free wiki space and web\npage hosting, pull requests, a nice interface to the git logs, etc.\nGithub user Hannes Ovr\u00e9n (hovren) pointed out some errors in a\nprevious version of this code and suggested some nice utility\nfunctions for rotation matrices, etc.  Github user Stijn van Drongelen\n(rhymoid) contributed some code that makes compilation work with\nMSVC++.  Github user Jon Long (longjon) has provided some elegant\ncontributions to substantially improve several tricky parts of this\ncode.  Rebecca Turner (9999years) and Leo Stein (duetosymmetry) did\nall the work in getting the documentation onto [Read the\nDocs](https://quaternion.readthedocs.io/).\n\nEvery change in this code is [automatically\ntested](https://github.com/moble/quaternion/actions) on Github\nActions.  The code is downloaded and installed fresh each time, and\nthen tested, on each of the different supported versions of python, on\neach of the supported platforms.  This ensures that no change I make\nto the code breaks either installation or any of the features that I\nhave written tests for.  Github Actions also automatically builds the\n`pip` versions of the code hosted on\n[pypi](https://pypi.python.org/pypi/numpy-quaternion).  Conda-forge\nalso uses Github Actions to build [the conda/mamba\nversion](https://github.com/conda-forge/quaternion-feedstock) hosted\non [anaconda.org](https://anaconda.org/conda-forge/quaternion).  These\nare all free services for open-source projects like this one.\n\nThe work of creating this code was supported in part by the Sherman\nFairchild Foundation and by NSF Grants No. PHY-1306125 and\nAST-1333129.\n\n---\n\n1 Euler angles are awful\n\nEuler angles are pretty much [the worst things\never](https://moble.github.io/spherical_functions/#euler-angles) and it\nmakes me feel bad even supporting them.  Quaternions are faster, more\naccurate, basically free of singularities, more intuitive, and\ngenerally easier to understand.  You can work entirely without Euler\nangles (I certainly do).  You absolutely never need them.  But if\nyou really can't give them up, they are mildly supported."}, {"name": "numpy-quaternion", "tags": ["math", "web"], "summary": "Add a quaternion dtype to NumPy", "text": "This library is used to extend NumPy with quaternion data type support, enabling developers to natively represent and perform operations on quaternions within their numerical computations. With numpy-quaternion, developers can now seamlessly integrate quaternion-based math into their applications."}, {"name": "numpy", "tags": ["dev", "math", "web"], "summary": "Fundamental package for array computing in Python", "text": "(\n(\n(\n(\n(\n(https://insights.linuxfoundation.org/project/numpy)\n(https://securityscorecards.dev/viewer/?uri=github.com/numpy/numpy)\n(https://pypi.org/project/numpy/)\n\nNumPy is the fundamental package for scientific computing with Python.\n\nIt provides:\n\n- a powerful N-dimensional array object\n- sophisticated (broadcasting) functions\n- tools for integrating C/C++ and Fortran code\n- useful linear algebra, Fourier transform, and random number capabilities\n\nTesting:\n\nNumPy requires `pytest` and `hypothesis`.  Tests can then be run after installation with:\n\nCode of Conduct\n----------------------\n\nNumPy is a community-driven open source project developed by a diverse group of\n[contributors](https://numpy.org/teams/). The NumPy leadership has made a strong\ncommitment to creating an open, inclusive, and positive community. Please read the\n[NumPy Code of Conduct](https://numpy.org/code-of-conduct/) for guidance on how to interact\nwith others in a way that makes our community thrive.\n\nCall for Contributions\n----------------------\n\nThe NumPy project welcomes your expertise and enthusiasm!\n\nSmall improvements or fixes are always appreciated. If you are considering larger contributions\nto the source code, please contact us through the [mailing\nlist](https://mail.python.org/mailman/listinfo/numpy-discussion) first.\n\nWriting code isn\u2019t the only way to contribute to NumPy. You can also:\n- review pull requests\n- help us stay on top of new and old issues\n- develop tutorials, presentations, and other educational materials\n- maintain and improve [our website](https://github.com/numpy/numpy.org)\n- develop graphic design for our brand assets and promotional materials\n- translate website content\n- help with outreach and onboard new contributors\n- write grant proposals and help with other fundraising efforts\n\nFor more information about the ways you can contribute to NumPy, visit [our website](https://numpy.org/contribute/). \nIf you\u2019re unsure where to start or how your skills fit in, reach out! You can\nask on the mailing list or here, on GitHub, by opening a new issue or leaving a\ncomment on a relevant issue that is already open.\n\nOur preferred channels of communication are all public, but if you\u2019d like to\nspeak to us in private first, contact our community coordinators at\nnumpy-team@googlegroups.com or on Slack (write numpy-team@googlegroups.com for\nan invitation).\n\nWe also have a biweekly community call, details of which are announced on the\nmailing list. You are very welcome to join.\n\nIf you are new to contributing to open source, [this\nguide](https://opensource.guide/how-to-contribute/) helps explain why, what,\nand how to successfully get involved."}, {"name": "numpy", "tags": ["dev", "math", "web"], "summary": "Fundamental package for array computing in Python", "text": "This library is used to perform efficient numerical computations in Python with its high-performance N-dimensional array object and sophisticated mathematical functions. With NumPy, developers can write concise and readable code for tasks such as data analysis, machine learning, and scientific simulations."}, {"name": "nvidia-cusparselt-cu12", "tags": ["math", "web"], "summary": "NVIDIA cuSPARSELt", "text": "###################################################################################\ncuSPARSELt: A High-Performance CUDA Library for Sparse Matrix-Matrix Multiplication\n###################################################################################\n\n**NVIDIA cuSPARSELt** is a high-performance CUDA library dedicated to general matrix-matrix operations in which at least one operand is a structured sparse matrix with 50\\% sparsity ratio:\n\n.. math::\n\n   D = Activation(\\alpha op(A) \\cdot op(B) + \\beta op(C) + bias)\n\nwhere :math:`op(A)/op(B)` refers to in-place operations such as transpose/non-transpose, and :math:`alpha, beta` are scalars or vectors.\n\nThe *cuSPARSELt APIs* allow flexibility in the algorithm/operation selection, epilogue, and matrix characteristics, including memory layout, alignment, and data types.\n\n**Download:** `developer.nvidia.com/cusparselt/downloads `_\n\n**Provide Feedback:** `Math-Libs-Feedback@nvidia.com `_\n\n**Examples**:\n`cuSPARSELt Example 1 `_,\n`cuSPARSELt Example 2 `_\n\n**Blog post**:\n\n- `Exploiting NVIDIA Ampere Structured Sparsity with cuSPARSELt `_\n- `Structured Sparsity in the NVIDIA Ampere Architecture and Applications in Search Engines `__\n- `Making the Most of Structured Sparsity in the NVIDIA Ampere Architecture `__\n\n================================================================================\nKey Features\n================================================================================\n\n* *NVIDIA Sparse MMA tensor core* support\n* Mixed-precision computation support:\n\n* Matrix pruning and compression functionalities\n* Activation functions, bias vector, and output scaling\n* Batched computation (multiple matrices in a single run)\n* GEMM Split-K mode\n* Auto-tuning functionality (see `cusparseLtMatmulSearch()`)\n* NVTX ranging and Logging functionalities\n\n================================================================================\nSupport\n================================================================================\n\n* *Supported SM Architectures*: `SM 8.0`, `SM 8.6`, `SM 8.7`, `SM 8.9`, `SM 9.0`, `SM 10.0`, `SM 10.1` (for CTK 12), `SM 11.0` (for CTK 13), `SM 12.0`, `SM 12.1`\n* *Supported CPU architectures and operating systems*:\n\n+------------+--------------------+\nOS\n+============+====================+\n`Windows`\n+------------+--------------------+\n`Linux`\n+------------+--------------------+\n\n================================================================================\nDocumentation\n================================================================================\n\nPlease refer to https://docs.nvidia.com/cuda/cusparselt/index.html for the cuSPARSELt documentation.\n\n================================================================================\nInstallation\n================================================================================\n\nThe cuSPARSELt wheel can be installed as follows:\n\n.. code-block:: bash\n\nwhere XX is the CUDA major version."}, {"name": "nvidia-cusparselt-cu12", "tags": ["math", "web"], "summary": "NVIDIA cuSPARSELt", "text": "This library is used to efficiently perform high-performance CUDA computations for sparse matrix-matrix multiplication operations. It allows developers to quickly implement complex sparse matrix algorithms with customizable options for operation selection, matrix characteristics, and more."}, {"name": "nvidia-nccl-cu12", "tags": ["math", "ml"], "summary": "NVIDIA Collective Communication Library (NCCL) Runtime", "text": "NCCL (pronounced \"Nickel\") is a stand-alone library of standard collective communication routines for GPUs, implementing all-reduce, all-gather, reduce, broadcast, and reduce-scatter. It has been optimized to achieve high bandwidth on any platform using PCIe, NVLink, NVswitch, as well as networking using InfiniBand Verbs or TCP/IP sockets."}, {"name": "nvidia-nccl-cu12", "tags": ["math", "ml"], "summary": "NVIDIA Collective Communication Library (NCCL) Runtime", "text": "This library is used to implement collective communication routines for GPUs, enabling developers to optimize data transfer and synchronization operations between multiple nodes. With NCCL, developers can achieve high-bandwidth GPU-to-GPU communication on various platforms, including PCIe, NVLink, NVswitch, InfiniBand Verbs, or TCP/IP sockets."}, {"name": "nvidia-nvshmem-cu12", "tags": ["math", "ml"], "summary": "NVSHMEM creates a global address space that provides efficient and scalable communication for NVIDIA GPU clusters.", "text": "NVSHMEM is a parallel programming interface based on OpenSHMEM that provides efficient and scalable communication for NVIDIA GPU clusters. NVSHMEM creates a global address space for data that spans the memory of multiple GPUs and can be accessed with fine-grained GPU-initiated operations, CPU-initiated operations, and operations on CUDA streams."}, {"name": "nvidia-nvshmem-cu12", "tags": ["math", "ml"], "summary": "NVSHMEM creates a global address space that provides efficient and scalable communication for NVIDIA GPU clusters.", "text": "This library is used to enable efficient and scalable communication between NVIDIA GPUs in clusters through a shared global address space. With NVSHMEM, developers can write applications that seamlessly access and share data across multiple GPUs using fine-grained GPU-initiated or CPU-initiated operations."}, {"name": "onnxruntime-gpu", "tags": ["math", "ml"], "summary": "ONNX Runtime is a runtime accelerator for Machine Learning models", "text": "ONNX Runtime\n============\n\nONNX Runtime is a performance-focused scoring engine for Open Neural Network Exchange (ONNX) models.\nFor more information on ONNX Runtime, please see `aka.ms/onnxruntime `_ or the `Github project `_.\n\nChanges\n-------\n1.23.2\n^^^^^^\n\nRelease Notes : https://github.com/Microsoft/onnxruntime/releases/tag/v1.23.2\n\n1.23.1\n^^^^^^\n\nRelease Notes : https://github.com/Microsoft/onnxruntime/releases/tag/v1.23.1\n\n1.23.0\n^^^^^^\n\nRelease Notes : https://github.com/Microsoft/onnxruntime/releases/tag/v1.23.0\n\n1.22.0\n^^^^^^\n\nRelease Notes : https://github.com/Microsoft/onnxruntime/releases/tag/v1.22.0\n\n1.21.0\n^^^^^^\n\nRelease Notes : https://github.com/Microsoft/onnxruntime/releases/tag/v1.21.0\n\n1.20.0\n^^^^^^\n\nRelease Notes : https://github.com/Microsoft/onnxruntime/releases/tag/v1.20.0\n\n1.19.0\n^^^^^^\n\nRelease Notes : https://github.com/Microsoft/onnxruntime/releases/tag/v1.19.0\n\n1.18.0\n^^^^^^\n\nRelease Notes : https://github.com/Microsoft/onnxruntime/releases/tag/v1.18.0\n\n1.17.0\n^^^^^^\n\nRelease Notes : https://github.com/Microsoft/onnxruntime/releases/tag/v1.17.0\n\n1.16.0\n^^^^^^\n\nRelease Notes : https://github.com/Microsoft/onnxruntime/releases/tag/v1.16.0\n\n1.15.0\n^^^^^^\n\nRelease Notes : https://github.com/Microsoft/onnxruntime/releases/tag/v1.15.0\n\n1.14.0\n^^^^^^\n\nRelease Notes : https://github.com/Microsoft/onnxruntime/releases/tag/v1.14.0\n\n1.13.0\n^^^^^^\n\nRelease Notes : https://github.com/Microsoft/onnxruntime/releases/tag/v1.13.0\n\n1.12.0\n^^^^^^\n\nRelease Notes : https://github.com/Microsoft/onnxruntime/releases/tag/v1.12.0\n\n1.11.0\n^^^^^^\n\nRelease Notes : https://github.com/Microsoft/onnxruntime/releases/tag/v1.11.0\n\n1.10.0\n^^^^^^\n\nRelease Notes : https://github.com/Microsoft/onnxruntime/releases/tag/v1.10.0\n\n1.9.0\n^^^^^\n\nRelease Notes : https://github.com/Microsoft/onnxruntime/releases/tag/v1.9.0\n\n1.8.2\n^^^^^\n\nRelease Notes : https://github.com/Microsoft/onnxruntime/releases/tag/v1.8.2\n\n1.8.1\n^^^^^\n\nRelease Notes : https://github.com/Microsoft/onnxruntime/releases/tag/v1.8.1\n\n1.8.0\n^^^^^\n\nRelease Notes : https://github.com/Microsoft/onnxruntime/releases/tag/v1.8.0\n\n1.7.0\n^^^^^\n\nRelease Notes : https://github.com/Microsoft/onnxruntime/releases/tag/v1.7.0\n\n1.6.0\n^^^^^\n\nRelease Notes : https://github.com/Microsoft/onnxruntime/releases/tag/v1.6.0\n\n1.5.3\n^^^^^\n\nRelease Notes : https://github.com/Microsoft/onnxruntime/releases/tag/v1.5.3\n\n1.5.2\n^^^^^\n\nRelease Notes : https://github.com/Microsoft/onnxruntime/releases/tag/v1.5.2\n\n1.5.1\n^^^^^\n\nRelease Notes : https://github.com/Microsoft/onnxruntime/releases/tag/v1.5.1\n\n1.4.0\n^^^^^\n\nRelease Notes : https://github.com/Microsoft/onnxruntime/releases/tag/v1.4.0\n\n1.3.1\n^^^^^\n\nRelease Notes : https://github.com/Microsoft/onnxruntime/releases/tag/v1.3.1\n\n1.3.0\n^^^^^\n\nRelease Notes : https://github.com/Microsoft/onnxruntime/releases/tag/v1.3.0\n\n1.2.0\n^^^^^\n\nRelease Notes : https://github.com/Microsoft/onnxruntime/releases/tag/v1.2.0\n\n1.1.0\n^^^^^\n\nRelease Notes : https://github.com/Microsoft/onnxruntime/releases/tag/v1.1.0\n\n1.0.0\n^^^^^\n\nRelease Notes : https://github.com/Microsoft/onnxruntime/releases/tag/v1.0.0\n\n0.5.0\n^^^^^\n\nRelease Notes : https://github.com/Microsoft/onnxruntime/releases/tag/v0.5.0\n\n0.4.0\n^^^^^\n\nRelease Notes : https://github.com/Microsoft/onnxruntime/releases/tag/v0.4.0"}, {"name": "onnxruntime-gpu", "tags": ["math", "ml"], "summary": "ONNX Runtime is a runtime accelerator for Machine Learning models", "text": "This library is used to accelerate the execution of Machine Learning models on NVIDIA GPU devices, providing a high-performance scoring engine for Open Neural Network Exchange (ONNX) models. Developers can utilize this library to optimize model inference and improve the overall efficiency of their machine learning applications running on GPUs."}, {"name": "onnxruntime", "tags": ["math", "ml"], "summary": "ONNX Runtime is a runtime accelerator for Machine Learning models", "text": "ONNX Runtime\n============\n\nONNX Runtime is a performance-focused scoring engine for Open Neural Network Exchange (ONNX) models.\nFor more information on ONNX Runtime, please see `aka.ms/onnxruntime `_ or the `Github project `_.\n\nChanges\n-------\n1.23.2\n^^^^^^\n\nRelease Notes : https://github.com/Microsoft/onnxruntime/releases/tag/v1.23.2\n\n1.23.1\n^^^^^^\n\nRelease Notes : https://github.com/Microsoft/onnxruntime/releases/tag/v1.23.1\n\n1.23.0\n^^^^^^\n\nRelease Notes : https://github.com/Microsoft/onnxruntime/releases/tag/v1.23.0\n\n1.22.0\n^^^^^^\n\nRelease Notes : https://github.com/Microsoft/onnxruntime/releases/tag/v1.22.0\n\n1.21.0\n^^^^^^\n\nRelease Notes : https://github.com/Microsoft/onnxruntime/releases/tag/v1.21.0\n\n1.20.0\n^^^^^^\n\nRelease Notes : https://github.com/Microsoft/onnxruntime/releases/tag/v1.20.0\n\n1.19.0\n^^^^^^\n\nRelease Notes : https://github.com/Microsoft/onnxruntime/releases/tag/v1.19.0\n\n1.18.0\n^^^^^^\n\nRelease Notes : https://github.com/Microsoft/onnxruntime/releases/tag/v1.18.0\n\n1.17.0\n^^^^^^\n\nRelease Notes : https://github.com/Microsoft/onnxruntime/releases/tag/v1.17.0\n\n1.16.0\n^^^^^^\n\nRelease Notes : https://github.com/Microsoft/onnxruntime/releases/tag/v1.16.0\n\n1.15.0\n^^^^^^\n\nRelease Notes : https://github.com/Microsoft/onnxruntime/releases/tag/v1.15.0\n\n1.14.0\n^^^^^^\n\nRelease Notes : https://github.com/Microsoft/onnxruntime/releases/tag/v1.14.0\n\n1.13.0\n^^^^^^\n\nRelease Notes : https://github.com/Microsoft/onnxruntime/releases/tag/v1.13.0\n\n1.12.0\n^^^^^^\n\nRelease Notes : https://github.com/Microsoft/onnxruntime/releases/tag/v1.12.0\n\n1.11.0\n^^^^^^\n\nRelease Notes : https://github.com/Microsoft/onnxruntime/releases/tag/v1.11.0\n\n1.10.0\n^^^^^^\n\nRelease Notes : https://github.com/Microsoft/onnxruntime/releases/tag/v1.10.0\n\n1.9.0\n^^^^^\n\nRelease Notes : https://github.com/Microsoft/onnxruntime/releases/tag/v1.9.0\n\n1.8.2\n^^^^^\n\nRelease Notes : https://github.com/Microsoft/onnxruntime/releases/tag/v1.8.2\n\n1.8.1\n^^^^^\n\nRelease Notes : https://github.com/Microsoft/onnxruntime/releases/tag/v1.8.1\n\n1.8.0\n^^^^^\n\nRelease Notes : https://github.com/Microsoft/onnxruntime/releases/tag/v1.8.0\n\n1.7.0\n^^^^^\n\nRelease Notes : https://github.com/Microsoft/onnxruntime/releases/tag/v1.7.0\n\n1.6.0\n^^^^^\n\nRelease Notes : https://github.com/Microsoft/onnxruntime/releases/tag/v1.6.0\n\n1.5.3\n^^^^^\n\nRelease Notes : https://github.com/Microsoft/onnxruntime/releases/tag/v1.5.3\n\n1.5.2\n^^^^^\n\nRelease Notes : https://github.com/Microsoft/onnxruntime/releases/tag/v1.5.2\n\n1.5.1\n^^^^^\n\nRelease Notes : https://github.com/Microsoft/onnxruntime/releases/tag/v1.5.1\n\n1.4.0\n^^^^^\n\nRelease Notes : https://github.com/Microsoft/onnxruntime/releases/tag/v1.4.0\n\n1.3.1\n^^^^^\n\nRelease Notes : https://github.com/Microsoft/onnxruntime/releases/tag/v1.3.1\n\n1.3.0\n^^^^^\n\nRelease Notes : https://github.com/Microsoft/onnxruntime/releases/tag/v1.3.0\n\n1.2.0\n^^^^^\n\nRelease Notes : https://github.com/Microsoft/onnxruntime/releases/tag/v1.2.0\n\n1.1.0\n^^^^^\n\nRelease Notes : https://github.com/Microsoft/onnxruntime/releases/tag/v1.1.0\n\n1.0.0\n^^^^^\n\nRelease Notes : https://github.com/Microsoft/onnxruntime/releases/tag/v1.0.0\n\n0.5.0\n^^^^^\n\nRelease Notes : https://github.com/Microsoft/onnxruntime/releases/tag/v0.5.0\n\n0.4.0\n^^^^^\n\nRelease Notes : https://github.com/Microsoft/onnxruntime/releases/tag/v0.4.0"}, {"name": "onnxruntime", "tags": ["math", "ml"], "summary": "ONNX Runtime is a runtime accelerator for Machine Learning models", "text": "This library is used to accelerate the scoring of Machine Learning models through efficient execution and optimized performance, allowing developers to deploy high-performance ML applications with ease. With ONNX Runtime, developers can optimize their models for various hardware platforms, including CPU, GPU, and other accelerators."}, {"name": "onnxslim", "tags": ["math", "ml"], "summary": "OnnxSlim: A Toolkit to Help Optimize Onnx Model", "text": "OnnxSlim\n\nOnnxSlim can help you slim your onnx model, with less operators, but same accuracy, better inference speed.\n\n-  2025/11/29: Top 1% on PyPI\n-  2025/01/28: Achieved 1M downloads\n\nBenchmark\n\nInstallation\n\nUsing Prebuilt\n\nInstall From Source\n\nInstall From Local\n\nHow to use\n\nBash\n\nInscript\n\nFor more usage, see onnxslim -h or refer to our [examples](./examples)\n\nProjects using OnnxSlim\n\nReferences\n\n> - [onnx-graphsurgeon](https://github.com/NVIDIA/TensorRT/tree/main/tools/onnx-graphsurgeon)\n> - [Polygraphy](https://github.com/NVIDIA/TensorRT/tree/main/tools/Polygraphy/polygraphy)\n> - [onnx-simplifier](https://github.com/daquexian/onnx-simplifier)\n> - [tabulate](https://github.com/astanin/python-tabulate)\n> - [onnxruntime](https://github.com/microsoft/onnxruntime)\n\nContact\n\nDiscord: https://discord.gg/nRw2Fd3VUS QQ Group: `873569894`"}, {"name": "onnxslim", "tags": ["math", "ml"], "summary": "OnnxSlim: A Toolkit to Help Optimize Onnx Model", "text": "This library is used to optimize Onnx models by reducing the number of operators while maintaining accuracy, resulting in improved inference speed. This optimization enables developers to create more efficient models that can run faster and with better performance without compromising on model quality."}, {"name": "opencv-contrib-python-headless", "tags": ["cli", "math", "ui"], "summary": "Wrapper package for OpenCV python bindings.", "text": "Keep OpenCV Free\n\nOpenCV is raising funds to keep the library free for everyone, and we need the support of the entire community to do it. [Donate to OpenCV on Github](https://github.com/sponsors/opencv) to show your support.\n\n- [OpenCV on Wheels](#opencv-on-wheels)\n  - [Installation and Usage](#installation-and-usage)\n- [Frequently Asked Questions](#frequently-asked-questions)\n- [Documentation for opencv-python](#documentation-for-opencv-python)\n  - [CI build process](#ci-build-process)\n  - [Manual builds](#manual-builds)\n  - [Licensing](#licensing)\n  - [Versioning](#versioning)\n  - [Releases](#releases)\n  - [Development builds](#development-builds)\n  - [Manylinux wheels](#manylinux-wheels)\n  - [Supported Python versions](#supported-python-versions)\n  - [Backward compatibility](#backward-compatibility)\n\nOpenCV on Wheels\n\nPre-built CPU-only OpenCV packages for Python.\n\nCheck the manual build section if you wish to compile the bindings from source to enable additional modules such as CUDA.\n\nInstallation and Usage\n\n1. If you have previous/other manually installed (= not installed via ``pip``) version of OpenCV installed (e.g. cv2 module in the root of Python's site-packages), remove it before installation to avoid conflicts.\n2. Make sure that your `pip` version is up-to-date (19.3 is the minimum supported version): `pip install --upgrade pip`. Check version with `pip -V`. For example Linux distributions ship usually with very old `pip` versions which cause a lot of unexpected problems especially with the `manylinux` format.\n3. Select the correct package for your environment:\n\n4. Import the package:\n\n5. Read [OpenCV documentation](https://docs.opencv.org/master/)\n\n6. Before opening a new issue, read the FAQ below and have a look at the other issues which are already open.\n\nFrequently Asked Questions\n--------------------------\n\n**Q: Do I need to install also OpenCV separately?**\n\nA: No, the packages are special wheel binary packages and they already contain statically built OpenCV binaries.\n\n**Q: Pip install fails with ``ModuleNotFoundError: No module named 'skbuild'``?**\n\nSince ``opencv-python`` version 4.3.0.\\*, ``manylinux1`` wheels were replaced by ``manylinux2014`` wheels. If your pip is too old, it will try to use the new source distribution introduced in 4.3.0.38 to manually build OpenCV because it does not know how to install ``manylinux2014`` wheels. However, source build will also fail because of too old ``pip`` because it does not understand build dependencies in ``pyproject.toml``. To use the new ``manylinux2014`` pre-built wheels (or to build from source), your ``pip`` version must be >= 19.3. Please upgrade ``pip`` with ``pip install --upgrade pip``.\n\n**Q: Import fails on Windows: ``ImportError: DLL load failed: The specified module could not be found.``?**\n\nA: If the import fails on Windows, make sure you have [Visual C++ redistributable 2015](https://www.microsoft.com/en-us/download/details.aspx?id=48145) installed. If you are using older Windows version than Windows 10 and latest system updates are not installed, [Universal C Runtime](https://support.microsoft.com/en-us/help/2999226/update-for-universal-c-runtime-in-windows) might be also required.\n\nWindows N and KN editions do not include Media Feature Pack which is required by OpenCV. If you are using Windows N or KN edition, please install also [Windows Media Feature Pack](https://support.microsoft.com/en-us/help/3145500/media-feature-pack-list-for-windows-n-editions)."}, {"name": "opencv-contrib-python-headless", "tags": ["cli", "math", "ui"], "summary": "Wrapper package for OpenCV python bindings.", "text": "If you have Windows Server 2012+, media DLLs are probably missing too; please install the Feature called \"Media Foundation\" in the Server Manager. Beware, some posts advise to install \"Windows Server Essentials Media Pack\", but this one requires the \"Windows Server Essentials Experience\" role, and this role will deeply affect your Windows Server configuration (by enforcing active directory integration etc.); so just installing the \"Media Foundation\" should be a safer choice.\n\nIf the above does not help, check if you are using Anaconda. Old Anaconda versions have a bug which causes the error, see [this issue](https://github.com/opencv/opencv-python/issues/36) for a manual fix.\n\nIf you still encounter the error after you have checked all the previous solutions, download [Dependencies](https://github.com/lucasg/Dependencies) and open the ``cv2.pyd`` (located usually at ``C:\\Users\\username\\AppData\\Local\\Programs\\Python\\PythonXX\\Lib\\site-packages\\cv2``) file with it to debug missing DLL issues.\n\n**Q: I have some other import errors?**\n\nA: Make sure you have removed old manual installations of OpenCV Python bindings (cv2.so or cv2.pyd in site-packages).\n\n**Q: Function foo() or method bar() returns wrong result, throws exception or crashes interpreter. What should I do?**\n\nA: The repository contains only OpenCV-Python package build scripts, but not OpenCV itself. Python bindings for OpenCV are developed in official OpenCV repository and it's the best place to report issues. Also please check [OpenCV wiki](https://github.com/opencv/opencv/wiki) and [the official OpenCV forum](https://forum.opencv.org/) before file new bugs.\n\n**Q: Why the packages do not include non-free algorithms?**\n\nA: Non-free algorithms such as SURF are not included in these packages because they are patented / non-free and therefore cannot be distributed as built binaries. Note that SIFT is included in the builds due to patent expiration since OpenCV versions 4.3.0 and 3.4.10. See this issue for more info: https://github.com/skvark/opencv-python/issues/126\n\n**Q: Why the package and import are different (opencv-python vs. cv2)?**\n\nA: It's easier for users to understand ``opencv-python`` than ``cv2`` and it makes it easier to find the package with search engines. `cv2` (old interface in old OpenCV versions was named as `cv`) is the name that OpenCV developers chose when they created the binding generators. This is kept as the import name to be consistent with different kind of tutorials around the internet. Changing the import name or behaviour would be also confusing to experienced users who are accustomed to the ``import cv2``.\n\nDocumentation for opencv-python\n\n(https://github.com/opencv/opencv-python/actions/workflows/build_wheels_windows.yml)\n(https://github.com/opencv/opencv-python/actions/workflows/build_wheels_linux.yml)\n(https://github.com/opencv/opencv-python/actions/workflows/build_wheels_macos.yml)\n\nThe aim of this repository is to provide means to package each new [OpenCV release](https://github.com/opencv/opencv/releases) for the most used Python versions and platforms.\n\nCI build process\n\nThe project is structured like a normal Python package with a standard ``setup.py`` file.\nThe build process for a single entry in the build matrices is as follows (see for example `.github/workflows/build_wheels_linux.yml` file):\n\n0. In Linux and MacOS build: get OpenCV's optional C dependencies that we compile against\n\n1. Checkout repository and submodules\n\n-  OpenCV is included as submodule and the version is updated\n   -  Contrib modules are also included as a submodule\n\n2. Find OpenCV version from the sources\n\n3. Build OpenCV"}, {"name": "opencv-contrib-python-headless", "tags": ["cli", "math", "ui"], "summary": "Wrapper package for OpenCV python bindings.", "text": "-  tests are disabled, otherwise build time increases too much\n   -  there are 4 build matrix entries for each build combination: with and without contrib modules, with and without GUI (headless)\n   -  Linux builds run in manylinux Docker containers (CentOS 5)\n   -  source distributions are separate entries in the build matrix\n\n4. Rearrange OpenCV's build result, add our custom files and generate wheel\n\n5. Linux and macOS wheels are transformed with auditwheel and delocate, correspondingly\n\n6. Install the generated wheel\n7. Test that Python can import the library and run some sanity checks\n8. Use twine to upload the generated wheel to PyPI (only in release builds)\n\nSteps 1--4 are handled by ``pip wheel``.\n\nThe build can be customized with environment variables. In addition to any variables that OpenCV's build accepts, we recognize:\n\n- ``CI_BUILD``. Set to ``1`` to emulate the CI environment build behaviour. Used only in CI builds to force certain build flags on in ``setup.py``. Do not use this unless you know what you are doing.\n- ``ENABLE_CONTRIB`` and ``ENABLE_HEADLESS``. Set to ``1`` to build the contrib and/or headless version\n- ``ENABLE_JAVA``, Set to ``1`` to enable the Java client build.  This is disabled by default.\n- ``CMAKE_ARGS``. Additional arguments for OpenCV's CMake invocation. You can use this to make a custom build.\n\nSee the next section for more info about manual builds outside the CI environment.\n\nManual builds\n\nIf some dependency is not enabled in the pre-built wheels, you can also run the build locally to create a custom wheel.\n\n1. Clone this repository: `git clone --recursive https://github.com/opencv/opencv-python.git`\n2. ``cd opencv-python``\n3. Add custom Cmake flags if needed, for example: `export CMAKE_ARGS=\"-DSOME_FLAG=ON -DSOME_OTHER_FLAG=OFF\"` (in Windows you need to set environment variables differently depending on Command Line or PowerShell)\n4. Select the package flavor which you wish to build with `ENABLE_CONTRIB` and `ENABLE_HEADLESS`: i.e. `export ENABLE_CONTRIB=1` if you wish to build `opencv-contrib-python`\n5. Run ``pip wheel . --verbose``. NOTE: make sure you have the latest ``pip`` version, the ``pip wheel`` command replaces the old ``python setup.py bdist_wheel`` command which does not support ``pyproject.toml``.\n6. Pip will print fresh wheel location at the end of build procedure. If you use old approach with `setup.py` file wheel package will be placed in `dist` folder. Package is ready and you can do with that whatever you wish.\n\nManual debug builds\n\nIn order to build `opencv-python` in an unoptimized debug build, you need to side-step the normal process a bit.\n\n1. Install the packages `scikit-build` and `numpy` via pip.\n2. Run the command `python setup.py bdist_wheel --build-type=Debug`.\n3. Install the generated wheel file in the `dist/` folder with `pip install dist/wheelname.whl`.\n\nIf you would like the build produce all compiler commands, then the following combination of flags and environment variables has been tested to work on Linux:\n\nSee this issue for more discussion: https://github.com/opencv/opencv-python/issues/424\n\nSource distributions"}, {"name": "opencv-contrib-python-headless", "tags": ["cli", "math", "ui"], "summary": "Wrapper package for OpenCV python bindings.", "text": "Since OpenCV version 4.3.0, also source distributions are provided in PyPI. This means that if your system is not compatible with any of the wheels in PyPI, ``pip`` will attempt to build OpenCV from sources. If you need a OpenCV version which is not available in PyPI as a source distribution, please follow the manual build guidance above instead of this one.\n\nYou can also force ``pip`` to build the wheels from the source distribution. Some examples:\n\n- ``pip install --no-binary opencv-python opencv-python``\n- ``pip install --no-binary :all: opencv-python``\n\nIf you need contrib modules or headless version, just change the package name (step 4 in the previous section is not needed). However, any additional CMake flags can be provided via environment variables as described in step 3 of the manual build section. If none are provided, OpenCV's CMake scripts will attempt to find and enable any suitable dependencies. Headless distributions have hard coded CMake flags which disable all possible GUI dependencies.\n\nOn slow systems such as Raspberry Pi the full build may take several hours. On a 8-core Ryzen 7 3700X the build takes about 6 minutes.\n\nLicensing\n\nOpencv-python package (scripts in this repository) is available under MIT license.\n\nOpenCV itself is available under [Apache 2](https://github.com/opencv/opencv/blob/master/LICENSE) license.\n\nThird party package licenses are at [LICENSE-3RD-PARTY.txt](https://github.com/opencv/opencv-python/blob/master/LICENSE-3RD-PARTY.txt).\n\nAll wheels ship with [FFmpeg](http://ffmpeg.org) licensed under the [LGPLv2.1](http://www.gnu.org/licenses/old-licenses/lgpl-2.1.html).\n\nNon-headless Linux wheels ship with [Qt 5](http://doc.qt.io/qt-5/lgpl.html) licensed under the [LGPLv3](http://www.gnu.org/licenses/lgpl-3.0.html).\n\nThe packages include also other binaries. Full list of licenses can be found from [LICENSE-3RD-PARTY.txt](https://github.com/opencv/opencv-python/blob/master/LICENSE-3RD-PARTY.txt).\n\nVersioning\n\n``find_version.py`` script searches for the version information from OpenCV sources and appends also a revision number specific to this repository to the version string. It saves the version information to ``version.py`` file under ``cv2`` in addition to some other flags.\n\nReleases\n\nA release is made and uploaded to PyPI when a new tag is pushed to master branch. These tags differentiate packages (this repo might have modifications but OpenCV version stays same) and should be incremented sequentially. In practice, release version numbers look like this:\n\n``cv_major.cv_minor.cv_revision.package_revision`` e.g. ``3.1.0.0``\n\nThe master branch follows OpenCV master branch releases. 3.4 branch follows OpenCV 3.4 bugfix releases.\n\nDevelopment builds\n\nEvery commit to the master branch of this repo will be built. Possible build artifacts use local version identifiers:\n\n``cv_major.cv_minor.cv_revision+git_hash_of_this_repo`` e.g. ``3.1.0+14a8d39``\n\nThese artifacts can't be and will not be uploaded to PyPI.\n\nManylinux wheels\n\nLinux wheels are built using [manylinux2014](https://github.com/pypa/manylinux). These wheels should work out of the box for most of the distros (which use GNU C standard library) out there since they are built against an old version of glibc.\n\nThe default ``manylinux2014`` images have been extended with some OpenCV dependencies. See [Docker folder](https://github.com/skvark/opencv-python/tree/master/docker) for more info.\n\nSupported Python versions\n\nPython 3.x compatible pre-built wheels are provided for the officially supported Python versions (not in EOL):\n\n- 3.7\n- 3.8\n- 3.9\n- 3.10\n- 3.11\n- 3.12\n- 3.13\n\nBackward compatibility"}, {"name": "opencv-contrib-python-headless", "tags": ["cli", "math", "ui"], "summary": "Wrapper package for OpenCV python bindings.", "text": "Starting from 4.2.0 and 3.4.9 builds the macOS Travis build environment was updated to XCode 9.4. The change effectively dropped support for older than 10.13 macOS versions.\n\nStarting from 4.3.0 and 3.4.10 builds the Linux build environment was updated from `manylinux1` to `manylinux2014`. This dropped support for old Linux distributions.\n\nStarting from version 4.7.0 the Mac OS GitHub Actions build environment was update to version 11. Mac OS 10.x support deprecated. See https://github.com/actions/runner-images/issues/5583\n\nStarting from version 4.9.0 the Mac OS GitHub Actions build environment was update to version 12. Mac OS 10.x support deprecated by Brew and most of used packages."}, {"name": "opencv-contrib-python-headless", "tags": ["cli", "math", "ui"], "summary": "Wrapper package for OpenCV python bindings.", "text": "This library is used to provide a free and open-source implementation of computer vision algorithms, allowing developers to easily integrate OpenCV into their projects. By leveraging this library, developers can unlock advanced image processing capabilities without the need for additional licensing or fees."}, {"name": "opencv-contrib-python", "tags": ["cli", "math", "ui"], "summary": "Wrapper package for OpenCV python bindings.", "text": "Keep OpenCV Free\n\nOpenCV is raising funds to keep the library free for everyone, and we need the support of the entire community to do it. [Donate to OpenCV on Github](https://github.com/sponsors/opencv) to show your support.\n\n- [OpenCV on Wheels](#opencv-on-wheels)\n  - [Installation and Usage](#installation-and-usage)\n- [Frequently Asked Questions](#frequently-asked-questions)\n- [Documentation for opencv-python](#documentation-for-opencv-python)\n  - [CI build process](#ci-build-process)\n  - [Manual builds](#manual-builds)\n  - [Licensing](#licensing)\n  - [Versioning](#versioning)\n  - [Releases](#releases)\n  - [Development builds](#development-builds)\n  - [Manylinux wheels](#manylinux-wheels)\n  - [Supported Python versions](#supported-python-versions)\n  - [Backward compatibility](#backward-compatibility)\n\nOpenCV on Wheels\n\nPre-built CPU-only OpenCV packages for Python.\n\nCheck the manual build section if you wish to compile the bindings from source to enable additional modules such as CUDA.\n\nInstallation and Usage\n\n1. If you have previous/other manually installed (= not installed via ``pip``) version of OpenCV installed (e.g. cv2 module in the root of Python's site-packages), remove it before installation to avoid conflicts.\n2. Make sure that your `pip` version is up-to-date (19.3 is the minimum supported version): `pip install --upgrade pip`. Check version with `pip -V`. For example Linux distributions ship usually with very old `pip` versions which cause a lot of unexpected problems especially with the `manylinux` format.\n3. Select the correct package for your environment:\n\n4. Import the package:\n\n5. Read [OpenCV documentation](https://docs.opencv.org/master/)\n\n6. Before opening a new issue, read the FAQ below and have a look at the other issues which are already open.\n\nFrequently Asked Questions\n--------------------------\n\n**Q: Do I need to install also OpenCV separately?**\n\nA: No, the packages are special wheel binary packages and they already contain statically built OpenCV binaries.\n\n**Q: Pip install fails with ``ModuleNotFoundError: No module named 'skbuild'``?**\n\nSince ``opencv-python`` version 4.3.0.\\*, ``manylinux1`` wheels were replaced by ``manylinux2014`` wheels. If your pip is too old, it will try to use the new source distribution introduced in 4.3.0.38 to manually build OpenCV because it does not know how to install ``manylinux2014`` wheels. However, source build will also fail because of too old ``pip`` because it does not understand build dependencies in ``pyproject.toml``. To use the new ``manylinux2014`` pre-built wheels (or to build from source), your ``pip`` version must be >= 19.3. Please upgrade ``pip`` with ``pip install --upgrade pip``.\n\n**Q: Import fails on Windows: ``ImportError: DLL load failed: The specified module could not be found.``?**\n\nA: If the import fails on Windows, make sure you have [Visual C++ redistributable 2015](https://www.microsoft.com/en-us/download/details.aspx?id=48145) installed. If you are using older Windows version than Windows 10 and latest system updates are not installed, [Universal C Runtime](https://support.microsoft.com/en-us/help/2999226/update-for-universal-c-runtime-in-windows) might be also required.\n\nWindows N and KN editions do not include Media Feature Pack which is required by OpenCV. If you are using Windows N or KN edition, please install also [Windows Media Feature Pack](https://support.microsoft.com/en-us/help/3145500/media-feature-pack-list-for-windows-n-editions)."}, {"name": "opencv-contrib-python", "tags": ["cli", "math", "ui"], "summary": "Wrapper package for OpenCV python bindings.", "text": "If you have Windows Server 2012+, media DLLs are probably missing too; please install the Feature called \"Media Foundation\" in the Server Manager. Beware, some posts advise to install \"Windows Server Essentials Media Pack\", but this one requires the \"Windows Server Essentials Experience\" role, and this role will deeply affect your Windows Server configuration (by enforcing active directory integration etc.); so just installing the \"Media Foundation\" should be a safer choice.\n\nIf the above does not help, check if you are using Anaconda. Old Anaconda versions have a bug which causes the error, see [this issue](https://github.com/opencv/opencv-python/issues/36) for a manual fix.\n\nIf you still encounter the error after you have checked all the previous solutions, download [Dependencies](https://github.com/lucasg/Dependencies) and open the ``cv2.pyd`` (located usually at ``C:\\Users\\username\\AppData\\Local\\Programs\\Python\\PythonXX\\Lib\\site-packages\\cv2``) file with it to debug missing DLL issues.\n\n**Q: I have some other import errors?**\n\nA: Make sure you have removed old manual installations of OpenCV Python bindings (cv2.so or cv2.pyd in site-packages).\n\n**Q: Function foo() or method bar() returns wrong result, throws exception or crashes interpreter. What should I do?**\n\nA: The repository contains only OpenCV-Python package build scripts, but not OpenCV itself. Python bindings for OpenCV are developed in official OpenCV repository and it's the best place to report issues. Also please check [OpenCV wiki](https://github.com/opencv/opencv/wiki) and [the official OpenCV forum](https://forum.opencv.org/) before file new bugs.\n\n**Q: Why the packages do not include non-free algorithms?**\n\nA: Non-free algorithms such as SURF are not included in these packages because they are patented / non-free and therefore cannot be distributed as built binaries. Note that SIFT is included in the builds due to patent expiration since OpenCV versions 4.3.0 and 3.4.10. See this issue for more info: https://github.com/skvark/opencv-python/issues/126\n\n**Q: Why the package and import are different (opencv-python vs. cv2)?**\n\nA: It's easier for users to understand ``opencv-python`` than ``cv2`` and it makes it easier to find the package with search engines. `cv2` (old interface in old OpenCV versions was named as `cv`) is the name that OpenCV developers chose when they created the binding generators. This is kept as the import name to be consistent with different kind of tutorials around the internet. Changing the import name or behaviour would be also confusing to experienced users who are accustomed to the ``import cv2``.\n\nDocumentation for opencv-python\n\n(https://github.com/opencv/opencv-python/actions/workflows/build_wheels_windows.yml)\n(https://github.com/opencv/opencv-python/actions/workflows/build_wheels_linux.yml)\n(https://github.com/opencv/opencv-python/actions/workflows/build_wheels_macos.yml)\n\nThe aim of this repository is to provide means to package each new [OpenCV release](https://github.com/opencv/opencv/releases) for the most used Python versions and platforms.\n\nCI build process\n\nThe project is structured like a normal Python package with a standard ``setup.py`` file.\nThe build process for a single entry in the build matrices is as follows (see for example `.github/workflows/build_wheels_linux.yml` file):\n\n0. In Linux and MacOS build: get OpenCV's optional C dependencies that we compile against\n\n1. Checkout repository and submodules\n\n-  OpenCV is included as submodule and the version is updated\n   -  Contrib modules are also included as a submodule\n\n2. Find OpenCV version from the sources\n\n3. Build OpenCV"}, {"name": "opencv-contrib-python", "tags": ["cli", "math", "ui"], "summary": "Wrapper package for OpenCV python bindings.", "text": "-  tests are disabled, otherwise build time increases too much\n   -  there are 4 build matrix entries for each build combination: with and without contrib modules, with and without GUI (headless)\n   -  Linux builds run in manylinux Docker containers (CentOS 5)\n   -  source distributions are separate entries in the build matrix\n\n4. Rearrange OpenCV's build result, add our custom files and generate wheel\n\n5. Linux and macOS wheels are transformed with auditwheel and delocate, correspondingly\n\n6. Install the generated wheel\n7. Test that Python can import the library and run some sanity checks\n8. Use twine to upload the generated wheel to PyPI (only in release builds)\n\nSteps 1--4 are handled by ``pip wheel``.\n\nThe build can be customized with environment variables. In addition to any variables that OpenCV's build accepts, we recognize:\n\n- ``CI_BUILD``. Set to ``1`` to emulate the CI environment build behaviour. Used only in CI builds to force certain build flags on in ``setup.py``. Do not use this unless you know what you are doing.\n- ``ENABLE_CONTRIB`` and ``ENABLE_HEADLESS``. Set to ``1`` to build the contrib and/or headless version\n- ``ENABLE_JAVA``, Set to ``1`` to enable the Java client build.  This is disabled by default.\n- ``CMAKE_ARGS``. Additional arguments for OpenCV's CMake invocation. You can use this to make a custom build.\n\nSee the next section for more info about manual builds outside the CI environment.\n\nManual builds\n\nIf some dependency is not enabled in the pre-built wheels, you can also run the build locally to create a custom wheel.\n\n1. Clone this repository: `git clone --recursive https://github.com/opencv/opencv-python.git`\n2. ``cd opencv-python``\n3. Add custom Cmake flags if needed, for example: `export CMAKE_ARGS=\"-DSOME_FLAG=ON -DSOME_OTHER_FLAG=OFF\"` (in Windows you need to set environment variables differently depending on Command Line or PowerShell)\n4. Select the package flavor which you wish to build with `ENABLE_CONTRIB` and `ENABLE_HEADLESS`: i.e. `export ENABLE_CONTRIB=1` if you wish to build `opencv-contrib-python`\n5. Run ``pip wheel . --verbose``. NOTE: make sure you have the latest ``pip`` version, the ``pip wheel`` command replaces the old ``python setup.py bdist_wheel`` command which does not support ``pyproject.toml``.\n6. Pip will print fresh wheel location at the end of build procedure. If you use old approach with `setup.py` file wheel package will be placed in `dist` folder. Package is ready and you can do with that whatever you wish.\n\nManual debug builds\n\nIn order to build `opencv-python` in an unoptimized debug build, you need to side-step the normal process a bit.\n\n1. Install the packages `scikit-build` and `numpy` via pip.\n2. Run the command `python setup.py bdist_wheel --build-type=Debug`.\n3. Install the generated wheel file in the `dist/` folder with `pip install dist/wheelname.whl`.\n\nIf you would like the build produce all compiler commands, then the following combination of flags and environment variables has been tested to work on Linux:\n\nSee this issue for more discussion: https://github.com/opencv/opencv-python/issues/424\n\nSource distributions"}, {"name": "opencv-contrib-python", "tags": ["cli", "math", "ui"], "summary": "Wrapper package for OpenCV python bindings.", "text": "Since OpenCV version 4.3.0, also source distributions are provided in PyPI. This means that if your system is not compatible with any of the wheels in PyPI, ``pip`` will attempt to build OpenCV from sources. If you need a OpenCV version which is not available in PyPI as a source distribution, please follow the manual build guidance above instead of this one.\n\nYou can also force ``pip`` to build the wheels from the source distribution. Some examples:\n\n- ``pip install --no-binary opencv-python opencv-python``\n- ``pip install --no-binary :all: opencv-python``\n\nIf you need contrib modules or headless version, just change the package name (step 4 in the previous section is not needed). However, any additional CMake flags can be provided via environment variables as described in step 3 of the manual build section. If none are provided, OpenCV's CMake scripts will attempt to find and enable any suitable dependencies. Headless distributions have hard coded CMake flags which disable all possible GUI dependencies.\n\nOn slow systems such as Raspberry Pi the full build may take several hours. On a 8-core Ryzen 7 3700X the build takes about 6 minutes.\n\nLicensing\n\nOpencv-python package (scripts in this repository) is available under MIT license.\n\nOpenCV itself is available under [Apache 2](https://github.com/opencv/opencv/blob/master/LICENSE) license.\n\nThird party package licenses are at [LICENSE-3RD-PARTY.txt](https://github.com/opencv/opencv-python/blob/master/LICENSE-3RD-PARTY.txt).\n\nAll wheels ship with [FFmpeg](http://ffmpeg.org) licensed under the [LGPLv2.1](http://www.gnu.org/licenses/old-licenses/lgpl-2.1.html).\n\nNon-headless Linux wheels ship with [Qt 5](http://doc.qt.io/qt-5/lgpl.html) licensed under the [LGPLv3](http://www.gnu.org/licenses/lgpl-3.0.html).\n\nThe packages include also other binaries. Full list of licenses can be found from [LICENSE-3RD-PARTY.txt](https://github.com/opencv/opencv-python/blob/master/LICENSE-3RD-PARTY.txt).\n\nVersioning\n\n``find_version.py`` script searches for the version information from OpenCV sources and appends also a revision number specific to this repository to the version string. It saves the version information to ``version.py`` file under ``cv2`` in addition to some other flags.\n\nReleases\n\nA release is made and uploaded to PyPI when a new tag is pushed to master branch. These tags differentiate packages (this repo might have modifications but OpenCV version stays same) and should be incremented sequentially. In practice, release version numbers look like this:\n\n``cv_major.cv_minor.cv_revision.package_revision`` e.g. ``3.1.0.0``\n\nThe master branch follows OpenCV master branch releases. 3.4 branch follows OpenCV 3.4 bugfix releases.\n\nDevelopment builds\n\nEvery commit to the master branch of this repo will be built. Possible build artifacts use local version identifiers:\n\n``cv_major.cv_minor.cv_revision+git_hash_of_this_repo`` e.g. ``3.1.0+14a8d39``\n\nThese artifacts can't be and will not be uploaded to PyPI.\n\nManylinux wheels\n\nLinux wheels are built using [manylinux2014](https://github.com/pypa/manylinux). These wheels should work out of the box for most of the distros (which use GNU C standard library) out there since they are built against an old version of glibc.\n\nThe default ``manylinux2014`` images have been extended with some OpenCV dependencies. See [Docker folder](https://github.com/skvark/opencv-python/tree/master/docker) for more info.\n\nSupported Python versions\n\nPython 3.x compatible pre-built wheels are provided for the officially supported Python versions (not in EOL):\n\n- 3.7\n- 3.8\n- 3.9\n- 3.10\n- 3.11\n- 3.12\n- 3.13\n\nBackward compatibility"}, {"name": "opencv-contrib-python", "tags": ["cli", "math", "ui"], "summary": "Wrapper package for OpenCV python bindings.", "text": "Starting from 4.2.0 and 3.4.9 builds the macOS Travis build environment was updated to XCode 9.4. The change effectively dropped support for older than 10.13 macOS versions.\n\nStarting from 4.3.0 and 3.4.10 builds the Linux build environment was updated from `manylinux1` to `manylinux2014`. This dropped support for old Linux distributions.\n\nStarting from version 4.7.0 the Mac OS GitHub Actions build environment was update to version 11. Mac OS 10.x support deprecated. See https://github.com/actions/runner-images/issues/5583\n\nStarting from version 4.9.0 the Mac OS GitHub Actions build environment was update to version 12. Mac OS 10.x support deprecated by Brew and most of used packages."}, {"name": "opencv-contrib-python", "tags": ["cli", "math", "ui"], "summary": "Wrapper package for OpenCV python bindings.", "text": "This library is used to enable developers to access and utilize OpenCV's computer vision capabilities in their Python projects, allowing for tasks such as image processing, object detection, and facial recognition. By using this library, developers can integrate the power of OpenCV into their applications with ease, without worrying about installing dependencies manually."}, {"name": "opencv-python-headless", "tags": ["cli", "math", "ui"], "summary": "Wrapper package for OpenCV python bindings.", "text": "Keep OpenCV Free\n\nOpenCV is raising funds to keep the library free for everyone, and we need the support of the entire community to do it. [Donate to OpenCV on Github](https://github.com/sponsors/opencv) to show your support.\n\n- [OpenCV on Wheels](#opencv-on-wheels)\n  - [Installation and Usage](#installation-and-usage)\n- [Frequently Asked Questions](#frequently-asked-questions)\n- [Documentation for opencv-python](#documentation-for-opencv-python)\n  - [CI build process](#ci-build-process)\n  - [Manual builds](#manual-builds)\n  - [Licensing](#licensing)\n  - [Versioning](#versioning)\n  - [Releases](#releases)\n  - [Development builds](#development-builds)\n  - [Manylinux wheels](#manylinux-wheels)\n  - [Supported Python versions](#supported-python-versions)\n  - [Backward compatibility](#backward-compatibility)\n\nOpenCV on Wheels\n\nPre-built CPU-only OpenCV packages for Python.\n\nCheck the manual build section if you wish to compile the bindings from source to enable additional modules such as CUDA.\n\nInstallation and Usage\n\n1. If you have previous/other manually installed (= not installed via ``pip``) version of OpenCV installed (e.g. cv2 module in the root of Python's site-packages), remove it before installation to avoid conflicts.\n2. Make sure that your `pip` version is up-to-date (19.3 is the minimum supported version): `pip install --upgrade pip`. Check version with `pip -V`. For example Linux distributions ship usually with very old `pip` versions which cause a lot of unexpected problems especially with the `manylinux` format.\n3. Select the correct package for your environment:\n\n4. Import the package:\n\n5. Read [OpenCV documentation](https://docs.opencv.org/master/)\n\n6. Before opening a new issue, read the FAQ below and have a look at the other issues which are already open.\n\nFrequently Asked Questions\n--------------------------\n\n**Q: Do I need to install also OpenCV separately?**\n\nA: No, the packages are special wheel binary packages and they already contain statically built OpenCV binaries.\n\n**Q: Pip install fails with ``ModuleNotFoundError: No module named 'skbuild'``?**\n\nSince ``opencv-python`` version 4.3.0.\\*, ``manylinux1`` wheels were replaced by ``manylinux2014`` wheels. If your pip is too old, it will try to use the new source distribution introduced in 4.3.0.38 to manually build OpenCV because it does not know how to install ``manylinux2014`` wheels. However, source build will also fail because of too old ``pip`` because it does not understand build dependencies in ``pyproject.toml``. To use the new ``manylinux2014`` pre-built wheels (or to build from source), your ``pip`` version must be >= 19.3. Please upgrade ``pip`` with ``pip install --upgrade pip``.\n\n**Q: Import fails on Windows: ``ImportError: DLL load failed: The specified module could not be found.``?**\n\nA: If the import fails on Windows, make sure you have [Visual C++ redistributable 2015](https://www.microsoft.com/en-us/download/details.aspx?id=48145) installed. If you are using older Windows version than Windows 10 and latest system updates are not installed, [Universal C Runtime](https://support.microsoft.com/en-us/help/2999226/update-for-universal-c-runtime-in-windows) might be also required.\n\nWindows N and KN editions do not include Media Feature Pack which is required by OpenCV. If you are using Windows N or KN edition, please install also [Windows Media Feature Pack](https://support.microsoft.com/en-us/help/3145500/media-feature-pack-list-for-windows-n-editions)."}, {"name": "opencv-python-headless", "tags": ["cli", "math", "ui"], "summary": "Wrapper package for OpenCV python bindings.", "text": "If you have Windows Server 2012+, media DLLs are probably missing too; please install the Feature called \"Media Foundation\" in the Server Manager. Beware, some posts advise to install \"Windows Server Essentials Media Pack\", but this one requires the \"Windows Server Essentials Experience\" role, and this role will deeply affect your Windows Server configuration (by enforcing active directory integration etc.); so just installing the \"Media Foundation\" should be a safer choice.\n\nIf the above does not help, check if you are using Anaconda. Old Anaconda versions have a bug which causes the error, see [this issue](https://github.com/opencv/opencv-python/issues/36) for a manual fix.\n\nIf you still encounter the error after you have checked all the previous solutions, download [Dependencies](https://github.com/lucasg/Dependencies) and open the ``cv2.pyd`` (located usually at ``C:\\Users\\username\\AppData\\Local\\Programs\\Python\\PythonXX\\Lib\\site-packages\\cv2``) file with it to debug missing DLL issues.\n\n**Q: I have some other import errors?**\n\nA: Make sure you have removed old manual installations of OpenCV Python bindings (cv2.so or cv2.pyd in site-packages).\n\n**Q: Function foo() or method bar() returns wrong result, throws exception or crashes interpreter. What should I do?**\n\nA: The repository contains only OpenCV-Python package build scripts, but not OpenCV itself. Python bindings for OpenCV are developed in official OpenCV repository and it's the best place to report issues. Also please check [OpenCV wiki](https://github.com/opencv/opencv/wiki) and [the official OpenCV forum](https://forum.opencv.org/) before file new bugs.\n\n**Q: Why the packages do not include non-free algorithms?**\n\nA: Non-free algorithms such as SURF are not included in these packages because they are patented / non-free and therefore cannot be distributed as built binaries. Note that SIFT is included in the builds due to patent expiration since OpenCV versions 4.3.0 and 3.4.10. See this issue for more info: https://github.com/skvark/opencv-python/issues/126\n\n**Q: Why the package and import are different (opencv-python vs. cv2)?**\n\nA: It's easier for users to understand ``opencv-python`` than ``cv2`` and it makes it easier to find the package with search engines. `cv2` (old interface in old OpenCV versions was named as `cv`) is the name that OpenCV developers chose when they created the binding generators. This is kept as the import name to be consistent with different kind of tutorials around the internet. Changing the import name or behaviour would be also confusing to experienced users who are accustomed to the ``import cv2``.\n\nDocumentation for opencv-python\n\n(https://github.com/opencv/opencv-python/actions/workflows/build_wheels_windows.yml)\n(https://github.com/opencv/opencv-python/actions/workflows/build_wheels_linux.yml)\n(https://github.com/opencv/opencv-python/actions/workflows/build_wheels_macos.yml)\n\nThe aim of this repository is to provide means to package each new [OpenCV release](https://github.com/opencv/opencv/releases) for the most used Python versions and platforms.\n\nCI build process\n\nThe project is structured like a normal Python package with a standard ``setup.py`` file.\nThe build process for a single entry in the build matrices is as follows (see for example `.github/workflows/build_wheels_linux.yml` file):\n\n0. In Linux and MacOS build: get OpenCV's optional C dependencies that we compile against\n\n1. Checkout repository and submodules\n\n-  OpenCV is included as submodule and the version is updated\n   -  Contrib modules are also included as a submodule\n\n2. Find OpenCV version from the sources\n\n3. Build OpenCV"}, {"name": "opencv-python-headless", "tags": ["cli", "math", "ui"], "summary": "Wrapper package for OpenCV python bindings.", "text": "-  tests are disabled, otherwise build time increases too much\n   -  there are 4 build matrix entries for each build combination: with and without contrib modules, with and without GUI (headless)\n   -  Linux builds run in manylinux Docker containers (CentOS 5)\n   -  source distributions are separate entries in the build matrix\n\n4. Rearrange OpenCV's build result, add our custom files and generate wheel\n\n5. Linux and macOS wheels are transformed with auditwheel and delocate, correspondingly\n\n6. Install the generated wheel\n7. Test that Python can import the library and run some sanity checks\n8. Use twine to upload the generated wheel to PyPI (only in release builds)\n\nSteps 1--4 are handled by ``pip wheel``.\n\nThe build can be customized with environment variables. In addition to any variables that OpenCV's build accepts, we recognize:\n\n- ``CI_BUILD``. Set to ``1`` to emulate the CI environment build behaviour. Used only in CI builds to force certain build flags on in ``setup.py``. Do not use this unless you know what you are doing.\n- ``ENABLE_CONTRIB`` and ``ENABLE_HEADLESS``. Set to ``1`` to build the contrib and/or headless version\n- ``ENABLE_JAVA``, Set to ``1`` to enable the Java client build.  This is disabled by default.\n- ``CMAKE_ARGS``. Additional arguments for OpenCV's CMake invocation. You can use this to make a custom build.\n\nSee the next section for more info about manual builds outside the CI environment.\n\nManual builds\n\nIf some dependency is not enabled in the pre-built wheels, you can also run the build locally to create a custom wheel.\n\n1. Clone this repository: `git clone --recursive https://github.com/opencv/opencv-python.git`\n2. ``cd opencv-python``\n3. Add custom Cmake flags if needed, for example: `export CMAKE_ARGS=\"-DSOME_FLAG=ON -DSOME_OTHER_FLAG=OFF\"` (in Windows you need to set environment variables differently depending on Command Line or PowerShell)\n4. Select the package flavor which you wish to build with `ENABLE_CONTRIB` and `ENABLE_HEADLESS`: i.e. `export ENABLE_CONTRIB=1` if you wish to build `opencv-contrib-python`\n5. Run ``pip wheel . --verbose``. NOTE: make sure you have the latest ``pip`` version, the ``pip wheel`` command replaces the old ``python setup.py bdist_wheel`` command which does not support ``pyproject.toml``.\n6. Pip will print fresh wheel location at the end of build procedure. If you use old approach with `setup.py` file wheel package will be placed in `dist` folder. Package is ready and you can do with that whatever you wish.\n\nManual debug builds\n\nIn order to build `opencv-python` in an unoptimized debug build, you need to side-step the normal process a bit.\n\n1. Install the packages `scikit-build` and `numpy` via pip.\n2. Run the command `python setup.py bdist_wheel --build-type=Debug`.\n3. Install the generated wheel file in the `dist/` folder with `pip install dist/wheelname.whl`.\n\nIf you would like the build produce all compiler commands, then the following combination of flags and environment variables has been tested to work on Linux:\n\nSee this issue for more discussion: https://github.com/opencv/opencv-python/issues/424\n\nSource distributions"}, {"name": "opencv-python-headless", "tags": ["cli", "math", "ui"], "summary": "Wrapper package for OpenCV python bindings.", "text": "Since OpenCV version 4.3.0, also source distributions are provided in PyPI. This means that if your system is not compatible with any of the wheels in PyPI, ``pip`` will attempt to build OpenCV from sources. If you need a OpenCV version which is not available in PyPI as a source distribution, please follow the manual build guidance above instead of this one.\n\nYou can also force ``pip`` to build the wheels from the source distribution. Some examples:\n\n- ``pip install --no-binary opencv-python opencv-python``\n- ``pip install --no-binary :all: opencv-python``\n\nIf you need contrib modules or headless version, just change the package name (step 4 in the previous section is not needed). However, any additional CMake flags can be provided via environment variables as described in step 3 of the manual build section. If none are provided, OpenCV's CMake scripts will attempt to find and enable any suitable dependencies. Headless distributions have hard coded CMake flags which disable all possible GUI dependencies.\n\nOn slow systems such as Raspberry Pi the full build may take several hours. On a 8-core Ryzen 7 3700X the build takes about 6 minutes.\n\nLicensing\n\nOpencv-python package (scripts in this repository) is available under MIT license.\n\nOpenCV itself is available under [Apache 2](https://github.com/opencv/opencv/blob/master/LICENSE) license.\n\nThird party package licenses are at [LICENSE-3RD-PARTY.txt](https://github.com/opencv/opencv-python/blob/master/LICENSE-3RD-PARTY.txt).\n\nAll wheels ship with [FFmpeg](http://ffmpeg.org) licensed under the [LGPLv2.1](http://www.gnu.org/licenses/old-licenses/lgpl-2.1.html).\n\nNon-headless Linux wheels ship with [Qt 5](http://doc.qt.io/qt-5/lgpl.html) licensed under the [LGPLv3](http://www.gnu.org/licenses/lgpl-3.0.html).\n\nThe packages include also other binaries. Full list of licenses can be found from [LICENSE-3RD-PARTY.txt](https://github.com/opencv/opencv-python/blob/master/LICENSE-3RD-PARTY.txt).\n\nVersioning\n\n``find_version.py`` script searches for the version information from OpenCV sources and appends also a revision number specific to this repository to the version string. It saves the version information to ``version.py`` file under ``cv2`` in addition to some other flags.\n\nReleases\n\nA release is made and uploaded to PyPI when a new tag is pushed to master branch. These tags differentiate packages (this repo might have modifications but OpenCV version stays same) and should be incremented sequentially. In practice, release version numbers look like this:\n\n``cv_major.cv_minor.cv_revision.package_revision`` e.g. ``3.1.0.0``\n\nThe master branch follows OpenCV master branch releases. 3.4 branch follows OpenCV 3.4 bugfix releases.\n\nDevelopment builds\n\nEvery commit to the master branch of this repo will be built. Possible build artifacts use local version identifiers:\n\n``cv_major.cv_minor.cv_revision+git_hash_of_this_repo`` e.g. ``3.1.0+14a8d39``\n\nThese artifacts can't be and will not be uploaded to PyPI.\n\nManylinux wheels\n\nLinux wheels are built using [manylinux2014](https://github.com/pypa/manylinux). These wheels should work out of the box for most of the distros (which use GNU C standard library) out there since they are built against an old version of glibc.\n\nThe default ``manylinux2014`` images have been extended with some OpenCV dependencies. See [Docker folder](https://github.com/skvark/opencv-python/tree/master/docker) for more info.\n\nSupported Python versions\n\nPython 3.x compatible pre-built wheels are provided for the officially supported Python versions (not in EOL):\n\n- 3.7\n- 3.8\n- 3.9\n- 3.10\n- 3.11\n- 3.12\n- 3.13\n\nBackward compatibility"}, {"name": "opencv-python-headless", "tags": ["cli", "math", "ui"], "summary": "Wrapper package for OpenCV python bindings.", "text": "Starting from 4.2.0 and 3.4.9 builds the macOS Travis build environment was updated to XCode 9.4. The change effectively dropped support for older than 10.13 macOS versions.\n\nStarting from 4.3.0 and 3.4.10 builds the Linux build environment was updated from `manylinux1` to `manylinux2014`. This dropped support for old Linux distributions.\n\nStarting from version 4.7.0 the Mac OS GitHub Actions build environment was update to version 11. Mac OS 10.x support deprecated. See https://github.com/actions/runner-images/issues/5583\n\nStarting from version 4.9.0 the Mac OS GitHub Actions build environment was update to version 12. Mac OS 10.x support deprecated by Brew and most of used packages."}, {"name": "opencv-python-headless", "tags": ["cli", "math", "ui"], "summary": "Wrapper package for OpenCV python bindings.", "text": "This library is used to install and use the OpenCV computer vision library in a headless (non-interactive) environment. With this package, developers can easily integrate OpenCV's image processing and analysis capabilities into their Python applications without requiring a graphical interface."}, {"name": "opencv-python", "tags": ["cli", "math", "ui"], "summary": "Wrapper package for OpenCV python bindings.", "text": "Keep OpenCV Free\n\nOpenCV is raising funds to keep the library free for everyone, and we need the support of the entire community to do it. [Donate to OpenCV on Github](https://github.com/sponsors/opencv) to show your support.\n\n- [OpenCV on Wheels](#opencv-on-wheels)\n  - [Installation and Usage](#installation-and-usage)\n- [Frequently Asked Questions](#frequently-asked-questions)\n- [Documentation for opencv-python](#documentation-for-opencv-python)\n  - [CI build process](#ci-build-process)\n  - [Manual builds](#manual-builds)\n  - [Licensing](#licensing)\n  - [Versioning](#versioning)\n  - [Releases](#releases)\n  - [Development builds](#development-builds)\n  - [Manylinux wheels](#manylinux-wheels)\n  - [Supported Python versions](#supported-python-versions)\n  - [Backward compatibility](#backward-compatibility)\n\nOpenCV on Wheels\n\nPre-built CPU-only OpenCV packages for Python.\n\nCheck the manual build section if you wish to compile the bindings from source to enable additional modules such as CUDA.\n\nInstallation and Usage\n\n1. If you have previous/other manually installed (= not installed via ``pip``) version of OpenCV installed (e.g. cv2 module in the root of Python's site-packages), remove it before installation to avoid conflicts.\n2. Make sure that your `pip` version is up-to-date (19.3 is the minimum supported version): `pip install --upgrade pip`. Check version with `pip -V`. For example Linux distributions ship usually with very old `pip` versions which cause a lot of unexpected problems especially with the `manylinux` format.\n3. Select the correct package for your environment:\n\n4. Import the package:\n\n5. Read [OpenCV documentation](https://docs.opencv.org/master/)\n\n6. Before opening a new issue, read the FAQ below and have a look at the other issues which are already open.\n\nFrequently Asked Questions\n--------------------------\n\n**Q: Do I need to install also OpenCV separately?**\n\nA: No, the packages are special wheel binary packages and they already contain statically built OpenCV binaries.\n\n**Q: Pip install fails with ``ModuleNotFoundError: No module named 'skbuild'``?**\n\nSince ``opencv-python`` version 4.3.0.\\*, ``manylinux1`` wheels were replaced by ``manylinux2014`` wheels. If your pip is too old, it will try to use the new source distribution introduced in 4.3.0.38 to manually build OpenCV because it does not know how to install ``manylinux2014`` wheels. However, source build will also fail because of too old ``pip`` because it does not understand build dependencies in ``pyproject.toml``. To use the new ``manylinux2014`` pre-built wheels (or to build from source), your ``pip`` version must be >= 19.3. Please upgrade ``pip`` with ``pip install --upgrade pip``.\n\n**Q: Import fails on Windows: ``ImportError: DLL load failed: The specified module could not be found.``?**\n\nA: If the import fails on Windows, make sure you have [Visual C++ redistributable 2015](https://www.microsoft.com/en-us/download/details.aspx?id=48145) installed. If you are using older Windows version than Windows 10 and latest system updates are not installed, [Universal C Runtime](https://support.microsoft.com/en-us/help/2999226/update-for-universal-c-runtime-in-windows) might be also required.\n\nWindows N and KN editions do not include Media Feature Pack which is required by OpenCV. If you are using Windows N or KN edition, please install also [Windows Media Feature Pack](https://support.microsoft.com/en-us/help/3145500/media-feature-pack-list-for-windows-n-editions)."}, {"name": "opencv-python", "tags": ["cli", "math", "ui"], "summary": "Wrapper package for OpenCV python bindings.", "text": "If you have Windows Server 2012+, media DLLs are probably missing too; please install the Feature called \"Media Foundation\" in the Server Manager. Beware, some posts advise to install \"Windows Server Essentials Media Pack\", but this one requires the \"Windows Server Essentials Experience\" role, and this role will deeply affect your Windows Server configuration (by enforcing active directory integration etc.); so just installing the \"Media Foundation\" should be a safer choice.\n\nIf the above does not help, check if you are using Anaconda. Old Anaconda versions have a bug which causes the error, see [this issue](https://github.com/opencv/opencv-python/issues/36) for a manual fix.\n\nIf you still encounter the error after you have checked all the previous solutions, download [Dependencies](https://github.com/lucasg/Dependencies) and open the ``cv2.pyd`` (located usually at ``C:\\Users\\username\\AppData\\Local\\Programs\\Python\\PythonXX\\Lib\\site-packages\\cv2``) file with it to debug missing DLL issues.\n\n**Q: I have some other import errors?**\n\nA: Make sure you have removed old manual installations of OpenCV Python bindings (cv2.so or cv2.pyd in site-packages).\n\n**Q: Function foo() or method bar() returns wrong result, throws exception or crashes interpreter. What should I do?**\n\nA: The repository contains only OpenCV-Python package build scripts, but not OpenCV itself. Python bindings for OpenCV are developed in official OpenCV repository and it's the best place to report issues. Also please check [OpenCV wiki](https://github.com/opencv/opencv/wiki) and [the official OpenCV forum](https://forum.opencv.org/) before file new bugs.\n\n**Q: Why the packages do not include non-free algorithms?**\n\nA: Non-free algorithms such as SURF are not included in these packages because they are patented / non-free and therefore cannot be distributed as built binaries. Note that SIFT is included in the builds due to patent expiration since OpenCV versions 4.3.0 and 3.4.10. See this issue for more info: https://github.com/skvark/opencv-python/issues/126\n\n**Q: Why the package and import are different (opencv-python vs. cv2)?**\n\nA: It's easier for users to understand ``opencv-python`` than ``cv2`` and it makes it easier to find the package with search engines. `cv2` (old interface in old OpenCV versions was named as `cv`) is the name that OpenCV developers chose when they created the binding generators. This is kept as the import name to be consistent with different kind of tutorials around the internet. Changing the import name or behaviour would be also confusing to experienced users who are accustomed to the ``import cv2``.\n\nDocumentation for opencv-python\n\n(https://github.com/opencv/opencv-python/actions/workflows/build_wheels_windows.yml)\n(https://github.com/opencv/opencv-python/actions/workflows/build_wheels_linux.yml)\n(https://github.com/opencv/opencv-python/actions/workflows/build_wheels_macos.yml)\n\nThe aim of this repository is to provide means to package each new [OpenCV release](https://github.com/opencv/opencv/releases) for the most used Python versions and platforms.\n\nCI build process\n\nThe project is structured like a normal Python package with a standard ``setup.py`` file.\nThe build process for a single entry in the build matrices is as follows (see for example `.github/workflows/build_wheels_linux.yml` file):\n\n0. In Linux and MacOS build: get OpenCV's optional C dependencies that we compile against\n\n1. Checkout repository and submodules\n\n-  OpenCV is included as submodule and the version is updated\n   -  Contrib modules are also included as a submodule\n\n2. Find OpenCV version from the sources\n\n3. Build OpenCV"}, {"name": "opencv-python", "tags": ["cli", "math", "ui"], "summary": "Wrapper package for OpenCV python bindings.", "text": "-  tests are disabled, otherwise build time increases too much\n   -  there are 4 build matrix entries for each build combination: with and without contrib modules, with and without GUI (headless)\n   -  Linux builds run in manylinux Docker containers (CentOS 5)\n   -  source distributions are separate entries in the build matrix\n\n4. Rearrange OpenCV's build result, add our custom files and generate wheel\n\n5. Linux and macOS wheels are transformed with auditwheel and delocate, correspondingly\n\n6. Install the generated wheel\n7. Test that Python can import the library and run some sanity checks\n8. Use twine to upload the generated wheel to PyPI (only in release builds)\n\nSteps 1--4 are handled by ``pip wheel``.\n\nThe build can be customized with environment variables. In addition to any variables that OpenCV's build accepts, we recognize:\n\n- ``CI_BUILD``. Set to ``1`` to emulate the CI environment build behaviour. Used only in CI builds to force certain build flags on in ``setup.py``. Do not use this unless you know what you are doing.\n- ``ENABLE_CONTRIB`` and ``ENABLE_HEADLESS``. Set to ``1`` to build the contrib and/or headless version\n- ``ENABLE_JAVA``, Set to ``1`` to enable the Java client build.  This is disabled by default.\n- ``CMAKE_ARGS``. Additional arguments for OpenCV's CMake invocation. You can use this to make a custom build.\n\nSee the next section for more info about manual builds outside the CI environment.\n\nManual builds\n\nIf some dependency is not enabled in the pre-built wheels, you can also run the build locally to create a custom wheel.\n\n1. Clone this repository: `git clone --recursive https://github.com/opencv/opencv-python.git`\n2. ``cd opencv-python``\n3. Add custom Cmake flags if needed, for example: `export CMAKE_ARGS=\"-DSOME_FLAG=ON -DSOME_OTHER_FLAG=OFF\"` (in Windows you need to set environment variables differently depending on Command Line or PowerShell)\n4. Select the package flavor which you wish to build with `ENABLE_CONTRIB` and `ENABLE_HEADLESS`: i.e. `export ENABLE_CONTRIB=1` if you wish to build `opencv-contrib-python`\n5. Run ``pip wheel . --verbose``. NOTE: make sure you have the latest ``pip`` version, the ``pip wheel`` command replaces the old ``python setup.py bdist_wheel`` command which does not support ``pyproject.toml``.\n6. Pip will print fresh wheel location at the end of build procedure. If you use old approach with `setup.py` file wheel package will be placed in `dist` folder. Package is ready and you can do with that whatever you wish.\n\nManual debug builds\n\nIn order to build `opencv-python` in an unoptimized debug build, you need to side-step the normal process a bit.\n\n1. Install the packages `scikit-build` and `numpy` via pip.\n2. Run the command `python setup.py bdist_wheel --build-type=Debug`.\n3. Install the generated wheel file in the `dist/` folder with `pip install dist/wheelname.whl`.\n\nIf you would like the build produce all compiler commands, then the following combination of flags and environment variables has been tested to work on Linux:\n\nSee this issue for more discussion: https://github.com/opencv/opencv-python/issues/424\n\nSource distributions"}, {"name": "opencv-python", "tags": ["cli", "math", "ui"], "summary": "Wrapper package for OpenCV python bindings.", "text": "Since OpenCV version 4.3.0, also source distributions are provided in PyPI. This means that if your system is not compatible with any of the wheels in PyPI, ``pip`` will attempt to build OpenCV from sources. If you need a OpenCV version which is not available in PyPI as a source distribution, please follow the manual build guidance above instead of this one.\n\nYou can also force ``pip`` to build the wheels from the source distribution. Some examples:\n\n- ``pip install --no-binary opencv-python opencv-python``\n- ``pip install --no-binary :all: opencv-python``\n\nIf you need contrib modules or headless version, just change the package name (step 4 in the previous section is not needed). However, any additional CMake flags can be provided via environment variables as described in step 3 of the manual build section. If none are provided, OpenCV's CMake scripts will attempt to find and enable any suitable dependencies. Headless distributions have hard coded CMake flags which disable all possible GUI dependencies.\n\nOn slow systems such as Raspberry Pi the full build may take several hours. On a 8-core Ryzen 7 3700X the build takes about 6 minutes.\n\nLicensing\n\nOpencv-python package (scripts in this repository) is available under MIT license.\n\nOpenCV itself is available under [Apache 2](https://github.com/opencv/opencv/blob/master/LICENSE) license.\n\nThird party package licenses are at [LICENSE-3RD-PARTY.txt](https://github.com/opencv/opencv-python/blob/master/LICENSE-3RD-PARTY.txt).\n\nAll wheels ship with [FFmpeg](http://ffmpeg.org) licensed under the [LGPLv2.1](http://www.gnu.org/licenses/old-licenses/lgpl-2.1.html).\n\nNon-headless Linux wheels ship with [Qt 5](http://doc.qt.io/qt-5/lgpl.html) licensed under the [LGPLv3](http://www.gnu.org/licenses/lgpl-3.0.html).\n\nThe packages include also other binaries. Full list of licenses can be found from [LICENSE-3RD-PARTY.txt](https://github.com/opencv/opencv-python/blob/master/LICENSE-3RD-PARTY.txt).\n\nVersioning\n\n``find_version.py`` script searches for the version information from OpenCV sources and appends also a revision number specific to this repository to the version string. It saves the version information to ``version.py`` file under ``cv2`` in addition to some other flags.\n\nReleases\n\nA release is made and uploaded to PyPI when a new tag is pushed to master branch. These tags differentiate packages (this repo might have modifications but OpenCV version stays same) and should be incremented sequentially. In practice, release version numbers look like this:\n\n``cv_major.cv_minor.cv_revision.package_revision`` e.g. ``3.1.0.0``\n\nThe master branch follows OpenCV master branch releases. 3.4 branch follows OpenCV 3.4 bugfix releases.\n\nDevelopment builds\n\nEvery commit to the master branch of this repo will be built. Possible build artifacts use local version identifiers:\n\n``cv_major.cv_minor.cv_revision+git_hash_of_this_repo`` e.g. ``3.1.0+14a8d39``\n\nThese artifacts can't be and will not be uploaded to PyPI.\n\nManylinux wheels\n\nLinux wheels are built using [manylinux2014](https://github.com/pypa/manylinux). These wheels should work out of the box for most of the distros (which use GNU C standard library) out there since they are built against an old version of glibc.\n\nThe default ``manylinux2014`` images have been extended with some OpenCV dependencies. See [Docker folder](https://github.com/skvark/opencv-python/tree/master/docker) for more info.\n\nSupported Python versions\n\nPython 3.x compatible pre-built wheels are provided for the officially supported Python versions (not in EOL):\n\n- 3.7\n- 3.8\n- 3.9\n- 3.10\n- 3.11\n- 3.12\n- 3.13\n\nBackward compatibility"}, {"name": "opencv-python", "tags": ["cli", "math", "ui"], "summary": "Wrapper package for OpenCV python bindings.", "text": "Starting from 4.2.0 and 3.4.9 builds the macOS Travis build environment was updated to XCode 9.4. The change effectively dropped support for older than 10.13 macOS versions.\n\nStarting from 4.3.0 and 3.4.10 builds the Linux build environment was updated from `manylinux1` to `manylinux2014`. This dropped support for old Linux distributions.\n\nStarting from version 4.7.0 the Mac OS GitHub Actions build environment was update to version 11. Mac OS 10.x support deprecated. See https://github.com/actions/runner-images/issues/5583\n\nStarting from version 4.9.0 the Mac OS GitHub Actions build environment was update to version 12. Mac OS 10.x support deprecated by Brew and most of used packages."}, {"name": "opencv-python", "tags": ["cli", "math", "ui"], "summary": "Wrapper package for OpenCV python bindings.", "text": "This library is used to leverage the OpenCV computer vision capabilities in Python, enabling developers to build applications with features such as image and video processing, object detection, and facial recognition. With this library, developers can easily integrate OpenCV's robust functionality into their projects, speeding up development time and enhancing application performance."}, {"name": "optax", "tags": ["dev", "math", "ml", "web"], "summary": "A gradient processing and optimization library in JAX.", "text": "Optax\n\n(http://optax.readthedocs.io)\n\nIntroduction\n\nOptax is a gradient processing and optimization library for JAX.\n\nOptax is designed to facilitate research by providing building blocks\nthat can be easily recombined in custom ways.\n\nOur goals are to\n\n*   Provide simple, well-tested, efficient implementations of core components.\n*   Improve research productivity by enabling to easily combine low-level\n*   Accelerate adoption of new ideas by making it easy for anyone to contribute.\n\nWe favor focusing on small composable building blocks that can be effectively\ncombined into custom solutions. Others may build upon these basic components\nin more complicated abstractions. Whenever reasonable, implementations prioritize\nreadability and structuring code to match standard equations, over code reuse.\n\nAn initial prototype of this library was made available in JAX's experimental\nfolder as `jax.experimental.optix`. Given the wide adoption across DeepMind\nof `optix`, and after a few iterations on the API, `optix` was eventually moved\nout of `experimental` as a standalone open-source library, and renamed `optax`.\n\nDocumentation on Optax can be found at [optax.readthedocs.io](https://optax.readthedocs.io/).\n\nInstallation\n\nYou can install the latest released version of Optax from PyPI via:\n\nor you can install the latest development version from GitHub:\n\nQuickstart\n\nOptax contains implementations of [many popular optimizers](https://optax.readthedocs.io/en/latest/api/optimizers.html) and\n[loss functions](https://optax.readthedocs.io/en/latest/api/losses.html).\nFor example, the following code snippet uses the Adam optimizer from `optax.adam`\nand the mean squared error from `optax.l2_loss`. We initialize the optimizer\nstate using the `init` function and `params` of the model.\n\nTo write the update loop we need a loss function that can be differentiated by\nJax (with `jax.grad` in this\nexample) to obtain the gradients.\n\nThe gradients are then converted via `optimizer.update` to obtain the updates\nthat should be applied to the current parameters to obtain the new ones.\n`optax.apply_updates` is a convenience utility to do this.\n\nYou can continue the quick start in [the Optax  Getting started notebook.](https://github.com/google-deepmind/optax/blob/main/docs/getting_started.ipynb)\n\nDevelopment\n\nWe welcome new contributors.\n\nSource code\n\nYou can check the latest sources with the following command.\n\nTesting\n\nTo run the tests, please execute the following script.\n\nDocumentation\n\nTo build the documentation, first ensure that all the dependencies are installed.\n\nThen, execute the following.\n\nBenchmarks\nIf you feel lost in the crowd of available optimizers for deep learning, there\nexist some extensive benchmarks:\n\n[Benchmarking Neural Network Training Algorithms, Dahl G. et al, 2023](https://arxiv.org/pdf/2306.07179),\n\n[Descending through a Crowded Valley \u2014 Benchmarking Deep Learning Optimizers, Schmidt R. et al, 2021](https://proceedings.mlr.press/v139/schmidt21a).\n\nIf you are interested in developing your own benchmark for some tasks,\nconsider the following framework\n\n[Benchopt: Reproducible, efficient and collaborative optimization benchmarks, Moreau T. et al, 2022](https://arxiv.org/abs/2206.13424).\n\nFinally, if you are searching for some recommendations on tuning optimizers,\nconsider taking a look at\n\n[Deep Learning Tuning Playbook, Godbole V. et al, 2023](https://github.com/google-research/tuning_playbook).\n\nCiting Optax\n\nThis repository is part of the DeepMind JAX Ecosystem, to cite Optax\nplease use the citation:"}, {"name": "optax", "tags": ["dev", "math", "ml", "web"], "summary": "A gradient processing and optimization library in JAX.", "text": "This library is used to process gradients and optimize models in JAX, enabling developers to efficiently combine low-level building blocks into custom solutions for research and development. With optax, researchers can improve productivity and accelerate the adoption of new ideas by leveraging simple, well-tested, and efficient implementations of core optimization components."}, {"name": "optimum", "tags": ["cli", "math", "ml", "web"], "summary": "Optimum Library is an extension of the Hugging Face Transformers library, providing a framework to integrate third-party libraries from Hardware Partners and interface with their specific functionality.", "text": "Installation\n\nOptimum can be installed using `pip` as follows:\n\nIf you'd like to use the accelerator-specific features of Optimum, you can check the documentation and install the required dependencies according to the table below:\n\nAccelerator\n:----------------------------------------------------------------------------------\n[ONNX](https://huggingface.co/docs/optimum-onnx/en/index)\n[ONNX Runtime](https://huggingface.co/docs/optimum-onnx/onnxruntime/overview)\n[ONNX Runtime GPU](https://huggingface.co/docs/optimum-onnx/onnxruntime/overview)\n[Intel Neural Compressor](https://huggingface.co/docs/optimum/intel/index)\n[OpenVINO](https://huggingface.co/docs/optimum/intel/index)\n[IPEX](https://huggingface.co/docs/optimum/intel/index)\n[NVIDIA TensorRT-LLM](https://huggingface.co/docs/optimum/main/en/nvidia_overview)\n[AMD Instinct GPUs and Ryzen AI NPU](https://huggingface.co/docs/optimum/amd/index)\n[AWS Trainum & Inferentia](https://huggingface.co/docs/optimum-neuron/index)\n[Intel Gaudi Accelerators (HPU)](https://huggingface.co/docs/optimum/habana/index)\n[FuriosaAI](https://huggingface.co/docs/optimum/furiosa/index)\n\nThe `--upgrade --upgrade-strategy eager` option is needed to ensure the different packages are upgraded to the latest possible version.\n\nTo install from source:\n\nFor the accelerator-specific features, append `optimum[accelerator_type]` to the above command:\n\nAccelerated Inference\n\nOptimum provides multiple tools to export and run optimized models on various ecosystems:\n\nThe [export](https://huggingface.co/docs/optimum/exporters/overview) and optimizations can be done both programmatically and with a command line.\n\nONNX + ONNX Runtime\n\n ONNX integration was moved to [`optimum-onnx`](https://github.com/huggingface/optimum-onnx) so make sure to follow the installation instructions \n\nBefore you begin, make sure you have all the necessary libraries installed :\n\nIt is possible to export Transformers, Diffusers, Sentence Transformers and Timm models to the [ONNX](https://onnx.ai/) format and perform graph optimization as well as quantization easily.\n\nFor more information on the ONNX export, please check the [documentation](https://huggingface.co/docs/optimum-onnx/en/onnx/usage_guides/export_a_model).\n\nOnce the model is exported to the ONNX format, we provide Python classes enabling you to run the exported ONNX model in a seamless manner using [ONNX Runtime](https://onnxruntime.ai/) in the backend.\n\nFor this make sure you have ONNX Runtime installed, fore more information check out the [installation instructions](https://onnxruntime.ai/docs/install/).\n\nMore details on how to run ONNX models with `ORTModelForXXX` classes [here](https://huggingface.co/docs/optimum-onnx/en/onnxruntime/usage_guides/models).\n\nIntel (OpenVINO + Neural Compressor + IPEX)\n\nBefore you begin, make sure you have all the necessary [libraries installed](https://huggingface.co/docs/optimum/main/en/intel/installation).\n\nYou can find more information on the different integration in our [documentation](https://huggingface.co/docs/optimum/main/en/intel/index) and in the examples of [`optimum-intel`](https://github.com/huggingface/optimum-intel).\n\nExecuTorch\n\nBefore you begin, make sure you have all the necessary libraries installed :\n\nUsers can export Transformers models to [ExecuTorch](https://github.com/pytorch/executorch) and run inference on edge devices within PyTorch's ecosystem.\n\nFor more information about export Transformers to ExecuTorch, please check the doc for [Optimum-ExecuTorch](https://huggingface.co/docs/optimum-executorch/guides/export).\n\nQuanto\n\n[Quanto](https://github.com/huggingface/optimum-quanto) is a pytorch quantization backend which allows you to quantize a model either using the python API or the `optimum-cli`.\n\nYou can see more details and [examples](https://github.com/huggingface/optimum-quanto/tree/main/examples) in the [Quanto](https://github.com/huggingface/optimum-quanto) repository.\n\nAccelerated training\n\nOptimum provides wrappers around the original Transformers [Trainer](https://huggingface.co/docs/transformers/main_classes/trainer) to enable training on powerful hardware easily.\nWe support many providers:\n\n- [Intel Gaudi Accelerators (HPU)](https://huggingface.co/docs/optimum/main/en/habana/usage_guides/accelerate_training) enabling optimal performance on first-gen Gaudi, Gaudi2 and Gaudi3.\n- [AWS Trainium](https://huggingface.co/docs/optimum-neuron/training_tutorials/sft_lora_finetune_llm) for accelerated training on Trn1 and Trn1n instances.\n- ONNX Runtime (optimized for GPUs).\n\nIntel Gaudi Accelerators\n\nBefore you begin, make sure you have all the necessary libraries installed :\n\nYou can find examples in the [documentation](https://huggingface.co/docs/optimum/habana/quickstart) and in the [examples](https://github.com/huggingface/optimum-habana/tree/main/examples).\n\nAWS Trainium\n\nBefore you begin, make sure you have all the necessary libraries installed :\n\nYou can find examples in the [documentation](https://huggingface.co/docs/optimum-neuron/index) and in the [tutorials](https://huggingface.co/docs/optimum-neuron/tutorials/fine_tune_bert)."}, {"name": "optimum", "tags": ["cli", "math", "ml", "web"], "summary": "Optimum Library is an extension of the Hugging Face Transformers library, providing a framework to integrate third-party libraries from Hardware Partners and interface with their specific functionality.", "text": "This library is used to integrate third-party hardware accelerators with Hugging Face Transformers, enabling developers to leverage specialized hardware for efficient and high-performance computing. By using Optimum, developers can interface with various accelerator-specific functionalities from Hardware Partners to optimize their models' performance."}, {"name": "optuna-integration", "tags": ["math", "ml", "web"], "summary": "Integration libraries of Optuna.", "text": "Optuna-Integration\n\n(https://www.python.org)\n(https://pypi.python.org/pypi/optuna-integration)\n(https://anaconda.org/conda-forge/optuna-integration)\n(https://github.com/optuna/optuna-integration)\n(https://optuna-integration.readthedocs.io/en/stable/)\n\nThis package is an integration module of [Optuna](https://github.com/optuna/optuna), an automatic Hyperparameter optimization software framework.\nThe modules in this package provide users with extended functionalities for Optuna in combination with third-party libraries such as PyTorch, sklearn, and TensorFlow.\n\n> [!NOTE]\n> You can find more information in [**our official documentations**](https://optuna-integration.readthedocs.io/en/stable/) and [**API reference**](https://optuna-integration.readthedocs.io/en/stable/reference/index.html).\n\nInstallation\n\nOptuna-Integration is available via [pip](https://pypi.org/project/optuna-integration/) and\non [conda](https://anaconda.org/conda-forge/optuna-integration).\n\n> [!IMPORTANT]\n> As dependencies of all the modules are large and complicated, the commands above install only the common dependencies.\n> Dependencies for each module can be installed via pip.\n> For example, if you would like to install the dependencies of `optuna_integration.botorch` and `optuna_integration.lightgbm`, you can install them via:\n> \n\n> [!NOTE]\n> Optuna-Integration supports from Python 3.9 to Python 3.13.\n> Optuna Docker image is also provided at [DockerHub](https://hub.docker.com/r/optuna/optuna).\n\nIntegration Modules\n\nHere is the table of `optuna-integration` modules:\n\nThird Party Library\n:--\n[BoTorch](https://optuna-integration.readthedocs.io/en/stable/reference/index.html#botorch)\n[CatBoost](https://optuna-integration.readthedocs.io/en/stable/reference/index.html#catboost)\n[Dask](https://optuna-integration.readthedocs.io/en/stable/reference/index.html#dask)\n[FastAI](https://optuna-integration.readthedocs.io/en/stable/reference/index.html#fast-ai)\n[Keras](https://optuna-integration.readthedocs.io/en/stable/reference/index.html#keras)\n[LightGBM](https://optuna-integration.readthedocs.io/en/stable/reference/index.html#lightgbm)\n[MLflow](https://optuna-integration.readthedocs.io/en/stable/reference/index.html#mlflow)\n[PyTorch Distributed](https://optuna-integration.readthedocs.io/en/stable/reference/index.html#pytorch)\n[PyTorch Ignite](https://optuna-integration.readthedocs.io/en/stable/reference/index.html#pytorch)\n[PyTorch Lightning](https://optuna-integration.readthedocs.io/en/stable/reference/index.html#pytorch)\n[pycma](https://optuna-integration.readthedocs.io/en/stable/reference/index.html#pycma)\n[SHAP](https://optuna-integration.readthedocs.io/en/stable/reference/index.html#shap)\n[scikit-learn](https://optuna-integration.readthedocs.io/en/stable/reference/index.html#sklearn)\n[skorch](https://optuna-integration.readthedocs.io/en/stable/reference/index.html#skorch)\n[TensorBoard](https://optuna-integration.readthedocs.io/en/stable/reference/index.html#tensorboard)\n[tf.keras](https://optuna-integration.readthedocs.io/en/stable/reference/index.html#tensorflow)\n[Weights & Biases](https://optuna-integration.readthedocs.io/en/stable/reference/index.html#wandb)\n[XGBoost](https://optuna-integration.readthedocs.io/en/stable/reference/index.html#xgboost)\n[AllenNLP](https://optuna-integration.readthedocs.io/en/stable/reference/index.html#allennlp)*\n[Chainer](https://optuna-integration.readthedocs.io/en/stable/reference/index.html#chainer)*\n[ChainerMN](https://optuna-integration.readthedocs.io/en/stable/reference/index.html#chainermn)*\n[MXNet](https://optuna-integration.readthedocs.io/en/stable/reference/index.html#mxnet)*\n\n> [!WARNING]\n> `*` shows deprecated modules and they might be removed in the future.\n\nCommunication\n\n* [GitHub Discussions] for questions.\n* [GitHub Issues] for bug reports and feature requests.\n\nContribution\n\nAny contributions to Optuna-Integration are more than welcome!\n\nFor general guidelines how to contribute to the project, take a look at [CONTRIBUTING.md](./CONTRIBUTING.md).\n\nReference\n\nIf you use Optuna in one of your research projects, please cite [our KDD paper](https://doi.org/10.1145/3292500.3330701) \"Optuna: A Next-generation Hyperparameter Optimization Framework\":\n\nBibTeX"}, {"name": "optuna-integration", "tags": ["math", "ml", "web"], "summary": "Integration libraries of Optuna.", "text": "This library is used to extend the functionalities of Optuna with third-party libraries, providing a seamless integration for hyperparameter optimization. With this library, developers can easily incorporate Optuna's automatic hyperparameter tuning capabilities into their projects using PyTorch, sklearn, and TensorFlow."}, {"name": "optuna", "tags": ["math", "ml", "visualization", "web"], "summary": "A hyperparameter optimization framework", "text": "Optuna: A hyperparameter optimization framework\n\n(https://www.python.org)\n(https://pypi.python.org/pypi/optuna)\n(https://anaconda.org/conda-forge/optuna)\n(https://github.com/optuna/optuna)\n(https://optuna.readthedocs.io/en/stable/)\n(https://codecov.io/gh/optuna/optuna)\n\n:link: [**Website**](https://optuna.org/)\n:page_with_curl: [**Docs**](https://optuna.readthedocs.io/en/stable/)\n:gear: [**Install Guide**](https://optuna.readthedocs.io/en/stable/installation.html)\n:pencil: [**Tutorial**](https://optuna.readthedocs.io/en/stable/tutorial/index.html)\n:bulb: [**Examples**](https://github.com/optuna/optuna-examples)\n[**Twitter**](https://twitter.com/OptunaAutoML)\n[**LinkedIn**](https://www.linkedin.com/showcase/optuna/)\n[**Medium**](https://medium.com/optuna)\n\n*Optuna* is an automatic hyperparameter optimization software framework, particularly designed\nfor machine learning. It features an imperative, *define-by-run* style user API. Thanks to our\n*define-by-run* API, the code written with Optuna enjoys high modularity, and the user of\nOptuna can dynamically construct the search spaces for the hyperparameters.\n\n:loudspeaker: News\nHelp us create the next version of Optuna!\n\nOptuna 5.0 Roadmap published for review. Please take a look at [the planned improvements to Optuna](https://medium.com/optuna/optuna-v5-roadmap-ac7d6935a878), and share your feedback in [the github issues](https://github.com/optuna/optuna/labels/v5). PR contributions also welcome!\n\nPlease take a few minutes to fill in [this survey](https://forms.gle/wVwLCQ9g6st6AXuq9), and let us know how you use Optuna now and what improvements you'd like.\nAll questions are optional. \u200d\u2642\ufe0f\n\n* **Oct 28, 2025**: A new article [AutoSampler: Full Support for Multi-Objective & Constrained Optimization](https://medium.com/optuna/autosampler-full-support-for-multi-objective-constrained-optimization-c1c4fc957ba2) has been published.\n* **Sep 22, 2025**: A new article [[Optuna v4.5] Gaussian Process-Based Sampler (GPSampler) Can Now Perform Constrained Multi-Objective Optimization](https://medium.com/optuna/optuna-v4-5-81e78d8e077a) has been published.\n* **Jun 16, 2025**: Optuna 4.4.0 has been released! Check out [the release blog](https://medium.com/optuna/announcing-optuna-4-4-ece661493126).\n* **May 26, 2025**: Optuna 5.0 roadmap has been published! See [the blog](https://medium.com/optuna/optuna-v5-roadmap-ac7d6935a878) for more details.\n* **Apr 14, 2025**: Optuna 4.3.0 is out! Check out [the release note](https://github.com/optuna/optuna/releases/tag/v4.3.0) for details.\n* **Mar 24, 2025**: A new article [Distributed Optimization in Optuna and gRPC Storage Proxy](https://medium.com/optuna/distributed-optimization-in-optuna-and-grpc-storage-proxy-08db83f1d608) has been published.\n\n:fire: Key Features\n\nOptuna has modern functionalities as follows:\n\n- [Lightweight, versatile, and platform agnostic architecture](https://optuna.readthedocs.io/en/stable/tutorial/10_key_features/001_first.html)\n  - Handle a wide variety of tasks with a simple installation that has few requirements.\n- [Pythonic search spaces](https://optuna.readthedocs.io/en/stable/tutorial/10_key_features/002_configurations.html)\n  - Define search spaces using familiar Python syntax including conditionals and loops.\n- [Efficient optimization algorithms](https://optuna.readthedocs.io/en/stable/tutorial/10_key_features/003_efficient_optimization_algorithms.html)\n  - Adopt state-of-the-art algorithms for sampling hyperparameters and efficiently pruning unpromising trials.\n- [Easy parallelization](https://optuna.readthedocs.io/en/stable/tutorial/10_key_features/004_distributed.html)\n  - Scale studies to tens or hundreds of workers with little or no changes to the code.\n- [Quick visualization](https://optuna.readthedocs.io/en/stable/tutorial/10_key_features/005_visualization.html)\n  - Inspect optimization histories from a variety of plotting functions.\n\nBasic Concepts\n\nWe use the terms *study* and *trial* as follows:\n\n- Study: optimization based on an objective function\n- Trial: a single execution of the objective function\n\nPlease refer to the sample code below. The goal of a *study* is to find out the optimal set of\nhyperparameter values (e.g., `regressor` and `svr_c`) through multiple *trials* (e.g.,\n`n_trials=100`). Optuna is a framework designed for automation and acceleration of\noptimization *studies*.\n\nSample code with scikit-learn\n\n(http://colab.research.google.com/github/optuna/optuna-examples/blob/main/quickstart.ipynb)\n\n> [!NOTE]\n> More examples can be found in [optuna/optuna-examples](https://github.com/optuna/optuna-examples).\n>\n> The examples cover diverse problem setups such as multi-objective optimization, constrained optimization, pruning, and distributed optimization.\n\nInstallation\n\nOptuna is available at [the Python Package Index](https://pypi.org/project/optuna/) and on [Anaconda Cloud](https://anaconda.org/conda-forge/optuna).\n\n> [!IMPORTANT]\n> Optuna supports Python 3.9 or newer.\n>\n> Also, we provide Optuna docker images on [DockerHub](https://hub.docker.com/r/optuna/optuna).\n\nIntegrations\n\nOptuna has integration features with various third-party libraries. Integrations can be found in [optuna/optuna-integration](https://github.com/optuna/optuna-integration) and the document is available [here](https://optuna-integration.readthedocs.io/en/stable/index.html).\n\nSupported integration libraries\n\n* [Catboost](https://github.com/optuna/optuna-examples/tree/main/catboost/catboost_pruning.py)\n* [Dask](https://github.com/optuna/optuna-examples/tree/main/dask/dask_simple.py)\n* [fastai](https://github.com/optuna/optuna-examples/tree/main/fastai/fastai_simple.py)\n* [Keras](https://github.com/optuna/optuna-examples/tree/main/keras/keras_integration.py)\n* [LightGBM](https://github.com/optuna/optuna-examples/tree/main/lightgbm/lightgbm_integration.py)\n* [MLflow](https://github.com/optuna/optuna-examples/tree/main/mlflow/keras_mlflow.py)\n* [PyTorch](https://github.com/optuna/optuna-examples/tree/main/pytorch/pytorch_simple.py)\n* [PyTorch Ignite](https://github.com/optuna/optuna-examples/tree/main/pytorch/pytorch_ignite_simple.py)\n* [PyTorch Lightning](https://github.com/optuna/optuna-examples/tree/main/pytorch/pytorch_lightning_simple.py)\n* [TensorBoard](https://github.com/optuna/optuna-examples/tree/main/tensorboard/tensorboard_simple.py)\n* [TensorFlow](https://github.com/optuna/optuna-examples/tree/main/tensorflow/tensorflow_estimator_integration.py)\n* [tf.keras](https://github.com/optuna/optuna-examples/tree/main/tfkeras/tfkeras_integration.py)\n* [Weights & Biases](https://github.com/optuna/optuna-examples/tree/main/wandb/wandb_integration.py)\n* [XGBoost](https://github.com/optuna/optuna-examples/tree/main/xgboost/xgboost_integration.py)\n\nWeb Dashboard\n\n[Optuna Dashboard](https://github.com/optuna/optuna-dashboard) is a real-time web dashboard for Optuna.\nYou can check the optimization history, hyperparameter importance, etc. in graphs and tables.\nYou don't need to create a Python script to call [Optuna's visualization](https://optuna.readthedocs.io/en/stable/reference/visualization/index.html) functions.\nFeature requests and bug reports are welcome!\n\n`optuna-dashboard` can be installed via pip:\n\n> [!TIP]\n> Please check out the convenience of Optuna Dashboard using the sample code below.\n\nSample code to launch Optuna Dashboard\n\nSave the following code as `optimize_toy.py`.\n\nThen try the commands below:\n\nOptunaHub\n\n[OptunaHub](https://hub.optuna.org/) is a feature-sharing platform for Optuna.\nYou can use the registered features and publish your packages.\n\nUse registered features\n\n`optunahub` can be installed via pip:\n\nYou can load registered module with `optunahub.load_module`.\n\nFor more details, please refer to [the optunahub documentation](https://optuna.github.io/optunahub/).\n\nPublish your packages\n\nYou can publish your package via [optunahub-registry](https://github.com/optuna/optunahub-registry).\nSee the [Tutorials for Contributors](https://optuna.github.io/optunahub/tutorials_for_contributors.html) in OptunaHub.\n\nCommunication\n\n- [GitHub Discussions] for questions.\n- [GitHub Issues] for bug reports and feature requests.\n\nContribution\n\nAny contributions to Optuna are more than welcome!\n\nIf you are new to Optuna, please check the [good first issues](https://github.com/optuna/optuna/labels/good%20first%20issue). They are relatively simple, well-defined, and often good starting points for you to get familiar with the contribution workflow and other developers.\n\nIf you already have contributed to Optuna, we recommend the other [contribution-welcome issues](https://github.com/optuna/optuna/labels/contribution-welcome).\n\nFor general guidelines on how to contribute to the project, take a look at [CONTRIBUTING.md](./CONTRIBUTING.md).\n\nReference\n\nIf you use Optuna in one of your research projects, please cite [our KDD paper](https://doi.org/10.1145/3292500.3330701) \"Optuna: A Next-generation Hyperparameter Optimization Framework\":\n\nBibTeX\n\nLicense\n\nMIT License (see [LICENSE](./LICENSE)).\n\nOptuna uses the codes from SciPy and fdlibm projects (see [LICENSE_THIRD_PARTY](./LICENSE_THIRD_PARTY))."}, {"name": "optuna", "tags": ["math", "ml", "visualization", "web"], "summary": "A hyperparameter optimization framework", "text": "This library is used to automatically optimize the performance of machine learning models by searching for optimal hyperparameters. With Optuna, developers can efficiently explore large hyperparameter spaces and find the best combinations that maximize their model's accuracy or other evaluation metrics."}, {"name": "optype", "tags": ["data", "math", "ui", "web"], "summary": "Building Blocks for Precise & Flexible Type Hints", "text": "Installation\n\nPyPI\n\nOptype is available as [`optype`][PYPI] on PyPI:\n\nFor optional [NumPy][NUMPY] support, ensure that you use the `optype[numpy]` extra.\nThis ensures that the installed `numpy` and the required [`numpy-typing-compat`][NPTC]\nversions are compatible with each other.\n\nSee the [`optype.numpy` docs](#optypenumpy) for more info.\n\nConda\n\nOptype can also be installed with `conda` from the [`conda-forge`][CONDA] channel:\n\nIf you want to use [`optype.numpy`](#optypenumpy), you should instead install\n[`optype-numpy`][CONDA-NP]:\n\nExample\n\nLet's say you're writing a `twice(x)` function, that evaluates `2 * x`.\nImplementing it is trivial, but what about the type annotations?\n\nBecause `twice(2) == 4`, `twice(3.14) == 6.28` and `twice('I') = 'II'`, it\nmight seem like a good idea to type it as `twice[T](x: T) -> T: ...`.\nHowever, that wouldn't include cases such as `twice(True) == 2` or\n`twice((42, True)) == (42, True, 42, True)`, where the input- and output types\ndiffer.\nMoreover, `twice` should accept *any* type with a custom `__rmul__` method\nthat accepts `2` as argument.\n\nThis is where `optype` comes in handy, which has single-method protocols for\n*all* the builtin special methods.\nFor `twice`, we can use `optype.CanRMul[T, R]`, which, as the name suggests,\nis a protocol with (only) the `def __rmul__(self, lhs: T) -> R: ...` method.\nWith this, the `twice` function can written as:\n\nPython 3.11\nPython 3.12+\n\nBut what about types that implement `__add__` but not `__radd__`?\nIn this case, we could return `x * 2` as fallback (assuming commutativity).\nBecause the `optype.Can*` protocols are runtime-checkable, the revised\n`twice2` function can be compactly written as:\n\nPython 3.11\nPython 3.12+\n\nSee [`examples/twice.py`](examples/twice.py) for the full example.\n\nReference\n\nThe API of `optype` is flat; a single `import optype as opt` is all you need\n(except for `optype.numpy`).\n\n- [`optype`](#optype)\n  - [`Just`](#just)\n  - [Builtin type conversion](#builtin-type-conversion)\n  - [Rich relations](#rich-relations)\n  - [Binary operations](#binary-operations)\n  - [Reflected operations](#reflected-operations)\n  - [Inplace operations](#inplace-operations)\n  - [Unary operations](#unary-operations)\n  - [Rounding](#rounding)\n  - [Callables](#callables)\n  - [Iteration](#iteration)\n  - [Awaitables](#awaitables)\n  - [Async Iteration](#async-iteration)\n  - [Containers](#containers)\n  - [Attributes](#attributes)\n  - [Context managers](#context-managers)\n  - [Descriptors](#descriptors)\n  - [Buffer types](#buffer-types)\n- [`optype.copy`](#optypecopy)\n- [`optype.dataclasses`](#optypedataclasses)\n- [`optype.inspect`](#optypeinspect)\n- [`optype.io`](#optypeio)\n- [`optype.json`](#optypejson)\n- [`optype.pickle`](#optypepickle)\n- [`optype.string`](#optypestring)\n- [`optype.typing`](#optypetyping)\n  - [`Any*` type aliases](#any-type-aliases)\n  - [`Empty*` type aliases](#empty-type-aliases)\n  - [Literal types](#literal-types)\n- [`optype.dlpack`](#optypedlpack)\n- [`optype.numpy`](#optypenumpy)\n  - [Shape-typing](#shape-typing)\n  - [Array-likes](#array-likes)\n  - [Literals](#literals)\n  - [`compat` submodule](#compat-submodule)\n  - [`random` submodule](#random-submodule)\n  - [`Any*Array` and `Any*DType`](#anyarray-and-anydtype)\n  - [Low-level interfaces](#low-level-interfaces)\n\n`optype`\n\nThere are five flavors of things that live within `optype`,"}, {"name": "optype", "tags": ["data", "math", "ui", "web"], "summary": "Building Blocks for Precise & Flexible Type Hints", "text": "- The `optype.Just[T]` and its `optype.Just{Int,Float,Complex}` subtypes only accept\n  instances of the type itself, while rejecting instances of strict subtypes.\n  This can be used to e.g. work around the `float` and `complex`\n  [type promotions][BAD], annotating `object()` sentinels with `Just[object]`,\n  rejecting `bool` in functions that accept `int`, etc.\n- `optype.Can{}` types describe *what can be done* with it.\n  For instance, any `CanAbs[T]` type can be used as argument to the `abs()`\n  builtin function with return type `T`. Most `Can{}` implement a single\n  special method, whose name directly matches that of the type. `CanAbs`\n  implements `__abs__`, `CanAdd` implements `__add__`, etc.\n- `optype.Has{}` is the analogue of `Can{}`, but for special *attributes*.\n  `HasName` has a `__name__` attribute, `HasDict` has a `__dict__`, etc.\n- `optype.Does{}` describe the *type of operators*.\n  So `DoesAbs` is the type of the `abs({})` builtin function,\n  and `DoesPos` the type of the `+{}` prefix operator.\n- `optype.do_{}` are the correctly-typed implementations of `Does{}`. For\n  each `do_{}` there is a `Does{}`, and vice-versa.\n  So `do_abs: DoesAbs` is the typed alias of `abs({})`,\n  and `do_pos: DoesPos` is a typed version of `operator.pos`.\n  The `optype.do_` operators are more complete than `operators`,\n  have runtime-accessible type annotations, and have names you don't\n  need to know by heart.\n\nThe reference docs are structured as follows:\n\nAll [typing protocols][PC] here live in the root `optype` namespace.\nThey are [runtime-checkable][RC] so that you can do e.g.\n`isinstance('snail', optype.CanAdd)`, in case you want to check whether\n`snail` implements `__add__`.\n\nUnlike`collections.abc`, `optype`'s protocols aren't abstract base classes,\ni.e. they don't extend `abc.ABC`, only `typing.Protocol`.\nThis allows the `optype` protocols to be used as building blocks for `.pyi`\ntype stubs.\n\n`Just`\n\n`Just` is an invariant type \"wrapper\", where `Just[T]` only accepts instances of `T`,\nand rejects instances of any strict subtypes of `T`.\n\nNote that e.g. `Literal[\"\"]` and `LiteralString` are not a strict `str` subtypes,\nand are therefore assignable to `Just[str]`, but instances of `class S(str): ...`\nare **not** assignable to `Just[str]`.\n\nDisallow passing `bool` as `int`:\n\nAnnotating a sentinel:\n\n> [!TIP]\n> The `Just{Bytes,Int,Float,Complex,Date,Object}` protocols are runtime-checkable,\n> so that `instance(42, JustInt) is True` and `instance(bool(), JustInt) is False`.\n> It's implemented through meta-classes, and type-checkers have no problem with it.\n\n`optype` type\n-------------\n`Just[T]`\n`JustInt`\n`JustFloat`\n`JustComplex`\n`JustBytes`\n`JustObject`\n`JustDate`\n\nBuiltin type conversion\n\nThe return type of these special methods is *invariant*. Python will raise an\nerror if some other (sub)type is returned.\nThis is why these `optype` interfaces don't accept generic type arguments.\n\nRich relations\n\nThe \"rich\" comparison special methods often return a `bool`.\nHowever, instances of any type can be returned (e.g. a numpy array).\nThis is why the corresponding `optype.Can*` interfaces accept a second type\nargument for the return type, that defaults to `bool` when omitted.\nThe first type parameter matches the passed method argument, i.e. the\nright-hand side operand, denoted here as `x`.\n\nBinary operations"}, {"name": "optype", "tags": ["data", "math", "ui", "web"], "summary": "Building Blocks for Precise & Flexible Type Hints", "text": "In the [Python docs][NT], these are referred to as \"arithmetic operations\".\nBut the operands aren't limited to numeric types, and because the\noperations aren't required to be commutative, might be non-deterministic, and\ncould have side-effects.\nClassifying them \"arithmetic\" is, at the very least, a bit of a stretch.\n\n> [!TIP]\n> Because `pow()` can take an optional third argument, `optype`\n> provides separate interfaces for `pow()` with two and three arguments.\n> Additionally, there is the overloaded intersection type\n> `type CanPow[-T, -M, +R, +RM] = CanPow2[T, R] & CanPow3[T, M, RM]`, as interface\n> for types that can take an optional third argument.\n\n> [!NOTE]\n> The `Can*Self` protocols method return `typing.Self` and optionally accept `T` and\n> `R`. The `Can*Same` protocols also return `Self`, but instead accept `Self | T`, with\n> `T` and `R` optional generic type parameters that default to `typing.Never`.\n> To illustrate, `CanAddSelf[T]` implements `__add__` as `(self, rhs: T, /) -> Self`,\n> while `CanAddSame[T, R]` implements it as `(self, rhs: Self | T, /) -> Self | R`, and\n> `CanAddSame` (without `T` and `R`) as `(self, rhs: Self, /) -> Self`.\n\nReflected operations\n\nFor the binary infix operators above, `optype` additionally provides\ninterfaces with *reflected* (swapped) operands, e.g. `__radd__` is a reflected\n`__add__`.\nThey are named like the original, but prefixed with `CanR` prefix, i.e.\n`__name__.replace('Can', 'CanR')`.\n\n> [!NOTE]\n> `CanRPow` corresponds to `CanPow2`; the 3-parameter \"modulo\" `pow` does not\n> reflect in Python.\n>\n> According to the relevant [python docs][RPOW]:\n>\n>> Note that ternary `pow()` will not try calling `__rpow__()` (the coercion\n>> rules would become too complicated).\n\nInplace operations\n\nSimilar to the reflected ops, the inplace/augmented ops are prefixed with\n`CanI`, namely:\n\nThese inplace operators usually return themselves (after some in-place mutation).\nBut unfortunately, it currently isn't possible to use `Self` for this (i.e.\nsomething like `type MyAlias[T] = optype.CanIAdd[T, Self]` isn't allowed).\nSo to help ease this unbearable pain, `optype` comes equipped with ready-made\naliases for you to use. They bear the same name, with an additional `*Self`\nsuffix, e.g. `optype.CanIAddSelf[T]`.\n\n> [!NOTE]\n> The `CanI*Self` protocols method return `typing.Self` and optionally accept `T`. The\n> `CanI*Same` protocols also return `Self`, but instead accept `rhs: Self | T`. Since\n> `T` defaults to `Never`, it will accept `rhs: Self | Never` if `T` is not provided,\n> which is equivalent to `rhs: Self`.\n>\n> *Available since `0.12.1`*\n\nUnary operations\n\nThe `Can*Self` variants return `-> Self` instead of `R`. Since optype 0.12.1 these\nalso accept an optional `R` type parameter (with a default of `Never`), which, when\nprovided, will result in a return type of `-> Self | R`.\n\nRounding\n\nThe `round()` built-in function takes an optional second argument.\nFrom a typing perspective, `round()` has two overloads, one with 1 parameter,\nand one with two.\nFor both overloads, `optype` provides separate operand interfaces:\n`CanRound1[R]` and `CanRound2[T, RT]`.\nAdditionally, `optype` also provides their (overloaded) intersection type:\n`CanRound[-T, +R1, +R2] = CanRound1[R1] & CanRound2[T, R2]`."}, {"name": "optype", "tags": ["data", "math", "ui", "web"], "summary": "Building Blocks for Precise & Flexible Type Hints", "text": "For example, type-checkers will mark the following code as valid (tested with\npyright in strict mode):\n\nFurthermore, there are the alternative rounding functions from the\n[`math`][MATH] standard library:\n\nAlmost all implementations use `int` for `R`.\nIn fact, if no type for `R` is specified, it will default in `int`.\nBut technically speaking, these methods can be made to return anything.\n\nCallables\n\nUnlike `operator`, `optype` provides an operator for callable objects:\n`optype.do_call(f, *args. **kwargs)`.\n\n`CanCall` is similar to `collections.abc.Callable`, but is runtime-checkable,\nand doesn't use esoteric hacks.\n\n> [!NOTE]\n> Pyright (and probably other typecheckers) tend to accept\n> `collections.abc.Callable` in more places than `optype.CanCall`.\n> This could be related to the lack of co/contra-variance specification for\n> `typing.ParamSpec` (they should almost always be contravariant, but\n> currently they can only be invariant).\n>\n> In case you encounter such a situation, please open an issue about it, so we\n> can investigate further.\n\nIteration\n\nThe operand `x` of `iter(_)` is within Python known as an *iterable*, which is\nwhat `collections.abc.Iterable[V]` is often used for (e.g. as base class, or\nfor instance checking).\n\nThe `optype` analogue is `CanIter[R]`, which as the name suggests,\nalso implements `__iter__`. But unlike `Iterable[V]`, its type parameter `R`\nbinds to the return type of `iter(_) -> R`. This makes it possible to annotate\nthe specific type of the *iterable* that `iter(_)` returns. `Iterable[V]` is\nonly able to annotate the type of the iterated value. To see why that isn't\npossible, see [python/typing#548](https://github.com/python/typing/issues/548).\n\nThe `collections.abc.Iterator[V]` is even more awkward; it is a subtype of\n`Iterable[V]`. For those familiar with `collections.abc` this might come as a\nsurprise, but an iterator only needs to implement `__next__`, `__iter__` isn't\nneeded. This means that the `Iterator[V]` is unnecessarily restrictive.\nApart from that being theoretically \"ugly\", it has significant performance\nimplications, because the time-complexity of `isinstance` on a\n`typing.Protocol` is $O(n)$, with the $n$ referring to the amount of members.\nSo even if the overhead of the inheritance and the `abc.ABC` usage is ignored,\n`collections.abc.Iterator` is twice as slow as it needs to be.\n\nThat's one of the (many) reasons that `optype.CanNext[V]` and\n`optype.CanIter[R]` are the better alternatives to `Iterable` and `Iterator`\nfrom the abracadabra collections. This is how they are defined:\n\nFor the sake of compatibility with `collections.abc`, there is\n`optype.CanIterSelf[V]`, which is a protocol whose `__iter__` returns\n`typing.Self`, as well as a `__next__` method that returns `T`.\nI.e. it is equivalent to `collections.abc.Iterator[V]`, but without the `abc`\nnonsense.\n\nAwaitables\n\nThe `optype.CanAwait[R]` is almost the same as `collections.abc.Awaitable[R]`, except\nthat `optype.CanAwait[R]` is a pure interface, whereas `Awaitable` is\nalso an abstract base class (making it absolutely useless when writing stubs).\n\nAsync Iteration\n\nYes, you guessed it right; the abracadabra collections made the exact same\nmistakes for the async iterablors (or was it \"iteramblers\"...?).\n\nBut fret not; the `optype` alternatives are right here:"}, {"name": "optype", "tags": ["data", "math", "ui", "web"], "summary": "Building Blocks for Precise & Flexible Type Hints", "text": "But wait, shouldn't `V` be a `CanAwait`? Well, only if you don't want to get\nfired...\nTechnically speaking, `__anext__` can return any type, and `anext` will pass\nit along without nagging. For details, see the discussion at [python/typeshed#7491][AN].\nJust because something is legal, doesn't mean it's a good idea (don't eat the\nyellow snow).\n\nAdditionally, there is `optype.CanAIterSelf[R]`, with both the\n`__aiter__() -> Self` and the `__anext__() -> V` methods.\n\nContainers\n\nBecause `CanMissing[K, D]` generally doesn't show itself without\n`CanGetitem[K, V]` there to hold its hand, `optype` conveniently stitched them\ntogether as `optype.CanGetMissing[K, V, D=V]`.\n\nSimilarly, there is `optype.CanSequence[K: CanIndex | slice, V]`, which is the\ncombination of both `CanLen` and `CanItem[I, V]`, and serves as a more\nspecific and flexible `collections.abc.Sequence[V]`.\n\nAttributes\n\nContext managers\n\nSupport for the `with` statement.\n\n`CanEnterSelf` and `CanWithSelf` are (runtime-checkable) aliases for\n`CanEnter[Self]` and `CanWith[Self, R]`, respectively.\n\nFor the `async with` statement the interfaces look very similar:\n\nDescriptors\n\nInterfaces for [descriptors](https://docs.python.org/3/howto/descriptor.html).\n\nBuffer types\n\nInterfaces for emulating buffer types using the [buffer protocol][BP].\n\n`optype.copy`\n\nFor the [`copy`][CP] standard library, `optype.copy` provides the following\nruntime-checkable interfaces:\n\n[1] *`copy.replace` requires `python>=3.13`\n(but `optype.copy.CanReplace` doesn't)*\n\nIn practice, it makes sense that a copy of an instance is the same type as the\noriginal.\nBut because `typing.Self` cannot be used as a type argument, this difficult\nto properly type.\nInstead, you can use the `optype.copy.Can{}Self` types, which are the\nruntime-checkable equivalents of the following (non-expressible) aliases:\n\n`optype.dataclasses`\n\nFor the [`dataclasses`][DC] standard library, `optype.dataclasses` provides the\n`HasDataclassFields[V: Mapping[str, Field]]` interface.\nIt can conveniently be used to check whether a type or instance is a\ndataclass, i.e. `isinstance(obj, HasDataclassFields)`.\n\n`optype.inspect`\n\nA collection of functions for runtime inspection of types, modules, and other\nobjects.\n\nA better alternative to [`typing.get_args()`][GET_ARGS], that\n\n- unpacks `typing.Annotated` and Python 3.12 `type _` alias types\n  (i.e. `typing.TypeAliasType`),\n- recursively flattens unions and nested `typing.Literal` types, and\n- raises `TypeError` if not a type expression.\n\nReturn a `tuple[type | object, ...]` of type arguments or parameters.\n\nTo illustrate one of the (many) issues with `typing.get_args`:\n\nBut this is in direct contradiction with the\n[official typing documentation][LITERAL-DOCS]:\n\n> When a Literal is parameterized with more than one value, it\u2019s treated as\n> exactly equivalent to the union of those types.\n> That is, `Literal[v1, v2, v3]` is equivalent to\n> `Literal[v1] | Literal[v2] | Literal[v3]`.\n\nSo this is why `optype.inspect.get_args` should be used\n\nAnother issue of `typing.get_args` is with Python 3.12 `type _ = ...` aliases,\nwhich are meant as a replacement for `_: typing.TypeAlias = ...`, and should\ntherefore be treated equally:\n\nClearly, `typing.get_args` fails miserably here; it would have been better\nif it would have raised an error, but it instead returns an empty tuple,\nhiding the fact that it doesn't support the new `type _ = ...` aliases.\nBut luckily, `optype.inspect.get_args` doesn't have this problem, and treats\nit just like it treats `typing.Alias` (and so do the other `optype.inspect`\nfunctions).\n\nA better alternative to [`typing.get_protocol_members()`][PROTO_MEM], that"}, {"name": "optype", "tags": ["data", "math", "ui", "web"], "summary": "Building Blocks for Precise & Flexible Type Hints", "text": "- doesn't require Python 3.13 or above,\n- supports [PEP 695][PEP695] `type _` alias types on Python 3.12 and above,\n- unpacks unions of `typing.Literal` ...\n- ... and flattens them if nested within another `typing.Literal`,\n- treats `typing.Annotated[T]` as `T`, and\n- raises a `TypeError` if the passed value isn't a type expression.\n\nReturns a `frozenset[str]` with member names.\n\nReturns a `frozenset[type]` of the public protocols within the passed module.\nPass `private=True` to also return the private protocols.\n\nCheck whether the object can be iterated over, i.e. if it can be used in a\n`for` loop, without attempting to do so.\nIf `True` is returned, then the object is a `optype.typing.AnyIterable`\ninstance.\n\nCheck if the type, method / classmethod / staticmethod / property, is\ndecorated with [`@typing.final`][@FINAL].\n\nNote that a `@property` won't be recognized unless the `@final` decorator is\nplaced *below* the `@property` decorator.\nSee the function docstring for more information.\n\nA backport of [`typing.is_protocol`][IS_PROTO] that was added in Python 3.13,\na re-export of [`typing_extensions.is_protocol`][IS_PROTO_EXT].\n\nCheck if the type expression is a *runtime-protocol*, i.e. a\n`typing.Protocol` *type*, decorated with `@typing.runtime_checkable` (also\nsupports `typing_extensions`).\n\nCheck if the type is a [`typing.Union`][UNION] type, e.g. `str | int`.\n\nUnlike `isinstance(_, types.Union)`, this function also returns `True` for\nunions of user-defined `Generic` or `Protocol` types (because those are\ndifferent union types for some reason).\n\nCheck if the type is a *subscripted* type, e.g. `list[str]` or\n`optype.CanNext[int]`, but not `list`, `CanNext`.\n\nUnlike `isinstance(_, typing.GenericAlias)`, this function also returns `True`\nfor user-defined `Generic` or `Protocol` types (because those are\nuse a different generic alias for some reason).\n\nEven though technically `T1 | T2` is represented as `typing.Union[T1, T2]`\n(which is a (special) generic alias), `is_generic_alias` will returns `False`\nfor such union types, because calling `T1 | T2` a subscripted type just\ndoesn't make much sense.\n\n> [!NOTE]\n> All functions in `optype.inspect` also work for Python 3.12 `type _` aliases\n> (i.e. `types.TypeAliasType`) and with `typing.Annotated`.\n\n`optype.io`\n\nA collection of protocols and type-aliases that, unlike their analogues in `_typeshed`,\nare accessible at runtime, and use a consistent naming scheme.\n\n`optype.json`\n\nType aliases for the `json` standard library:\n\nThe `(Any)Value` can be any json input, i.e. `Value | Array | Object` is\nequivalent to `Value`.\nIt's also worth noting that `Value` is a subtype of `AnyValue`, which means\nthat `AnyValue | Value` is equivalent to `AnyValue`.\n\n`optype.pickle`\n\nFor the [`pickle`][PK] standard library, `optype.pickle` provides the following\ninterfaces:\n\n`optype.string`\n\nThe [`string`](https://docs.python.org/3/library/string.html) standard\nlibrary contains practical constants, but it has two issues:\n\n- The constants contain a collection of characters, but are represented as\n  a single string. This makes it practically impossible to type-hint the\n  individual characters, so typeshed currently types these constants as a\n  `LiteralString`.\n- The names of the constants are inconsistent, and doesn't follow\n  [PEP 8](https://peps.python.org/pep-0008/#constants).\n\nSo instead, `optype.string` provides an alternative interface, that is\ncompatible with `string`, but with slight differences:"}, {"name": "optype", "tags": ["data", "math", "ui", "web"], "summary": "Building Blocks for Precise & Flexible Type Hints", "text": "- For each constant, there is a corresponding `Literal` type alias for\n  the *individual* characters. Its name matches the name of the constant,\n  but is singular instead of plural.\n- Instead of a single string, `optype.string` uses a `tuple` of characters,\n  so that each character has its own `typing.Literal` annotation.\n  Note that this is only tested with (based)pyright / pylance, so it might\n  not work with mypy (it has more bugs than it has lines of codes).\n- The names of the constant are consistent with PEP 8, and use a postfix\n  notation for variants, e.g. `DIGITS_HEX` instead of `hexdigits`.\n- Unlike `string`, `optype.string` has a constant (and type alias) for\n  binary digits `'0'` and `'1'`; `DIGITS_BIN` (and `DigitBin`). Because\n  besides `oct` and `hex` functions in `builtins`, there's also the\n  `builtins.bin` function.\n\nEach of the `optype.string` constants is exactly the same as the corresponding\n`string` constant (after concatenation / splitting), e.g.\n\nSimilarly, the values within a constant's `Literal` type exactly match the\nvalues of its constant:\n\nThe `optype.inspect.get_args` is a non-broken variant of `typing.get_args`\nthat correctly flattens nested literals, type-unions, and PEP 695 type aliases,\nso that it matches the official typing specs.\n*In other words; `typing.get_args` is yet another fundamentally broken\npython-typing feature that's useless in the situations where you need it\nmost.*\n\n`optype.typing`\n\n`Any*` type aliases\n\nType aliases for anything that can *always* be passed to\n`int`, `float`, `complex`, `iter`, or `typing.Literal`\n\n> [!NOTE]\n> Even though *some* `str` and `bytes` can be converted to `int`, `float`,\n> `complex`, most of them can't, and are therefore not included in these\n> type aliases.\n\n`Empty*` type aliases\n\nThese are builtin types or collections that are empty, i.e. have length 0 or\nyield no elements.\n\nLiteral types\n\n`optype.dlpack`\n\nA collection of low-level types for working [DLPack](DOC-DLPACK).\n\nProtocols\n\nThe `+` prefix indicates that the type parameter is *co*variant.\n\nEnums\n\nThere are also two convenient\n[`IntEnum`](https://docs.python.org/3/library/enum.html#enum.IntEnum)s\nin `optype.dlpack`: `DLDeviceType` for the device types, and `DLDataTypeCode` for the\ninternal type-codes of the `DLPack` data types.\n\n`optype.numpy`\n\nOptype supports both NumPy 1 and 2. The current minimum supported version is `1.25`,\nfollowing [NEP 29][NEP29] and [SPEC 0][SPEC0].\n\n`optype.numpy` uses [`numpy-typing-compat`][NPTC] package to ensure compatibility for\nolder versions of NumPy. To ensure that the correct versions of `numpy` and\n`numpy-typing-compat` are installed, you should install `optype` with the `numpy` extra:\n\nIf you're using `conda`, the [`optype-numpy`][CONDA-NP] package can be used, which\nwill also install the required `numpy` and `numpy-typing-compat` versions:\n\n> [!NOTE]\n> For the remainder of the `optype.numpy` docs, assume that the following\n> import aliases are available.\n>\n> \n>\n> For the sake of brevity and readability, the [PEP 695][PEP695] and\n> [PEP 696][PEP696] type parameter syntax will be used, which is supported\n> since Python 3.13.\n\nShape-typing\n\nArray aliases\n\nOptype provides the generic `onp.Array` type alias for `np.ndarray`.\nIt is similar to `npt.NDArray`, but includes two (optional) type parameters:\none that matches the *shape type* (`ND: tuple[int, ...]`),\nand one that matches the *scalar type* (`ST: np.generic`)."}, {"name": "optype", "tags": ["data", "math", "ui", "web"], "summary": "Building Blocks for Precise & Flexible Type Hints", "text": "When we put the definitions of `npt.NDArray` and `onp.Array` side-by-side,\ntheir differences become clear:\n\n`numpy.typing.NDArray`[^1]\n\n`optype.numpy.Array`\n\n`optype.numpy.ArrayND`\n\nAdditionally, there are the four `Array{0,1,2,3}D` aliases, which are\nequivalent to `Array` with `tuple[()]`, `tuple[int]`, `tuple[int, int]` and\n`tuple[int, int, int]` as shape-type, respectively.\n\n[^1]: Since `numpy>=2.2` the `NDArray` alias uses `tuple[int, ...]` as shape-type\n\n> [!TIP]\n> Before NumPy 2.1, the shape type parameter of `ndarray` (i.e. the type of\n> `ndarray.shape`) was invariant. It is therefore recommended to not use `Literal`\n> within shape types on `numpy=2.1` you can use\n> `tuple[Literal[3], Literal[3]]` without problem, but with `numpy `tuple[int, int]` instead.\n>\n> See [numpy/numpy#25729](https://github.com/numpy/numpy/issues/25729) and\n> [numpy/numpy#26081](https://github.com/numpy/numpy/pull/26081) for details.\n\nIn the same way as `ArrayND` for `ndarray` (shown for reference), its subtypes\n`np.ma.MaskedArray` and `np.matrix` are also aliased:\n\n`ArrayND` (`np.ndarray`)\n\n`MArray` (`np.ma.MaskedArray`)\n\n`Matrix` (`np.matrix`)\n\nFor masked arrays with specific `ndim`, you could also use one of the four\n`MArray{0,1,2,3}D` aliases.\n\nArray typeguards\n\nTo check whether a given object is an instance of `Array{0,1,2,3,N}D`, in a way that\nstatic type-checkers also understand it, the following [PEP 742][PEP742] typeguards can\nbe used:\n\nThese functions additionally accept an optional `dtype` argument, that can either be\na `np.dtype[ST]` instance, a `type[ST]`, or something that has a `dtype: np.dtype[ST]`\nattribute.\nThe signatures are almost identical to each other, and in the `0d` case it roughly\nlooks like this:\n\nShape aliases\n\nA *shape* is nothing more than a tuple of (non-negative) integers, i.e.\nan instance of `tuple[int, ...]` such as `(42,)`, `(480, 720, 3)` or `()`.\nThe length of a shape is often referred to as the *number of dimensions*\nor the *dimensionality* of the array or scalar.\nFor arrays this is accessible through the `np.ndarray.ndim`, which is\nan alias for `len(np.ndarray.shape)`.\n\n> [!NOTE]\n> Before NumPy 2, the maximum number of dimensions was `32`, but has since\n> been increased to `ndim = N`\n- `AtMost{N}D` is a `tuple[int, ...]` with `ndim\n\n0\n\n1\n\n2\n\n3\n\nThe `AtLeast{}D` optionally accepts a type argument that can either be `int` (default),\nor `Any`. Passing `Any` turns it from a *gradual tuple type*, so that they can also be\nassigned to compatible bounded shape-types. So `AtLeast1D[Any]` is assignable to\n`tuple[int]`, whereas `AtLeast1D` (equiv. `AtLeast1D[int]`) is not.\n\nHowever, mypy currently has a [bug](https://github.com/python/mypy/issues/19109),\ncausing it to falsely reject such gradual shape-type assignment for N=1 or up.\n\nArray-likes\n\nSimilar to the `numpy._typing._ArrayLike{}_co` *coercible array-like* types,\n`optype.numpy` provides the `optype.numpy.To{}ND`. Unlike the ones in `numpy`, these\ndon't accept \"bare\" scalar types (the `__len__` method is required).\nAdditionally, there are the `To{}1D`, `To{}2D`, and `To{}3D` for vector-likes,\nmatrix-likes, and cuboid-likes, and the `To{}` aliases for \"bare\" scalar types.\n\n> [!NOTE]\n> The `To*Strict{1,2,3}D` aliases were added in `optype 0.7.3`.\n>\n> These array-likes with *strict shape-type* require the shape-typed input to be\n> shape-typed.\n> This means that e.g. `ToFloat1D` and `ToFloat2D` are disjoint (non-overlapping),\n> and makes them suitable to overload array-likes of a particular dtype for different\n> numbers of dimensions."}, {"name": "optype", "tags": ["data", "math", "ui", "web"], "summary": "Building Blocks for Precise & Flexible Type Hints", "text": "> [!NOTE]\n> The `ToJust{Bool,Float,Complex}*` type aliases were added in `optype 0.8.0`.\n>\n> See [`optype.Just`](#just) for more information.\n\n> [!NOTE]\n> The `To[Just]{False,True}` type aliases were added in `optype 0.9.1`.\n>\n> These only include the `np.bool` types on `numpy>=2.2`. Before that, `np.bool`\n> wasn't generic, making it impossible to distinguish between `np.False_` and `np.True_`\n> using static typing.\n\n> [!NOTE]\n> The `ToArrayStrict{1,2,3}D` types are generic since `optype 0.9.1`, analogous to\n> their non-strict dual type, `ToArray{1,2,3}D`.\n\n> [!NOTE]\n> The `To[Just]{Float16,Float32,Complex64}*` type aliases were added in `optype 0.12.0`.\n\nSource code: [`optype/numpy/_to.py`][CODE-NP-TO]\n\nLiterals\n\nType Alias\n---------------\n`ByteOrder`\n`ByteOrderChar`\n`ByteOrderName`\n`Casting`\n`CastingUnsafe`\n`CastingSafe`\n`ConvolveMode`\n`Device`\n`IndexMode`\n`OrderCF`\n`OrderACF`\n`OrderKACF`\n`PartitionKind`\n`SortKind`\n`SortSide`\n\n`compat` submodule\n\nCompatibility module for supporting a wide range of numpy versions (currently `>=1.25`).\nIt contains the abstract numeric scalar types, with `numpy>=2.2`\ntype-parameter defaults, which I explained in the [`release notes`][NP-REL22].\n\n`random` submodule\n\n[SPEC 7](https://scientific-python.org/specs/spec-0007/) -compatible type aliases.\nThe `optype.numpy.random` module provides three type aliases: `RNG`, `ToRNG`, and\n`ToSeed`.\n\nIn general, the most useful one is `ToRNG`, which describes what can be\npassed to `numpy.random.default_rng`. It is defined as the union of `RNG`, `ToSeed`,\nand `numpy.random.BitGenerator`.\n\nThe `RNG` is the union type of `numpy.random.Generator` and its legacy dual type,\n`numpy.random.RandomState`.\n\n`ToSeed` accepts integer-like scalars, sequences, and arrays, as well as instances of\n`numpy.random.SeedSequence`.\n\n`DType`\n\nIn NumPy, a *dtype* (data type) object, is an instance of the\n`numpy.dtype[ST: np.generic]` type.\nIt's commonly used to convey metadata of a scalar type, e.g. within arrays.\n\nBecause the type parameter of `np.dtype` isn't optional, it could be more\nconvenient to use the alias `optype.numpy.DType`, which is defined as:\n\nApart from the \"CamelCase\" name, the only difference with `np.dtype` is that\nthe type parameter can be omitted, in which case it's equivalent to\n`np.dtype[np.generic]`, but shorter.\n\n`Scalar`\n\nThe `optype.numpy.Scalar` interface is a generic runtime-checkable protocol,\nthat can be seen as a \"more specific\" `np.generic`, both in name, and from\na typing perspective.\n\nIts type signature looks roughly like this:\n\nIt can be used as e.g.\n\n> [!NOTE]\n> The second type argument for `itemsize` can be omitted, which is equivalent\n> to setting it to `int`, so `Scalar[PT]` and `Scalar[PT, int]` are equivalent.\n\n`UFunc`\n\nA large portion of numpy's public API consists of *universal functions*, often\ndenoted as [ufuncs][DOC-UFUNC], which are (callable) instances of\n[`np.ufunc`][REF_UFUNC].\n\n> [!TIP]\n> Custom ufuncs can be created using [`np.frompyfunc`][REF_FROMPY], but also\n> through a user-defined class that implements the required attributes and\n> methods (i.e., duck typing).\n\nBut `np.ufunc` has a big issue; it accepts no type parameters.\nThis makes it very difficult to properly annotate its callable signature and\nits literal attributes (e.g. `.nin` and `.identity`).\n\nThis is where `optype.numpy.UFunc` comes into play:\nIt's a runtime-checkable generic typing protocol, that has been thoroughly\ntype- and unit-tested to ensure compatibility with all of numpy's ufunc\ndefinitions.\nIts generic type signature looks roughly like:"}, {"name": "optype", "tags": ["data", "math", "ui", "web"], "summary": "Building Blocks for Precise & Flexible Type Hints", "text": "> [!NOTE]\n> Unfortunately, the extra callable methods of `np.ufunc` (`at`, `reduce`,\n> `reduceat`, `accumulate`, and `outer`), are incorrectly annotated (as `None`\n> *attributes*, even though at runtime they're methods that raise a\n> `ValueError` when called).\n> This currently makes it impossible to properly type these in\n> `optype.numpy.UFunc`; doing so would make it incompatible with numpy's\n> ufuncs.\n\n`Any*Array` and `Any*DType`\n\nThe `Any{Scalar}Array` type aliases describe array-likes that are coercible to an\n`numpy.ndarray` with specific [dtype][REF-DTYPES].\n\nUnlike `numpy.typing.ArrayLike`, these `optype.numpy` aliases **don't**\naccept \"bare\" scalar types such as `float` and `np.float64`. However, arrays of\n\"zero dimensions\" like `onp.Array[tuple[()], np.float64]` will be accepted.\nThis is in line with the behavior of [`numpy.isscalar`][REF-ISSCALAR] on `numpy >= 2`.\n\n> [!NOTE]\n> The [`numpy.dtypes` docs][REF-DTYPES] exists since NumPy 1.25, but its\n> type annotations were incorrect before NumPy 2.1 (see\n> [numpy/numpy#27008](https://github.com/numpy/numpy/pull/27008))\n\nSee the [docs][REF-SCT] for more info on the NumPy scalar type hierarchy.\n\nAbstract types\n\nUnsigned integers\n\n`uint_`[^5]\n\n`uint32`[^6]\n\n`uintc`[^6]\n\n`ulong`[^7]\n\nSigned integers\n\n`int_`[^5]\n\n`int32`[^6]\n\n`intc`[^6]\n\n`long`[^7]\n\n[^5]: Since NumPy 2, `np.uint` and `np.int_` are aliases for `np.uintp` and `np.intp`, respectively.\n\n[^6]: On unix-based platforms `np.[u]intc` are aliases for `np.[u]int32`.\n\n[^7]: On NumPy 1 `np.uint` and `np.int_` are what in NumPy 2 are now the `np.ulong` and `np.long` types, respectively.\n\nReal floats\n\n`longdouble`[^13]\n\n[^13]: Depending on the platform, `np.longdouble` is (almost always) an alias for **either** `float128`,\n\nComplex floats\n\n`clongdouble`[^16]\n\n[^16]: Depending on the platform, `np.clongdouble` is (almost always) an alias for **either** `complex256`,\n\n\"Flexible\"\n\nScalar types with \"flexible\" length, whose values have a (constant) length\nthat depends on the specific `np.dtype` instantiation.\n\nOther types\n\n`bool_`[^0]\n\n*`generic`*[^22]\n\n[^2056]\n\n[^0]: Since NumPy 2, `np.bool` is preferred over `np.bool_`, which only exists for backwards compatibility.\n\n[^22]: At runtime `np.timedelta64` is a subclass of `np.signedinteger`, but this is currently not\n\n[^2056]: The `np.dypes.StringDType` has no associated numpy scalar type, and its `.type` attribute returns the\n\nLow-level interfaces\n\nWithin `optype.numpy` there are several `Can*` (single-method) and `Has*`\n(single-attribute) protocols, related to the `__array_*__` dunders of the\nNumPy Python API.\nThese typing protocols are, just like the `optype.Can*` and `optype.Has*` ones,\nruntime-checkable and extensible (i.e. not `@final`).\n\n> [!TIP]\n> All type parameters of these protocols can be omitted, which is equivalent\n> to passing its upper type bound.\n\n[User Guide: Interoperability with NumPy][DOC-ARRAY]\n\n[NEP 13][NEP13]\n\n[NEP 18][NEP18]\n\n[User Guide: Subclassing ndarray][DOC-AFIN]\n\n[API: Standard array subclasses][REF_ARRAY-WRAP]\n\n[API: The array interface protocol][REF_ARRAY-INTER]\n\n[API: Standard array subclasses][REF_ARRAY-PRIO]\n\n[API: Specifying and constructing data types][REF_DTYPE]"}, {"name": "optype", "tags": ["data", "math", "ui", "web"], "summary": "Building Blocks for Precise & Flexible Type Hints", "text": "This library is used to provide precise and flexible type hints for Python developers, enabling them to accurately express complex type relationships in their code. With optype, developers can create robust and maintainable type annotations that handle edge cases and variations in input types with ease."}, {"name": "oras", "tags": ["math"], "summary": "OCI Registry as Storage Python SDK", "text": "ORAS Python\n\n(#contributors-)\n\nOCI Registry as Storage enables libraries to push OCI Artifacts to [OCI Conformant](https://github.com/opencontainers/oci-conformance) registries. This is a Python SDK for Python developers to empower them to do this in their applications.\n\nSee our \u2b50\ufe0f [Documentation](https://oras-project.github.io/oras-py/) \u2b50\ufe0f to get started.\n\nCode of Conduct\n\nPlease note that this project has adopted the [CNCF Code of Conduct](https://github.com/cncf/foundation/blob/master/code-of-conduct.md).\nPlease follow it in all your interactions with the project members and users.\n\nContributing\n\nTo contribute to oras python, if you want to have discussion about a change, feature, or fix, you can open an issue first. We then ask that you open a pull request against the main branch. In the description please include the details of your change, e.g., why it is needed, what you did, and any further points for discussion. In addition:\n\n- For changes to the code:\n  - Please bump the version in the `oras/version.py` file\n  - Please also make a corresponding note in the `CHANGELOG.md`\n\nFor any changes to functionality or code that are not tested, please add one or more tests. Thank you for your contributions!\n\n\ufe0f Contributors \ufe0f\n\nWe use the [all-contributors](https://github.com/all-contributors/all-contributors)\ntool to generate a contributors graphic below.\n\nLicense\n\nThis code is licensed under the Apache 2.0 [LICENSE](LICENSE)."}, {"name": "oras", "tags": ["math"], "summary": "OCI Registry as Storage Python SDK", "text": "This library is used to enable Python developers to push OCI Artifacts to OCI Conformant registries from their applications. With this SDK, developers can empower their applications to store and manage OCI Artifacts in compliant registries."}, {"name": "orbax-checkpoint", "tags": ["math", "ml", "web"], "summary": "Orbax Checkpoint", "text": "Orbax Checkpointing\n\n`pip install orbax-checkpoint` (latest PyPi release) OR\n\n`pip install 'git+https://github.com/google/orbax/#subdirectory=checkpoint'` (from this repository, at HEAD)\n\n`import orbax.checkpoint`\n\nOrbax includes a checkpointing library oriented towards JAX users, supporting a\nvariety of different features required by different frameworks, including\nasynchronous checkpointing, various types, and various storage formats.\nWe aim to provide a highly customizable and composable API which maximizes\nflexibility for diverse use cases."}, {"name": "orbax-checkpoint", "tags": ["math", "ml", "web"], "summary": "Orbax Checkpoint", "text": "This library is used to implement efficient and flexible checkpointing functionality in JAX applications, supporting asynchronous operations and multiple data types. Developers can leverage this library to maximize flexibility in their use cases while ensuring seamless preservation of application state."}, {"name": "ortools", "tags": ["math"], "summary": "Google OR-Tools python libraries and modules", "text": "This project hosts operations research tools developed at Google and\nmade available as open source under the Apache 2.0 License.\n\nOR-Tools includes solvers for:\n\n- Constraint Programming\n  - CP-SAT solver: A constraint programming solver that uses SAT (satisfiability)\n  methods.\n  - Original CP solver: A constraint programming solver.\n- Linear and Mixed-Integer Programming\n  - Glop: A linear optimizer to find the optimal value of a linear objective\n  function, given a set of linear inequalities as constraints.\n  - MPSolver, ModelBuilder: Wrappers around commercial and other open source\n  solvers, including mixed integer solvers: CBC, CLP, GLPK, Gurobi or SCIP.\n- Vehicle Routing\n  A specialized library for identifying best vehicle routes given constraints.\n- Graph Algorithms\n  Code for finding shortest paths in graphs, min-cost flows, max flows, and\n  linear sum assignments."}, {"name": "ortools", "tags": ["math"], "summary": "Google OR-Tools python libraries and modules", "text": "This library is used to efficiently solve complex optimization problems in various domains such as linear and integer programming, constraint programming, and vehicle routing. Developers can leverage OR-Tools to optimize resource allocation, scheduling, and route planning, among other applications."}, {"name": "osmnx", "tags": ["math", "visualization", "web"], "summary": "Download, model, analyze, and visualize street networks and other geospatial features from OpenStreetMap", "text": "OSMnx\n\n(https://pypi.org/project/osmnx/)\n(https://pepy.tech/project/osmnx)\n(https://osmnx.readthedocs.io/)\n(https://github.com/gboeing/osmnx/actions/workflows/ci.yml)\n(https://codecov.io/gh/gboeing/osmnx)\n\n**OSMnx** is a Python package to easily download, model, analyze, and visualize street networks and other geospatial features from OpenStreetMap. You can download and model walking, driving, or biking networks with a single line of code then analyze and visualize them. You can just as easily work with urban amenities/points of interest, building footprints, transit stops, elevation data, street orientations, speed/travel time, and routing.\n\nOSMnx 2.0 is released: read the [migration guide](https://github.com/gboeing/osmnx/issues/1123).\n\nCitation\n\nIf you use OSMnx in your work, please cite the paper:\n\nBoeing, G. (2025). [Modeling and Analyzing Urban Networks and Amenities with OSMnx](https://doi.org/10.1111/gean.70009). *Geographical Analysis* 57 (4), 567-577. doi:10.1111/gean.70009\n\nGetting Started\n\nFirst read the [Getting Started](https://osmnx.readthedocs.io/en/stable/getting-started.html) guide for an introduction to the package and FAQ.\n\nThen work through the [Examples Gallery](https://github.com/gboeing/osmnx-examples) for step-by-step tutorials and sample code.\n\nInstallation\n\nFollow the [Installation](https://osmnx.readthedocs.io/en/stable/installation.html) guide to install OSMnx.\n\nSupport\n\nIf you have any trouble, consult the [User Reference](https://osmnx.readthedocs.io/en/stable/user-reference.html). The OSMnx repository is hosted on [GitHub](https://github.com/gboeing/osmnx). If you have a \"how-to\" or usage question, please ask it on [StackOverflow](https://stackoverflow.com/search?q=osmnx), as we reserve the repository's issue tracker for bug tracking and feature development.\n\nLicense\n\nOSMnx is open source and licensed under the MIT license. OpenStreetMap's open data [license](https://www.openstreetmap.org/copyright/) requires that derivative works provide proper attribution. Refer to the [Getting Started](https://osmnx.readthedocs.io/en/stable/getting-started.html) guide for usage limitations."}, {"name": "osmnx", "tags": ["math", "visualization", "web"], "summary": "Download, model, analyze, and visualize street networks and other geospatial features from OpenStreetMap", "text": "This library is used to download, model, analyze, and visualize street networks and other geospatial features from OpenStreetMap. It enables developers to easily access and interact with a wide range of urban data types through a simple and unified interface."}, {"name": "paddlex", "tags": ["data", "math", "ml", "web"], "summary": "Low-code development tool based on PaddlePaddle.", "text": "\u7b80\u4ecb\n\nPaddleX 3.0 \u662f\u57fa\u4e8e\u98de\u6868\u6846\u67b6\u6784\u5efa\u7684\u4f4e\u4ee3\u7801\u5f00\u53d1\u5de5\u5177\uff0c\u5b83\u96c6\u6210\u4e86\u4f17\u591a**\u5f00\u7bb1\u5373\u7528\u7684\u9884\u8bad\u7ec3\u6a21\u578b**\uff0c\u53ef\u4ee5\u5b9e\u73b0\u6a21\u578b\u4ece\u8bad\u7ec3\u5230\u63a8\u7406\u7684**\u5168\u6d41\u7a0b\u5f00\u53d1**\uff0c\u652f\u6301\u56fd\u5185\u5916**\u591a\u6b3e\u4e3b\u6d41\u786c\u4ef6**\uff0c\u52a9\u529bAI \u5f00\u53d1\u8005\u8fdb\u884c\u4ea7\u4e1a\u5b9e\u8df5\u3002\n\n\u7279\u6027\n   **\u6a21\u578b\u4e30\u5bcc\u4e00\u952e\u8c03\u7528**\uff1a\u5c06\u8986\u76d6\u6587\u672c\u56fe\u50cf\u667a\u80fd\u5206\u6790\u3001OCR\u3001\u76ee\u6807\u68c0\u6d4b\u3001\u65f6\u5e8f\u9884\u6d4b\u7b49\u591a\u4e2a\u5173\u952e\u9886\u57df\u7684 **200+ \u98de\u6868\u6a21\u578b**\u6574\u5408\u4e3a **33 \u6761\u6a21\u578b\u4ea7\u7ebf**\uff0c\u901a\u8fc7\u6781\u7b80\u7684 Python API \u4e00\u952e\u8c03\u7528\uff0c\u5feb\u901f\u4f53\u9a8c\u6a21\u578b\u6548\u679c\u3002\u540c\u65f6\u652f\u6301 **39 \u79cd\u5355\u529f\u80fd\u6a21\u5757**\uff0c\u65b9\u4fbf\u5f00\u53d1\u8005\u8fdb\u884c\u6a21\u578b\u7ec4\u5408\u4f7f\u7528\u3002\n\n   **\u63d0\u9ad8\u6548\u7387\u964d\u4f4e\u95e8\u69db**\uff1a\u5b9e\u73b0\u57fa\u4e8e\u7edf\u4e00\u547d\u4ee4\u548c\u56fe\u5f62\u754c\u9762\u7684\u6a21\u578b**\u5168\u6d41\u7a0b\u5f00\u53d1**\uff0c\u6253\u9020\u5927\u5c0f\u6a21\u578b\u7ed3\u5408\u3001\u5927\u6a21\u578b\u534a\u76d1\u7763\u5b66\u4e60\u548c\u591a\u6a21\u578b\u878d\u5408\u7684[**8 \u6761\u7279\u8272\u6a21\u578b\u4ea7\u7ebf**](https://aistudio.baidu.com/intro/paddlex)\uff0c\u5927\u5e45\u5ea6\u964d\u4f4e\u8fed\u4ee3\u6a21\u578b\u7684\u6210\u672c\u3002\n\n   **\u591a\u79cd\u573a\u666f\u7075\u6d3b\u90e8\u7f72**\uff1a\u652f\u6301**\u9ad8\u6027\u80fd\u63a8\u7406**\u3001**\u670d\u52a1\u5316\u90e8\u7f72**\u548c**\u7aef\u4fa7\u90e8\u7f72**\u7b49\u591a\u79cd\u90e8\u7f72\u65b9\u5f0f\uff0c\u786e\u4fdd\u4e0d\u540c\u5e94\u7528\u573a\u666f\u4e0b\u6a21\u578b\u7684\u9ad8\u6548\u8fd0\u884c\u548c\u5feb\u901f\u54cd\u5e94\u3002\n\n   **\u4e3b\u6d41\u786c\u4ef6\u9ad8\u6548\u652f\u6301**\uff1a\u652f\u6301\u82f1\u4f1f\u8fbe GPU\u3001\u6606\u4ed1\u82af\u3001\u6607\u817e\u548c\u5bd2\u6b66\u7eaa\u7b49**\u591a\u79cd\u4e3b\u6d41\u786c\u4ef6**\u7684\u65e0\u7f1d\u5207\u6362\uff0c\u786e\u4fdd\u9ad8\u6548\u8fd0\u884c\u3002\n\n\u8fd1\u671f\u66f4\u65b0\n\n **2025.10.16\uff0c\u53d1\u5e03 PaddleX v3.3.0**\uff0c\u65b0\u589e\u80fd\u529b\u5982\u4e0b\uff1a\n\n- **\u652f\u6301PaddleOCR-VL\u3001PP-OCRv5\u591a\u8bed\u79cd\u6a21\u578b\u7684\u63a8\u7406\u90e8\u7f72\u80fd\u529b\u3002**\n\n **2025.8.20\uff0c\u53d1\u5e03 PaddleX v3.2.0**\uff0c\u65b0\u589e\u80fd\u529b\u5982\u4e0b\uff1a\n\n- **\u90e8\u7f72\u80fd\u529b\u5347\u7ea7\uff1a**\n\n- **\u91cd\u8981\u6a21\u578b\u65b0\u589e\uff1a**\n\n- **Benchmark\u5347\u7ea7\uff1a**\n\n- **Bug\u4fee\u590d\uff1a**\n\n- **\u5176\u4ed6\u5347\u7ea7\uff1a**\n\n **2025.6.28\uff0c\u53d1\u5e03 PaddleX v3.1.0**\uff0c\u65b0\u589e\u80fd\u529b\u5982\u4e0b\uff1a\n\n- **\u91cd\u8981\u6a21\u578b\uff1a**\n  - **\u65b0\u589ePP-OCRv5\u591a\u8bed\u79cd\u6587\u672c\u8bc6\u522b\u6a21\u578b**\uff0c\u652f\u6301\u6cd5\u8bed\u3001\u897f\u73ed\u7259\u8bed\u3001\u8461\u8404\u7259\u8bed\u3001\u4fc4\u8bed\u3001\u97e9\u8bed\u7b4937\u79cd\u8bed\u8a00\u7684\u6587\u5b57\u8bc6\u522b\u6a21\u578b\u7684\u8bad\u63a8\u6d41\u7a0b\u3002**\u5e73\u5747\u7cbe\u5ea6\u6da8\u5e45\u8d8530%\u3002**\n  - \u5347\u7ea7PP-StructureV3\u4e2d\u7684**PP-Chart2Table\u6a21\u578b**\uff0c\u56fe\u8868\u8f6c\u8868\u80fd\u529b\u8fdb\u4e00\u6b65\u5347\u7ea7\uff0c\u5728\u5185\u90e8\u81ea\u5efa\u6d4b\u8bc4\u96c6\u5408\u4e0a\u6307\u6807\uff08RMS-F1\uff09**\u63d0\u53479.36\u4e2a\u767e\u5206\u70b9\uff0871.24% -> 80.60%\uff09**\n- **\u91cd\u8981\u4ea7\u7ebf\uff1a**\n  - \u65b0\u589e\u57fa\u4e8ePP-StructureV3\u548cERNIE 4.5 Turbo\u7684**\u6587\u6863\u7ffb\u8bd1\u4ea7\u7ebfPP-DocTranslation\uff0c\u652f\u6301\u7ffb\u8bd1Markdown\u6587\u6863\u3001\u5404\u79cd\u590d\u6742\u7248\u5f0f\u7684PDF\u6587\u6863\u548c\u6587\u6863\u56fe\u50cf\uff0c\u7ed3\u679c\u4fdd\u5b58\u4e3aMarkdown\u683c\u5f0f\u6587\u6863\u3002**\n\n **2025.5.20\uff0c\u53d1\u5e03 PaddleX v3.0.0**\uff0c\u76f8\u6bd4PaddleX v2.x\uff0c\u6838\u5fc3\u5347\u7ea7\u5982\u4e0b\uff1a\n\n**\u4e30\u5bcc\u7684\u6a21\u578b\u5e93\uff1a**\n- **\u6a21\u578b\u4e30\u5bcc\uff1a** PaddleX3.0 \u5305\u542b270+\u6a21\u578b\uff0c\u6db5\u76d6\u4e86\u56fe\u50cf\uff08\u89c6\u9891\uff09\u5206\u7c7b/\u68c0\u6d4b/\u5206\u5272\u3001OCR\u3001\u8bed\u97f3\u8bc6\u522b\u3001\u65f6\u5e8f\u7b49\u591a\u79cd\u573a\u666f\u3002\n- **\u65b9\u6848\u6210\u719f\uff1a** PaddleX3.0 \u57fa\u4e8e\u4e30\u5bcc\u7684\u6a21\u578b\u5e93\uff0c**\u63d0\u4f9b\u4e86\u901a\u7528\u6587\u6863\u89e3\u6790\u3001\u5173\u952e\u4fe1\u606f\u62bd\u53d6\u3001\u6587\u6863\u7406\u89e3\u3001\u8868\u683c\u8bc6\u522b\u3001\u901a\u7528\u56fe\u50cf\u8bc6\u522b\u7b49\u591a\u79cd\u91cd\u8981\u4e14\u6210\u719f\u7684AI\u89e3\u51b3\u65b9\u6848\u3002**\n\n**\u7edf\u4e00\u63a8\u7406\u63a5\u53e3\uff0c\u91cd\u6784\u90e8\u7f72\u80fd\u529b\uff1a**\n- **\u63a8\u7406\u63a5\u53e3\u6807\u51c6\u5316**\uff0c\u964d\u4f4e\u4e0d\u540c\u79cd\u7c7b\u6a21\u578b\u5e26\u6765\u7684API\u63a5\u53e3\u5dee\u5f02\uff0c\u51cf\u5c11\u7528\u6237\u5b66\u4e60\u6210\u672c\uff0c\u63d0\u5347\u4f01\u4e1a\u843d\u5730\u6548\u7387\u3002\n- **\u63d0\u4f9b\u591a\u6a21\u578b\u7ec4\u5408\u80fd\u529b**\uff0c\u590d\u6742\u4efb\u52a1\u53ef\u4ee5\u901a\u8fc7\u4e0d\u540c\u7684\u6a21\u578b\u65b9\u4fbf\u5730\u8fdb\u884c\u7ec4\u5408\u4f7f\u7528\uff0c\u5b9e\u73b01+1>2 \u7684\u80fd\u529b\u3002\n- **\u90e8\u7f72\u80fd\u529b\u5347\u7ea7\uff0c\u591a\u79cd\u6a21\u578b\u90e8\u7f72\u53ef\u4ee5\u4f7f\u7528\u7edf\u4e00\u7684\u547d\u4ee4\u7ba1\u7406\uff0c\u652f\u6301\u591a\u5361\u63a8\u7406\uff0c\u652f\u6301\u591a\u5361\u591a\u5b9e\u4f8b\u670d\u52a1\u5316\u90e8\u7f72\u3002**\n\n**\u5168\u9762\u9002\u914d\u98de\u6868\u6846\u67b63.0\uff1a**\n- **\u5168\u9762\u9002\u914d\u98de\u6868\u6846\u67b63.0\u65b0\u7279\u6027\uff1a** \u652f\u6301\u7f16\u8bd1\u5668\u8bad\u7ec3\uff0c\u8bad\u7ec3\u547d\u4ee4\u901a\u8fc7\u8ffd\u52a0 `-o Global.dy2st=True` \u5373\u53ef\u5f00\u542f\u7f16\u8bd1\u5668\u8bad\u7ec3\uff0c\u5728 GPU \u4e0a\uff0c\u591a\u6570\u6a21\u578b\u8bad\u7ec3\u901f\u5ea6\u53ef\u63d0\u5347 10% \u4ee5\u4e0a\uff0c\u5c11\u90e8\u5206\u6a21\u578b\u8bad\u7ec3\u901f\u5ea6\u53ef\u4ee5\u63d0\u5347 30% \u4ee5\u4e0a\u3002\u63a8\u7406\u65b9\u9762\uff0c\u6a21\u578b\u6574\u4f53\u9002\u914d\u98de\u6868 3.0 \u4e2d\u95f4\u8868\u793a\u6280\u672f\uff08PIR\uff09\uff0c\u62e5\u6709\u66f4\u52a0\u7075\u6d3b\u7684\u6269\u5c55\u80fd\u529b\u548c\u517c\u5bb9\u6027\uff0c\u9759\u6001\u56fe\u6a21\u578b\u5b58\u50a8\u6587\u4ef6\u540d\u7531 `xxx.pdmodel` \u6539\u4e3a `xxx.json`\u3002\n- **\u5168\u9762\u652f\u6301 ONNX \u683c\u5f0f\u6a21\u578b\uff1a** \u652f\u6301\u901a\u8fc7Paddle2ONNX\u63d2\u4ef6\u8f6c\u6362\u6a21\u578b\u683c\u5f0f\u3002\n\n**\u91cd\u78c5\u80fd\u529b\u652f\u6491\uff1a**\n- **\u652f\u6491PP-OCRv5\u7684\u4e32\u8054\u903b\u8f91\u548c\u591a\u786c\u4ef6\u63a8\u7406\u3001\u591a\u540e\u7aef\u63a8\u7406\u3001\u670d\u52a1\u5316\u90e8\u7f72\u80fd\u529b\u3002**\n- **\u652f\u6491PP-StructureV3\u7684\u590d\u6742\u6a21\u578b\u4e32\u8054\u548c\u5e76\u8054\u7684\u903b\u8f91\uff0c\u9996\u6b21\u4e32\u8054\u5e76\u8054\u517115\u4e2a\u6a21\u578b\uff0c\u5b9e\u73b0\u591a\u6a21\u578b\u534f\u540c\u7684\u590d\u6742pipeline\u3002\u7cbe\u5ea6\u5728 OmniDocBench \u699c\u5355\u4e0a\u8fbe\u5230 SOTA \u6c34\u5e73\u3002**\n- **\u652f\u6491PP-ChatOCRv4\u7684\u5927\u6a21\u578b\u4e32\u8054\u903b\u8f91\uff0c\u7ed3\u5408\u6587\u5fc3\u5927\u6a21\u578b4.5Turbo\uff0c\u7ed3\u5408\u65b0\u589e\u7684PP-DocBee2\uff0c\u5173\u952e\u4fe1\u606f\u62bd\u53d6\u7cbe\u5ea6\u76f8\u6bd4\u4e0a\u4e00\u4ee3\u63d0\u534715.7\u4e2a\u767e\u5206\u70b9\u3002**\n\n**\u591a\u786c\u4ef6\u652f\u6301\uff1a**\n- **\u6574\u4f53\u652f\u6301\u82f1\u4f1f\u8fbe\u3001\u82f1\u7279\u5c14\u3001\u82f9\u679cM\u7cfb\u5217\u3001\u6606\u4ed1\u82af\u3001\u6607\u817e\u3001\u5bd2\u6b66\u7eaa\u3001\u6d77\u5149\u3001\u71e7\u539f\u7b49\u82af\u7247\u7684\u8bad\u7ec3\u548c\u63a8\u7406\u3002**\n- **\u5728\u6607\u817e\u4e0a\uff0c\u5168\u9762\u9002\u914d\u7684\u6a21\u578b\u8fbe\u5230200\u4e2a\uff0c** \u652f\u6301OM\u9ad8\u6027\u80fd\u63a8\u7406\u7684\u6a21\u578b\u8fbe\u523021\u4e2a\u3002\u6b64\u5916\u652f\u6301PP-OCRv5\u3001PP-StructureV3\u7b49\u91cd\u8981\u6a21\u578b\u65b9\u6848\u3002\n- \u5728\u6606\u4ed1\u82af\u4e0a\u652f\u6301\u91cd\u8981\u5206\u7c7b\u3001\u68c0\u6d4b\u3001OCR\u7c7b\u6a21\u578b\uff08\u542bPP-OCRv5\uff09\u3002\n\n ##  \u6a21\u578b\u4ea7\u7ebf\u8bf4\u660e\n\n **PaddleX \u81f4\u529b\u4e8e\u5b9e\u73b0\u4ea7\u7ebf\u7ea7\u522b\u7684\u6a21\u578b\u8bad\u7ec3\u3001\u63a8\u7406\u4e0e\u90e8\u7f72\u3002\u6a21\u578b\u4ea7\u7ebf\u662f\u6307\u4e00\u7cfb\u5217\u9884\u5b9a\u4e49\u597d\u7684\u3001\u9488\u5bf9\u7279\u5b9aAI\u4efb\u52a1\u7684\u5f00\u53d1\u6d41\u7a0b\uff0c\u5176\u4e2d\u5305\u542b\u80fd\u591f\u72ec\u7acb\u5b8c\u6210\u67d0\u7c7b\u4efb\u52a1\u7684\u5355\u6a21\u578b\uff08\u5355\u529f\u80fd\u6a21\u5757\uff09\u7ec4\u5408\u3002**\n\n ##  \u80fd\u529b\u652f\u6301\n\nPaddleX\u7684\u5404\u4e2a\u4ea7\u7ebf\u5747\u652f\u6301\u672c\u5730**\u5feb\u901f\u63a8\u7406**\uff0c\u90e8\u5206\u6a21\u578b\u652f\u6301\u5728[AI Studio\u661f\u6cb3\u793e\u533a](https://aistudio.baidu.com/overview)\u4e0a\u8fdb\u884c**\u5728\u7ebf\u4f53\u9a8c**\uff0c\u60a8\u53ef\u4ee5\u5feb\u901f\u4f53\u9a8c\u5404\u4e2a\u4ea7\u7ebf\u7684\u9884\u8bad\u7ec3\u6a21\u578b\u6548\u679c\uff0c\u5982\u679c\u60a8\u5bf9\u4ea7\u7ebf\u7684\u9884\u8bad\u7ec3\u6a21\u578b\u6548\u679c\u6ee1\u610f\uff0c\u53ef\u4ee5\u76f4\u63a5\u5bf9\u4ea7\u7ebf\u8fdb\u884c[\u9ad8\u6027\u80fd\u63a8\u7406](https://paddlepaddle.github.io/PaddleX/latest/pipeline_deploy/high_performance_inference.html)/[\u670d\u52a1\u5316\u90e8\u7f72](https://paddlepaddle.github.io/PaddleX/latest/pipeline_deploy/serving.html)/[\u7aef\u4fa7\u90e8\u7f72](https://paddlepaddle.github.io/PaddleX/latest/pipeline_deploy/on_device_deployment.html)\uff0c\u5982\u679c\u4e0d\u6ee1\u610f\uff0c\u60a8\u4e5f\u53ef\u4ee5\u4f7f\u7528\u4ea7\u7ebf\u7684**\u4e8c\u6b21\u5f00\u53d1**\u80fd\u529b\uff0c\u63d0\u5347\u6548\u679c\u3002\u5b8c\u6574\u7684\u4ea7\u7ebf\u5f00\u53d1\u6d41\u7a0b\u8bf7\u53c2\u8003[PaddleX\u4ea7\u7ebf\u4f7f\u7528\u6982\u89c8](https://paddlepaddle.github.io/PaddleX/latest/pipeline_usage/pipeline_develop_guide.html)\u6216\u5404\u4ea7\u7ebf\u4f7f\u7528[\u6559\u7a0b](#-\u6587\u6863)\u3002\n\n\u6b64\u5916\uff0cPaddleX\u5728[AI Studio\u661f\u6cb3\u793e\u533a](https://aistudio.baidu.com/overview)\u4e3a\u5f00\u53d1\u8005\u63d0\u4f9b\u4e86\u57fa\u4e8e[\u4e91\u7aef\u56fe\u5f62\u5316\u5f00\u53d1\u754c\u9762](https://aistudio.baidu.com/pipeline/mine)\u7684\u5168\u6d41\u7a0b\u5f00\u53d1\u5de5\u5177, \u70b9\u51fb\u3010\u521b\u5efa\u4ea7\u7ebf\u3011\uff0c\u9009\u62e9\u5bf9\u5e94\u7684\u4efb\u52a1\u573a\u666f\u548c\u6a21\u578b\u4ea7\u7ebf\uff0c\u5c31\u53ef\u4ee5\u5f00\u542f\u5168\u6d41\u7a0b\u5f00\u53d1\u3002\u8be6\u7ec6\u8bf7\u53c2\u8003[\u6559\u7a0b\u300a\u96f6\u95e8\u69db\u5f00\u53d1\u4ea7\u4e1a\u7ea7AI\u6a21\u578b\u300b](https://aistudio.baidu.com/practical/introduce/546656605663301)\n\n> \u6ce8\uff1a\u4ee5\u4e0a\u529f\u80fd\u5747\u57fa\u4e8e GPU/CPU \u5b9e\u73b0\u3002PaddleX \u8fd8\u53ef\u5728\u6606\u4ed1\u82af\u3001\u6607\u817e\u3001\u5bd2\u6b66\u7eaa\u548c\u6d77\u5149\u7b49\u4e3b\u6d41\u786c\u4ef6\u4e0a\u8fdb\u884c\u5feb\u901f\u63a8\u7406\u548c\u4e8c\u6b21\u5f00\u53d1\u3002\u4e0b\u8868\u8be6\u7ec6\u5217\u51fa\u4e86\u6a21\u578b\u4ea7\u7ebf\u7684\u652f\u6301\u60c5\u51b5\uff0c\u5177\u4f53\u652f\u6301\u7684\u6a21\u578b\u5217\u8868\u8bf7\u53c2\u9605[\u6a21\u578b\u5217\u8868(\u6606\u4ed1\u82afXPU)](https://paddlepaddle.github.io/PaddleX/latest/support_list/model_list_xpu.html)/[\u6a21\u578b\u5217\u8868(\u6607\u817eNPU)](https://paddlepaddle.github.io/PaddleX/latest/support_list/model_list_npu.html)/[\u6a21\u578b\u5217\u8868(\u5bd2\u6b66\u7eaaMLU)](https://paddlepaddle.github.io/PaddleX/latest/support_list/model_list_mlu.html)/[\u6a21\u578b\u5217\u8868(\u6d77\u5149DCU)](https://paddlepaddle.github.io/PaddleX/latest/support_list/model_list_dcu.html)\u3002\u6211\u4eec\u6b63\u5728\u9002\u914d\u66f4\u591a\u7684\u6a21\u578b\uff0c\u5e76\u5728\u4e3b\u6d41\u786c\u4ef6\u4e0a\u63a8\u52a8\u9ad8\u6027\u80fd\u548c\u670d\u52a1\u5316\u90e8\u7f72\u7684\u5b9e\u65bd\u3002\n\n **\u56fd\u4ea7\u5316\u786c\u4ef6\u80fd\u529b\u652f\u6301**\n\n\u23ed\ufe0f \u5feb\u901f\u5f00\u59cb\n\n\ufe0f \u5b89\u88c5\n\n> \u5728\u5b89\u88c5 PaddleX \u4e4b\u524d\uff0c\u8bf7\u786e\u4fdd\u60a8\u5df2\u5177\u5907\u57fa\u672c\u7684 **Python \u8fd0\u884c\u73af\u5883**\uff08\u6ce8\uff1a\u76ee\u524d\u652f\u6301 Python 3.8 \u81f3 Python 3.12\uff09\u3002PaddleX 3.0.x \u7248\u672c\u4f9d\u8d56\u7684 PaddlePaddle \u7248\u672c\u4e3a 3.0.0 \u53ca\u4ee5\u4e0a\u7248\u672c\uff0c\u8bf7\u5728\u4f7f\u7528\u524d\u52a1\u5fc5\u4fdd\u8bc1\u7248\u672c\u7684\u5bf9\u5e94\u5173\u7cfb\u3002\n\n* **\u5b89\u88c5 PaddlePaddle**\n\n> \u65e0\u9700\u5173\u6ce8\u7269\u7406\u673a\u4e0a\u7684 CUDA \u7248\u672c\uff0c\u53ea\u9700\u5173\u6ce8\u663e\u5361\u9a71\u52a8\u7a0b\u5e8f\u7248\u672c\u3002\u66f4\u591a\u98de\u6868 Wheel \u7248\u672c\u4fe1\u606f\uff0c\u8bf7\u53c2\u8003[\u98de\u6868\u5b98\u7f51](https://www.paddlepaddle.org.cn/install/quick?docurl=/documentation./docs/zh/install/pip/linux-pip.html)\u3002\n\n* **\u5b89\u88c5PaddleX**\n\n>  \u66f4\u591a\u5b89\u88c5\u65b9\u5f0f\u53c2\u8003 [PaddleX \u5b89\u88c5\u6559\u7a0b](https://paddlepaddle.github.io/PaddleX/latest/installation/installation.html)\n\n\u547d\u4ee4\u884c\u4f7f\u7528\n\n\u4e00\u884c\u547d\u4ee4\u5373\u53ef\u5feb\u901f\u4f53\u9a8c\u4ea7\u7ebf\u6548\u679c\uff0c\u7edf\u4e00\u7684\u547d\u4ee4\u884c\u683c\u5f0f\u4e3a\uff1a\n\nPaddleX\u7684\u6bcf\u4e00\u6761\u4ea7\u7ebf\u5bf9\u5e94\u7279\u5b9a\u7684\u53c2\u6570\uff0c\u60a8\u53ef\u4ee5\u5728\u5404\u81ea\u7684\u4ea7\u7ebf\u6587\u6863\u4e2d\u67e5\u770b\u5177\u4f53\u7684\u53c2\u6570\u8bf4\u660e\u3002\u6bcf\u6761\u4ea7\u7ebf\u9700\u6307\u5b9a\u5fc5\u8981\u7684\u4e09\u4e2a\u53c2\u6570\uff1a\n* `pipeline`\uff1a\u4ea7\u7ebf\u540d\u79f0\u6216\u4ea7\u7ebf\u914d\u7f6e\u6587\u4ef6\n* `input`\uff1a\u5f85\u5904\u7406\u7684\u8f93\u5165\u6587\u4ef6\uff08\u5982\u56fe\u7247\uff09\u7684\u672c\u5730\u8def\u5f84\u3001\u76ee\u5f55\u6216 URL\n* `device`\uff1a\u4f7f\u7528\u7684\u786c\u4ef6\u8bbe\u5907\u53ca\u5e8f\u53f7\uff08\u4f8b\u5982`gpu:0`\u8868\u793a\u4f7f\u7528\u7b2c 0 \u5757 GPU\uff09\uff0c\u4e5f\u53ef\u9009\u62e9\u4f7f\u7528 NPU(`npu:0`)\u3001 XPU(`xpu:0`)\u3001CPU(`cpu`)\u7b49\u3002\n\n\u4ee5\u901a\u7528 OCR \u4ea7\u7ebf\u4e3a\u4f8b\uff1a\n\n   \u70b9\u51fb\u67e5\u770b\u8fd0\u884c\u7ed3\u679c \n\n\u53ef\u89c6\u5316\u7ed3\u679c\u5982\u4e0b\uff1a\n\n\u5176\u4ed6\u4ea7\u7ebf\u7684\u547d\u4ee4\u884c\u4f7f\u7528\uff0c\u53ea\u9700\u5c06 `pipeline` \u53c2\u6570\u8c03\u6574\u4e3a\u76f8\u5e94\u4ea7\u7ebf\u7684\u540d\u79f0\uff0c\u53c2\u6570\u8c03\u6574\u4e3a\u5bf9\u5e94\u7684\u4ea7\u7ebf\u7684\u53c2\u6570\u5373\u53ef\u3002\u4e0b\u9762\u5217\u51fa\u4e86\u6bcf\u4e2a\u4ea7\u7ebf\u5bf9\u5e94\u7684\u547d\u4ee4\uff1a\n\n   \u66f4\u591a\u4ea7\u7ebf\u7684\u547d\u4ee4\u884c\u4f7f\u7528\n\n\u4ea7\u7ebf\u540d\u79f0\n--------------------\n\u901a\u7528\u56fe\u50cf\u5206\u7c7b\n\u901a\u7528\u76ee\u6807\u68c0\u6d4b\n\u901a\u7528\u5b9e\u4f8b\u5206\u5272\n\u901a\u7528\u8bed\u4e49\u5206\u5272\n\u56fe\u50cf\u591a\u6807\u7b7e\u5206\u7c7b\n\u5c0f\u76ee\u6807\u68c0\u6d4b\n\u56fe\u50cf\u5f02\u5e38\u68c0\u6d4b\n\u884c\u4eba\u5c5e\u6027\u8bc6\u522b\n\u8f66\u8f86\u5c5e\u6027\u8bc6\u522b\n3D\u591a\u6a21\u6001\u878d\u5408\u68c0\u6d4b\n\u4eba\u4f53\u5173\u952e\u70b9\u68c0\u6d4b\n\u5f00\u653e\u8bcd\u6c47\u68c0\u6d4b\n\u5f00\u653e\u8bcd\u6c47\u5206\u5272\n\u65cb\u8f6c\u76ee\u6807\u68c0\u6d4b\n\u901a\u7528OCR\n\u6587\u6863\u56fe\u50cf\u9884\u5904\u7406\n\u901a\u7528\u8868\u683c\u8bc6\u522b\n\u901a\u7528\u8868\u683c\u8bc6\u522bv2\n\u901a\u7528\u7248\u9762\u89e3\u6790\n\u901a\u7528\u7248\u9762\u89e3\u6790v3\n\u516c\u5f0f\u8bc6\u522b\n\u5370\u7ae0\u6587\u672c\u8bc6\u522b\n\u65f6\u5e8f\u9884\u6d4b\n\u65f6\u5e8f\u5f02\u5e38\u68c0\u6d4b\n\u65f6\u5e8f\u5206\u7c7b\n\u591a\u8bed\u79cd\u8bed\u97f3\u8bc6\u522b\n\u901a\u7528\u89c6\u9891\u5206\u7c7b\n\u901a\u7528\u89c6\u9891\u68c0\u6d4b\n\nPython \u811a\u672c\u4f7f\u7528\n\n\u51e0\u884c\u4ee3\u7801\u5373\u53ef\u5b8c\u6210\u4ea7\u7ebf\u7684\u5feb\u901f\u63a8\u7406\uff0c\u7edf\u4e00\u7684 Python \u811a\u672c\u683c\u5f0f\u5982\u4e0b\uff1a\n\n\u6267\u884c\u4e86\u5982\u4e0b\u51e0\u4e2a\u6b65\u9aa4\uff1a\n\n* `create_pipeline()` \u5b9e\u4f8b\u5316\u4ea7\u7ebf\u5bf9\u8c61\n* \u4f20\u5165\u56fe\u7247\u5e76\u8c03\u7528\u4ea7\u7ebf\u5bf9\u8c61\u7684 `predict()` \u65b9\u6cd5\u8fdb\u884c\u63a8\u7406\u9884\u6d4b\n* \u5bf9\u9884\u6d4b\u7ed3\u679c\u8fdb\u884c\u5904\u7406\n\n\u5176\u4ed6\u4ea7\u7ebf\u7684 Python \u811a\u672c\u4f7f\u7528\uff0c\u53ea\u9700\u5c06 `create_pipeline()` \u65b9\u6cd5\u7684 `pipeline` \u53c2\u6570\u8c03\u6574\u4e3a\u76f8\u5e94\u4ea7\u7ebf\u7684\u540d\u79f0\uff0c\u53c2\u6570\u8c03\u6574\u4e3a\u5bf9\u5e94\u7684\u4ea7\u7ebf\u7684\u53c2\u6570\u5373\u53ef\u3002\u4e0b\u9762\u5217\u51fa\u4e86\u6bcf\u4e2a\u4ea7\u7ebf\u5bf9\u5e94\u7684\u53c2\u6570\u540d\u79f0\u53ca\u8be6\u7ec6\u7684\u4f7f\u7528\u89e3\u91ca\uff1a\n\n   \u66f4\u591a\u4ea7\u7ebf\u7684Python\u811a\u672c\u4f7f\u7528\n\n\u4ea7\u7ebf\u540d\u79f0\n--------------------\n\u6587\u6863\u573a\u666f\u4fe1\u606f\u62bd\u53d6v4\n\u6587\u6863\u573a\u666f\u4fe1\u606f\u62bd\u53d6v3\n\u901a\u7528\u56fe\u50cf\u5206\u7c7b\n\u901a\u7528\u76ee\u6807\u68c0\u6d4b\n\u901a\u7528\u5b9e\u4f8b\u5206\u5272\n\u901a\u7528\u8bed\u4e49\u5206\u5272\n\u56fe\u50cf\u591a\u6807\u7b7e\u5206\u7c7b\n\u5c0f\u76ee\u6807\u68c0\u6d4b\n\u56fe\u50cf\u5f02\u5e38\u68c0\u6d4b\n\u901a\u7528\u56fe\u50cf\u8bc6\u522b\n\u4eba\u8138\u8bc6\u522b\n\u8f66\u8f86\u5c5e\u6027\u8bc6\u522b\n\u884c\u4eba\u5c5e\u6027\u8bc6\u522b\n3D\u591a\u6a21\u6001\u878d\u5408\u68c0\u6d4b\n\u4eba\u4f53\u5173\u952e\u70b9\u68c0\u6d4b\n\u5f00\u653e\u8bcd\u6c47\u68c0\u6d4b\n\u5f00\u653e\u8bcd\u6c47\u5206\u5272\n\u65cb\u8f6c\u76ee\u6807\u68c0\u6d4b\n\u901a\u7528OCR\n\u6587\u6863\u56fe\u50cf\u9884\u5904\u7406\n\u901a\u7528\u8868\u683c\u8bc6\u522b\n\u901a\u7528\u8868\u683c\u8bc6\u522bv2\n\u901a\u7528\u7248\u9762\u89e3\u6790\n\u901a\u7528\u7248\u9762\u89e3\u6790v3\n\u516c\u5f0f\u8bc6\u522b\n\u5370\u7ae0\u6587\u672c\u8bc6\u522b\n\u65f6\u5e8f\u9884\u6d4b\n\u65f6\u5e8f\u5f02\u5e38\u68c0\u6d4b\n\u65f6\u5e8f\u5206\u7c7b\n\u591a\u8bed\u79cd\u8bed\u97f3\u8bc6\u522b\n\u901a\u7528\u89c6\u9891\u5206\u7c7b\n\u901a\u7528\u89c6\u9891\u68c0\u6d4b\n\u6587\u6863\u7406\u89e3\n\n\u6587\u6863\n\n  * [ PaddlePaddle \u5b89\u88c5\u6559\u7a0b](https://paddlepaddle.github.io/PaddleX/latest/installation/paddlepaddle_install.html)\n  * [ PaddleX \u5b89\u88c5\u6559\u7a0b](https://paddlepaddle.github.io/PaddleX/latest/installation/installation.html)\n\n   \u4ea7\u7ebf\u4f7f\u7528 \n\n* [ PaddleX \u4ea7\u7ebf\u4f7f\u7528\u6982\u89c8](https://paddlepaddle.github.io/PaddleX/latest/pipeline_usage/pipeline_develop_guide.html)\n\n* \n\n   * [ \u6587\u6863\u573a\u666f\u4fe1\u606f\u62bd\u53d6v3\u4ea7\u7ebf\u4f7f\u7528\u6559\u7a0b](https://paddlepaddle.github.io/PaddleX/latest/pipeline_usage/tutorials/information_extraction_pipelines/document_scene_information_extraction_v3.html)\n\n   * [ \u6587\u6863\u573a\u666f\u4fe1\u606f\u62bd\u53d6v4\u4ea7\u7ebf\u4f7f\u7528\u6559\u7a0b](https://paddlepaddle.github.io/PaddleX/latest/pipeline_usage/tutorials/information_extraction_pipelines/document_scene_information_extraction_v4.html)\n\n* \n\n  * [ \u901a\u7528 OCR \u4ea7\u7ebf\u4f7f\u7528\u6559\u7a0b](https://paddlepaddle.github.io/PaddleX/latest/pipeline_usage/tutorials/ocr_pipelines/OCR.html )\n\n  * [ \u901a\u7528\u8868\u683c\u8bc6\u522b\u4ea7\u7ebf\u4f7f\u7528\u6559\u7a0b](https://paddlepaddle.github.io/PaddleX/latest/pipeline_usage/tutorials/ocr_pipelines/table_recognition.html )\n\n  * [\ufe0f \u901a\u7528\u8868\u683c\u8bc6\u522bv2\u4ea7\u7ebf\u4f7f\u7528\u6559\u7a0b](https://paddlepaddle.github.io/PaddleX/latest/pipeline_usage/tutorials/ocr_pipelines/table_recognition_v2.html)\n\n  * [ \u901a\u7528\u7248\u9762\u89e3\u6790\u4ea7\u7ebf\u4f7f\u7528\u6559\u7a0b](https://paddlepaddle.github.io/PaddleX/latest/pipeline_usage/tutorials/ocr_pipelines/layout_parsing.html)\n\n  * [\ufe0f \u901a\u7528\u7248\u9762\u89e3\u6790\u4ea7\u7ebfv3\u4f7f\u7528\u6559\u7a0b](https://paddlepaddle.github.io/PaddleX/latest/pipeline_usage/tutorials/ocr_pipelines/PP-StructureV3.html)\n  * [ \u516c\u5f0f\u8bc6\u522b\u4ea7\u7ebf\u4f7f\u7528\u6559\u7a0b](https://paddlepaddle.github.io/PaddleX/latest/pipeline_usage/tutorials/ocr_pipelines/formula_recognition.html)\n  * [ \u5370\u7ae0\u6587\u672c\u8bc6\u522b\u4ea7\u7ebf\u4f7f\u7528\u6559\u7a0b](https://paddlepaddle.github.io/PaddleX/latest/pipeline_usage/tutorials/ocr_pipelines/seal_recognition.html)\n  * [\ufe0f \u6587\u6863\u56fe\u50cf\u9884\u5904\u7406\u4ea7\u7ebf\u4f7f\u7528\u6559\u7a0b](https://paddlepaddle.github.io/PaddleX/latest/pipeline_usage/tutorials/ocr_pipelines/doc_preprocessor.html)\n\n* \n\n   * [\ufe0f \u901a\u7528\u56fe\u50cf\u5206\u7c7b\u4ea7\u7ebf\u4f7f\u7528\u6559\u7a0b](https://paddlepaddle.github.io/PaddleX/latest/pipeline_usage/tutorials/cv_pipelines/image_classification.html)\n\n   * [ \u901a\u7528\u5b9e\u4f8b\u5206\u5272\u4ea7\u7ebf\u4f7f\u7528\u6559\u7a0b](https://paddlepaddle.github.io/PaddleX/latest/pipeline_usage/tutorials/cv_pipelines/instance_segmentation.html)\n\n   * [\ufe0f \u56fe\u50cf\u591a\u6807\u7b7e\u5206\u7c7b\u4ea7\u7ebf\u4f7f\u7528\u6559\u7a0b](https://paddlepaddle.github.io/PaddleX/latest/pipeline_usage/tutorials/cv_pipelines/image_multi_label_classification.html)\n\n   * [\ufe0f \u56fe\u50cf\u5f02\u5e38\u68c0\u6d4b\u4ea7\u7ebf\u4f7f\u7528\u6559\u7a0b](https://paddlepaddle.github.io/PaddleX/latest/pipeline_usage/tutorials/cv_pipelines/image_anomaly_detection.html)\n\n   * [ \u4eba\u4f53\u5173\u952e\u70b9\u68c0\u6d4b\u4ea7\u7ebf\u4f7f\u7528\u6559\u7a0b](https://paddlepaddle.github.io/PaddleX/latest/pipeline_usage/tutorials/cv_pipelines/human_keypoint_detection.html)\n\n   * [ \u5f00\u653e\u8bcd\u6c47\u5206\u5272\u4ea7\u7ebf\u4f7f\u7528\u6559\u7a0b](https://paddlepaddle.github.io/PaddleX/latest/pipeline_usage/tutorials/cv_pipelines/open_vocabulary_segmentation.html)\n\n   * [\ufe0f \u901a\u7528\u56fe\u50cf\u8bc6\u522b\u4ea7\u7ebf\u4f7f\u7528\u6559\u7a0b](https://paddlepaddle.github.io/PaddleX/latest/pipeline_usage/tutorials/cv_pipelines/general_image_recognition.html)\n\n   * [ \u8f66\u8f86\u5c5e\u6027\u8bc6\u522b\u4ea7\u7ebf\u4f7f\u7528\u6559\u7a0b](https://paddlepaddle.github.io/PaddleX/latest/pipeline_usage/tutorials/cv_pipelines/vehicle_attribute_recognition.html)\n\n* \n\n   * [ \u65f6\u5e8f\u9884\u6d4b\u4ea7\u7ebf\u4f7f\u7528\u6559\u7a0b](https://paddlepaddle.github.io/PaddleX/latest/pipeline_usage/tutorials/time_series_pipelines/time_series_forecasting.html)\n\n   * [ \u65f6\u5e8f\u5f02\u5e38\u68c0\u6d4b\u4ea7\u7ebf\u4f7f\u7528\u6559\u7a0b](https://paddlepaddle.github.io/PaddleX/latest/pipeline_usage/tutorials/time_series_pipelines/time_series_anomaly_detection.html)\n\n* \n\n* \n\n* \n\n   * [ \u6587\u6863\u7406\u89e3\u4ea7\u7ebf\u4f7f\u7528\u6559\u7a0b](https://paddlepaddle.github.io/PaddleX/latest/pipeline_usage/tutorials/vlm_pipelines/doc_understanding.html)\n\n* \n\n   * [\ufe0f PaddleX \u4ea7\u7ebf\u547d\u4ee4\u884c\u4f7f\u7528\u8bf4\u660e](https://paddlepaddle.github.io/PaddleX/latest/pipeline_usage/instructions/pipeline_CLI_usage.html)\n\n  * [ PaddleX \u4ea7\u7ebf Python \u811a\u672c\u4f7f\u7528\u8bf4\u660e](https://paddlepaddle.github.io/PaddleX/latest/pipeline_usage/instructions/pipeline_python_API.html)\n\n  * [ \u4ea7\u7ebf\u5e76\u884c\u63a8\u7406](https://paddlepaddle.github.io/PaddleX/latest/pipeline_usage/instructions/parallel_inference.html)\n\n  \u2699\ufe0f \u5355\u529f\u80fd\u6a21\u5757\u4f7f\u7528 \n\n* \n\n  * [ \u6587\u672c\u68c0\u6d4b\u6a21\u5757\u4f7f\u7528\u6559\u7a0b](https://paddlepaddle.github.io/PaddleX/latest/module_usage/tutorials/ocr_modules/text_detection.html)\n\n  * [ \u5370\u7ae0\u6587\u672c\u68c0\u6d4b\u6a21\u5757\u4f7f\u7528\u6559\u7a0b](https://paddlepaddle.github.io/PaddleX/latest/module_usage/tutorials/ocr_modules/seal_text_detection.html)\n\n  * [ \u6587\u672c\u8bc6\u522b\u6a21\u5757\u4f7f\u7528\u6559\u7a0b](https://paddlepaddle.github.io/PaddleX/latest/module_usage/tutorials/ocr_modules/text_recognition.html)\n\n  * [\ufe0f \u7248\u9762\u533a\u57df\u68c0\u6d4b\u6a21\u5757\u4f7f\u7528\u6559\u7a0b](https://paddlepaddle.github.io/PaddleX/latest/module_usage/tutorials/ocr_modules/layout_detection.html)\n\n  * [ \u8868\u683c\u7ed3\u6784\u8bc6\u522b\u6a21\u5757\u4f7f\u7528\u6559\u7a0b](https://paddlepaddle.github.io/PaddleX/latest/module_usage/tutorials/ocr_modules/table_structure_recognition.html)\n\n  * [ \u8868\u683c\u5355\u5143\u683c\u68c0\u6d4b\u6a21\u5757\u4f7f\u7528\u6559\u7a0b](https://paddlepaddle.github.io/PaddleX/latest/module_usage/tutorials/ocr_modules/table_cells_detection.html)\n\n  * [ \u8868\u683c\u5206\u7c7b\u6a21\u5757\u4f7f\u7528\u6559\u7a0b](https://paddlepaddle.github.io/PaddleX/latest/module_usage/tutorials/ocr_modules/table_classification.html)\n\n  * [ \u6587\u6863\u56fe\u50cf\u65b9\u5411\u5206\u7c7b\u4f7f\u7528\u6559\u7a0b](https://paddlepaddle.github.io/PaddleX/latest/module_usage/tutorials/ocr_modules/doc_img_orientation_classification.html)\n\n  * [ \u6587\u672c\u56fe\u50cf\u77eb\u6b63\u6a21\u5757\u4f7f\u7528\u6559\u7a0b](https://paddlepaddle.github.io/PaddleX/latest/module_usage/tutorials/ocr_modules/text_image_unwarping.html)\n\n  * [ \u6587\u672c\u884c\u65b9\u5411\u5206\u7c7b\u6a21\u5757\u4f7f\u7528\u6559\u7a0b](https://paddlepaddle.github.io/PaddleX/latest/module_usage/tutorials/ocr_modules/textline_orientation_classification.html)\n\n  * [ \u516c\u5f0f\u8bc6\u522b\u6a21\u5757\u4f7f\u7528\u6559\u7a0b](https://paddlepaddle.github.io/PaddleX/latest/module_usage/tutorials/ocr_modules/formula_recognition.html)\n\n* \n\n  * [ \u56fe\u50cf\u5206\u7c7b\u6a21\u5757\u4f7f\u7528\u6559\u7a0b](https://paddlepaddle.github.io/PaddleX/latest/module_usage/tutorials/cv_modules/image_classification.html)\n\n  * [\ufe0f \u56fe\u50cf\u591a\u6807\u7b7e\u5206\u7c7b\u6a21\u5757\u4f7f\u7528\u6559\u7a0b](https://paddlepaddle.github.io/PaddleX/latest/module_usage/tutorials/cv_modules/image_multilabel_classification.html)\n\n  * [ \u884c\u4eba\u5c5e\u6027\u8bc6\u522b\u6a21\u5757\u4f7f\u7528\u6559\u7a0b](https://paddlepaddle.github.io/PaddleX/latest/module_usage/tutorials/cv_modules/pedestrian_attribute_recognition.html)\n\n  * [ \u8f66\u8f86\u5c5e\u6027\u8bc6\u522b\u6a21\u5757\u4f7f\u7528\u6559\u7a0b](https://paddlepaddle.github.io/PaddleX/latest/module_usage/tutorials/cv_modules/vehicle_attribute_recognition.html)\n\n* \n\n* \n\n  * [ \u76ee\u6807\u68c0\u6d4b\u6a21\u5757\u4f7f\u7528\u6559\u7a0b](https://paddlepaddle.github.io/PaddleX/latest/module_usage/tutorials/cv_modules/object_detection.html)\n\n  * [ \u5c0f\u76ee\u6807\u68c0\u6d4b\u6a21\u5757\u4f7f\u7528\u6559\u7a0b](https://paddlepaddle.github.io/PaddleX/latest/module_usage/tutorials/cv_modules/small_object_detection.html)\n\n  * [\u200d\u200d \u4eba\u8138\u68c0\u6d4b\u6a21\u5757\u4f7f\u7528\u6559\u7a0b](https://paddlepaddle.github.io/PaddleX/latest/module_usage/tutorials/cv_modules/face_detection.html)\n\n  * [ \u4e3b\u4f53\u68c0\u6d4b\u6a21\u5757\u4f7f\u7528\u6559\u7a0b](https://paddlepaddle.github.io/PaddleX/latest/module_usage/tutorials/cv_modules/mainbody_detection.html)\n\n  * [ \u884c\u4eba\u68c0\u6d4b\u6a21\u5757\u4f7f\u7528\u6559\u7a0b](https://paddlepaddle.github.io/PaddleX/latest/module_usage/tutorials/cv_modules/human_detection.html)\n\n  * [\u200d\u2642\ufe0f \u4eba\u4f53\u5173\u952e\u70b9\u68c0\u6d4b\u6a21\u5757\u4f7f\u7528\u6559\u7a0b](https://paddlepaddle.github.io/PaddleX/latest/module_usage/tutorials/cv_modules/human_keypoint_detection.html)\n\n  * [ \u5f00\u653e\u8bcd\u6c47\u76ee\u6807\u68c0\u6d4b\u6a21\u5757\u4f7f\u7528\u6559\u7a0b](https://paddlepaddle.github.io/PaddleX/latest/module_usage/tutorials/cv_modules/open_vocabulary_detection.html)\n\n* \n\n  * [\ufe0f \u8bed\u4e49\u5206\u5272\u6a21\u5757\u4f7f\u7528\u6559\u7a0b](https://paddlepaddle.github.io/PaddleX/latest/module_usage/tutorials/cv_modules/semantic_segmentation.html)\n\n  * [ \u5b9e\u4f8b\u5206\u5272\u6a21\u5757\u4f7f\u7528\u6559\u7a0b](https://paddlepaddle.github.io/PaddleX/latest/module_usage/tutorials/cv_modules/instance_segmentation.html)\n\n  * [ \u56fe\u50cf\u5f02\u5e38\u68c0\u6d4b\u6a21\u5757\u4f7f\u7528\u6559\u7a0b](https://paddlepaddle.github.io/PaddleX/latest/module_usage/tutorials/cv_modules/anomaly_detection.html)\n\n  * [ \u5f00\u653e\u8bcd\u6c47\u5206\u5272\u6a21\u5757\u4f7f\u7528\u6559\u7a0b](https://paddlepaddle.github.io/PaddleX/latest/module_usage/tutorials/cv_modules/open_vocabulary_segmentation.html)\n\n* \n\n  * [ \u65f6\u5e8f\u9884\u6d4b\u6a21\u5757\u4f7f\u7528\u6559\u7a0b](https://paddlepaddle.github.io/PaddleX/latest/module_usage/tutorials/time_series_modules/time_series_forecasting.html)\n\n  * [ \u65f6\u5e8f\u5f02\u5e38\u68c0\u6d4b\u6a21\u5757\u4f7f\u7528\u6559\u7a0b](https://paddlepaddle.github.io/PaddleX/latest/module_usage/tutorials/time_series_modules/time_series_anomaly_detection.html)\n\n  * [ \u65f6\u5e8f\u5206\u7c7b\u6a21\u5757\u4f7f\u7528\u6559\u7a0b](https://paddlepaddle.github.io/PaddleX/latest/module_usage/tutorials/time_series_modules/time_series_classification.html)\n\n* \n\n  * [ \u591a\u8bed\u79cd\u8bed\u97f3\u8bc6\u522b\u6a21\u5757\u4f7f\u7528\u6559\u7a0b](https://paddlepaddle.github.io/PaddleX/latest/module_usage/tutorials/speech_modules/multilingual_speech_recognition.html)\n\n* \n\n  * [ 3D\u591a\u6a21\u6001\u878d\u5408\u68c0\u6d4b\u6a21\u5757\u4f7f\u7528\u6559\u7a0b](https://paddlepaddle.github.io/PaddleX/latest/module_usage/tutorials/cv_modules/3d_bev_detection.html)\n\n* \n\n  * [ \u6587\u6863\u7c7b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u6a21\u5757\u4f7f\u7528\u6559\u7a0b](https://paddlepaddle.github.io/PaddleX/latest/module_usage/tutorials/vlm_modules/doc_vlm.html)\n\n  * [ \u56fe\u8868\u89e3\u6790\u6a21\u5757\u4f7f\u7528\u6559\u7a0b](https://paddlepaddle.github.io/PaddleX/latest/module_usage/tutorials/vlm_modules/chart_parsing.html)\n\n* \n\n  * [ PaddleX \u5355\u6a21\u578b Python \u811a\u672c\u4f7f\u7528\u8bf4\u660e](https://paddlepaddle.github.io/PaddleX/latest/module_usage/instructions/model_python_API.html)\n\n  * [ PaddleX \u901a\u7528\u6a21\u578b\u914d\u7f6e\u6587\u4ef6\u53c2\u6570\u8bf4\u660e](https://paddlepaddle.github.io/PaddleX/latest/module_usage/instructions/config_parameters_common.html)\n\n  * [ PaddleX \u65f6\u5e8f\u4efb\u52a1\u6a21\u578b\u914d\u7f6e\u6587\u4ef6\u53c2\u6570\u8bf4\u660e](https://paddlepaddle.github.io/PaddleX/latest/module_usage/instructions/config_parameters_time_series.html)\n\n  * [ PaddleX 3d\u4efb\u52a1\u6a21\u578b\u914d\u7f6e\u6587\u4ef6\u53c2\u6570\u8bf4\u660e](https://paddlepaddle.github.io/PaddleX/latest/module_usage/instructions/config_parameters_3d.html)\n\n  * [ \u6a21\u578b\u63a8\u7406 Benchmark](https://paddlepaddle.github.io/PaddleX/latest/module_usage/instructions/benchmark.html)\n\n  * [ PaddleX \u9ad8\u6027\u80fd\u63a8\u7406\u6307\u5357](https://paddlepaddle.github.io/PaddleX/latest/pipeline_deploy/high_performance_inference.html)\n  * [\ufe0f PaddleX \u670d\u52a1\u5316\u90e8\u7f72\u6307\u5357](https://paddlepaddle.github.io/PaddleX/latest/pipeline_deploy/serving.html)\n  * [ PaddleX \u7aef\u4fa7\u90e8\u7f72\u6307\u5357](https://paddlepaddle.github.io/PaddleX/latest/pipeline_deploy/on_device_deployment.html)\n  * [ \u83b7\u53d6 ONNX \u6a21\u578b](https://paddlepaddle.github.io/PaddleX/latest/pipeline_deploy/paddle2onnx.html)\n\n  * [ \u591a\u786c\u4ef6\u4f7f\u7528\u6307\u5357](https://paddlepaddle.github.io/PaddleX/latest/other_devices_support/multi_devices_use_guide.html)\n  * [\ufe0f \u6d77\u5149 DCU \u98de\u6868\u5b89\u88c5\u6559\u7a0b](https://paddlepaddle.github.io/PaddleX/latest/other_devices_support/paddlepaddle_install_DCU.html)\n  * [ \u5bd2\u6b66\u7eaa MLU \u98de\u6868\u5b89\u88c5\u6559\u7a0b](https://paddlepaddle.github.io/PaddleX/latest/other_devices_support/paddlepaddle_install_MLU.html)\n  * [ \u6607\u817e NPU \u98de\u6868\u5b89\u88c5\u6559\u7a0b](https://paddlepaddle.github.io/PaddleX/latest/other_devices_support/paddlepaddle_install_NPU.html)\n  * [ \u6606\u4ed1 XPU \u98de\u6868\u5b89\u88c5\u6559\u7a0b](https://paddlepaddle.github.io/PaddleX/latest/other_devices_support/paddlepaddle_install_XPU.html)\n  * [ \u71e7\u539f GCU \u98de\u6868\u5b89\u88c5\u6559\u7a0b](https://paddlepaddle.github.io/PaddleX/latest/other_devices_support/paddlepaddle_install_GCU.html)\n\n   \u6570\u636e\u6807\u6ce8\u6559\u7a0b \n\n- \n\n  - [ \u56fe\u50cf\u5206\u7c7b\u4efb\u52a1\u6a21\u5757](https://paddlepaddle.github.io/PaddleX/latest/data_annotations/cv_modules/image_classification.html)\n\n  - [ \u56fe\u50cf\u7279\u5f81\u4efb\u52a1\u6a21\u5757](https://paddlepaddle.github.io/PaddleX/latest/data_annotations/cv_modules/image_feature.html)\n\n  - [ \u5b9e\u4f8b\u5206\u5272\u4efb\u52a1\u6a21\u5757](https://paddlepaddle.github.io/PaddleX/latest/data_annotations/cv_modules/instance_segmentation.html)\n\n  - [ \u56fe\u50cf\u591a\u6807\u7b7e\u5206\u7c7b\u6a21\u5757](https://paddlepaddle.github.io/PaddleX/latest/data_annotations/cv_modules/ml_classification.html)\n\n  - [ \u76ee\u6807\u68c0\u6d4b\u4efb\u52a1\u6a21\u5757](https://paddlepaddle.github.io/PaddleX/latest/data_annotations/cv_modules/object_detection.html)\n\n  - [ \u8bed\u4e49\u5206\u5272\u4efb\u52a1\u6a21\u5757](https://paddlepaddle.github.io/PaddleX/latest/data_annotations/cv_modules/semantic_segmentation.html)\n\n- \n\n  - [ \u8868\u683c\u8bc6\u522b\u4efb\u52a1\u6a21\u5757](https://paddlepaddle.github.io/PaddleX/latest/data_annotations/ocr_modules/table_recognition.html)\n\n  - [ \u6587\u672c\u68c0\u6d4b/\u8bc6\u522b\u4efb\u52a1\u6a21\u5757](https://paddlepaddle.github.io/PaddleX/latest/data_annotations/ocr_modules/text_detection_recognition.html)\n\n- \n\n  - [ \u65f6\u5e8f\u5f02\u5e38\u68c0\u6d4b\u4efb\u52a1\u6a21\u5757](https://paddlepaddle.github.io/PaddleX/latest/data_annotations/time_series_modules/time_series_anomaly_detection.html)\n\n  - [\u65f6\u5e8f\u5206\u7c7b\u4efb\u52a1\u6a21\u5757](https://paddlepaddle.github.io/PaddleX/latest/data_annotations/time_series_modules/time_series_classification.html)\n\n  - [ \u65f6\u5e8f\u9884\u6d4b\u4efb\u52a1\u6a21\u5757](https://paddlepaddle.github.io/PaddleX/latest/data_annotations/time_series_modules/time_series_forecasting.html)\n\n  * [\ufe0f PaddleX\u4ea7\u7ebf\u5217\u8868(CPU/GPU)](https://paddlepaddle.github.io/PaddleX/latest/support_list/pipelines_list.html)\n  * [ PaddleX\u4ea7\u7ebf\u5217\u8868(DCU)](https://paddlepaddle.github.io/PaddleX/latest/support_list/pipelines_list_dcu.html)\n  * [ PaddleX\u4ea7\u7ebf\u5217\u8868(MLU)](https://paddlepaddle.github.io/PaddleX/latest/support_list/pipelines_list_mlu.html)\n  * [ PaddleX\u4ea7\u7ebf\u5217\u8868(NPU)](https://paddlepaddle.github.io/PaddleX/latest/support_list/pipelines_list_npu.html)\n  * [ PaddleX\u4ea7\u7ebf\u5217\u8868(XPU)](https://paddlepaddle.github.io/PaddleX/latest/support_list/pipelines_list_xpu.html)\n\n  * [\ufe0f PaddleX\u6a21\u578b\u5217\u8868\uff08CPU/GPU\uff09](https://paddlepaddle.github.io/PaddleX/latest/support_list/models_list.html)\n  * [ PaddleX\u6a21\u578b\u5217\u8868\uff08\u6d77\u5149 DCU\uff09](https://paddlepaddle.github.io/PaddleX/latest/support_list/model_list_dcu.html)\n  * [ PaddleX\u6a21\u578b\u5217\u8868\uff08\u5bd2\u6b66\u7eaa MLU\uff09](https://paddlepaddle.github.io/PaddleX/latest/support_list/model_list_mlu.html)\n  * [ PaddleX\u6a21\u578b\u5217\u8868\uff08\u6607\u817e NPU\uff09](https://paddlepaddle.github.io/PaddleX/latest/support_list/model_list_npu.html)\n  * [ PaddleX\u6a21\u578b\u5217\u8868\uff08\u6606\u4ed1 XPU\uff09](https://paddlepaddle.github.io/PaddleX/latest/support_list/model_list_xpu.html)\n  * [ PaddleX\u6a21\u578b\u5217\u8868\uff08\u71e7\u539f GCU\uff09](https://paddlepaddle.github.io/PaddleX/latest/support_list/model_list_gcu.html)\n\n* [ \u6587\u6863\u573a\u666f\u4fe1\u606f\u62bd\u53d6v3\u6a21\u578b\u4ea7\u7ebf\u2014\u2014\u2014\u8bba\u6587\u6587\u732e\u4fe1\u606f\u62bd\u53d6\u5e94\u7528\u6559\u7a0b](https://paddlepaddle.github.io/PaddleX/3.0/practical_tutorials/document_scene_information_extraction%28layout_detection%29_tutorial.html)\n\n* [ \u6587\u6863\u573a\u666f\u4fe1\u606f\u62bd\u53d6v3\u6a21\u578b\u4ea7\u7ebf\u2014\u2014\u2014\u5370\u7ae0\u4fe1\u606f\u62bd\u53d6\u5e94\u7528\u6559\u7a0b](https://paddlepaddle.github.io/PaddleX/3.0/practical_tutorials/document_scene_information_extraction%28seal_recognition%29_tutorial.html)\n\n* [ \u6587\u6863\u573a\u666f\u4fe1\u606f\u62bd\u53d6v3\u6a21\u578b\u4ea7\u7ebf\u2014\u2014\u2014DeepSeek \u7bc7](https://paddlepaddle.github.io/PaddleX/latest/practical_tutorials/document_scene_information_extraction(deepseek)_tutorial.html)\n\n* [ \u901a\u7528 OCR \u6a21\u578b\u4ea7\u7ebf\u2014\u2014\u2014\u8f66\u724c\u8bc6\u522b\u6559\u7a0b](https://paddlepaddle.github.io/PaddleX/latest/practical_tutorials/ocr_det_license_tutorial.html)\n\n* [\ufe0f \u901a\u7528 OCR \u6a21\u578b\u4ea7\u7ebf\u2014\u2014\u2014\u624b\u5199\u4e2d\u6587\u8bc6\u522b\u6559\u7a0b](https://paddlepaddle.github.io/PaddleX/latest/practical_tutorials/ocr_rec_chinese_tutorial.html)\n\n* [ \u516c\u5f0f\u8bc6\u522b\u6a21\u578b\u4ea7\u7ebf\u5b9e\u8df5\u6559\u7a0b](https://paddlepaddle.github.io/PaddleX/latest/practical_tutorials/formula_recognition_tutorial.html)\n\n* [ \u7248\u9762\u533a\u57df\u68c0\u6d4b\u6a21\u578b\u4f7f\u7528\u5b9e\u8df5\u6559\u7a0b\u2014\u2014\u2014\u5927\u6a21\u578b\u8bad\u7ec3\u6570\u636e\u6784\u5efa\u6559\u7a0b](https://paddlepaddle.github.io/PaddleX/latest/practical_tutorials/layout_detection.html)\n\n* [ \u4eba\u8138\u8bc6\u522b\u4e4b\u5361\u901a\u4eba\u8138\u8bc6\u522b\u5b9e\u8df5\u6559\u7a0b\u2014\u2014\u2014\u5361\u901a\u4eba\u8138\u8bc6\u522b\u6559\u7a0b](https://paddlepaddle.github.io/PaddleX/latest/practical_tutorials/face_recognition_tutorial.html)\n\n* [\ufe0f \u901a\u7528\u56fe\u50cf\u5206\u7c7b\u6a21\u578b\u4ea7\u7ebf\u2014\u2014\u2014\u5783\u573e\u5206\u7c7b\u6559\u7a0b](https://paddlepaddle.github.io/PaddleX/latest/practical_tutorials/image_classification_garbage_tutorial.html)\n\n* [ \u901a\u7528\u5b9e\u4f8b\u5206\u5272\u6a21\u578b\u4ea7\u7ebf\u2014\u2014\u2014\u9065\u611f\u56fe\u50cf\u5b9e\u4f8b\u5206\u5272\u6559\u7a0b](https://paddlepaddle.github.io/PaddleX/latest/practical_tutorials/instance_segmentation_remote_sensing_tutorial.html)\n\n* [ \u901a\u7528\u76ee\u6807\u68c0\u6d4b\u6a21\u578b\u4ea7\u7ebf\u2014\u2014\u2014\u884c\u4eba\u8dcc\u5012\u68c0\u6d4b\u6559\u7a0b](https://paddlepaddle.github.io/PaddleX/latest/practical_tutorials/object_detection_fall_tutorial.html)\n\n* [ \u901a\u7528\u76ee\u6807\u68c0\u6d4b\u6a21\u578b\u4ea7\u7ebf\u2014\u2014\u2014\u670d\u88c5\u65f6\u5c1a\u5143\u7d20\u68c0\u6d4b\u6559\u7a0b](https://paddlepaddle.github.io/PaddleX/latest/practical_tutorials/object_detection_fashion_pedia_tutorial.html)\n\n* [\ufe0f \u901a\u7528\u8bed\u4e49\u5206\u5272\u6a21\u578b\u4ea7\u7ebf\u2014\u2014\u2014\u8f66\u9053\u7ebf\u5206\u5272\u6559\u7a0b](https://paddlepaddle.github.io/PaddleX/latest/practical_tutorials/semantic_segmentation_road_tutorial.html)\n\n* [\ufe0f \u65f6\u5e8f\u5f02\u5e38\u68c0\u6d4b\u6a21\u578b\u4ea7\u7ebf\u2014\u2014\u2014\u8bbe\u5907\u5f02\u5e38\u68c0\u6d4b\u5e94\u7528\u6559\u7a0b](https://paddlepaddle.github.io/PaddleX/latest/practical_tutorials/ts_anomaly_detection.html)\n\n* [ \u65f6\u5e8f\u5206\u7c7b\u6a21\u578b\u4ea7\u7ebf\u2014\u2014\u2014\u5fc3\u8df3\u76d1\u6d4b\u65f6\u5e8f\u6570\u636e\u5206\u7c7b\u5e94\u7528\u6559\u7a0b](https://paddlepaddle.github.io/PaddleX/latest/practical_tutorials/ts_classification.html)\n\n* [ \u65f6\u5e8f\u9884\u6d4b\u6a21\u578b\u4ea7\u7ebf\u2014\u2014\u2014\u7528\u7535\u91cf\u957f\u671f\u9884\u6d4b\u5e94\u7528\u6559\u7a0b](https://paddlepaddle.github.io/PaddleX/latest/practical_tutorials/ts_forecast.html)\n\n* [ \u4ea7\u7ebf\u90e8\u7f72\u5b9e\u8df5\u6559\u7a0b](https://paddlepaddle.github.io/PaddleX/latest/practical_tutorials/deployment_tutorial.html)\n\nFAQ\n\n\u5173\u4e8e\u6211\u4eec\u9879\u76ee\u7684\u4e00\u4e9b\u5e38\u89c1\u95ee\u9898\u89e3\u7b54\uff0c\u8bf7\u53c2\u8003[FAQ](https://paddlepaddle.github.io/PaddleX/latest/FAQ.html)\u3002\u5982\u679c\u60a8\u7684\u95ee\u9898\u6ca1\u6709\u5f97\u5230\u89e3\u7b54\uff0c\u8bf7\u968f\u65f6\u5728 [Issues](https://github.com/PaddlePaddle/PaddleX/issues) \u4e2d\u63d0\u51fa\n\nDiscussion\n\n\u6211\u4eec\u975e\u5e38\u6b22\u8fce\u5e76\u9f13\u52b1\u793e\u533a\u6210\u5458\u5728 [Discussions](https://github.com/PaddlePaddle/PaddleX/discussions) \u677f\u5757\u4e2d\u63d0\u51fa\u95ee\u9898\u3001\u5206\u4eab\u60f3\u6cd5\u548c\u53cd\u9988\u3002\u65e0\u8bba\u60a8\u662f\u60f3\u8981\u62a5\u544a\u4e00\u4e2a bug\u3001\u8ba8\u8bba\u4e00\u4e2a\u529f\u80fd\u8bf7\u6c42\u3001\u5bfb\u6c42\u5e2e\u52a9\u8fd8\u662f\u4ec5\u4ec5\u60f3\u8981\u4e86\u89e3\u9879\u76ee\u7684\u6700\u65b0\u52a8\u6001\uff0c\u8fd9\u91cc\u90fd\u662f\u4e00\u4e2a\u7edd\u4f73\u7684\u5e73\u53f0\u3002\n\n\u8bb8\u53ef\u8bc1\u4e66\n\n\u672c\u9879\u76ee\u7684\u53d1\u5e03\u53d7 [Apache 2.0 license](./LICENSE) \u8bb8\u53ef\u8ba4\u8bc1\u3002"}, {"name": "paddlex", "tags": ["data", "math", "ml", "web"], "summary": "Low-code development tool based on PaddlePaddle.", "text": "This library is used to enable developers to quickly and efficiently build, train, and deploy AI models with a low-code development tool based on PaddlePaddle. With paddlex, developers can implement model development workflows from training to inference using over 200 pre-trained models and support various mainstream hardware platforms."}, {"name": "palettable", "tags": ["math", "visualization", "web"], "summary": "Color palettes for Python", "text": "Palettable\n==========\n\n   :target: https://github.com/jiffyclub/palettable/actions/workflows/ci.yml\n   :alt: Build Status\n\n   :target: https://coveralls.io/r/jiffyclub/palettable\n   :alt: Coveralls\n\n.. image:: https://img.shields.io/pypi/v/palettable.svg\n   :target: https://pypi.python.org/pypi/palettable/\n   :alt: Latest Version\n\n.. image:: https://img.shields.io/pypi/pyversions/snakeviz.svg\n   :target: https://pypi.python.org/pypi/snakeviz/\n   :alt: Supported Python versions\n\n.. image:: https://img.shields.io/pypi/wheel/palettable.svg\n\nColor Palettes for Python\n-------------------------\n\nPalettable (formerly brewer2mpl) is a library of color palettes for Python.\nIt's written in pure Python with no dependencies, but it can supply color maps\nfor matplotlib. You can use Palettable to customize matplotlib plots or supply\ncolors for a web application.\n\nFor more information see the\n`documentation `_."}, {"name": "palettable", "tags": ["math", "visualization", "web"], "summary": "Color palettes for Python", "text": "This library is used to generate and customize color palettes for Python applications, particularly for use with the popular data visualization library matplotlib. With Palettable, developers can easily create consistent and visually appealing color schemes for their plots and visualizations."}, {"name": "pandas-datareader", "tags": ["dev", "math"], "summary": "Data readers extracted from the pandas codebase,should be compatible with recent pandas versions", "text": "pandas-datareader\n\nUp to date remote data access for pandas, works for multiple versions of\npandas.\n\n(https://pypi.python.org/pypi/pandas-datareader/)\n(https://codecov.io/gh/pydata/pandas-datareader)\n(https://pandas-datareader.readthedocs.io/en/latest/)\n(https://github.com/psf/black)\n(https://pypi.org/project/pandas-datareader/)\n\nInstallation\n\nInstall using `pip`\n\nUsage\n\nDocumentation\n\n[Stable documentation](https://pydata.github.io/pandas-datareader/) is available on\n[github.io](https://pydata.github.io/pandas-datareader/). A second copy of the stable\ndocumentation is hosted on [read the docs](https://pandas-datareader.readthedocs.io/)\nfor more details.\n\n[Development documentation](https://pydata.github.io/pandas-datareader/devel/) is available\nfor the latest changes in master.\n\nRequirements\n\nUsing pandas datareader requires the following packages:\n\n-   pandas>=1.0\n-   lxml\n-   requests>=2.19.0\n\nBuilding the documentation additionally requires:\n\n-   matplotlib\n-   ipython\n-   requests_cache\n-   sphinx\n-   pydata_sphinx_theme\n\nDevelopment and testing additionally requires:\n\n-   black\n-   coverage\n-   codecov\n-   coveralls\n-   flake8\n-   pytest\n-   pytest-cov\n-   wrapt\n\nInstall latest development version\n\nor"}, {"name": "pandas-datareader", "tags": ["dev", "math"], "summary": "Data readers extracted from the pandas codebase,should be compatible with recent pandas versions", "text": "This library is used to provide up-to-date remote data access for pandas, allowing developers to seamlessly fetch and manipulate external datasets with ease. With this library, developers can easily extract and integrate various types of data from multiple sources into their pandas-based applications."}, {"name": "pandas-gbq", "tags": ["math", "web"], "summary": "Google BigQuery connector for pandas", "text": "pandas-gbq\n==========\n\npreview\n\n**pandas-gbq** is a package providing an interface to the Google BigQuery API from pandas.\n\n-  `Library Documentation`_\n-  `Product Documentation`_\n\n   :target: https://github.com/googleapis/google-cloud-python/blob/main/README.rst#beta-support\n.. |pypi| image:: https://img.shields.io/pypi/v/pandas-gbq.svg\n   :target: https://pypi.org/project/pandas-gbq/\n.. |versions| image:: https://img.shields.io/pypi/pyversions/pandas-gbq.svg\n   :target: https://pypi.org/project/pandas-gbq/\n.. _Library Documentation: https://googleapis.dev/python/pandas-gbq/latest/\n.. _Product Documentation: https://cloud.google.com/bigquery/docs/reference/v2/\n\nInstallation\n------------\n\nInstall latest release version via pip\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n.. code-block:: shell\n\n   $ pip install pandas-gbq\n\nInstall latest development version\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n.. code-block:: shell\n\nUsage\n-----\n\nPerform a query\n~~~~~~~~~~~~~~~\n\n.. code:: python\n\nUpload a dataframe\n~~~~~~~~~~~~~~~~~~\n\n.. code:: python\n\nMore samples\n~~~~~~~~~~~~\n\nSee the `pandas-gbq documentation `_ for more details."}, {"name": "pandas-gbq", "tags": ["math", "web"], "summary": "Google BigQuery connector for pandas", "text": "This library is used to read and write data between pandas DataFrames and Google BigQuery, enabling seamless integration of the two for big data analytics. With pandas-gbq, developers can easily load, manipulate, and analyze large datasets in BigQuery using familiar pandas APIs."}, {"name": "pandas-stubs", "tags": ["dev", "math", "web"], "summary": "Type annotations for pandas", "text": "pandas-stubs: Public type stubs for pandas\n\n(https://pypi.org/project/pandas-stubs/)\n(https://anaconda.org/conda-forge/pandas-stubs)\n(https://pypi.org/project/pandas-stubs/)\n(https://github.com/pandas-dev/pandas-stubs/blob/main/LICENSE)\n(https://pepy.tech/project/pandas-stubs)\n(https://gitter.im/pydata/pandas)\n(https://numfocus.org)\n(https://github.com/psf/black)\n(https://pycqa.github.io/isort/)\n(https://pypi.org/project/pandas-stubs/)\n\nWhat is it?\n\nThese are public type stubs for [**pandas**](http://pandas.pydata.org/), following the\nconvention of providing stubs in a separate package, as specified in [PEP 561](https://peps.python.org/pep-0561/#stub-only-packages).  The stubs cover the most typical use cases of\npandas.  In general, these stubs are narrower than what is possibly allowed by pandas,\nbut follow a convention of suggesting best recommended practices for using pandas.\n\nThe stubs are likely incomplete in terms of covering the published API of pandas.  NOTE: The current 2.0.x releases of pandas-stubs do not support all of the new features of pandas 2.0.  See this [tracker](https://github.com/pandas-dev/pandas-stubs/issues/624) to understand the current compatibility with version 2.0.\n\nThe stubs are tested with [mypy](http://mypy-lang.org/) and [pyright](https://github.com/microsoft/pyright#readme) and are currently shipped with the Visual Studio Code extension\n[pylance](https://github.com/microsoft/pylance-release#readme).\n\nUsage\n\nLet\u2019s take this example piece of code in file `round.py`\n\nMypy won't see any issues with that, but after installing pandas-stubs and running it again:\n\nwe get the following error message:\n\nAnd, if you use pyright:\n\nyou get the following error message:\n\nAnd after confirming with the [docs](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.round.html)\nwe can fix the code:\n\nVersion Numbering Convention\n\nThe version number x.y.z.yymmdd corresponds to a test done with pandas version x.y.z, with the stubs released on the date mm/yy/dd.\nIt is anticipated that the stubs will be released more frequently than pandas as the stubs are expected to evolve due to more\npublic visibility.\n\nWhere to get it\n\nThe source code is currently hosted on GitHub at: \n\nBinary installers for the latest released version are available at the [Python\nPackage Index (PyPI)](https://pypi.org/project/pandas-stubs) and on [conda-forge](https://conda-forge.org/).\n\nDependencies\n\n- [pandas: powerful Python data analysis toolkit](https://pandas.pydata.org/)\n- [typing-extensions >= 4.2.0 - supporting the latest typing extensions](https://github.com/python/typing_extensions#readme)\n\nInstallation from sources\n\n- Make sure you have `python >= 3.10` installed.\n- Install poetry\n\n- Install the project dependencies\n\n- Build and install the distribution\n\nLicense\n\n[BSD 3](LICENSE)\n\nDocumentation\n\nDocumentation is a work-in-progress.\n\nBackground\n\nThese stubs are the result of a strategic effort led by the core pandas team to integrate [Microsoft type stub repository](https://github.com/microsoft/python-type-stubs) with the [VirtusLabs pandas_stubs repository](https://github.com/VirtusLab/pandas-stubs).\n\nThese stubs were initially forked from the Microsoft project at  as of [this commit](https://github.com/microsoft/python-type-stubs/tree/6b800063bde687cd1846122431e2a729a9de625a).\n\nWe are indebted to Microsoft and that project for providing the initial set of public type stubs.  We are also grateful for the original pandas-stubs project at , which created the framework for testing the stubs.\n\nDifferences between type declarations in pandas and pandas-stubs\n\nThe  project has type declarations for some parts of pandas, both for the internal and public API's.  Those type declarations are used to make sure that the pandas code is _internally_ consistent.\n\nThe  project provides type declarations for the pandas _public_ API.  The philosophy of these stubs can be found at . While it would be ideal if the `pyi` files in this project would be part of the `pandas` distribution, this would require consistency between the internal type declarations and the public declarations, and the scope of a project to create that consistency is quite large.  That is a long term goal.  Finally, another goal is to do more frequent releases of the pandas-stubs than is done for pandas, in order to make the stubs more useful.\n\nIf issues are found with the public stubs, pull requests to correct those issues are welcome.  In addition, pull requests on the pandas repository to fix the same issue are welcome there as well.  However, since the goals of typing in the two projects are different (internal consistency vs. public usage), it may be a challenge to create consistent type declarations across both projects.  See  for a discussion of typing standards used within the pandas code.\n\nGetting help\n\nAsk questions and report issues on the [pandas-stubs repository](https://github.com/pandas-dev/pandas-stubs/issues).\n\nDiscussion and Development\n\nMost development discussions take place on GitHub in the [pandas-stubs repository](https://github.com/pandas-dev/pandas-stubs/).\n\nFurther, the [pandas-dev mailing list](https://mail.python.org/mailman/listinfo/pandas-dev) can also be used for specialized discussions or design issues, and a [Slack channel](https://pandas.pydata.org/docs/dev/development/community.html?highlight=slack#community-slack) is available for quick development related questions.\n\nThere are also frequent [community meetings](https://pandas.pydata.org/docs/dev/development/community.html#community-meeting) for project maintainers open to the community as well as monthly [new contributor meetings](https://pandas.pydata.org/docs/dev/development/community.html#new-contributor-meeting) to help support new contributors.\n\nAdditional information on the communication channels can be found on the [contributor community](https://pandas.pydata.org/docs/development/community.html) page.\n\nContributing to pandas-stubs\n\nAll contributions, bug reports, bug fixes, documentation improvements, enhancements, and ideas are welcome.  See  for instructions."}, {"name": "pandas-stubs", "tags": ["dev", "math", "web"], "summary": "Type annotations for pandas", "text": "This library is used to provide type annotations for pandas, allowing developers to write more maintainable and self-documenting code with features like static type checking. With this library, developers can ensure their code is well-documented and follows established coding standards."}, {"name": "pandas", "tags": ["dev", "math", "web"], "summary": "Powerful data structures for data analysis, time series, and statistics", "text": "pandas: powerful Python data analysis toolkit\n\n---\nTesting\nPackage\nMeta\n\nWhat is it?\n\n**pandas** is a Python package that provides fast, flexible, and expressive data\nstructures designed to make working with \"relational\" or \"labeled\" data both\neasy and intuitive. It aims to be the fundamental high-level building block for\ndoing practical, **real world** data analysis in Python. Additionally, it has\nthe broader goal of becoming **the most powerful and flexible open source data\nanalysis / manipulation tool available in any language**. It is already well on\nits way towards this goal.\n\nTable of Contents\n\n- [Main Features](#main-features)\n- [Where to get it](#where-to-get-it)\n- [Dependencies](#dependencies)\n- [Installation from sources](#installation-from-sources)\n- [License](#license)\n- [Documentation](#documentation)\n- [Background](#background)\n- [Getting Help](#getting-help)\n- [Discussion and Development](#discussion-and-development)\n- [Contributing to pandas](#contributing-to-pandas)\n\nMain Features\nHere are just a few of the things that pandas does well:\n\n  - Easy handling of [**missing data**][missing-data] (represented as\n  - Size mutability: columns can be [**inserted and\n  - Automatic and explicit [**data alignment**][alignment]: objects can\n  - Powerful, flexible [**group by**][groupby] functionality to perform\n  - Make it [**easy to convert**][conversion] ragged,\n  - Intelligent label-based [**slicing**][slicing], [**fancy\n  - Intuitive [**merging**][merging] and [**joining**][joining] data\n  - Flexible [**reshaping**][reshape] and [**pivoting**][pivot-table] of\n  - [**Hierarchical**][mi] labeling of axes (possible to have multiple\n  - Robust IO tools for loading data from [**flat files**][flat-files]\n  - [**Time series**][timeseries]-specific functionality: date range\n\nWhere to get it\nThe source code is currently hosted on GitHub at:\n\nBinary installers for the latest released version are available at the [Python\nPackage Index (PyPI)](https://pypi.org/project/pandas) and on [Conda](https://docs.conda.io/en/latest/).\n\nThe list of changes to pandas between each release can be found\n[here](https://pandas.pydata.org/pandas-docs/stable/whatsnew/index.html). For full\ndetails, see the commit logs at https://github.com/pandas-dev/pandas.\n\nDependencies\n\nSee the [full installation instructions](https://pandas.pydata.org/pandas-docs/stable/install.html#dependencies) for minimum supported versions of required, recommended and optional dependencies.\n\nInstallation from sources\nTo install pandas from source you need [Cython](https://cython.org/) in addition to the normal\ndependencies above. Cython can be installed from PyPI:\n\nIn the `pandas` directory (same one where you found this file after\ncloning the git repo), execute:\n\nor for installing in [development mode](https://pip.pypa.io/en/latest/cli/pip_install/#install-editable):\n\nSee the full instructions for [installing from source](https://pandas.pydata.org/docs/dev/development/contributing_environment.html).\n\nLicense\n[BSD 3](LICENSE)\n\nDocumentation\nThe official documentation is hosted on [PyData.org](https://pandas.pydata.org/pandas-docs/stable/).\n\nBackground\nWork on ``pandas`` started at [AQR](https://www.aqr.com/) (a quantitative hedge fund) in 2008 and\nhas been under active development since then.\n\nGetting Help\n\nFor usage questions, the best place to go to is [StackOverflow](https://stackoverflow.com/questions/tagged/pandas).\nFurther, general questions and discussions can also take place on the [pydata mailing list](https://groups.google.com/forum/?fromgroups#!forum/pydata).\n\nDiscussion and Development\nMost development discussions take place on GitHub in this repo, via the [GitHub issue tracker](https://github.com/pandas-dev/pandas/issues).\n\nFurther, the [pandas-dev mailing list](https://mail.python.org/mailman/listinfo/pandas-dev) can also be used for specialized discussions or design issues, and a [Slack channel](https://pandas.pydata.org/docs/dev/development/community.html?highlight=slack#community-slack) is available for quick development related questions.\n\nThere are also frequent [community meetings](https://pandas.pydata.org/docs/dev/development/community.html#community-meeting) for project maintainers open to the community as well as monthly [new contributor meetings](https://pandas.pydata.org/docs/dev/development/community.html#new-contributor-meeting) to help support new contributors.\n\nAdditional information on the communication channels can be found on the [contributor community](https://pandas.pydata.org/docs/development/community.html) page.\n\nContributing to pandas\n\n(https://www.codetriage.com/pandas-dev/pandas)\n\nAll contributions, bug reports, bug fixes, documentation improvements, enhancements, and ideas are welcome.\n\nA detailed overview on how to contribute can be found in the **[contributing guide](https://pandas.pydata.org/docs/dev/development/contributing.html)**.\n\nIf you are simply looking to start working with the pandas codebase, navigate to the [GitHub \"issues\" tab](https://github.com/pandas-dev/pandas/issues) and start looking through interesting issues. There are a number of issues listed under [Docs](https://github.com/pandas-dev/pandas/issues?labels=Docs&sort=updated&state=open) and [good first issue](https://github.com/pandas-dev/pandas/issues?labels=good+first+issue&sort=updated&state=open) where you could start out.\n\nYou can also triage issues which may include reproducing bug reports, or asking for vital information such as version numbers or reproduction instructions. If you would like to start triaging issues, one easy way to get started is to [subscribe to pandas on CodeTriage](https://www.codetriage.com/pandas-dev/pandas).\n\nOr maybe through using pandas you have an idea of your own or are looking for something in the documentation and thinking \u2018this can be improved\u2019...you can do something about it!\n\nFeel free to ask questions on the [mailing list](https://groups.google.com/forum/?fromgroups#!forum/pydata) or on [Slack](https://pandas.pydata.org/docs/dev/development/community.html?highlight=slack#community-slack).\n\nAs contributors and maintainers to this project, you are expected to abide by pandas' code of conduct. More information can be found at: [Contributor Code of Conduct](https://github.com/pandas-dev/.github/blob/master/CODE_OF_CONDUCT.md)\n\n[Go to Top](#table-of-contents)"}, {"name": "pandas", "tags": ["dev", "math", "web"], "summary": "Powerful data structures for data analysis, time series, and statistics", "text": "This library is used to provide fast, flexible, and expressive data structures for performing practical and real-world data analysis in Python. It enables developers to efficiently work with labeled or relational data for tasks such as time series and statistical analysis."}, {"name": "pandera", "tags": ["math", "web"], "summary": "A light-weight and flexible data validation and testing tool for statistical data objects.", "text": "Install\n\nPandera supports [multiple dataframe libraries](https://pandera.readthedocs.io/en/stable/supported_libraries.html), including [pandas](http://pandas.pydata.org), [polars](https://docs.pola.rs/), [pyspark](https://spark.apache.org/docs/latest/api/python/index.html), and more. To validate `pandas` DataFrames, install Pandera with the `pandas` extra:\n\n**With `pip`:**\n\n**With `uv`:**\n\n**With `conda`:**\n\nGet started\n\nFirst, create a dataframe:\n\nValidate the data using the object-based API:\n\nOr validate the data using the class-based API:\n\n> [!WARNING]\n> Pandera `v0.24.0` introduces the `pandera.pandas` module, which is now the\n> (highly) recommended way of defining `DataFrameSchema`s and `DataFrameModel`s\n> for `pandas` data structures like `DataFrame`s. Defining a dataframe schema from\n> the top-level `pandera` module will produce a `FutureWarning`:\n>\n> \n>\n> Update your import to:\n>\n> \n>\n> And all of the rest of your pandera code should work. Using the top-level\n> `pandera` module to access `DataFrameSchema` and the other pandera classes\n> or functions will be deprecated in version `0.29.0`\n\nNext steps\n\nSee the [official documentation](https://pandera.readthedocs.io) to learn more."}, {"name": "pandera", "tags": ["math", "web"], "summary": "A light-weight and flexible data validation and testing tool for statistical data objects.", "text": "This library is used to validate and test statistical data objects in various formats, including pandas DataFrames, using a light-weight and flexible framework. With pandera, developers can ensure the integrity and accuracy of their data across different libraries and frameworks."}, {"name": "panel", "tags": ["math", "ui", "visualization", "web"], "summary": "The powerful data exploration & web app framework for Python.", "text": "Panel: The powerful data exploration & web app framework for Python\n\nPanel is an [open-source](https://github.com/holoviz/panel/blob/main/LICENSE.txt) Python library that lets you **easily build powerful tools, dashboards and complex applications entirely in Python**. It has a batteries-included philosophy, putting the PyData ecosystem, powerful data tables and much more at your fingertips. High-level reactive APIs and lower-level callback based APIs ensure you can quickly build exploratory applications, but you aren't limited if you build complex, multi-page apps with rich interactivity. Panel is a member of the [HoloViz](https://holoviz.org/) ecosystem, your gateway into a connected ecosystem of data exploration tools.\n\n---\n\nEnjoying Panel? Show your support with a [Github star](https://github.com/holoviz/panel) \u2014 it\u2019s a simple click that means the world to us and helps others discover it too! \u2b50\ufe0f\n\n---\n\nDownloads\n\nBuild Status\n\nCoverage\n\nLatest dev release\n \n\nLatest release\n\nDocs\n\nNotebooks\n\nSupport\n \n\n[Home](https://panel.holoviz.org/) | [Installation instructions](#installation-instructions) | [Getting Started Guide](https://panel.holoviz.org/getting_started/index.html) | [Reference Guides](https://panel.holoviz.org/reference/index.html) | [Examples](#examples) | [License](#license) | [Support](#support--feedback)\n\nPanel works with the tools you know and love\n\n[Panel](https://panel.holoviz.org/) makes it easy to combine widgets, plots, tables and other viewable Python objects into custom analysis tools, applications, and dashboards.\n\n(https://panel.holoviz.org/reference/templates/FastGridTemplate.html)\n\nPanel works really well with the visualization tools you already know and love like [Altair/ Vega](https://panel.holoviz.org/reference/panes/Vega.html), [Bokeh](https://panel.holoviz.org/reference/panes/Bokeh.html), [Datashader](https://datashader.org/), [Deck.gl/ pydeck](https://panel.holoviz.org/reference/panes/DeckGL.html), [Echarts/ pyecharts](https://panel.holoviz.org/reference/panes/ECharts.html), [Folium](https://panel.holoviz.org/reference/panes/Folium.html), [HoloViews](https://holoviews.org/), [hvPlot](https://hvplot.holoviz.org), [plotnine](https://panel.holoviz.org/reference/panes/Matplotlib.html), [Matplotlib](https://panel.holoviz.org/reference/panes/Matplotlib.html), [Plotly](https://panel.holoviz.org/reference/panes/Plotly.html), [PyVista/ VTK](https://panel.holoviz.org/reference/panes/VTK.html), [Seaborn](https://panel.holoviz.org/gallery/styles/SeabornStyle.html) and more. Panel also works with the [ipywidgets](https://panel.holoviz.org/reference/panes/IPyWidget.html) ecosystem.\n\n(https://panel.holoviz.org/reference/index.html#panes)\n\nPanel provides bi-directional communication making it possible to react to clicks, selections, hover etc. events.\n\n(https://panel.holoviz.org/reference/panes/Vega.html)\n\nYou can develop in [Jupyter Notebooks](http://jupyter.org) as well as editors like [VS Code](https://code.visualstudio.com/), [PyCharm](https://www.jetbrains.com/pycharm/) or [Spyder](https://www.spyder-ide.org/).\n\n  \n  \n \n\nPanel provides a unique combination of deployment options. You can share your data and models as\n\n- a web application running on the [Tornado](https://www.tornadoweb.org/en/stable/) (default), [Flask](https://flask.palletsprojects.com/), [Django](https://www.djangoproject.com/) or [Fast API](https://fastapi.tiangolo.com/) web server.\n- a stand alone client side application powered by [Pyodide](https://pyodide.org/en/stable/) or [PyScript](https://pyscript.net/) via [`panel convert`](https://panel.holoviz.org/how_to/wasm/convert.html).\n- an interactive Jupyter notebook component.\n- a static `.html` web page, a `.gif` video, a `.png` image and more.\n\nPanel has something to offer for every one *from beginner to data pro*.\n\nPanel is a member of the HoloViz ecosystem\n\nPanel is a member of the ambitious [HoloViz](https://holoviz.org/) dataviz ecosystem and has first class support for the other members like [hvPlot](https://hvplot.holoviz.org) (simple .hvplot plotting api), [HoloViews](https://holoviews.org/) (powerful plotting api), and [Datashader](https://datashader.org/) (big data viz).\n\nPanel is built on top of [Param](https://param.holoviz.org). Param enables you to annotate your code with parameter ranges, documentation, and dependencies between parameters and code. With this approach,\n\n- you don't ever have to commit to whether your code will be used in a notebook, a data app, in batch processing, or reports.\n- you will write less code and be able to develop large, maintainable code bases!\n\nMini getting-started\n\nHead over to the [getting started guide](https://panel.holoviz.org/getting_started/index.html) for more!\n\nInstallation Instructions\n\nPanel can be installed on Linux, Windows, or Mac with ``conda``:\n\nor with ``pip``:\n\nSee the [Environments](#environments) section below for additional instructions for your environment.\n\nInteractive data apps\n\nBring your data or model\n\nBind it to a Panel *widget* and *lay it out*.\n\nFor deployment on a web server wrap it in a nice template.\n\nStart the server with\n\nor\n\nExamples\n\n(https://panel.holoviz.org/gallery/index.html)\n\n(https://holoviz-topics.github.io/panel-chat-examples/)\n\n(https://www.awesome-panel.org)\n\n(https://examples.holoviz.org)\n\nGet started\n\nDevelop applications in your favorite notebook or editor environment, including Jupyter(Lab) notebooks, VSCode, Google Colab and many more, [see our getting started guide](https://panel.holoviz.org/getting_started/installation.html#developing-in-different-editors) for more details.\n\nSupport & Feedback\n\nFor more detail check out the [HoloViz Community Guide](https://holoviz.org/community.html).\n\nContributing \ufe0f\n\nCheck out the [Contributing Guide](CONTRIBUTING.MD).\n\nLicense\n\nPanel is completely free and open-source. It is licensed under the [BSD 3-Clause License](https://opensource.org/licenses/BSD-3-Clause).\n\nSponsors\n\nThe Panel project is also very grateful for the sponsorship by the organizations and companies below:"}, {"name": "panel", "tags": ["math", "ui", "visualization", "web"], "summary": "The powerful data exploration & web app framework for Python.", "text": "This library is used to easily build powerful tools, dashboards, and complex applications entirely in Python. It allows developers to quickly create exploratory applications or complex, multi-page apps with rich interactivity using high-level reactive APIs or lower-level callback based APIs."}, {"name": "param", "tags": ["math", "ui", "web"], "summary": "Declarative parameters for robust Python classes and a rich API for reactive programming", "text": "Param\n\n**Param** is a zero-dependency Python library that provides two main features:\n\n- Easily create classes with **rich, declarative attributes** - `Parameter` objects - that include extended metadata for various purposes such as runtime type and range validation, documentation strings, default values or factories, nullability, etc. In this sense, Param is conceptually similar to libraries like Pydantic, Python's dataclasses, or Traitlets.\n- A suite of expressive and composable APIs for **reactive programming**, enabling automatic updates on attribute changes, and declaring complex reactive dependencies and expressions that can be introspected by other frameworks to implement their own reactive workflows.\n\nThis combination of **rich attributes** and **reactive APIs** makes Param a solid foundation for constructing user interfaces, graphical applications, and responsive systems where data integrity and automatic synchronization are paramount. In fact, Param serves as the backbone of HoloViz\u2019s [Panel](https://panel.holoviz.org) and [HoloViews](https://holoviews.org) libraries, powering their rich interactivity and data-driven workflows.\n\nHere is a very simple example showing both features at play. We declare a UserForm class with three parameters: `age` as an *Integer* parameter and and `name` as a *String* parameter for user data, and `submit` as an *Event* parameter to simulate a button in a user interface. We also declare that the `save_user_to_db` method should be called automatically when the value of the `submit` attribute changes.\n\n---\n\nEnjoying Param? Show your support with a [Github star](https://github.com/holoviz/param) to help others discover it too! \u2b50\ufe0f\n\n---\n\n---\nDownloads\nBuild Status\nCoverage\nLatest dev release\nLatest release\nPython\nDocs\nBinder\nSupport\n\nRich class attributes for runtime validation and more\n\nParam lets you create classes and declare facts about each of their attributes through rich `Parameter` objects. Once you have done that, Param can handle runtime attribute validation (type checking, range validation, etc.) and more (documentation, serialization, etc.). Let's see how to use `Parameter` objects with a simple example, a `Processor` class that has three attributes.\n\nRuntime attribute validation is a great feature that helps build defendable code bases! Alternative libraries, like [Pydantic](https://docs.pydantic.dev) and others, excel at input validation, and if this is only what you need, you should probably look into them. Where Param shines is when you also need:\n\n- Attributes that are also available at the class level, allowing to easily configure a hierarchy of classes and their instances.\n- Parameters with rich metadata (`default`, `doc`, `label`, `bounds`, etc.) that downstream tooling can inspect to build configuration UIs, CLIs, or documentation automatically.\n- Parameterized subclasses that inherit Parameter metadata from their parents, and can selectively override certain attributes (e.g. overriding `default` in a subclass).\n\nLet's see this in action by extending the example above with a custom processor subclass:\n\nReactive Programming\n\nParam extends beyond rich class attributes with a suite of APIs for reactive programming. Let's do a quick tour!\n\nWe'll start with APIs that trigger side-effects only, which either have the noun watch in their name or are invoked with `watch=True`:"}, {"name": "param", "tags": ["math", "ui", "web"], "summary": "Declarative parameters for robust Python classes and a rich API for reactive programming", "text": "1. `.param.watch(fn, *parameters, ...)`: Low-level, imperative API to attach callbacks to parameter changes, the callback receives one or more rich `Event` objects.\n2. `@depends(*parameter_names, watch=True)`: In a Parameterized class, declare dependencies and automatically watch parameters for changes to call the decorated method.\n3. `bind(fn, *references, watch=True, **kwargs)`: Function binding with automatic references (parameters, bound functions, reactive expressions) watching and triggering on changes.\n\nLet's continue the tour with what we'll call \"reactive APIs\". Contrary to the APIs presented above, in this group parameter updates do not immediately trigger side effects. Instead, these APIs let you declare relationships, dependencies, and expressions, which can be introspected by other frameworks to set up their own reactive workflows.\n\n1. `@depends(*parameter_names)`: In a Parameterized class, declare parameter dependencies by decorating a method.\n2. `bind(fn, *references, **kwargs)`: Create a bound function, that when called, will always use the current parameter/reference value. `bind` is essentially a reactive version of [`functools.partial`](https://docs.python.org/3/library/functools.html#functools.partial).\n3. `rx()`: Fluent API to create reactive expressions, which allow chaining and composing operations.\n\n`rx()` is the highest-level reactive API Param offers. We'll show a simple example first, using literal values as input of three source reactive expressions, that we combine with simple arithmetic operations.\n\nThe snippet below shows a slightly more complex example that reveals the power of reactive expressions.\n\nA `Parameter` does not have to refer to a specific static value but can reference another object and update reactively when its value changes. We'll show a few examples of supported *references* (Parameters, bound functions, reactive expressions, etc.). Setting parameter values with references is an effective way to establish automatic one-way linking between a reference and a parameter (the reference being often driven by another parameter).\n\nSupport & Feedback\n\nFor more detail check out the [HoloViz Community Guide](https://holoviz.org/community.html).\n\nContributing\n\nCheck out the [Contributing Guide](CONTRIBUTING.MD).\n\nLicense\n\nParam is completely free and open-source. It is licensed under the [BSD 3-Clause License](https://opensource.org/licenses/BSD-3-Clause).\n\nSponsors\n\nThe Param project is also very grateful for the sponsorship by the organizations and companies below:"}, {"name": "param", "tags": ["math", "ui", "web"], "summary": "Declarative parameters for robust Python classes and a rich API for reactive programming", "text": "This library is used to create robust Python classes with rich, declarative attributes for metadata management and validation, as well as enable reactive programming through automatic updates on attribute changes. It provides a suite of expressive APIs for declaring complex reactive dependencies and expressions."}, {"name": "parsimonious", "tags": ["dev", "math", "web"], "summary": "(Soon to be) the fastest pure-Python PEG parser I could muster", "text": "============\nParsimonious\n============\n\nParsimonious aims to be the fastest arbitrary-lookahead parser written in pure\nPython\u2014and the most usable. It's based on parsing expression grammars (PEGs),\nwhich means you feed it a simplified sort of EBNF notation. Parsimonious was\ndesigned to undergird a MediaWiki parser that wouldn't take 5 seconds or a GB\nof RAM to do one page, but it's applicable to all sorts of languages.\n\n:Code:    https://github.com/erikrose/parsimonious/\n:Issues:  https://github.com/erikrose/parsimonious/issues\n:License: MIT License (MIT)\n:Package: https://pypi.org/project/parsimonious/\n\nGoals\n=====\n\n* Speed\n* Frugal RAM use\n* Minimalistic, understandable, idiomatic Python code\n* Readable grammars\n* Extensible grammars\n* Complete test coverage\n* Separation of concerns. Some Python parsing kits mix recognition with\n  instructions about how to turn the resulting tree into some kind of other\n  representation. This is limiting when you want to do several different things\n  with a tree: for example, render wiki markup to HTML *or* to text.\n* Good error reporting. I want the parser to work *with* me as I develop a\n  grammar.\n\nInstall\n=======\n\nTo install Parsimonious, run::\n\nExample Usage\n=============\n\nHere's how to build a simple grammar:\n\n.. code:: python\n\nYou can have forward references and even right recursion; it's all taken care\nof by the grammar compiler. The first rule is taken to be the default start\nsymbol, but you can override that.\n\nNext, let's parse something and get an abstract syntax tree:\n\n.. code:: python\n\nYou'd typically then use a ``nodes.NodeVisitor`` subclass (see below) to walk\nthe tree and do something useful with it.\n\nAnother example would be to implement a parser for ``.ini``-files. Consider the following:\n\n.. code:: python\n\nWe could now implement a subclass of ``NodeVisitor`` like so:\n\n.. code:: python\n\nAnd call it like that:\n\n.. code:: python\n\nThis would yield\n\n.. code:: python\n\nStatus\n======\n\n* Everything that exists works. Test coverage is good.\n* I don't plan on making any backward-incompatible changes to the rule syntax\n  in the future, so you can write grammars with confidence.\n* It may be slow and use a lot of RAM; I haven't measured either yet. However,\n  I have yet to begin optimizing in earnest.\n* Error reporting is now in place. ``repr`` methods of expressions, grammars,\n  and nodes are clear and helpful as well. The ``Grammar`` ones are\n  even round-trippable!\n* The grammar extensibility story is underdeveloped at the moment. You should\n  be able to extend a grammar by simply concatenating more rules onto the\n  existing ones; later rules of the same name should override previous ones.\n  However, this is untested and may not be the final story.\n* Sphinx docs are coming, but the docstrings are quite useful now.\n* Note that there may be API changes until we get to 1.0, so be sure to pin to\n  the version you're using.\n\nComing Soon\n-----------\n\n* Optimizations to make Parsimonious worthy of its name\n* Tighter RAM use\n* Better-thought-out grammar extensibility story\n* Amazing grammar debugging\n\nA Little About PEG Parsers\n=========================="}, {"name": "parsimonious", "tags": ["dev", "math", "web"], "summary": "(Soon to be) the fastest pure-Python PEG parser I could muster", "text": "PEG parsers don't draw a distinction between lexing and parsing; everything is\ndone at once. As a result, there is no lookahead limit, as there is with, for\ninstance, Yacc. And, due to both of these properties, PEG grammars are easier\nto write: they're basically just a more practical dialect of EBNF. With\ncaching, they take O(grammar size * text length) memory (though I plan to do\nbetter), but they run in O(text length) time.\n\nMore Technically\n----------------\n\nPEGs can describe a superset of *LL(k)* languages, any deterministic *LR(k)*\nlanguage, and many others\u2014including some that aren't context-free\n(http://www.brynosaurus.com/pub/lang/peg.pdf). They can also deal with what\nwould be ambiguous languages if described in canonical EBNF. They do this by\ntrading the ``|`` alternation operator for the ``/`` operator, which works the\nsame except that it makes priority explicit: ``a / b / c`` first tries matching\n``a``. If that fails, it tries ``b``, and, failing that, moves on to ``c``.\nThus, ambiguity is resolved by always yielding the first successful recognition.\n\nWriting Grammars\n================\n\nGrammars are defined by a series of rules. The syntax should be familiar to\nanyone who uses regexes or reads programming language manuals. An example will\nserve best:\n\n.. code:: python\n\nYou can wrap a rule across multiple lines if you like; the syntax is very\nforgiving.\n\nIf you want to save your grammar into a separate file, you should name it using\n``.ppeg`` extension.\n\nSyntax Reference\n----------------\n\n====================    ========================================================\n``\"some literal\"``      Used to quote literals. Backslash escaping and Python\n\n``b\"some literal\"``     A bytes literal. Using bytes literals and regular\n\n[space]                 Sequences are made out of space- or tab-delimited\n\n``a / b / c``           Alternatives. The first to succeed of ``a / b / c``\n\n``thing?``              An optional expression. This is greedy, always consuming\n\n``&thing``              A lookahead assertion. Ensures ``thing`` matches at the\n\n``!thing``              A negative lookahead assertion. Matches if ``thing``\n\n``things*``             Zero or more things. This is greedy, always consuming as\n\n``things+``             One or more things. This is greedy, always consuming as\n\n``~r\"regex\"ilmsuxa``    Regexes have ``~`` in front and are quoted like\n\n``~br\"regex\"``          A bytes regex; required if your grammar parses\n\n``(things)``            Parentheses are used for grouping, like in every other\n\n``thing{n}``            Exactly ``n`` repetitions of ``thing``.\n\n``thing{n,m}``          Between ``n`` and ``m`` repititions (inclusive.)\n\n``thing{,m}``           At most ``m`` repetitions of ``thing``.\n\n``thing{n,}``           At least ``n`` repetitions of ``thing``.\n\n====================    ========================================================\n\n.. _flags: https://docs.python.org/3/howto/regex.html#compilation\n.. _regex: https://github.com/mrabarnett/mrab-regex\n\nOptimizing Grammars\n===================\n\nDon't Repeat Expressions\n------------------------\n\nIf you need a ``~\"[a-z0-9]\"i`` at two points in your grammar, don't type it\ntwice. Make it a rule of its own, and reference it from wherever you need it.\nYou'll get the most out of the caching this way, since cache lookups are by\nexpression object identity (for speed)."}, {"name": "parsimonious", "tags": ["dev", "math", "web"], "summary": "(Soon to be) the fastest pure-Python PEG parser I could muster", "text": "Even if you have an expression that's very simple, not repeating it will\nsave RAM, as there can, at worst, be a cached int for every char in the text\nyou're parsing. In the future, we may identify repeated subexpressions\nautomatically and factor them up while building the grammar.\n\nHow much should you shove into one regex, versus how much should you break them\nup to not repeat yourself? That's a fine balance and worthy of benchmarking.\nMore stuff jammed into a regex will execute faster, because it doesn't have to\nrun any Python between pieces, but a broken-up one will give better cache\nperformance if the individual pieces are re-used elsewhere. If the pieces of a\nregex aren't used anywhere else, by all means keep the whole thing together.\n\nQuantifiers\n-----------\n\nBring your ``?`` and ``*`` quantifiers up to the highest level you\ncan. Otherwise, lower-level patterns could succeed but be empty and put a bunch\nof useless nodes in your tree that didn't really match anything.\n\nProcessing Parse Trees\n======================\n\nA parse tree has a node for each expression matched, even if it matched a\nzero-length string, like ``\"thing\"?`` might.\n\nThe ``NodeVisitor`` class provides an inversion-of-control framework for\nwalking a tree and returning a new construct (tree, string, or whatever) based\non it. For now, have a look at its docstrings for more detail. There's also a\ngood example in ``grammar.RuleVisitor``. Notice how we take advantage of nodes'\niterability by using tuple unpacks in the formal parameter lists:\n\n.. code:: python\n\nFor reference, here is the production the above unpacks::\n\nWhen something goes wrong in your visitor, you get a nice error like this::\n\nThe parse tree is tacked onto the exception, and the node whose visitor method\nraised the error is pointed out.\n\nWhy No Streaming Tree Processing?\n---------------------------------\n\nSome have asked why we don't process the tree as we go, SAX-style. There are\ntwo main reasons:\n\n1. It wouldn't work. With a PEG parser, no parsing decision is final until the\n   whole text is parsed. If we had to change a decision, we'd have to backtrack\n   and redo the SAX-style interpretation as well, which would involve\n   reconstituting part of the AST and quite possibly scuttling whatever you\n   were doing with the streaming output. (Note that some bursty SAX-style\n   processing may be possible in the future if we use cuts.)\n\n2. It interferes with the ability to derive multiple representations from the\n   AST: for example, turning wiki markup into first HTML and then text.\n\nFuture Directions\n=================\n\nRule Syntax Changes\n-------------------\n\n* Maybe support left-recursive rules like PyMeta, if anybody cares.\n* Ultimately, I'd like to get rid of explicit regexes and break them into more\n  atomic things like character classes. Then we can dynamically compile bits\n  of the grammar into regexes as necessary to boost speed.\n\nOptimizations\n-------------"}, {"name": "parsimonious", "tags": ["dev", "math", "web"], "summary": "(Soon to be) the fastest pure-Python PEG parser I could muster", "text": "* Make RAM use almost constant by automatically inserting \"cuts\", as described\n  in\n  This would also improve error reporting, as we wouldn't backtrack out of\n  everything informative before finally failing.\n* Find all the distinct subexpressions, and unify duplicates for a better cache\n  hit ratio.\n* Think about having the user (optionally) provide some representative input\n  along with a grammar. We can then profile against it, see which expressions\n  are worth caching, and annotate the grammar. Perhaps there will even be\n  positions at which a given expression is more worth caching. Or we could keep\n  a count of how many times each cache entry has been used and evict the most\n  useless ones as RAM use grows.\n* We could possibly compile the grammar into VM instructions, like in \"A\n  parsing machine for PEGs\" by Medeiros.\n* If the recursion gets too deep in practice, use trampolining to dodge it.\n\nNiceties\n--------\n\n* Pijnu has a raft of tree manipulators. I don't think I want all of them, but\n  a judicious subset might be nice. Don't get into mixing formatting with tree\n  manipulation.\n  parsing lib exposes a sane subset:\n\nVersion History\n===============\n\n0.11.0\n  * Correctly handle `/` expressions with multiple terms in a row. (lucaswiman)\n  * Start using pyproject.toml. (Kolanich)\n  * Add a ``ParsimoniousError`` exception base class. (Kevin Kirsche)\n  * Fall back to ``re`` when the ``regex`` lib is not available. (Pavel Kirienko)\n\n0.10.0\n  * Fix infinite recursion in __eq__ in some cases. (FelisNivalis)\n  * Improve error message in left-recursive rules. (lucaswiman)\n  * Add support for range ``{min,max}`` repetition expressions (righthandabacus)\n  * Fix bug in ``*`` and ``+`` for token grammars (lucaswiman)\n  * Add support for grammars on bytestrings (lucaswiman)\n  * Fix LazyReference resolution bug #134 (righthandabacus)\n  * ~15% speedup on benchmarks with a faster node cache (ethframe)\n\n.. warning::\n\n0.9.0\n  * Add support for Python 3.7, 3.8, 3.9, 3.10 (righthandabacus, Lonnen)\n  * Drop support for Python 2.x, 3.3, 3.4 (righthandabacus, Lonnen)\n  * Remove six and go all in on Python 3 idioms (Lonnen)\n  * Replace re with regex for improved handling of unicode characters\n  * Dropped nose for unittest (swayson)\n  * `Grammar.__repr__()` now correctly escapes backslashes (ingolemo)\n  * Custom rules can now be class methods in addition to\n  * Make the ascii flag available in the regex syntax (Roman Inflianskas)\n\n0.8.1\n  * Switch to a function-style ``print`` in the benchmark tests so we work\n\n0.8.0\n  * Make Grammar iteration ordered, making the ``__repr__`` more like the\n  * Improve text representation and error messages for anonymous\n  * Expose BadGrammar and VisitationError as top-level imports.\n  * No longer crash when you try to compare a Node to an instance of a\n  * Pin ``six`` at 1.9.0 to ensure we have ``python_2_unicode_compatible``.\n  * Drop Python 2.6 support."}, {"name": "parsimonious", "tags": ["dev", "math", "web"], "summary": "(Soon to be) the fastest pure-Python PEG parser I could muster", "text": "0.7.0\n  * Add experimental token-based parsing, via TokenGrammar class, for those\n  * Common codebase for Python 2 and 3: no more 2to3 translation step (Mattias\n  * Drop Python 3.1 and 3.2 support.\n  * Fix a bug in ``Grammar.__repr__`` which fails to work on Python 3 since the\n  * Don't lose parentheses when printing representations of expressions.\n  * Make Grammar an immutable mapping (until we add automatic recompilation).\n\n0.6.2\n  * Make grammar compilation 100x faster. Thanks to dmoisset for the initial\n\n0.6.1\n  * Fix bug which made the default rule of a grammar invalid when it\n\n0.6\n  .. warning::\n\n* Add support for \"custom rules\" in Grammars. These provide a hook for simple\n  * Allow grammars without a default rule (in cases where there are no string\n  * Add ``@rule`` decorator, allowing grammars to be constructed out of\n  * Add ``parse()`` and ``match()`` convenience methods to ``NodeVisitor``.\n  * Improve exception message when you forget to declare a visitor method.\n  * Add ``unwrapped_exceptions`` attribute to ``NodeVisitor``, letting you\n  * Expose much more of the library in ``__init__``, making your imports\n  * Drastically simplify reference resolution machinery. (Vladimir Keleshev)\n\n0.5\n  .. warning::\n\n* Add alpha-quality error reporting. Now, rather than returning ``None``,\n  * Grammar construction now raises ``ParseError`` rather than ``BadGrammar``\n  * ``parse()`` now takes an optional ``pos`` argument, like ``match()``.\n  * Make the ``_str__()`` method of ``UndefinedLabel`` return the right type.\n  * Support splitting rules across multiple lines, interleaving comments,\n  * Tolerate whitespace after opening parens.\n  * Add support for single-quoted literals.\n\n0.4\n  * Support Python 3.\n  * Fix ``import *`` for ``parsimonious.expressions``.\n  * Rewrite grammar compiler so right-recursive rules can be compiled and\n\n0.3\n  * Support comments, the ``!`` (\"not\") operator, and parentheses in grammar\n  * Change the ``&`` operator to a prefix operator to conform to the original\n  * Take the ``print`` statements out of the benchmark tests.\n  * Give Node an evaluate-able ``__repr__``.\n\n0.2\n  * Support matching of prefixes and other not-to-the-end slices of strings by\n  * Report a ``BadGrammar`` exception (rather than crashing) when there are\n  * Simplify grammar compilation internals: get rid of superfluous visitor\n  * Add ``NodeVisitor.lift_child`` convenience method.\n  * Rename ``VisitationException`` to ``VisitationError`` for consistency with\n  * Rework ``repr`` and ``str`` values for grammars and expressions. Now they\n  * Add tox for testing. Stop advertising Python 2.5 support, which never\n  * Settle (hopefully) on the term \"rule\" to mean \"the string representation of\n\n0.1\n  * A rough but useable preview release\n\nThanks to Wiki Loves Monuments Panama for showing their support with a generous\ngift."}, {"name": "parsimonious", "tags": ["dev", "math", "web"], "summary": "(Soon to be) the fastest pure-Python PEG parser I could muster", "text": "This library is used to parse languages efficiently using parsing expression grammars (PEGs), enabling developers to create high-performance parsers for various languages with minimal memory usage. With Parsimonious, developers can build fast and lightweight parsers that meet their specific language parsing needs."}, {"name": "pdbp", "tags": ["dev", "math"], "summary": "pdbp (Pdb+): A drop-in replacement for pdb and pdbpp.", "text": "pdbp (Pdb+) (https://pypi.python.org/pypi/pdbp)\n\n**[pdbp (Pdb+)](https://github.com/mdmintz/pdbp)** is an advanced console debugger for Python. It can be used as a drop-in replacement for [pdb](https://docs.python.org/3/library/pdb.html) and [pdbpp](https://github.com/pdbpp/pdbpp).\n\npdbp (Pdb+) makes Python debugging a lot easier (and more fun!)\n\n--------\n\nInstallation:\n\nThen add ``import pdbp`` to an ``__init__.py`` of your project, which will automatically make **``Pdb+``** the default debugger at breakpoints:\n\n(If using ``flake8`` for code-linting, you may want to add ``# noqa`` to that line):\n\nYou can also make ``pdbp`` the default debugger by setting an environmental variable:\n\nUsage:\n\nTo trigger a breakpoint in your code with ``pytest``, add ``--trace`` (to start tests with a breakpoint) or ``--pdb`` (to trigger a breakpoint if a test fails).\n\nTo trigger a breakpoint from a pure ``python`` run, use:\n\n--------\n\nBasic **``Pdb+``** console commands:\n``n``, ``c``, ``s``, ``u``, ``d`` => ``next``, ``continue``, ``step``, ``up``, ``down``\n\n(To learn more **Pdb+** console commands, type ``help`` in the **Pdb+** console and press ``Enter/Return``.)\n\n--------\n\n**``pdbp`` (Pdb+)** makes improvements to ``pdbpp`` so that it works in all environments. It also includes other bug-fixes. \"Sticky\" mode is the default option, which shows multiple lines of code while letting you see where you're going (while typing ``n`` + ``Enter``).\n\nIf you somehow reset ``pdb`` to Python's built-in version, you can always replace ``pdb`` with **``pdbp``** again as the default debugger by running this:\n\nHere's how to customize **``pdbp``**/``pdb`` options if you don't like the default settings: (Shown below are the default settings.)\n\nYou can also trigger **``Pdb+``** activation like this:\n\npdbp (Pdb+) commands:\n\nPost Mortem Debug Mode:\n\nThe ``where`` / ``w`` command, which displays the current stack:\n\n--------\n\nSticky Mode vs Non-Sticky Mode:\n\nThe default mode (``sticky``) lets you see a lot more lines of code from the debugger when active. In Non-Sticky mode, only one line of code is shown at a time. You can switch between the two modes by typing ``sticky`` in the **Pdb+** console prompt and pressing ``Enter/Return``.\n\n> **Sticky Mode:**\n\n> **Non-Sticky Mode:**\n\n--------\n\nTab completion:\n\n--------\n\nMulti-layer highlighting in the same stack:\n\nMore examples:\n\n**``Pdb+``** is used by packages such as **``seleniumbase``**:\n\n* https://pypi.org/project/seleniumbase/\n* https://github.com/seleniumbase/SeleniumBase\n\n--------\n\n--------\n\n(**Pdb+** is maintained by the [SeleniumBase Dev Team](https://github.com/seleniumbase/SeleniumBase))"}, {"name": "pdbp", "tags": ["dev", "math"], "summary": "pdbp (Pdb+): A drop-in replacement for pdb and pdbpp.", "text": "This library is used to provide an advanced console debugger for Python, serving as a drop-in replacement for pdb and pdbpp. With this library, developers can make Python debugging easier and more enjoyable by setting it as the default debugger at breakpoints or through environmental variables."}, {"name": "peft", "tags": ["data", "math", "ml"], "summary": "Parameter-Efficient Fine-Tuning (PEFT)", "text": "Quickstart\n\nInstall PEFT from pip:\n\nPrepare a model for training with a PEFT method such as LoRA by wrapping the base model and PEFT configuration with `get_peft_model`. For the bigscience/mt0-large model, you're only training 0.19% of the parameters!\n\nTo load a PEFT model for inference:\n\nWhy you should use PEFT\n\nThere are many benefits of using PEFT but the main one is the huge savings in compute and storage, making PEFT applicable to many different use cases.\n\nHigh performance on consumer hardware\n\nConsider the memory requirements for training the following models on the [ought/raft/twitter_complaints](https://huggingface.co/datasets/ought/raft/viewer/twitter_complaints) dataset with an A100 80GB GPU with more than 64GB of CPU RAM.\n\nModel\n---------\nbigscience/T0_3B (3B params)\nbigscience/mt0-xxl (12B params)\nbigscience/bloomz-7b1 (7B params)\n\nWith LoRA you can fully finetune a 12B parameter model that would've otherwise run out of memory on the 80GB GPU, and comfortably fit and train a 3B parameter model. When you look at the 3B parameter model's performance, it is comparable to a fully finetuned model at a fraction of the GPU memory.\n\nSubmission Name\n---------\nHuman baseline (crowdsourced)\nFlan-T5\nlora-t0-3b\n\n> [!TIP]\n> The bigscience/T0_3B model performance isn't optimized in the table above. You can squeeze even more performance out of it by playing around with the input instruction templates, LoRA hyperparameters, and other training related hyperparameters. The final checkpoint size of this model is just 19MB compared to 11GB of the full bigscience/T0_3B model. Learn more about the advantages of finetuning with PEFT in this [blog post](https://www.philschmid.de/fine-tune-flan-t5-peft).\n\nQuantization\n\nQuantization is another method for reducing the memory requirements of a model by representing the data in a lower precision. It can be combined with PEFT methods to make it even easier to train and load LLMs for inference.\n\n* Learn how to finetune [meta-llama/Llama-2-7b-hf](https://huggingface.co/meta-llama/Llama-2-7b-hf) with QLoRA and the [TRL](https://huggingface.co/docs/trl/index) library on a 16GB GPU in the [Finetune LLMs on your own consumer hardware using tools from PyTorch and Hugging Face ecosystem](https://pytorch.org/blog/finetune-llms/) blog post.\n* Learn how to finetune a [openai/whisper-large-v2](https://huggingface.co/openai/whisper-large-v2) model for multilingual automatic speech recognition with LoRA and 8-bit quantization in this [notebook](https://colab.research.google.com/drive/1DOkD_5OUjFa0r5Ik3SgywJLJtEo2qLxO?usp=sharing) (see this [notebook](https://colab.research.google.com/drive/1vhF8yueFqha3Y3CpTHN6q9EVcII9EYzs?usp=sharing) instead for an example of streaming a dataset).\n\nSave compute and storage\n\nPEFT can help you save storage by avoiding full finetuning of models on each of downstream task or dataset. In many cases, you're only finetuning a very small fraction of a model's parameters and each checkpoint is only a few MBs in size (instead of GBs). These smaller PEFT adapters demonstrate performance comparable to a fully finetuned model. If you have many datasets, you can save a lot of storage with a PEFT model and not have to worry about catastrophic forgetting or overfitting the backbone or base model.\n\nPEFT integrations\n\nPEFT is widely supported across the Hugging Face ecosystem because of the massive efficiency it brings to training and inference.\n\nDiffusers"}, {"name": "peft", "tags": ["data", "math", "ml"], "summary": "Parameter-Efficient Fine-Tuning (PEFT)", "text": "The iterative diffusion process consumes a lot of memory which can make it difficult to train. PEFT can help reduce the memory requirements and reduce the storage size of the final model checkpoint. For example, consider the memory required for training a Stable Diffusion model with LoRA on an A100 80GB GPU with more than 64GB of CPU RAM. The final model checkpoint size is only 8.8MB!\n\nModel\n---------\nCompVis/stable-diffusion-v1-4\n\n> [!TIP]\n> Take a look at the [examples/lora_dreambooth/train_dreambooth.py](examples/lora_dreambooth/train_dreambooth.py) training script to try training your own Stable Diffusion model with LoRA, and play around with the [smangrul/peft-lora-sd-dreambooth](https://huggingface.co/spaces/smangrul/peft-lora-sd-dreambooth) Space which is running on a T4 instance. Learn more about the PEFT integration in Diffusers in this [tutorial](https://huggingface.co/docs/peft/main/en/tutorial/peft_integrations#diffusers).\n\nTransformers\n\nPEFT is directly integrated with [Transformers](https://huggingface.co/docs/transformers/main/en/peft). After loading a model, call `add_adapter` to add a new PEFT adapter to the model:\n\nTo load a trained PEFT adapter, call `load_adapter`:\n\nAnd to switch between different adapters, call `set_adapter`:\n\nThe Transformers integration doesn't include all the functionalities offered in PEFT, such as methods for merging the adapter into the base model.\n\nAccelerate\n\n[Accelerate](https://huggingface.co/docs/accelerate/index) is a library for distributed training and inference on various training setups and hardware (GPUs, TPUs, Apple Silicon, etc.). PEFT models work with Accelerate out of the box, making it really convenient to train really large models or use them for inference on consumer hardware with limited resources.\n\nTRL\n\nPEFT can also be applied to training LLMs with RLHF components such as the ranker and policy. Get started by reading:\n\n* [Fine-tune a Mistral-7b model with Direct Preference Optimization](https://towardsdatascience.com/fine-tune-a-mistral-7b-model-with-direct-preference-optimization-708042745aac) with PEFT and the [TRL](https://huggingface.co/docs/trl/index) library to learn more about the Direct Preference Optimization (DPO) method and how to apply it to a LLM.\n* [Fine-tuning 20B LLMs with RLHF on a 24GB consumer GPU](https://huggingface.co/blog/trl-peft) with PEFT and the [TRL](https://huggingface.co/docs/trl/index) library, and then try out the [gpt2-sentiment_peft.ipynb](https://github.com/huggingface/trl/blob/main/examples/notebooks/gpt2-sentiment.ipynb) notebook to optimize GPT2 to generate positive movie reviews.\n* [StackLLaMA: A hands-on guide to train LLaMA with RLHF](https://huggingface.co/blog/stackllama) with PEFT, and then try out the [stack_llama/scripts](https://github.com/huggingface/trl/tree/main/examples/research_projects/stack_llama/scripts) for supervised finetuning, reward modeling, and RL finetuning.\n\nModel support\n\nUse this [Space](https://stevhliu-peft-methods.hf.space) or check out the [docs](https://huggingface.co/docs/peft/main/en/index) to find which models officially support a PEFT method out of the box. Even if you don't see a model listed below, you can manually configure the model config to enable PEFT for a model. Read the [New transformers architecture](https://huggingface.co/docs/peft/main/en/developer_guides/custom_models#new-transformers-architectures) guide to learn how.\n\nContribute\n\nIf you would like to contribute to PEFT, please check out our [contribution guide](https://huggingface.co/docs/peft/developer_guides/contributing).\n\nCiting  PEFT\n\nTo use  PEFT in your publication, please cite it by using the following BibTeX entry."}, {"name": "peft", "tags": ["data", "math", "ml"], "summary": "Parameter-Efficient Fine-Tuning (PEFT)", "text": "This library is used to efficiently fine-tune large language models by reducing the number of parameters that need to be updated during training, resulting in significant savings in compute and storage resources. With PEFT, developers can achieve high performance on consumer hardware while working with large models."}, {"name": "pillow", "tags": [], "summary": "Python Imaging Library (fork)", "text": "Pillow\n\nPython Imaging Library (Fork)\n\nPillow is the friendly PIL fork by [Jeffrey A. Clark and\ncontributors](https://github.com/python-pillow/Pillow/graphs/contributors).\nPIL is the Python Imaging Library by Fredrik Lundh and contributors.\nAs of 2019, Pillow development is\n[supported by Tidelift](https://tidelift.com/subscription/pkg/pypi-pillow?utm_source=pypi-pillow&utm_medium=readme&utm_campaign=enterprise).\n\nOverview\n\nThe Python Imaging Library adds image processing capabilities to your Python interpreter.\n\nThis library provides extensive file format support, an efficient internal representation, and fairly powerful image processing capabilities.\n\nThe core image library is designed for fast access to data stored in a few basic pixel formats. It should provide a solid foundation for a general image processing tool.\n\nMore information\n\nReport a vulnerability\n\nTo report a security vulnerability, please follow the procedure described in the [Tidelift security policy](https://tidelift.com/docs/security)."}, {"name": "pillow", "tags": [], "summary": "Python Imaging Library (fork)", "text": "This library is used to add extensive image processing capabilities to Python applications, including support for various file formats and efficient internal representations. This enables developers to load, edit, and manipulate images within their code with high performance."}, {"name": "pint", "tags": ["math", "web"], "summary": "Physical quantities module", "text": ".. image:: https://img.shields.io/pypi/v/pint.svg\n\n.. image:: https://img.shields.io/pypi/l/pint.svg\n\n.. image:: https://img.shields.io/pypi/pyversions/pint.svg\n\nPint: makes units easy\n======================\n\nPint is a Python package to define, operate and manipulate physical\nquantities: the product of a numerical value and a unit of measurement.\nIt allows arithmetic operations between them and conversions from and\nto different units.\n\nIt is distributed with a comprehensive list of physical units, prefixes\nand constants. Due to its modular design, you can extend (or even rewrite!)\nthe complete list without changing the source code. It supports a lot of\nnumpy mathematical operations **without monkey patching or wrapping numpy**.\n\nIt has a complete test coverage. It runs in Python 3.9+ with no other dependency.\nIt is licensed under BSD.\n\nIt is extremely easy and natural to use:\n\n.. code-block:: python\n\nand you can make good use of numpy if you want:\n\n.. code-block:: python\n\nQuick Installation\n------------------\n\nTo install Pint, simply:\n\n.. code-block:: bash\n\nor utilizing conda, with the conda-forge channel:\n\n.. code-block:: bash\n\nand then simply enjoy it!\n\nDocumentation\n-------------\n\nFull documentation is available at http://pint.readthedocs.org/\n\nCommand-line converter\n----------------------\n\nA command-line script `pint-convert` provides a quick way to convert between\nunits or get conversion factors.\n\nDesign principles\n-----------------\n\nAlthough there are already a few very good Python packages to handle physical\nquantities, no one was really fitting my needs. Like most developers, I\nprogrammed Pint to scratch my own itches.\n\n**Unit parsing**: prefixed and pluralized forms of units are recognized without\nexplicitly defining them. In other words: as the prefix *kilo* and the unit\n*meter* are defined, Pint understands *kilometers*. This results in a much\nshorter and maintainable unit definition list as compared to other packages.\n\n**Standalone unit definitions**: units definitions are loaded from a text file\nwhich is simple and easy to edit. Adding and changing units and their\ndefinitions does not involve changing the code.\n\n**Advanced string formatting**: a quantity can be formatted into string using\n`PEP 3101`_ syntax. Extended conversion flags are given to provide symbolic,\nLaTeX and pretty formatting. Unit name translation is available if Babel_ is\ninstalled.\n\n**Free to choose the numerical type**: You can use any numerical type\n(`fraction`, `float`, `decimal`, `numpy.ndarray`, etc). NumPy_ is not required\nbut supported.\n\n**Awesome NumPy integration**: When you choose to use a NumPy_ ndarray, its methods and\nufuncs are supported including automatic conversion of units. For example\n`numpy.arccos(q)` will require a dimensionless `q` and the units of the output\nquantity will be radian.\n\n**Uncertainties integration**:  transparently handles calculations with\nquantities with uncertainties (like 3.14\u00b10.01 meter) via the `uncertainties\npackage`_.\n\n**Handle temperature**: conversion between units with different reference\npoints, like positions on a map or absolute temperature scales.\n\n**Dependency free**: it depends only on Python and its standard library. It interacts with other packages\nlike numpy and uncertainties if they are installed\n\n**Pandas integration**: Thanks to `Pandas Extension Types`_ it is now possible to use Pint with Pandas. Operations on DataFrames and between columns are units aware, providing even more convenience for users of Pandas DataFrames. For full details, see the `pint-pandas Jupyter notebook`_.\n\nPint is maintained by a community of scientists, programmers and enthusiasts around the world.\nSee AUTHORS_ for a complete list.\n\nTo review an ordered list of notable changes for each version of a project,\nsee CHANGES_\n\n.. _Website: http://www.dimensionalanalysis.org/\n.. _`comprehensive list of physical units, prefixes and constants`: https://github.com/hgrecco/pint/blob/master/pint/default_en.txt\n.. _`uncertainties package`: https://pythonhosted.org/uncertainties/\n.. _`NumPy`: http://www.numpy.org/\n.. _`PEP 3101`: https://www.python.org/dev/peps/pep-3101/\n.. _`Babel`: http://babel.pocoo.org/\n.. _`Pandas Extension Types`: https://pandas.pydata.org/pandas-docs/stable/development/extending.html#extension-types\n.. _`pint-pandas Jupyter notebook`: https://github.com/hgrecco/pint-pandas/blob/master/notebooks/pint-pandas.ipynb\n.. _`AUTHORS`: https://github.com/hgrecco/pint/blob/master/AUTHORS\n.. _`CHANGES`: https://github.com/hgrecco/pint/blob/master/CHANGES"}, {"name": "pint", "tags": ["math", "web"], "summary": "Physical quantities module", "text": "This library is used to define, operate and manipulate physical quantities with ease by allowing arithmetic operations between them and conversions from and to different units. Developers can use Pint to easily work with various physical units and constants without modifying the source code or requiring additional libraries like numpy."}, {"name": "plotly-express", "tags": ["math", "visualization", "web"], "summary": "Plotly Express - a high level wrapper for Plotly.py", "text": "Plotly Express\n\nPlotly Express is now part of Plotly.py version 4 and so the `plotly_express` module now just re-exports the contents of `plotly.express`\n\nInstallation\n\nIf you follow the [`plotly` Getting Started](https://plot.ly/python/getting-started/) instructions for installation, you will get access to `plotly.express`.\n\nHowever, if you have existing code that imports from `plotly_express` explicitly and you don't wish to change it, you can still install the latest version, which just exposes `plotly.express` under the `plotly_express` namespace.\n\nVia `pip`\n\nJust running `pip install plotly_express==0.4.0` in your terminal should do it!\n\nVia `conda`\n\nYou'll have to install from the `plotly` channel with `conda install -c plotly plotly_express==0.4.0`\n\nGetting Help\n\nPlease join our [Community Forum](https://community.plot.ly/c/api/python) or file a [Github Issue](https://github.com/plotly/plotly.py/issues/new) if you've found a bug."}, {"name": "plotly-express", "tags": ["math", "visualization", "web"], "summary": "Plotly Express - a high level wrapper for Plotly.py", "text": "This library is used to create interactive, web-based visualizations with ease through a high-level interface, allowing developers to focus on data analysis rather than complex plotting code. With Plotly Express, developers can generate a wide range of plots, including scatter plots, bar charts, histograms, and more, in a simple and intuitive way."}, {"name": "plotly", "tags": ["math", "visualization"], "summary": "An open-source interactive data visualization library for Python", "text": "plotly.py\n\nQuickstart\n\n`pip install plotly`\n\nSee the [Python documentation](https://plotly.com/python/) for more examples.\n\nOverview\n\n[plotly.py](https://plotly.com/python/) is an interactive, open-source, and browser-based graphing library for Python :sparkles:\n\nBuilt on top of [plotly.js](https://github.com/plotly/plotly.js), `plotly.py` is a high-level, declarative charting library. plotly.js ships with over 30 chart types, including scientific charts, 3D graphs, statistical charts, SVG maps, financial charts, and more.\n\n`plotly.py` is [MIT Licensed](https://github.com/plotly/plotly.py/blob/main/LICENSE.txt). Plotly graphs can be viewed in [Jupyter notebooks](https://jupyter.org), other Python notebook software such as [marimo](https://marimo.io), as standalone HTML files, or integrated into [Dash applications](https://dash.plotly.com/).\n\n[Contact us](https://plotly.com/consulting-and-oem/) for consulting, dashboard development, application integration, and feature additions.\n\n---\n\n---\n\nInstallation\n\nplotly.py may be installed using pip\n\nor conda.\n\nJupyter Widget Support\n\nFor use as a Jupyter widget, install `jupyter` and `anywidget`\npackages using `pip`:\n\nor `conda`:\n\nStatic Image Export\n\nplotly.py supports [static image export](https://plotly.com/python/static-image-export/),\nusing either the [`kaleido`](https://github.com/plotly/Kaleido)\npackage (recommended, supported as of `plotly` version 4.9) or the [orca](https://github.com/plotly/orca)\ncommand line utility (legacy as of `plotly` version 4.9).\n\nKaleido\n\nThe [`kaleido`](https://github.com/plotly/Kaleido) package has no dependencies and can be installed\nusing pip\n\nor conda\n\nExtended Geo Support\n\nSome plotly.py features rely on fairly large geographic shape files. The county\nchoropleth figure factory is one such example. These shape files are distributed as a\nseparate `plotly-geo` package. This package can be installed using pip...\n\nor conda\n\n`plotly-geo` can be found on Github at https://github.com/plotly/plotly-geo.\n\nCopyright and Licenses\n\nCode and documentation copyright 2019 Plotly, Inc.\n\nCode released under the [MIT license](https://github.com/plotly/plotly.py/blob/main/LICENSE.txt).\n\nDocs released under the [Creative Commons license](https://github.com/plotly/documentation/blob/source/LICENSE)."}, {"name": "plotly", "tags": ["math", "visualization"], "summary": "An open-source interactive data visualization library for Python", "text": "This library is used to create interactive and customizable visualizations of data with over 30 chart types, including scientific charts, 3D graphs, and statistical charts. With plotly, developers can easily embed these visualizations into Jupyter notebooks, other Python notebook software, or as standalone HTML files."}, {"name": "plotnine", "tags": ["dev", "math", "visualization", "web"], "summary": "A Grammar of Graphics for Python", "text": "plotnine \n\n(https://pypi.python.org/pypi/plotnine)\n(https://pypi.python.org/pypi/plotnine)\n\n(https://github.com/has2k1/plotnine/actions?query=branch%3Amain+workflow%3A%22build%22)\n(https://codecov.io/github/has2k1/plotnine?branch=main)\n\nplotnine is an implementation of a *grammar of graphics* in Python\nbased on [ggplot2](https://github.com/tidyverse/ggplot2).\nThe grammar allows you to compose plots by explicitly mapping variables in a\ndataframe to the visual characteristics (position, color, size etc.) of objects that make up the plot.\n\nPlotting with a *grammar of graphics* is powerful. Custom (and otherwise\ncomplex) plots are easy to think about and build incrementally, while the\nsimple plots remain simple to create.\n\nTo learn more about how to use plotnine, check out the\n[documentation](https://plotnine.org). Since plotnine\nhas an API similar to ggplot2, where it lacks in coverage the\n[ggplot2 documentation](http://ggplot2.tidyverse.org/reference/index.html)\nmay be helpful.\n\nExample\n\nBuilding a complex plot piece by piece.\n\n1. Scatter plot\n\n   \n\n   \n\n2. Scatter plot colored according some variable\n\n   \n\n   \n\n3. Scatter plot colored according some variable and\n   smoothed with a linear model with confidence intervals.\n\n   \n\n   \n\n4. Scatter plot colored according some variable,\n   smoothed with a linear model with confidence intervals and\n   plotted on separate panels.\n\n   \n\n   \n\n5. Adjust the themes\n\n   I) Make it playful\n\n   \n\n   \n\n   II) Or professional\n\nInstallation\n\nOfficial release\n\nDevelopment version\n\nContributing\n\nOur documentation could use some examples, but we are looking for something\na little bit special. We have two criteria:\n\n1. Simple looking plots that otherwise require a trick or two.\n2. Plots that are part of a data analytic narrative. That is, they provide\n   some form of clarity showing off the `geom`, `stat`, ... at their\n   differential best.\n\nIf you come up with something that meets those criteria, we would love to\nsee it. See [plotnine-examples](https://github.com/has2k1/plotnine-examples).\n\nIf you discover a bug checkout the [issues](https://github.com/has2k1/plotnine/issues)\nif it has not been reported, yet please file an issue.\n\nAnd if you can fix a bug, your contribution is welcome.\n\nTesting\n-------\n\nPlotnine has tests that generate images which are compared to baseline images known\nto be correct. To generate images that are consistent across all systems you have\nto install matplotlib from source. You can do that with ``pip`` using the command.\n\nOtherwise there may be small differences in the text rendering that throw off the\nimage comparisons."}, {"name": "plotnine", "tags": ["dev", "math", "visualization", "web"], "summary": "A Grammar of Graphics for Python", "text": "This library is used to compose custom plots by explicitly mapping variables in a dataframe to visual characteristics of objects that make up the plot. This allows developers to easily create complex and incremental plots while maintaining simplicity for straightforward ones."}, {"name": "plpygis", "tags": ["data", "math"], "summary": "Python tools for PostGIS", "text": "plpygis\n\n`plpygis` is a pure Python module with no dependencies that can convert geometries between [Well-known binary](https://en.wikipedia.org/wiki/Well-known_binary) (WKB/EWKB), [Well-known Text](https://en.wikipedia.org/wiki/Well-known_text_representation_of_geometry) (WKT/EWKT) and GeoJSON representations. `plpygis` is mainly intended for use in PostgreSQL [PL/Python](https://www.postgresql.org/docs/current/plpython.html) functions to augment [PostGIS](https://postgis.net/)'s native capabilities.\n\nBasic usage\n\n`plpygis` implements several subclasses of the `Geometry` class, such as `Point`, `LineString`, `MultiPolygon` and so on:\n\nUsage with PostGIS\n\n`plpygis` is designed to provide an easy way to implement PL/Python functions that accept `geometry` arguments or return `geometry` results. The following example will take a PostGIS `geometry(Point)` and use an external service to create a `geometry(PointZ)`.\n\nThe `Geometry()` constructor will convert a PostGIS `geometry` that has been passed as a parameter to the PL/Python function into one of its `plpygis` subclasses. A `Geometry` that is returned from the PL/Python function will automatically be converted back to a PostGIS `geometry`.\n\nThe function above can be called as part of an SQL query:\n\nDocumentation\n\nFull `plpygis` documentation is available at ."}, {"name": "plpygis", "tags": ["data", "math"], "summary": "Python tools for PostGIS", "text": "This library is used to convert geometries between different formats such as WKB, WKT, and GeoJSON for use in PostgreSQL PL/Python functions with PostGIS. With plpygis, developers can easily manipulate and transform spatial data within their PostGIS applications."}, {"name": "plyfile", "tags": ["math", "web"], "summary": "PLY file reader/writer", "text": "Quick start\n\nTo install the latest official release:\n\nTo install from source:\n\nQuick links\n\nPLY format reference\n\n[Link](https://web.archive.org/web/20161221115231/http://www.cs.virginia.edu/~gfx/Courses/2001/Advanced.spring.01/plylib/Ply.txt)\n\nProject documentation\n\n[Link](https://python-plyfile.readthedocs.io)\n\nGetting help\n\nHave questions? Feel free to ask in\n[Discussions](https://github.com/dranjan/python-plyfile/discussions).\n\nReporting bugs\n\n[Issues](https://github.com/dranjan/python-plyfile/issues)\n\nContributing\n\n[Information for developers](https://python-plyfile.readthedocs.io/en/latest/developing.html)\n\nCopyright and license\n\nCopyright Darsh Ranjan and `plyfile` authors.\n\nThis software is released under the terms of the GNU General Public\nLicense, version 3.  See the file `COPYING` for details."}, {"name": "plyfile", "tags": ["math", "web"], "summary": "PLY file reader/writer", "text": "This library is used to read and write files in PLY format, a widely-used binary mesh format for 3D models. With this library, developers can easily import and export 3D model data in PLY format from their Python applications."}, {"name": "pmdarima", "tags": ["data", "math", "ui"], "summary": "Python's forecast::auto.arima equivalent", "text": "pmdarima\n\n(https://circleci.com/gh/alkaline-ml/pmdarima)\n(https://github.com/alkaline-ml/pmdarima/actions/workflows/build_and_deploy.yml)\n(https://codecov.io/gh/alkaline-ml/pmdarima)\n\nPmdarima (originally `pyramid-arima`, for the anagram of 'py' + 'arima') is a statistical\nlibrary designed to fill the void in Python's time series analysis capabilities. This includes:\n\n  * The equivalent of R's [`auto.arima`](https://www.rdocumentation.org/packages/forecast/versions/7.3/topics/auto.arima) functionality\n  * A collection of statistical tests of stationarity and seasonality\n  * Time series utilities, such as differencing and inverse differencing\n  * Numerous endogenous and exogenous transformers and featurizers, including Box-Cox and Fourier transformations\n  * Seasonal time series decompositions\n  * Cross-validation utilities\n  * A rich collection of built-in time series datasets for prototyping and examples\n  * Scikit-learn-esque pipelines to consolidate your estimators and promote productionization\n  \nPmdarima wraps [statsmodels](https://github.com/statsmodels/statsmodels/blob/master/statsmodels)\nunder the hood, but is designed with an interface that's familiar to users coming\nfrom a scikit-learn background.\n\nInstallation\n\npip\n\nPmdarima has binary and source distributions for Windows, Mac and Linux (`manylinux`) on pypi\nunder the package name `pmdarima` and can be downloaded via `pip`:\n\nconda\n\nPmdarima also has Mac and Linux builds available via `conda` and can be installed like so:\n\n**Note:** We do not maintain our own Conda binaries, they are maintained at https://github.com/conda-forge/pmdarima-feedstock.\nSee that repo for further documentation on working with Pmdarima on Conda.\n\nQuickstart Examples\n\nFitting a simple auto-ARIMA on the [`wineind`](https://alkaline-ml.com/pmdarima/modules/generated/pmdarima.datasets.load_wineind.html#pmdarima.datasets.load_wineind) dataset:\n\nFitting a more complex pipeline on the [`sunspots`](https://www.rdocumentation.org/packages/datasets/versions/3.6.1/topics/sunspots) dataset,\nserializing it, and then loading it from disk to make predictions:\n\nAvailability\n\n`pmdarima` is available on PyPi in pre-built Wheel files for Python 3.10+ for the following platforms:\n\n* Mac (64-bit)\n* Linux (64-bit manylinux)\n* Windows (64-bit)\n  * 32-bit wheels are available for pmdarima versions below 2.0.0 and Python versions below 3.10\n\nIf a wheel doesn't exist for your platform, you can still `pip install` and it\nwill build from the source distribution tarball, however you'll need `cython>=0.29`\nand `gcc` (Mac/Linux) or `MinGW` (Windows) in order to build the package from source.\n\nNote that legacy versions (<1.0.0) are available under the name\n\"`pyramid-arima`\" and can be pip installed via:\n\nHowever, this is not recommended.\n\nDocumentation\n\nAll of your questions and more (including examples and guides) can be answered by\nthe [`pmdarima` documentation](https://www.alkaline-ml.com/pmdarima). If not, always\nfeel free to file an issue."}, {"name": "pmdarima", "tags": ["data", "math", "ui"], "summary": "Python's forecast::auto.arima equivalent", "text": "This library is used to automatically fit ARIMA models to time series data using the equivalent of R's auto.arima functionality. With pmdarima, developers can easily perform robust forecasting and analysis of time series data in Python."}, {"name": "polars-lts-cpu", "tags": ["cli", "data", "math", "ui", "web"], "summary": "Blazingly fast DataFrame library", "text": "Polars: Blazingly fast DataFrames in Rust, Python, Node.js, R, and SQL\n\nPolars is a DataFrame interface on top of an OLAP Query Engine implemented in Rust using\n[Apache Arrow Columnar Format](https://arrow.apache.org/docs/format/Columnar.html) as the memory\nmodel.\n\n- Lazy | eager execution\n- Multi-threaded\n- SIMD\n- Query optimization\n- Powerful expression API\n- Hybrid Streaming (larger-than-RAM datasets)\n- Rust | Python | NodeJS | R | ...\n\nTo learn more, read the [user guide](https://docs.pola.rs/).\n\nPython\n\nSQL\n\nSQL commands can also be run directly from your terminal using the Polars CLI:\n\nRefer to the [Polars CLI repository](https://github.com/pola-rs/polars-cli) for more information.\n\nPerformance\n\nBlazingly fast\n\nPolars is very fast. In fact, it is one of the best performing solutions available. See the\n[PDS-H benchmarks](https://www.pola.rs/benchmarks.html) results.\n\nLightweight\n\nPolars is also very lightweight. It comes with zero required dependencies, and this shows in the\nimport times:\n\n- polars: 70ms\n- numpy: 104ms\n- pandas: 520ms\n\nHandles larger-than-RAM data\n\nIf you have data that does not fit into memory, Polars' query engine is able to process your query\n(or parts of your query) in a streaming fashion. This drastically reduces memory requirements, so\nyou might be able to process your 250GB dataset on your laptop. Collect with\n`collect(engine='streaming')` to run the query streaming. (This might be a little slower, but it is\nstill very fast!)\n\nSetup\n\nPython\n\nInstall the latest Polars version with:\n\nWe also have a conda package (`conda install -c conda-forge polars`), however pip is the preferred\nway to install Polars.\n\nInstall Polars with all optional dependencies.\n\nYou can also install a subset of all optional dependencies.\n\nSee the [User Guide](https://docs.pola.rs/user-guide/installation/#feature-flags) for more details\non optional dependencies\n\nTo see the current Polars version and a full list of its optional dependencies, run:\n\nReleases happen quite often (weekly / every few days) at the moment, so updating Polars regularly to\nget the latest bugfixes / features might not be a bad idea.\n\nRust\n\nYou can take latest release from `crates.io`, or if you want to use the latest features /\nperformance improvements point to the `main` branch of this repo.\n\nRequires Rust version `>=1.80`.\n\nContributing\n\nWant to contribute? Read our [contributing guide](https://docs.pola.rs/development/contributing/).\n\nPython: compile Polars from source\n\nIf you want a bleeding edge release or maximal performance you should compile Polars from source.\n\nThis can be done by going through the following steps in sequence:\n\n1. Install the latest [Rust compiler](https://www.rust-lang.org/tools/install)\n2. Install [maturin](https://maturin.rs/): `pip install maturin`\n3. `cd py-polars` and choose one of the following:\n   - `make build`, slow binary with debug assertions and symbols, fast compile times\n   - `make build-release`, fast binary without debug assertions, minimal debug symbols, long compile\n   - `make build-nodebug-release`, same as build-release but without any debug symbols, slightly\n   - `make build-debug-release`, same as build-release but with full debug symbols, slightly slower\n   - `make build-dist-release`, fastest binary, extreme compile times\n\nBy default the binary is compiled with optimizations turned on for a modern CPU. Specify `LTS_CPU=1`\nwith the command if your CPU is older and does not support e.g. AVX2.\n\nNote that the Rust crate implementing the Python bindings is called `py-polars` to distinguish from\nthe wrapped Rust crate `polars` itself. However, both the Python package and the Python module are\nnamed `polars`, so you can `pip install polars` and `import polars`.\n\nUsing custom Rust functions in Python\n\nExtending Polars with UDFs compiled in Rust is easy. We expose PyO3 extensions for `DataFrame` and\n`Series` data structures. See more in https://github.com/pola-rs/polars/tree/main/pyo3-polars.\n\nGoing big...\n\nDo you expect more than 2^32 (~4.2 billion) rows? Compile Polars with the `bigidx` feature flag or,\nfor Python users, install `pip install polars-u64-idx`.\n\nDon't use this unless you hit the row boundary as the default build of Polars is faster and consumes\nless memory.\n\nLegacy\n\nDo you want Polars to run on an old CPU (e.g. dating from before 2011), or on an `x86-64` build of\nPython on Apple Silicon under Rosetta? Install `pip install polars-lts-cpu`. This version of Polars\nis compiled without [AVX](https://en.wikipedia.org/wiki/Advanced_Vector_Extensions) target features.\n\nSponsors\n\n[](https://www.jetbrains.com)"}, {"name": "polars-lts-cpu", "tags": ["cli", "data", "math", "ui", "web"], "summary": "Blazingly fast DataFrame library", "text": "This library is used to create and manage high-performance DataFrames in various programming languages, providing features like lazy execution, multi-threading, and query optimization for efficient data processing. With Polars-LTS-CPU, developers can achieve blazingly fast performance for complex data operations, making it suitable for large-scale data analysis and manipulation tasks."}, {"name": "polars", "tags": ["data", "math", "web"], "summary": "Blazingly fast DataFrame library", "text": "Polars: Extremely fast Query Engine for DataFrames, written in Rust\n\nPolars is an analytical query engine written for DataFrames. It is designed to be fast, easy to use\nand expressive. Key features are:\n\n- Lazy | Eager execution\n- Streaming (larger-than-RAM datasets)\n- Query optimization\n- Multi-threaded\n- Written in Rust\n- SIMD\n- Powerful expression API\n- Front end in Python | Rust | NodeJS | R | SQL\n- [Apache Arrow Columnar Format](https://arrow.apache.org/docs/format/Columnar.html)\n\nTo learn more, read the [user guide](https://docs.pola.rs/).\n\nPerformance\n\nBlazingly fast\n\nPolars is very fast. In fact, it is one of the best performing solutions available. See the\n[PDS-H benchmarks](https://www.pola.rs/benchmarks.html) results.\n\nLightweight\n\nPolars is also very lightweight. It comes with zero required dependencies, and this shows in the\nimport times:\n\n- polars: 70ms\n- numpy: 104ms\n- pandas: 520ms\n\nHandles larger-than-RAM data\n\nIf you have data that does not fit into memory, Polars' query engine is able to process your query\n(or parts of your query) in a streaming fashion. This drastically reduces memory requirements, so\nyou might be able to process your 250GB dataset on your laptop. Collect with\n`collect(engine='streaming')` to run the query streaming.\n\nSetup\n\nPython\n\nInstall the latest Polars version with:\n\nSee the [User Guide](https://docs.pola.rs/user-guide/installation/#feature-flags) for more details\non optional dependencies\n\nTo see the current Polars version and a full list of its optional dependencies, run:\n\nContributing\n\nWant to contribute? Read our [contributing guide](https://docs.pola.rs/development/contributing/).\n\nManaged/Distributed Polars\n\nDo you want a managed solution or scale out to distributed clusters? Consider our\n[offering](https://cloud.pola.rs/) and help the project!\n\nPython: compile Polars from source\n\nIf you want a bleeding edge release or maximal performance you should compile Polars from source.\n\nThis can be done by going through the following steps in sequence:\n\n1. Install the latest [Rust compiler](https://www.rust-lang.org/tools/install)\n2. Install [maturin](https://maturin.rs/): `pip install maturin`\n3. `cd py-polars` and choose one of the following:\n   - `make build`, slow binary with debug assertions and symbols, fast compile times\n   - `make build-release`, fast binary without debug assertions, minimal debug symbols, long compile\n   - `make build-nodebug-release`, same as build-release but without any debug symbols, slightly\n   - `make build-debug-release`, same as build-release but with full debug symbols, slightly slower\n   - `make build-dist-release`, fastest binary, extreme compile times\n\nBy default the binary is compiled with optimizations turned on for a modern CPU. Specify `LTS_CPU=1`\nwith the command if your CPU is older and does not support e.g. AVX2.\n\nNote that the Rust crate implementing the Python bindings is called `py-polars` to distinguish from\nthe wrapped Rust crate `polars` itself. However, both the Python package and the Python module are\nnamed `polars`, so you can `pip install polars` and `import polars`.\n\nUsing custom Rust functions in Python\n\nExtending Polars with UDFs compiled in Rust is easy. We expose PyO3 extensions for `DataFrame` and\n`Series` data structures. See more in https://github.com/pola-rs/polars/tree/main/pyo3-polars.\n\nGoing big...\n\nDo you expect more than 2^32 (~4.2 billion) rows? Compile Polars with the `bigidx` feature flag or,\nfor Python users, install `pip install polars[rt64]`.\n\nDon't use this unless you hit the row boundary as the default build of Polars is faster and consumes\nless memory.\n\nLegacy\n\nDo you want Polars to run on an old CPU (e.g. dating from before 2011), or on an `x86-64` build of\nPython on Apple Silicon under Rosetta? Install `pip install polars[rtcompat]`. This version of\nPolars is compiled without [AVX](https://en.wikipedia.org/wiki/Advanced_Vector_Extensions) target\nfeatures."}, {"name": "polars", "tags": ["data", "math", "web"], "summary": "Blazingly fast DataFrame library", "text": "This library is used to efficiently query and manipulate large datasets in various programming languages, leveraging Rust's performance and Apache Arrow's columnar format for fast data processing. With polars, developers can achieve blazingly fast performance while working with DataFrames, making it suitable for demanding analytical tasks."}, {"name": "pooch", "tags": ["data", "math"], "summary": "A friend to fetch your data files", "text": "About\n\n> Just want to download a file without messing with `requests` and `urllib`?\n> Trying to add sample datasets to your Python package?\n> **Pooch is here to help!**\n\n*Pooch* is a **Python library** that can manage data by **downloading files**\nfrom a server (only when needed) and storing them locally in a data **cache**\n(a folder on your computer).\n\n* Pure Python and minimal dependencies.\n* Download files over HTTP, FTP, and from data repositories like Zenodo and figshare.\n* Built-in post-processors to unzip/decompress the data after download.\n* Designed to be extended: create custom downloaders and post-processors.\n\nAre you a **scientist** or researcher? Pooch can help you too!\n\n* Host your data on a repository and download using the DOI.\n* Automatically download data using code instead of telling colleagues to do it themselves.\n* Make sure everyone running the code has the same version of the data files.\n\nProjects using Pooch\n\n[SciPy](https://github.com/scipy/scipy), \n[scikit-image](https://github.com/scikit-image/scikit-image),\n[xarray](https://github.com/pydata/xarray),\n[Ensaio](https://github.com/fatiando/ensaio),\n[GemPy](https://github.com/cgre-aachen/gempy),\n[MetPy](https://github.com/Unidata/MetPy),\n[napari](https://github.com/napari/napari),\n[Satpy](https://github.com/pytroll/satpy),\n[yt](https://github.com/yt-project/yt),\n[PyVista](https://github.com/pyvista/pyvista),\n[icepack](https://github.com/icepack/icepack),\n[histolab](https://github.com/histolab/histolab),\n[seaborn-image](https://github.com/SarthakJariwala/seaborn-image),\n[Open AR-Sandbox](https://github.com/cgre-aachen/open_AR_Sandbox),\n[climlab](https://github.com/climlab/climlab),\n[mne-python](https://github.com/mne-tools/mne-python),\n[GemGIS](https://github.com/cgre-aachen/gemgis),\n[SHTOOLS](https://github.com/SHTOOLS/SHTOOLS),\n[MOABB](https://github.com/NeuroTechX/moabb),\n[GeoViews](https://github.com/holoviz/geoviews),\n[ScopeSim](https://github.com/AstarVienna/ScopeSim),\n[Brainrender](https://github.com/brainglobe/brainrender),\n[pyxem](https://github.com/pyxem/pyxem),\n[cellfinder](https://github.com/brainglobe/cellfinder),\n[PVGeo](https://github.com/OpenGeoVis/PVGeo),\n[geosnap](https://github.com/oturns/geosnap),\n[BioCypher](https://github.com/biocypher/biocypher),\n[cf-xarray](https://github.com/xarray-contrib/cf-xarray),\n[Scirpy](https://github.com/scverse/scirpy),\n[rembg](https://github.com/danielgatis/rembg),\n[DASCore](https://github.com/DASDAE/dascore),\n[scikit-mobility](https://github.com/scikit-mobility/scikit-mobility),\n[Py-ART](https://github.com/ARM-DOE/pyart),\n[HyperSpy](https://github.com/hyperspy/hyperspy),\n[RosettaSciIO](https://github.com/hyperspy/rosettasciio),\n[eXSpy](https://github.com/hyperspy/exspy)\n\n> If you're using Pooch, **send us a pull request** adding your project to the list.\n\nExample\n\nFor a **scientist downloading a data file** for analysis:\n\nFor **package developers** including sample data in their projects:\n\nGetting involved\n\n\ufe0f **Contact us:**\nFind out more about how to reach us at\n[fatiando.org/contact](https://www.fatiando.org/contact/).\n\n\u200d **Contributing to project development:**\nPlease read our\n[Contributing Guide](https://github.com/fatiando/pooch/blob/main/CONTRIBUTING.md)\nto see how you can help and give feedback.\n\n\u200d\u200d **Code of conduct:**\nThis project is released with a\n[Code of Conduct](https://github.com/fatiando/community/blob/main/CODE_OF_CONDUCT.md).\nBy participating in this project you agree to abide by its terms.\n\n> **Imposter syndrome disclaimer:**\n> We want your help. **No, really.** There may be a little voice inside your\n> head that is telling you that you're not ready, that you aren't skilled\n> enough to contribute. We assure you that the little voice in your head is\n> wrong. Most importantly, **there are many valuable ways to contribute besides\n> writing code**.\n>\n> *This disclaimer was adapted from the*\n> [MetPy project](https://github.com/Unidata/MetPy).\n\nLicense\n\nThis is free software: you can redistribute it and/or modify it under the terms\nof the **BSD 3-clause License**. A copy of this license is provided in\n[`LICENSE.txt`](https://github.com/fatiando/pooch/blob/main/LICENSE.txt)."}, {"name": "pooch", "tags": ["data", "math"], "summary": "A friend to fetch your data files", "text": "This library is used to manage data files by downloading them from a server and storing them locally in a cache, with built-in support for various protocols and file types. Pooch enables developers to easily retrieve data without worrying about manual requests or urllib configurations."}, {"name": "portion", "tags": ["dev", "math", "web"], "summary": "Python data structure and operations for intervals", "text": "portion - data structure and operations for intervals\n\n(https://github.com/AlexandreDecan/portion/actions/workflows/test.yaml)\n(https://coveralls.io/github/AlexandreDecan/portion?branch=master)\n(https://github.com/AlexandreDecan/portion/blob/master/LICENSE.txt)\n(https://pypi.org/project/portion)\n(https://github.com/AlexandreDecan/portion/commits/)\n\nThe `portion` library provides data structure and operations for intervals in Python.\n\n- Support intervals of any (comparable) objects.\n - Closed or open, finite or (semi-)infinite intervals.\n - Interval sets (union of atomic intervals) are supported.\n - Automatic simplification of intervals.\n - Support comparison, transformation, intersection, union, complement, difference and containment.\n - Provide test for emptiness, atomicity, overlap and adjacency.\n - Discrete iterations on the values of an interval.\n - Dict-like structure to map intervals to data.\n - Import and export intervals to strings and to Python built-in data types.\n - Heavily tested with high code coverage.\n\nTable of contents\n\n* [Installation](#installation)\n  * [Documentation & usage](#documentation--usage)\n  * [Changelog](#changelog)\n  * [Contributions](#contributions)\n  * [License](#license)\n\nInstallation\n\nYou can use `pip` to install it, as usual: `pip install portion`. This will install the latest available version from [PyPI](https://pypi.org/project/portion).\nPre-releases are available from the *master* branch on [GitHub](https://github.com/AlexandreDecan/portion) and can be installed with `pip install git+https://github.com/AlexandreDecan/portion`.\n\nYou can install `portion` and its development environment using `pip install --group dev` at the root of this repository. This automatically installs [pytest](https://docs.pytest.org/en/latest/) (for the test suites) and [ruff](https://docs.astral.sh/ruff/) (for code style).\n\nDocumentation & usage\n\nInterval creation\n\nAssuming this library is imported using `import portion as P`, intervals can be easily created using one of the following helpers:\n\nThe bounds of an interval can be any arbitrary values, as long as they are comparable:\n\nInfinite and semi-infinite intervals are supported using `P.inf` and `-P.inf` as upper or lower bounds.\nThese two objects support comparison with any other object.\nWhen infinities are used as a lower or upper bound, the corresponding boundary is automatically converted to an open one.\n\nIntervals created with this library are `Interval` instances.\nAn `Interval` instance is a disjunction of atomic intervals each representing a single interval (e.g. `[1,2]`).\nIntervals can be iterated to access the underlying atomic intervals, sorted by their lower and upper bounds.\n\nNested (sorted) intervals can also be retrieved with a position or a slice:\n\nFor convenience, intervals are automatically simplified:\n\nNote that, by default, simplification of discrete intervals is **not** supported by `portion` (but it can be simulated though, see [#24](https://github.com/AlexandreDecan/portion/issues/24#issuecomment-604456362)).\nFor example, combining `[0,1]` with `[2,3]` will **not** result in `[0,3]` even if there is no integer between `1` and `2`.\nRefer to [Specialize & customize intervals](#specialize--customize-intervals) to see how to create and use specialized discrete intervals.\n\n[&uparrow; back to top](#table-of-contents)\n\nInterval bounds & attributes\n\nAn `Interval` defines the following properties:\n\n- `i.empty` is `True` if and only if the interval is empty.\n\n- `i.atomic` is `True` if and only if the interval is empty or is a disjunction of a single interval.\n\n- `i.enclosure` refers to the smallest atomic interval that includes the current one."}, {"name": "portion", "tags": ["dev", "math", "web"], "summary": "Python data structure and operations for intervals", "text": "The left and right boundaries, and the lower and upper bounds of an interval can be respectively accessed with its `left`, `right`, `lower` and `upper` attributes.\nThe `left` and `right` bounds are either `P.CLOSED` or `P.OPEN`.\nBy definition, `P.CLOSED == ~P.OPEN` and vice-versa.\n\nBy convention, empty intervals resolve to `(P.inf, -P.inf)`:\n\nIf the interval is not atomic, then `left` and `lower` refer to the lower bound of its enclosure, while `right` and `upper` refer to the upper bound of its enclosure:\n\nOne can easily check for some interval properties based on the bounds of an interval:\n\n[&uparrow; back to top](#table-of-contents)\n\nInterval operations\n\n`Interval` instances support the following operations:\n\n- `i.intersection(other)` and `i & other` return the intersection of two intervals.\n\n- `i.union(other)` and `i | other` return the union of two intervals.\n\n- `i.complement(other)` and `~i` return the complement of the interval.\n\n- `i.difference(other)` and `i - other` return the difference between `i` and `other`.\n\n- `i.contains(other)` and `other in i` hold if given item is contained in the interval.\n It supports intervals and arbitrary comparable values.\n\n- `i.adjacent(other)` tests if the two intervals are adjacent, i.e., if they do not overlap and their union form a single atomic interval.\n While this definition corresponds to the usual notion of adjacency for atomic  intervals, it has stronger requirements for non-atomic ones since it requires  all underlying atomic intervals to be adjacent (i.e. that one  interval fills the gaps between the atomic intervals of the other one).\n\n- `i.overlaps(other)` tests if there is an overlap between two intervals.\n\nFinally, intervals are hashable as long as their bounds are hashable (and we have defined a hash value for `P.inf` and `-P.inf`).\n\n[&uparrow; back to top](#table-of-contents)\n\nComparison operators\n\nEquality between intervals can be checked with the classical `==` operator:\n\nMoreover, intervals are comparable using `>`, `>=`, `>> P.closed(0, 1) >> P.closed(0, 1) >> P.closed(0, 1) >> P.closed(0, 2) >> P.closed(0, 3) >> P.singleton(0) >> P.singleton(0) >> P.singleton(5) >> P.closed(0, 1) ` nor `>=` than any other interval, and no interval is ``, `=` when compared to the empty interval.\n\nMoreover, some non-empty intervals are also not comparable in the classical sense, as illustrated hereafter:\n\nAs a general rule, if `a  a`, `b >= a`, `not (a > b)`, `not (b = b)`, and `not (b >> i = P.closed(0, 2)\n>>> i.replace(P.OPEN, -1, 3, P.CLOSED)\n(-1,3]\n>>> i.replace(lower=1, right=P.OPEN)\n[1,2)\n\npython\n>>> P.closed(0, 2).replace(upper=lambda x: 2 * x)\n[0,4]\n\npython\n>>> i = P.closedopen(0, P.inf)\n>>> i.replace(upper=lambda x: 10)  # No change, infinity is ignored\n[0,+inf)\n>>> i.replace(upper=lambda x: 10, ignore_inf=False)  # Infinity is not ignored\n[0,10)\n\npython\n>>> i = P.openclosed(0, 1) | P.closed(5, 10)\n>>> i.replace(P.CLOSED, -1, 8, P.OPEN)\n[-1,1] | [5,8)\n>>> i.replace(lower=4)\n(4,10]\n\npython\n>>> i = P.closed(2, 3) | P.open(4, 5)\n>>> # Increment bound values\n>>> i.apply(lambda x: (x.left, x.lower + 1, x.upper + 1, x.right))\n[3,4] | (5,6)\n>>> # Invert bounds\n>>> i.apply(lambda x: (~x.left, x.lower, x.upper, ~x.right))\n(2,3) | [4,5]"}, {"name": "portion", "tags": ["dev", "math", "web"], "summary": "Python data structure and operations for intervals", "text": "python\n>>> i = P.openclosed(-P.inf, 0) | P.closed(3, 4) | P.closedopen(8, P.inf)\n>>> # Increment bound values\n>>> i.apply(lambda x: x.replace(upper=lambda v: v + 1))\n(-inf,1] | [3,5] | [8,+inf)\n>>> # Intervals are still automatically simplified\n>>> i.apply(lambda x: x.replace(lower=lambda v: v * 2))\n(-inf,0] | [16,+inf)\n>>> # Invert bounds\n>>> i.apply(lambda x: x.replace(left=lambda v: ~v, right=lambda v: ~v))\n(-inf,0) | (3,4) | (8,+inf)\n>>> # Replace infinities with -10 and 10\n>>> conv = lambda v: -10 if v == -P.inf else (10 if v == P.inf else v)\n>>> i.apply(lambda x: x.replace(lower=conv, upper=conv, ignore_inf=False))\n(-10,0] | [3,4] | [8,10)\n\npython\n>>> list(P.iterate(P.closed(0, 3), step=1))\n[0, 1, 2, 3]\n>>> list(P.iterate(P.closed(0, 3), step=2))\n[0, 2]\n>>> list(P.iterate(P.open(0, 3), step=2))\n[2]\n\npython\n>>> list(P.iterate(P.singleton(0) | P.singleton(3) | P.singleton(5), step=2))  # Won't be [0]\n[0, 3, 5]\n>>> list(P.iterate(P.closed(0, 2) | P.closed(4, 6), step=3))  # Won't be [0, 6]\n[0, 4]\n\npython\n>>> # Align on integers\n>>> list(P.iterate(P.closed(0.3, 4.9), step=1, base=int))\n[1, 2, 3, 4]\n>>> # Restrict values of a (semi-)infinite interval\n>>> list(P.iterate(P.openclosed(-P.inf, 2), step=1, base=lambda x: max(0, x)))\n[0, 1, 2]\n\npython\n>>> base = lambda x: 0\n>>> list(P.iterate(P.singleton(0) | P.singleton(3) | P.singleton(5), step=2, base=base))\n[0]\n>>> list(P.iterate(P.closed(0, 2) | P.closed(4, 6), step=3, base=base))\n[0, 6]\n\npython\n>>> list(P.iterate(P.closed(0, 3), step=-1, reverse=True))  # Mind step=-1\n[3, 2, 1, 0]\n>>> list(P.iterate(P.closed(0, 3), step=-2, reverse=True))  # Mind step=-2\n[3, 1]\n\npython\n>>> list(P.iterate(P.closed('a', 'd'), step=lambda d: chr(ord(d) + 1)))\n['a', 'b', 'c', 'd']\n>>> # Since we reversed the order, we changed \"+\" to \"-\" in the lambda.\n>>> list(P.iterate(P.closed('a', 'd'), step=lambda d: chr(ord(d) - 1), reverse=True))\n['d', 'c', 'b', 'a']\n\npython\n>>> d = P.IntervalDict()\n>>> d[P.closed(0, 3)] = 'banana'\n>>> d[4] = 'apple'\n>>> d\n{[0,3]: 'banana', [4]: 'apple'}\n\npython\n>>> d[P.closed(2, 4)] = 'orange'\n>>> d\n{[0,2): 'banana', [2,4]: 'orange'}\n\npython\n>>> d[2]\n'orange'\n>>> d[5]  # Key does not exist\nTraceback (most recent call last):\n ...\nKeyError: 5\n>>> d.get(5, default=0)\n0\n\npython\n>>> d[~P.empty()]  # Get all values, similar to d.copy()\n{[0,2): 'banana', [2,4]: 'orange'}\n>>> d[P.closed(1, 3)]\n{[1,2): 'banana', [2,3]: 'orange'}\n>>> d[P.closed(-2, 1)]\n{[0,1]: 'banana'}\n>>> d[P.closed(-2, -1)]\n{}\n\npython\n>>> d.get(P.closed(-2, 1), default='peach')\n{[-2,0): 'peach', [0,1]: 'banana'}\n>>> d.get(P.closed(-2, -1), default='peach')\n{[-2,-1]: 'peach'}\n>>> d.get(P.singleton(1), default='peach')  # Key is covered, default is not used\n{[1]: 'banana'}\n\npython\n>>> d.find('banana')\n[0,2)\n>>> d.find('orange')\n[2,4]\n>>> d.find('carrot')\n()\n\npython\n>>> d.domain()\n[0,4]\n>>> list(d.keys())\n[[0,2), [2,4]]\n>>> list(d.values())\n['banana', 'orange']\n>>> list(d.items())\n[([0,2), 'banana'), ([2,4], 'orange')]\n\npython\n>>> d = P.IntervalDict()\n>>> d[P.closed(0, 1)] = d[P.closed(2, 3)] = 'peach'\n>>> list(d.items())\n[([0,1] | [2,3], 'peach')]\n\npython\n>>> d1 = P.IntervalDict({P.closed(0, 2): 'banana'})\n>>> d2 = P.IntervalDict({P.closed(1, 3): 'orange'})\n>>> concat = lambda x, y: x + '/' + y\n>>> d1.combine(d2, how=concat)\n{[0,1): 'banana', [1,2]: 'banana/orange', (2,3]: 'orange'}\n\npython\n>>> d1.combine(d2, how=concat, missing='kiwi')\n{[0,1): 'banana/kiwi', [1,2]: 'banana/orange', (2,3]: 'kiwi/orange'}"}, {"name": "portion", "tags": ["dev", "math", "web"], "summary": "Python data structure and operations for intervals", "text": "python\n>>> d = d1.combine(d2, how=concat)\n>>> d[d1.domain()]  # Left join\n{[0,1): 'banana', [1,2]: 'banana/orange'}\n>>> d[d2.domain()]  # Right join\n{[1,2]: 'banana/orange', (2,3]: 'orange'}\n>>> d[d1.domain() & d2.domain()]  # Inner join\n{[1,2]: 'banana/orange'}\n\npython\n>>> P.to_string(P.closedopen(0, 1))\n'[0,1)'\n\npython\n>>> params = {\n...   'disj': ' or ',\n...   'sep': ' - ',\n...   'left_closed': '',\n...   'left_open': '..',\n...   'right_open': '..',\n...   'pinf': '+oo',\n...   'ninf': '-oo',\n...   'conv': lambda v: '\"{}\"'.format(v),\n... }\n>>> x = P.openclosed(0, 1) | P.closed(2, P.inf)\n>>> P.to_string(x, **params)\n'..\"0\" - \"1\"> or >> P.from_string('[0, 1]', conv=int) == P.closed(0, 1)\nTrue\n>>> P.from_string('[1.2]', conv=float) == P.singleton(1.2)\nTrue\n>>> converter = lambda s: datetime.datetime.strptime(s, '%Y/%m/%d')\n>>> P.from_string('[2011/03/15, 2013/10/10]', conv=converter)\n[datetime.datetime(2011, 3, 15, 0, 0),datetime.datetime(2013, 10, 10, 0, 0)]\n\npython\n>>> s = '..\"0\" - \"1\"> or >> params = {\n...   'disj': ' or ',\n...   'sep': ' - ',\n...   'left_closed': '',\n...   'left_open': r'\\.\\.',  # from_string expects regular expression patterns\n...   'right_open': r'\\.\\.',  # from_string expects regular expression patterns\n...   'pinf': r'\\+oo',  # from_string expects regular expression patterns\n...   'ninf': '-oo',\n...   'conv': lambda v: int(v[1:-1]),\n... }\n>>> P.from_string(s, **params)\n(0,1] | [2,+inf)\n\npython\n>>> s = '[(0, 1), (2, 3)]'  # Bounds are expected to be tuples\n>>> P.from_string(s, conv=eval, bound=r'\\(.+?\\)')\n[(0, 1),(2, 3)]\n\npython\n>>> P.to_data(P.openclosed(0, 2))\n[(False, 0, 2, True)]\n\npython\n>>> x = P.openclosed(0, 1) | P.closedopen(2, P.inf)\n>>> P.to_data(x)\n[(False, 0, 1, True), (True, 2, inf, False)]\n\npython\n>>> x = P.closedopen(datetime.date(2011, 3, 15), datetime.date(2013, 10, 10))\n>>> P.to_data(x, conv=lambda v: (v.year, v.month, v.day))\n[(True, (2011, 3, 15), (2013, 10, 10), False)]\n\npython\n>>> x = [(True, (2011, 3, 15), (2013, 10, 10), False)]\n>>> P.from_data(x, conv=lambda v: datetime.date(*v))\n[datetime.date(2011, 3, 15),datetime.date(2013, 10, 10))\n\npython\n>>> P.singleton(0) | P.singleton(1)  # Case 1: should be [0,1] for discrete numbers\n[0] | [1]\n>>> P.open(0, 1)  # Case 2: should be empty\n(0,1)\n>>> P.closedopen(0, 1)  # Case 3: should be singleton [0]\n[0,1)\n\npython\n>>> class IntInterval(P.AbstractDiscreteInterval):\n...     _step = 1\n\npython\n>>> IntInterval.from_atomic(P.CLOSED, 0, 0, P.CLOSED) | IntInterval.from_atomic(P.CLOSED, 1, 1, P.CLOSED)\n[0,1]\n>>> IntInterval.from_atomic(P.OPEN, 0, 1, P.OPEN)\n()\n>>> IntInterval.from_atomic(P.CLOSED, 0, 1, P.OPEN)\n[0]\n\npython\n>>> class CharInterval(P.AbstractDiscreteInterval):\n...     _incr = lambda v: chr(ord(v) + 1)\n...     _decr = lambda v: chr(ord(v) - 1)\n>>> CharInterval.from_atomic(P.OPEN, 'a', 'z', P.OPEN)\n['b','y']\n\npython\n>>> D = P.create_api(IntInterval)\n>>> D.singleton(0) | D.singleton(1)\n[0,1]\n>>> D.open(0, 1)\n()\n>>> D.closedopen(0, 1)\n[0]\n\npython\n>>> class NaturalInterval(IntInterval):\n...    @classmethod\n...    def from_atomic(cls, left, lower, upper, right):\n...        return super().from_atomic(\n...            P.CLOSED if lower >> N = P.create_api(NaturalInterval)\n>>> N.closed(-10, 2)\n[0,2]\n>>> N.open(-10, 2)\n[0,1]\n>>> ~N.empty()\n[0,+inf)\n\npython\n>>> N.closed(1.5, 2.5)  # Bounds are not natural numbers\n[1.5,2.5]\n>>> 0.5 in N.closed(0, 1)  # Given value is not a natural number\nTrue\n>>> ~N.singleton(0.5)\n[1.5,+inf)\n\n@software{portion,\n  author = {Decan, Alexandre},\n  title = {portion: Python data structure and operations for intervals},\n  url = {https://github.com/AlexandreDecan/portion},\n}\n```"}, {"name": "portion", "tags": ["dev", "math", "web"], "summary": "Python data structure and operations for intervals", "text": "This library is used to efficiently represent and manipulate intervals in Python, supporting various types of intervals and operations. With portion, developers can easily create, compare, and combine intervals to perform tasks such as data filtering and validation."}, {"name": "pox", "tags": ["cli", "math"], "summary": "utilities for filesystem exploration and automated builds", "text": "--------------------------------------------------------------\npox: utilities for filesystem exploration and automated builds\n--------------------------------------------------------------\n\nAbout Pox\n=========\n\n``pox`` provides a collection of utilities for navigating and manipulating\nfilesystems. This module is designed to facilitate some of the low level\noperating system interactions that are useful when exploring a filesystem\non a remote host, where queries such as *\"what is the root of the filesystem?\"*,\n*\"what is the user's name?\"*, and *\"what login shell is preferred?\"* become\nessential in allowing a remote user to function as if they were logged in\nlocally. While ``pox`` is in the same vein of both the ``os`` and ``shutil``\nbuiltin modules, the majority of its functionality is unique and compliments\nthese two modules.\n\n``pox`` provides Python equivalents of several unix shell commands such as\n``which`` and ``find``. These commands allow automated discovery of what has\nbeen installed on an operating system, and where the essential tools are\nlocated. This capability is useful not only for exploring remote hosts,\nbut also locally as a helper utility for automated build and installation.\n\nSeveral high-level operations on files and filesystems are also provided.\nExamples of which are: finding the location of an installed Python package,\ndetermining if and where the source code resides on the filesystem, and\ndetermining what version the installed package is.\n\n``pox`` also provides utilities to enable the abstraction of commands sent\nto a remote filesystem.  In conjunction with a registry of environment\nvariables and installed utilites, ``pox`` enables the user to interact with\na remote filesystem as if they were logged in locally. \n\n``pox`` is part of ``pathos``, a Python framework for heterogeneous computing.\n``pox`` is in active development, so any user feedback, bug reports, comments,\nor suggestions are highly appreciated.  A list of issues is located at https://github.com/uqfoundation/pox/issues, with a legacy list maintained at https://uqfoundation.github.io/project/pathos/query.\n\nMajor Features\n==============\n\n``pox`` provides utilities for discovering the user's environment:\n\n``pox`` also provides utilities for filesystem exploration and manipulation:\n\nCurrent Release\n===============\n\nThe latest released version of ``pox`` is available from:\n\n``pox`` is distributed under a 3-clause BSD license.\n\nDevelopment Version\n===================\n\nYou can get the latest development version with all the shiny new features at:\n\nIf you have a new contribution, please submit a pull request.\n\nInstallation\n============\n\n``pox`` can be installed with ``pip``::\n\nRequirements\n============\n\n``pox`` requires:\n\nBasic Usage\n===========\n\n``pox`` includes some basic utilities to connect to and automate exploration\non local and remote filesystems. There are some basic functions to discover\nimportant locations::\n\nor, you can interact with local and global environment variables::\n\nand perform some basic search functions::\n\n``pox`` also has a specialized `which` command just for Python::\n\nAny of the ``pox`` functions can be launched from the command line,\nwhich facilitates executing commands across parallel and distributed pipes\n(such as `pathos.connection.Pipe` and `pathos.secure.connection.Pipe`)::\n\nThe functions in ``pox`` that help make interactions with filesystems and\nenvironment varialbles programmatic and abstract become especially relevant\nwhen trying to execute complex commands remotely. \n\nMore Information\n================\n\nProbably the best way to get started is to look at the documentation at\nhow ``pox`` can be used to interact with the operating system. You can run the\ntest suite with ``python -m pox.tests``.  All ``pox`` utilities\ncan be launched directly from an operating system terminal, using the ``pox``\nscript (or with ``python -m pox``).  The source code is also generally well\ndocumented, so further questions may be resolved by inspecting the code\nitself.  Please feel free to submit a ticket on github, or ask a\nquestion on stackoverflow (**@Mike McKerns**).\nIf you would like to share how you use ``pox`` in your work, please send an\nemail (to **mmckerns at uqfoundation dot org**).\n\nCitation\n========\n\nIf you use ``pox`` to do research that leads to publication, we ask that you\nacknowledge use of ``pox`` by citing the following in your publication::\n\nPlease see https://uqfoundation.github.io/project/pathos or"}, {"name": "pox", "tags": ["cli", "math"], "summary": "utilities for filesystem exploration and automated builds", "text": "This library is used to navigate and manipulate filesystems on remote hosts for tasks such as automating build processes. It provides low-level operating system interactions for essential operations like determining file system roots, user names, and login shells."}, {"name": "ppft", "tags": ["math"], "summary": "distributed and parallel Python", "text": "-------------------------------------\nppft: distributed and parallel Python\n-------------------------------------\n\nAbout Ppft\n==========\n\n``ppft`` is a friendly fork of Parallel Python (``pp``). ``ppft`` extends Parallel Python to provide packaging and distribution with ``pip`` and ``setuptools``, support for Python 3, and enhanced serialization using ``dill.source``. ``ppft`` uses Parallel Python to provide mechanisms for the parallel execution of Python code on SMP (systems with multiple processors or cores) and clusters (computers connected via network).\n\nSoftware written in Python finds applications in a broad range of the categories including business logic, data analysis, and scientific calculations. This together with wide availability of SMP computers (multi-processor or multi-core) and clusters (computers connected via network) on the market create the demand in parallel execution of Python code.\n\nThe most common way to write parallel applications for SMP computers is to use threads. However, the Python interpreter uses the GIL (Global Interpreter Lock) for internal bookkeeping, where the GIL only allows one Python byte-code instruction to execute at a time, even on an SMP computer. Parallel Python overcomes this limitation, and provides a simple way to write parallel Python applications. Internally, processes and IPC (Inter Process Communications) are used to organize parallel computations. Parallel Python is written so that the details and complexity of IPC are handled internally, and the calling application just submits jobs and retrieves the results. Software written with Parallel Python works in parallel on many computers connected via a local network or the Internet. Cross-platform portability and dynamic load-balancing allows Parallel Python to parallelize computations efficiently even on heterogeneous and multi-platform clusters. Visit http://www.parallelpython.com for further information on Parallel Python.\n\n``ppft`` is part of ``pathos``, a Python framework for heterogeneous computing.\n``ppft`` is in active development, so any user feedback, bug reports, comments,\nor suggestions are highly appreciated.  A list of issues is located at https://github.com/uqfoundation/ppft/issues, with a legacy list maintained at https://uqfoundation.github.io/project/pathos/query.\n\nMajor Features\n==============\n\n``ppft`` provides:\n\nCurrent Release\n===============\n\nThe latest released version of ``ppft`` is available from:\n\n``ppft`` is distributed under a 3-clause BSD license, and is a fork of ``pp-1.6.6``.\n\nDevelopment Version\n===================\n\nYou can get the latest development version with all the shiny new features at:\n\nIf you have a new contribution, please submit a pull request.\n\nInstallation\n============\n\n``ppft`` can be installed with ``pip``::\n\nTo include enhanced serialization, using ``dill.source``, install::\n\nIf Parallel Python is already installed, it should be uninstalled before ``ppft`` is installed -- otherwise, ``import pp`` may point to the original and not to the ``ppft`` fork.\n\nRequirements\n============\n\n``ppft`` requires:\n\nOptional requirements:\n\nBasic Usage\n===========\n\n``ppft`` is a fork of the Parallel Python package (``pp``) that has been\nconverted from Python 2 to Python 3, made PEP 517 compliant, and augmented\nwith ``dill.source``.  For simple parallel execution, first create a job\n``Server`` where the number nodes available is autodetected::"}, {"name": "ppft", "tags": ["math"], "summary": "distributed and parallel Python", "text": "The number of nodes can be specified by passing an int as the first argument\nwhen creating the server (i.e. ``Server(4)`` creates a server with four nodes).\nThe server uses ``submit`` to execute jobs in parallel. ``submit`` takes a\nfunction, a tuple of the arguments to pass to the function, a tuple of any\nfunctions used but not imported in the function, and a tuple of any modules\nrequired to produce the function::\n\nThe functions are serialized by ``dill.source`` (as opposed to ``dill``), by\nextracting and passing the source code to the server. The server compiles\nand executes the source code, and then calls the function with the arguments\npassed in the tuple. Any function and module dependencies are imported\nbefore ``exec`` is called on the source code. Results are retrieved by\ncalling the object returned from ``submit``::\n\nJob server execution statistics can be printed with::\n\n``ppft`` also can execute jobs on remote computational nodes, if a ``ppserver``\nis first started on the node. Here the ``ppserver`` is started on 127.0.0.1,\nand will listen on port 35000::\n\nThen, locally, instantiate a ``Server`` with the connection information\nfor the remote node, submit some jobs, and retrieve the results::\n\nHowever, the stats show that all of the jobs were run locally::\n\nThis is due because we don't specify the number of nodes. The number of nodes\nare specified both in the ``ppserver`` and in the local job ``Server``. Thus,\nthe above is actually \"autobalance\" between 4 local nodes and 4 remote nodes.\nThe former is naturally going to be preferred; however, if the local server is\nflooded with jobs, some will get sent to the remote ``ppserver``, and that will\nbe reflected in the stats.  To run all jobs remotely, set the number of local\nnodes to zero::\n\n>>>\n\nGet help on the command line options for ``ppserver``::\n\nMore Information\n================\n\nProbably the best way to get started is to look at the documentation at\n``ppft.tests``. You can run the test suite with ``python -m ppft.tests``.\n``ppft`` will create and execute jobs on local workers (automatically created\nusing ``python -u -m ppft``). Additionally, remote servers can be created with \n``ppserver`` (or ``python -m ppft.server``), and then jobs can be distributed\nto remote workers. See ``--help`` for more details on how to configure a server.\nPlease feel free to submit a ticket on github, or ask a question on\nstackoverflow (**@Mike McKerns**).  If you would like to share how you use\n``ppft`` in your work, please send an email (to **mmckerns at uqfoundation dot org**).\n\nCitation\n========\n\nIf you use ``ppft`` to do research that leads to publication, we ask that you\nacknowledge use of ``ppft`` by citing the following in your publication::\n\nPlease see https://uqfoundation.github.io/project/pathos or"}, {"name": "ppft", "tags": ["math"], "summary": "distributed and parallel Python", "text": "This library is used to enable the parallel execution of Python code on both systems with multiple processors or cores and clusters of connected computers. With ppft, developers can distribute and execute complex tasks in parallel across multiple machines, significantly improving processing efficiency and speed."}, {"name": "pubchempy", "tags": ["data", "math", "web"], "summary": "A simple Python wrapper around the PubChem PUG REST API.", "text": "PubChemPy\n\n(https://pypi.python.org/pypi/PubChemPy)\n(https://anaconda.org/conda-forge/pubchempy)\n(https://github.com/mcs07/PubChemPy/blob/main/LICENSE)\n\n(https://github.com/mcs07/PubChemPy/actions/workflows/test.yml)\n(https://docs.pubchempy.org)\n\nPubChemPy provides a way to interact with PubChem in Python. It allows chemical searches by name, substructure and similarity, chemical standardization, conversion between chemical file formats, depiction and retrieval of chemical properties.\n\nInstallation\n\nInstall PubChemPy with pip:\n\nOr with conda:\n\nFor detailed instructions, see the [installation guide](https://docs.pubchempy.org/en/latest/guide/install.html).\n\nExample usage\n\nRetrieve a compound by its PubChem Compound Identifier (CID) and print its SMILES and IUPAC name:\n\nSearch compounds by name and print the SMILES and molecular weight of the first result:\n\nDocumentation\n\nFull documentation is available at .\n\nThis includes a [step-by-step guide on how to use PubChemPy](https://docs.pubchempy.org/en/latest/guide/gettingstarted.html), as well as a [complete API reference](https://docs.pubchempy.org/en/latest/api.html).\n\nContributing\n\n- Feature ideas and bug reports are welcome on the [Issue Tracker](https://github.com/mcs07/PubChemPy/issues).\n- Fork the [source code](https://github.com/mcs07/PubChemPy) on GitHub, make changes and file a pull request.\n\nLicense\n\nPubChemPy is licensed under the [MIT license](https://github.com/mcs07/PubChemPy/blob/main/LICENSE)."}, {"name": "pubchempy", "tags": ["data", "math", "web"], "summary": "A simple Python wrapper around the PubChem PUG REST API.", "text": "This library is used to interact with the PubChem database through Python, enabling chemical searches, standardization, file format conversions, and property retrievals. With PubChemPy, developers can programmatically retrieve and manipulate chemical information, facilitating tasks such as data mining and analysis in cheminformatics applications."}, {"name": "pulp", "tags": ["math", "ui"], "summary": "PuLP is an LP modeler written in python. PuLP can generate MPS or LP files and call GLPK, COIN CLP/CBC, CPLEX, and GUROBI to solve linear problems.", "text": "pulp\n**************************\n\n.. image:: https://travis-ci.org/coin-or/pulp.svg?branch=master\n.. image:: https://img.shields.io/pypi/v/pulp\n.. image:: https://img.shields.io/pypi/dm/pulp\n\nPuLP is an linear and mixed integer programming modeler written in Python. With PuLP, it is simple to create MILP optimisation problems and solve them with the latest open-source (or proprietary) solvers.  PuLP can generate MPS or LP files and call solvers such as GLPK_, COIN-OR CLP/`CBC`_, CPLEX_, GUROBI_, MOSEK_, XPRESS_, CHOCO_, MIPCL_, HiGHS_, SCIP_/FSCIP_.\n\nThe documentation for PuLP can be `found here `_.\n\nPuLP is part of the `COIN-OR project `_. \n\nInstallation\n================\n\nPuLP requires Python 3.9 or newer.\n\nThe easiest way to install PuLP is with ``pip``. If ``pip`` is available on your system, type::\n\nOtherwise follow the download instructions on the `PyPi page `_.\n\nInstalling solvers\n----------------------\n\nPuLP can use a variety of solvers. The default solver is the COIN-OR CBC solver, which is included with PuLP. If you want to use other solvers, PuLP offers a quick way to install most solvers via their pypi package (some require a commercial license for running or for running large models)::\n\nIf you want to install all open source solvers (scip, highs, cylp), you can use the shortcut::\n\nFor more information on how to install solvers, see the `guide on configuring solvers `_.\n\nQuickstart \n===============\n\nUse ``LpVariable`` to create new variables. To create a variable x with 0  \u2264  x  \u2264  3::\n\nTo create a binary variable, y, with values either 0 or 1::\n\nUse ``LpProblem`` to create new problems. Create a problem called \"myProblem\" like so::\n\nCombine variables in order to create expressions and constraints, and then add them to the problem.::\n\nTo solve the problem  with the default included solver::\n\nIf you want to try another solver to solve the problem::\n\nDisplay the status of the solution::\n\nYou can get the value of the variables using ``value``. ex::\n\nEssential Classes\n------------------\n\n* ``LpProblem`` -- Container class for a Linear or Integer programming problem\n* ``LpVariable`` -- Variables that are added into constraints in the LP problem\n* ``LpConstraint`` -- Constraints of the general form\n\n* ``LpConstraintVar`` -- A special type of constraint for constructing column of the model in column-wise modelling\n\nUseful Functions\n------------------\n\n* ``value()`` -- Finds the value of a variable or expression\n* ``lpSum()`` -- Given a list of the form [a1*x1, a2*x2, ..., an*xn] will construct a linear expression to be used as a constraint or variable\n* ``lpDot()`` -- Given two lists of the form [a1, a2, ..., an] and [x1, x2, ..., xn] will construct a linear expression to be used as a constraint or variable\n\nMore Examples\n================\n\nSeveral tutorial are given in `documentation `_ and pure code examples are available in `examples/ directory `_ .\n\nThe examples use the default solver (CBC). To use other solvers they must be available (installed and accessible). For more information on how to do that, see the `guide on configuring solvers `_.\n\nFor Developers \n================\n\nIf you want to install the latest version from GitHub you can run::\n\nOn Linux and MacOS systems, you must run the tests to make the default solver executable::\n\nBuilding the documentation\n--------------------------\n\nThe PuLP documentation is built with `Sphinx `_.  We recommended using a\n`virtual environment `_ to build the documentation locally.\n\nTo build, run the following in a terminal window, in the PuLP root directory\n\n::\n\nA folder named html will be created inside the ``build/`` directory.\nThe home page for the documentation is ``doc/build/html/index.html`` which can be opened in a browser.\n\nContributing to PuLP\n-----------------------\nInstructions for making your first contribution to PuLP are given `here `_.\n\n**Comments, bug reports, patches and suggestions are very welcome!**\n\n* Comments and suggestions: https://github.com/coin-or/pulp/discussions\n* Bug reports: https://github.com/coin-or/pulp/issues\n* Patches: https://github.com/coin-or/pulp/pulls\n\nCopyright and License \n=======================\nPuLP is distributed under an MIT license. \n\n.. _Python: http://www.python.org/\n\n.. _GLPK: http://www.gnu.org/software/glpk/glpk.html\n.. _CBC: https://github.com/coin-or/Cbc\n.. _CPLEX: http://www.cplex.com/\n.. _GUROBI: http://www.gurobi.com/\n.. _MOSEK: https://www.mosek.com/\n.. _XPRESS: https://www.fico.com/es/products/fico-xpress-solver\n.. _CHOCO: https://choco-solver.org/\n.. _MIPCL: http://mipcl-cpp.appspot.com/\n.. _SCIP: https://www.scipopt.org/\n.. _HiGHS: https://highs.dev\n.. _FSCIP: https://ug.zib.de"}, {"name": "pulp", "tags": ["math", "ui"], "summary": "PuLP is an LP modeler written in python. PuLP can generate MPS or LP files and call GLPK, COIN CLP/CBC, CPLEX, and GUROBI to solve linear problems.", "text": "This library is used to model and solve linear and mixed-integer programming problems in Python, allowing developers to optimize complex systems with various solvers. With PuLP, developers can easily generate MPS or LP files and leverage the capabilities of top-tier open-source and proprietary solvers."}, {"name": "pvlib", "tags": ["math"], "summary": "A set of functions and classes for simulating the performance of photovoltaic energy systems.", "text": "pvlib python is a community developed toolbox that provides a set of\nfunctions and classes for simulating the performance of photovoltaic\nenergy systems and accomplishing related tasks.  The core mission of pvlib\npython is to provide open, reliable, interoperable, and benchmark\nimplementations of PV system models.\n\nWe need your help to make pvlib-python a great tool!\n\nDocumentation: http://pvlib-python.readthedocs.io\n\nSource code: https://github.com/pvlib/pvlib-python"}, {"name": "pvlib", "tags": ["math"], "summary": "A set of functions and classes for simulating the performance of photovoltaic energy systems.", "text": "This library is used to simulate the performance of photovoltaic energy systems and accomplish related tasks. It provides open, reliable, and interoperable implementations of PV system models, enabling developers to accurately model and optimize solar panel performance."}, {"name": "pyclipper", "tags": ["math"], "summary": "Cython wrapper for the C++ translation of the Angus Johnson's Clipper library (ver. 6.4.2)", "text": "About\n=====\n\nPyclipper is a Cython wrapper exposing public functions and classes of\nthe C++ translation of the `Angus Johnson's Clipper library (ver.\n6.4.2) `__.\n\nSource code is available on\n`GitHub `__. The package is published on \n`PyPI `__.\n\nAbout Clipper\n-------------\n\nInstall\n=======\n\nFrom PyPI\n---------\n\n::\n\nFrom source\n-----------\n\nClone the repository:\n\n::\n\nInstall:\n\n::\n\nFor development, use an editable install:\n\n::\n\nClippers' preprocessor directives\n---------------------------------\nClipper can be compiled with the following preprocessor directives: ``use_int32``, ``use_xyz``, ``use_lines`` and ``use_deprecated``. \nAmong these the ``use_int32`` and ``use_lines`` can be used with Pyclipper.\n\n-  ``use_int32`` - when enabled 32bit ints are used instead of 64bit ints. This improve performance but coordinate values are limited to the range +/- 46340. In Pyclipper this directive is **disabled** by default.\n\n-  ``use_lines`` - enables line clipping. Adds a very minor cost to performance. In Pyclipper this directive is **enabled** by default (since version 0.9.2b0).\n\nIn case you would want to change these settings, clone this repository and change the ``define_macros`` collection (``setup.py``, pyclipper extension definition). Add a set like ``('use_int32', 1)`` to enable the directive, or remove the set to disable it. After that you need to rebuild the package.\n\nHow to use\n==========\n\nThis wrapper library tries to follow naming conventions of the original\nlibrary.\n\n-  ``ClipperLib`` namespace is represented by the ``pyclipper`` module,\n-  classes ``Clipper`` and ``ClipperOffset`` -> \n   ``Pyclipper`` and ``PyclipperOffset``,\n-  when Clipper is overloading functions with different number of\n   parameters or different types (eg. ``Clipper.Execute``, one function\n   fills a list of paths the other PolyTree) that becomes\n   ``Pyclipper.Execute`` and ``Pyclipper.Execute2``.\n\nBasic clipping example (based on `Angus Johnson's Clipper\nlibrary `__):\n\n.. code:: python\n\nBasic offset example:\n\n.. code:: python\n\nThe Clipper library uses integers instead of floating point values to\npreserve numerical robustness. If you need to scale coordinates of your polygons, this library provides helper functions ``scale_to_clipper()`` and ``scale_from_clipper()`` to achieve that. \n\nMigrating from Pyclipper ``0.9.3b0``\n------------------------------------\n\nIn previous version of Pyclipper (``0.9.3b0``) polygons could be automatically scaled using the ``SCALING_FACTOR`` variable. This was removed in version ``1.0.0`` due to inexact conversions related to floating point operations. This way the library now provides the original numerical robustness of the base library.\n\nThe ``SCALING_FACTOR`` removal **breaks backward compatibility**. \nFor an explanation and help with migration, see https://github.com/fonttools/pyclipper/wiki/Deprecating-SCALING_FACTOR.\n\nAuthors\n=======\n\n-  The Clipper library is written by `Angus\n   Johnson `__,\n-  This wrapper was initially written by `Maxime\n   Chalton `__,\n-  Adaptions to make it work with version 5 written by `Lukas\n   Treyer `__,\n-  Adaptions to make it work with version 6.2.1 and PyPI package written by `Gregor Ratajc `__,\n-  ``SCALING_FACTOR`` removal and additions to documentation by Michael Schwarz (@Feuermurmel),\n-  Bug fix `sympy.Zero` is not a collection by Jamie Bull (@jamiebull1),\n-  Travis CI and Appveyor CI integration for continuous builds of wheel packages by Cosimo Lupo (@anthrotype).\n\nThe package is maintained by Cosimo Lupo (`@anthrotype `__).\n\nLicense\n=======\n\n-  Pyclipper is available under `MIT\n   license `__.\n-  The core Clipper library is available under `Boost Software\n   License `__. Freeware for both\n   open source and commercial applications.\n\nChangelog\n=========\n\nFor recent versions, see the `GitHub Releases page `__.\n\n1.1.0\n-------\n\n- Updated embedded Clipper library to version 6.4.2.\n\n1.0.6\n-------\n-  Added support for Python 3.6.\n\n1.0.3\n-------\n-  added Travis CI and Appveyor CI to build wheel packages (thanks to @anthrotype)\n\n1.0.2\n-------\n-  bug fix: `sympy.Zero` recognized as a collection (thanks to @jamiebull1)\n\n1.0.0\n-------\n- **(breaks backwards compatibility)** removes SCALING_FACTOR (thanks to @Feuermurmel)\n\n0.9.3b0\n-------\n-  Applied SCALING_FACTOR to the relevant function parameters and class properties\n-  Refactored tests\n\n0.9.2b1\n-------\n-  bug fix: Fix setting of the PyPolyNode.IsHole property\n\n0.9.2b0\n-------\n-  enable preprocessor directive ``use_lines`` by default,\n-  bug fix: PyPolyNode.Contour that is now one path and not a list of paths as it was previously."}, {"name": "pyclipper", "tags": ["math"], "summary": "Cython wrapper for the C++ translation of the Angus Johnson's Clipper library (ver. 6.4.2)", "text": "This library is used to perform polygon clipping operations, allowing developers to efficiently clip complex polygons against each other or simple shapes. With pyclipper, developers can create and manipulate geometric objects in their applications with precision and accuracy."}, {"name": "pydantic-ai", "tags": ["math", "ml", "web"], "summary": "Agent Framework / shim to use Pydantic with LLMs", "text": "Pydantic AI is a Python agent framework designed to help you quickly, confidently, and painlessly build production grade applications and workflows with Generative AI.\n\nFastAPI revolutionized web development by offering an innovative and ergonomic design, built on the foundation of [Pydantic Validation](https://docs.pydantic.dev) and modern Python features like type hints.\n\nYet despite virtually every Python agent framework and LLM library using Pydantic Validation, when we began to use LLMs in [Pydantic Logfire](https://pydantic.dev/logfire), we couldn't find anything that gave us the same feeling.\n\nWe built Pydantic AI with one simple aim: to bring that FastAPI feeling to GenAI app and agent development.\n\nWhy use Pydantic AI\n\n1. **Built by the Pydantic Team**:\n[Pydantic Validation](https://docs.pydantic.dev/latest/) is the validation layer of the OpenAI SDK, the Google ADK, the Anthropic SDK, LangChain, LlamaIndex, AutoGPT, Transformers, CrewAI, Instructor and many more. _Why use the derivative when you can go straight to the source?_ :smiley:\n\n2. **Model-agnostic**:\nSupports virtually every [model](https://ai.pydantic.dev/models/overview) and provider: OpenAI, Anthropic, Gemini, DeepSeek, Grok, Cohere, Mistral, and Perplexity; Azure AI Foundry, Amazon Bedrock, Google Vertex AI, Ollama, LiteLLM, Groq, OpenRouter, Together AI, Fireworks AI, Cerebras, Hugging Face, GitHub, Heroku, Vercel, Nebius, OVHcloud, Alibaba Cloud, and Outlines. If your favorite model or provider is not listed, you can easily implement a [custom model](https://ai.pydantic.dev/models/overview#custom-models).\n\n3. **Seamless Observability**:\nTightly [integrates](https://ai.pydantic.dev/logfire) with [Pydantic Logfire](https://pydantic.dev/logfire), our general-purpose OpenTelemetry observability platform, for real-time debugging, evals-based performance monitoring, and behavior, tracing, and cost tracking. If you already have an observability platform that supports OTel, you can [use that too](https://ai.pydantic.dev/logfire#alternative-observability-backends).\n\n4. **Fully Type-safe**:\nDesigned to give your IDE or AI coding agent as much context as possible for auto-completion and [type checking](https://ai.pydantic.dev/agents#static-type-checking), moving entire classes of errors from runtime to write-time for a bit of that Rust \"if it compiles, it works\" feel.\n\n5. **Powerful Evals**:\nEnables you to systematically test and [evaluate](https://ai.pydantic.dev/evals) the performance and accuracy of the agentic systems you build, and monitor the performance over time in Pydantic Logfire.\n\n6. **MCP, A2A, and UI**:\nIntegrates the [Model Context Protocol](https://ai.pydantic.dev/mcp/overview), [Agent2Agent](https://ai.pydantic.dev/a2a), and various [UI event stream](https://ai.pydantic.dev/ui/overview) standards to give your agent access to external tools and data, let it interoperate with other agents, and build interactive applications with streaming event-based communication.\n\n7. **Human-in-the-Loop Tool Approval**:\nEasily lets you flag that certain tool calls [require approval](https://ai.pydantic.dev/deferred-tools#human-in-the-loop-tool-approval) before they can proceed, possibly depending on tool call arguments, conversation history, or user preferences.\n\n8. **Durable Execution**:\nEnables you to build [durable agents](https://ai.pydantic.dev/durable_execution/overview/) that can preserve their progress across transient API failures and application errors or restarts, and handle long-running, asynchronous, and human-in-the-loop workflows with production-grade reliability.\n\n9. **Streamed Outputs**:\nProvides the ability to [stream](https://ai.pydantic.dev/output#streamed-results) structured output continuously, with immediate validation, ensuring real time access to generated data.\n\n10. **Graph Support**:\nProvides a powerful way to define [graphs](https://ai.pydantic.dev/graph) using type hints, for use in complex applications where standard control flow can degrade to spaghetti code.\n\nRealistically though, no list is going to be as convincing as [giving it a try](#next-steps) and seeing how it makes you feel!\n\nHello World Example\n\nHere's a minimal example of Pydantic AI:\n\n_(This example is complete, it can be run \"as is\", assuming you've [installed the `pydantic_ai` package](https://ai.pydantic.dev/install))_\n\nThe exchange will be very short: Pydantic AI will send the instructions and the user prompt to the LLM, and the model will return a text response.\n\nNot very interesting yet, but we can easily add [tools](https://ai.pydantic.dev/tools), [dynamic instructions](https://ai.pydantic.dev/agents#instructions), and [structured outputs](https://ai.pydantic.dev/output) to build more powerful agents.\n\nTools & Dependency Injection Example\n\nHere is a concise example using Pydantic AI to build a support agent for a bank:\n\n**(Better documented example [in the docs](https://ai.pydantic.dev/#tools-dependency-injection-example))**\n\nNext Steps\n\nTo try Pydantic AI for yourself, [install it](https://ai.pydantic.dev/install) and follow the instructions [in the examples](https://ai.pydantic.dev/examples/setup).\n\nRead the [docs](https://ai.pydantic.dev/agents/) to learn more about building applications with Pydantic AI.\n\nRead the [API Reference](https://ai.pydantic.dev/api/agent/) to understand Pydantic AI's interface.\n\nJoin [Slack](https://logfire.pydantic.dev/docs/join-slack/) or file an issue on [GitHub](https://github.com/pydantic/pydantic-ai/issues) if you have any questions."}, {"name": "pydantic-ai", "tags": ["math", "ml", "web"], "summary": "Agent Framework / shim to use Pydantic with LLMs", "text": "This library is used to help developers quickly build production-grade applications and workflows with Generative AI, leveraging the power of Pydantic validation. With Pydantic AI, developers can confidently create GenAI apps and agents with an ergonomic design reminiscent of FastAPI's innovative approach."}, {"name": "pydicom", "tags": ["data", "math", "web"], "summary": "A pure Python package for reading and writing DICOM data", "text": "*pydicom*\n\n*pydicom* is a pure Python package for working with [DICOM](https://www.dicomstandard.org/) files.\nIt lets you read, modify and write DICOM data in an easy \"pythonic\" way. As a pure Python package,\n*pydicom* can run anywhere Python runs without any other requirements, although if you're working\nwith *Pixel Data* then we recommend you also install [NumPy](https://numpy.org).\n\nNote that *pydicom* is a general-purpose DICOM framework concerned with\nreading and writing DICOM datasets. In order to keep the\nproject manageable, it does not handle the specifics of individual SOP classes\nor other aspects of DICOM. Other libraries both inside and outside the\n[pydicom organization](https://github.com/pydicom) are based on *pydicom*\nand provide support for other aspects of DICOM, and for more\nspecific applications.\n\nExamples are [pynetdicom](https://github.com/pydicom/pynetdicom), which\nis a Python library for DICOM networking, and [deid](https://github.com/pydicom/deid),\nwhich supports the anonymization of DICOM files.\n\nInstallation\n\nUsing [pip](https://pip.pypa.io/en/stable/):\n\nUsing [conda](https://docs.conda.io/en/latest/):\n\nFor more information, including installation instructions for the development version, see the [installation guide](https://pydicom.github.io/pydicom/stable/tutorials/installation.html).\n\nDocumentation\n\nThe *pydicom* [user guide](https://pydicom.github.io/pydicom/stable/guides/user/index.html), [tutorials](https://pydicom.github.io/pydicom/stable/tutorials/index.html), [examples](https://pydicom.github.io/pydicom/stable/auto_examples/index.html) and [API reference](https://pydicom.github.io/pydicom/stable/reference/index.html) documentation is available for both the [current release](https://pydicom.github.io/pydicom/stable) and the [development version](https://pydicom.github.io/pydicom/dev) on GitHub Pages.\n\n*Pixel Data*\n\nCompressed and uncompressed *Pixel Data* is always available to\nbe read, changed and written as [bytes](https://docs.python.org/3/library/stdtypes.html#bytes-objects):\n\nIf [NumPy](https://www.numpy.org) is installed, *Pixel Data* can be converted to an [ndarray](https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html) using the [Dataset.pixel_array](https://pydicom.github.io/pydicom/stable/reference/generated/pydicom.dataset.Dataset.html#pydicom.dataset.Dataset.pixel_array) property:\n\nDecompressing *Pixel Data*\n\nJPEG, JPEG-LS and JPEG 2000\nConverting JPEG, JPEG-LS or JPEG 2000 compressed *Pixel Data* to an ``ndarray`` requires installing one or more additional Python libraries. For information on which libraries are required, see the [pixel data handler documentation](https://pydicom.github.io/pydicom/stable/guides/user/image_data_handlers.html#guide-compressed).\n\nRLE\nDecompressing RLE *Pixel Data* only requires NumPy, however it can be quite slow. You may want to consider [installing one or more additional Python libraries](https://pydicom.github.io/pydicom/stable/guides/user/image_data_compression.html) to speed up the process.\n\nCompressing *Pixel Data*\nInformation on compressing *Pixel Data* using one of the below formats can be found in the corresponding [encoding guides](https://pydicom.github.io/pydicom/stable/guides/encoding/index.html). These guides cover the specific requirements for each encoding method and we recommend you be familiar with them when performing image compression.\n\nJPEG-LS, JPEG 2000\nCompressing image data from an ``ndarray`` or ``bytes`` object to JPEG-LS or JPEG 2000 requires installing the following:\n\n* JPEG-LS requires [pyjpegls](https://github.com/pydicom/pyjpegls)\n* JPEG 2000 requires [pylibjpeg](https://github.com/pydicom/pylibjpeg) and the [pylibjpeg-openjpeg](https://github.com/pydicom/pylibjpeg-openjpeg) plugin\n\nRLE\nCompressing using RLE requires no additional packages but can be quite slow. It can be sped up by installing [pylibjpeg](https://github.com/pydicom/pylibjpeg) with the [pylibjpeg-rle](https://github.com/pydicom/pylibjpeg-rle) plugin, or [gdcm](https://github.com/tfmoraes/python-gdcm).\n\nExamples\nMore [examples](https://pydicom.github.io/pydicom/stable/auto_examples/index.html) are available in the documentation.\n\n**Change a patient's ID**\n\n**Display the Pixel Data**\n\nWith [NumPy](https://numpy.org) and [matplotlib](https://matplotlib.org/)\n\nContributing\n\nWe are all volunteers working on *pydicom* in our free time. As our\nresources are limited, we very much value your contributions, be it bug fixes, new\ncore features, or documentation improvements. For more information, please\nread our [contribution guide](https://github.com/pydicom/pydicom/blob/main/CONTRIBUTING.md)."}, {"name": "pydicom", "tags": ["data", "math", "web"], "summary": "A pure Python package for reading and writing DICOM data", "text": "This library is used to read, modify, and write DICOM data in a straightforward Pythonic manner, allowing developers to easily work with DICOM files without requiring additional dependencies. It provides a general-purpose framework for handling DICOM datasets, enabling developers to focus on their specific use case requirements."}, {"name": "pydot", "tags": ["math", "visualization", "web"], "summary": "Python interface to Graphviz's Dot", "text": "Pydot\n\n`pydot` is a Python interface to Graphviz and its DOT language. You can use `pydot` to create, read, edit, and visualize graphs.\n\n- It's made in pure Python, with only one dependency \u2013 pyparsing \u2013 other than Graphviz itself.\n- It's compatible with `networkx`, which can convert its graphs to `pydot`.\n\nTo see what Graphviz is capable of, check the [Graphviz Gallery](https://graphviz.org/gallery/)!\n\nDependencies\n\n- Python: The latest version of pydot supports Python 3.9+. It may work with Python 3.6-3.8, but we can't guarantee full support. If you're using one of these older versions, feel free to experiment, but we won't be able to address issues specific to them. Older versions of pydot are also an option.\n- [`pyparsing`][pyparsing]: used only for *loading* DOT files, installed automatically during `pydot` installation.\n- GraphViz: used to render graphs in a variety of formats, including PNG, SVG, PDF, and more.\n  Should be installed separately, using your system's [package manager][pkg], something similar (e.g., [MacPorts][mac]), or from [its source][src].\n\nInstallation\n\n- Latest release, from [PyPI][pypi]:\n\n  \n\n- Current development code, from this repository:\n\n  \n\n- Development installation, to modify the code or contribute changes:\n\nQuickstart\n\n1. Input\n\nNo matter what you want to do with `pydot`, you'll need some input to start with. Here are the common ways to get some data to work with.\n\nImport a graph from an existing DOT file\n\nLet's say you already have a file `example.dot` (based on an [example from Wikipedia][wiki_example]):\n\nYou can read the graph from the file in this way:\n\nParse a graph from an existing DOT string\n\nUse this method if you already have a string describing a DOT graph:\n\nCreate a graph from scratch using pydot objects\n\nThis is where the cool stuff starts. Use this method if you want to build new graphs with Python code.\n\nYou can use these basic building blocks in your Python program\nto dynamically generate a graph. For example, start with a\nbasic `pydot.Dot` graph object, then loop through your data\nas you add nodes and edges. Use values from your data as labels to\ndetermine shapes, edges and so on. This allows you to easily create\nvisualizations of thousands of related objects.\n\nConvert a NetworkX graph to a pydot graph\n\nNetworkX has conversion methods for pydot graphs:\n\n2. Edit\n\nYou can now further manipulate your graph using pydot methods:\n\nAdd more nodes and edges:\n\nEdit attributes of graphs, nodes and edges:\n\n3. Output\n\nHere are three different output options:\n\nGenerate an image\n\nIf you just want to save the image to a file, use one of the `write_*` methods:\n\nIf you need to further process the image output, the `create_*` methods will get you a Python `bytes` object:\n\nRetrieve the DOT string\n\nThere are two different DOT strings you can retrieve:\n\n- The \"raw\" pydot DOT: This is generated the fastest and will\n  usually still look quite similar to the DOT you put in. It is\n  generated by pydot itself, without calling Graphviz.\n\n  \n\n- The Graphviz DOT: You can use it to check how Graphviz lays out\n  the graph before it produces an image. It is generated by\n  Graphviz.\n\nConvert to a NetworkX graph\n\nNetworkX has a conversion method for pydot graphs:\n\nMore help\n\nFor more help, see the docstrings of the various pydot objects and\nmethods. For example, `help(pydot)`, `help(pydot.Graph)` and\n`help(pydot.Dot.write)`.\n\nMore [documentation contributions welcome][contrib].\n\nTroubleshooting\n\nEnable logging\n\n`pydot` uses Python's standard `logging` module. To see the logs,\nassuming logging has not been configured already:\n\n**Warning**: When `DEBUG` level logging is enabled, `pydot` may log the\ndata that it processes, such as graph contents or DOT strings. This can\ncause the log to become very large or contain sensitive information.\n\nAdvanced logging configuration\n\n- Check out the [Python logging documentation][log] and the\n  [`logging_tree`][log_tree] visualizer.\n- `pydot` does not add any handlers to its loggers, nor does it setup\n  or modify your root logger. The `pydot` loggers are created with the\n  default level `NOTSET`.\n- `pydot` registers the following loggers:\n  - `pydot`: Parent logger. Emits a few messages during startup.\n  - `pydot.core`: Messages related to pydot objects, Graphviz execution\n  - `pydot.dot_parser`: Messages related to the parsing of DOT strings.\n\nLicense\n\nDistributed under the [MIT license][MIT].\n\nThe module [`pydot._vendor.tempfile`][tempfile]\nis based on the Python 3.12 standard library module\n[`tempfile.py`][tempfile-src],\nCopyright \u00a9 2001-2023 Python Software Foundation. All rights reserved.\nLicensed under the terms of the [Python-2.0][Python-2.0] license.\n\nContacts\n\nCurrent maintainer(s): \n- \u0141ukasz \u0141api\u0144ski \n\nPast maintainers:\n- Sebastian Kalinowski  (GitHub: @prmtl)\n- Peter Nowee  (GitHub: @peternowee)\n\nOriginal author: Ero Carrera"}, {"name": "pydot", "tags": ["math", "visualization", "web"], "summary": "Python interface to Graphviz's Dot", "text": "This library is used to create, read, edit, and visualize graphs with Python, leveraging the capabilities of Graphviz's DOT language. It provides a pure Python interface that supports compatibility with other libraries like `networkx` and comes with only one dependency - `pyparsing`."}, {"name": "pyerfa", "tags": ["dev", "math"], "summary": "Python bindings for ERFA", "text": "======\nPyERFA\n======\n\nPyPI Status\n\nPyERFA is the Python_ wrapper for the ERFA_ library (Essential Routines for\nFundamental Astronomy), a C library containing key algorithms for astronomy,\nwhich is based on the SOFA library published by the International Astronomical\nUnion (IAU).  All C routines are wrapped as Numpy_ `universal functions\n`_, so that they can be\ncalled with scalar or array inputs.\n\nThe project is a split of ``astropy._erfa`` module, developed in the\ncontext of Astropy_ project, into a standalone package.  It contains\nthe ERFA_ C source code as a git submodule.  The wrapping is done\nwith help of the Jinja2_ template engine.\n\nIf you use this package in your research, please cita it via DOI\n`10.5281/zenodo.3940699 `_.\n\n.. Installation\n\nInstallation instructions\n-------------------------\n\nThe package can be installed from the package directory using a simple::\n\n  $ pip install .\n\nand similarly a wheel_ can be created with::\n\n  $ pip wheel .\n\n.. note:: If you already have the C library ``liberfa`` on your\n  system, you can use that by setting environment variable\n  ``PYERFA_USE_SYSTEM_LIBERFA=1``.\n\n.. _wheel: https://github.com/pypa/wheel\n\nThe package can be obtained from PyPI_ or directly from the git repository::\n\n  $ git clone --recursive https://github.com/liberfa/pyerfa/\n\nThe package also has nightly wheel that can be obtained as follows::\n\n  $ pip install --upgrade --index-url https://pypi.anaconda.org/liberfa/simple pyerfa --pre\n\nTesting\n-------\n\nFor testing, one can install the packages together with its testing\ndependencies and then test it with::\n\n  $ pip install .[test]\n  $ pytest\n\nAlternatively, one can use ``tox``, which will set up a separate testing\nenvironment for you, with::\n\n  $ tox -e test\n\nUsage\n-----\n\nThe package can be imported as ``erfa`` which has all ERFA_ ufuncs wrapped with\npython code that tallies errors and warnings.  Also exposed are the constants\ndefined by ERFA_ in `erfam.h\n`_, as well\nas `numpy.dtype` corresponding to structures used by ERFA_.  Examples::\n\n  >>> import erfa\n  >>> erfa.jd2cal(2460000., [0, 1, 2, 3])\n  (array([2023, 2023, 2023, 2023], dtype=int32),\n   array([2, 2, 2, 2], dtype=int32),\n   array([24, 25, 26, 27], dtype=int32),\n   array([0.5, 0.5, 0.5, 0.5]))\n  >>> erfa.plan94(2460000., [0, 1, 2, 3], 1)\n  array([([ 0.09083713, -0.39041392, -0.21797389], [0.02192341, 0.00705449, 0.00149618]),\n  dtype([('p', '>> erfa.dt_eraLDBODY\n  dtype([('bm', '>> erfa.DAYSEC\n  86400.0\n\nIt is also possible to use the ufuncs directly, though then one has to\ndeal with the warning and error states explicitly.  For instance, compare::\n\n  >>> erfa.jd2cal(-600000., [0, 1, 2, 3])\n  Traceback (most recent call last):\n  ...\n  ErfaError: ERFA function \"jd2cal\" yielded 4 of \"unacceptable date (Note 1)\"\n  >>> erfa.ufunc.jd2cal(-600000., [0, 1, 2, 3])\n  (array([-1, -1, -1, -1], dtype=int32),\n   ...,\n   array([-1, -1, -1, -1], dtype=int32))\n\nLicense\n-------\n\nPyERFA is licensed under a 3-clause BSD style license - see the\n`LICENSE.rst `_ file.\n\n.. References\n.. _Python: https://www.python.org/\n.. _ERFA: https://github.com/liberfa/erfa\n.. _Numpy: https://numpy.org/\n.. _Astropy: https://www.astropy.org\n.. _PyPI: https://pypi.org/project/pyerfa/\n.. _Jinja2: https://palletsprojects.com/p/jinja/\n.. |PyPI Status| image:: https://img.shields.io/pypi/v/pyerfa.svg\n\n   :alt: DOI 10.5281/zenodo.3940699\n\n.. |Documentation Status| image:: https://img.shields.io/readthedocs/pyerfa/latest.svg?logo=read%20the%20docs&logoColor=white&label=Docs&version=stable"}, {"name": "pyerfa", "tags": ["dev", "math"], "summary": "Python bindings for ERFA", "text": "This library is used to provide Python bindings for ERFA (Essential Routines for Fundamental Astronomy), allowing developers to perform astronomy-related calculations and operations with ease. With pyerfa, developers can leverage the ERFA library's key algorithms and routines in their Python applications."}, {"name": "pyfaidx", "tags": ["math", "web"], "summary": "pyfaidx: efficient pythonic random access to fasta subsequences", "text": "CI\n\nDescription\n-----------\n\nSamtools provides a function \"faidx\" (FAsta InDeX), which creates a\nsmall flat index file \".fai\" allowing for fast random access to any\nsubsequence in the indexed FASTA file, while loading a minimal amount of the\nfile in to memory. This python module implements pure Python classes for\nindexing, retrieval, and in-place modification of FASTA files using a samtools\ncompatible index. The pyfaidx module is API compatible with the `pygr`_ seqdb module.\nA command-line script \"`faidx`_\" is installed alongside the pyfaidx module, and\nfacilitates complex manipulation of FASTA files without any programming knowledge.\n\n.. _`pygr`: https://github.com/cjlee112/pygr\n\nIf you use pyfaidx in your publication, please cite:\n\n`Shirley MD`_, `Ma Z`_, `Pedersen B`_, `Wheelan S`_. `Efficient \"pythonic\" access to FASTA files using pyfaidx `_. PeerJ PrePrints 3:e1196. 2015.\n\n.. _`Shirley MD`: http://github.com/mdshw5\n.. _`Ma Z`: http://github.com/azalea\n.. _`Pedersen B`: http://github.com/brentp\n.. _`Wheelan S`: http://github.com/swheelan\n\nInstallation\n------------\n\nThis package is tested under Linux and macOS using Python 3.7+, and and is available from the PyPI:\n\n::\n\nor download a `release `_ and:\n\n::\n\nIf using ``pip install --user`` make sure to add ``/home/$USER/.local/bin`` to your ``$PATH`` (on linux) or ``/Users/$USER/Library/Python/{python version}/bin`` (on macOS) if you want to run the ``faidx`` script.\n\nPython 2.6 and 2.7 users may choose to use a package version from `v0.7.2 `_ or earier.\n\nUsage\n-----\n\n.. code:: python\n\nActs like a dictionary.\n\n.. code:: python\n\nNote that start and end coordinates of Sequence objects are [1, 0]. This can be changed to [0, 0] by passing ``one_based_attributes=False`` to ``Fasta`` or ``Faidx``. This argument only affects the ``Sequence .start/.end`` attributes, and has no effect on slicing coordinates.\n\nIndexes like a list:\n\n.. code:: python\n\nSlices just like a string:\n\n.. code:: python\n\n- Slicing start and end coordinates are 0-based, just like Python sequences.\n\nComplements and reverse complements just like DNA\n\n.. code:: python\n\n``Fasta`` objects can also be accessed using method calls:\n\n.. code:: python\n\nSpliced sequences can be retrieved from a list of [start, end] coordinates:\n**TODO** update this section\n\n.. code:: python\n\n.. _keyfn:\n\nCustom key functions provide cleaner access:\n\n.. code:: python\n\nYou can specify a character to split names on, which will generate additional entries:\n\n.. code:: python\n\nIf your `key_function` or `split_char` generates duplicate entries, you can choose what action to take:\n\n.. code:: python\n\nFilter functions (returning True) limit the index:\n\n.. code:: python\n\nOr just get a Python string:\n\n.. code:: python\n\nYou can make sure that you always receive an uppercase sequence, even if your fasta file has lower case\n\n.. code:: python\n\nYou can also perform line-based iteration, receiving the sequence lines as they appear in the FASTA file:\n\n.. code:: python\n\nSequence names are truncated on any whitespace. This is a limitation of the indexing strategy. However, full names can be recovered:\n\n.. code:: python\n\nRecords can be accessed efficiently as numpy arrays:\n\n.. code:: python\n\nSequence can be buffered in memory using a read-ahead buffer\nfor fast sequential access:\n\n.. code:: python"}, {"name": "pyfaidx", "tags": ["math", "web"], "summary": "pyfaidx: efficient pythonic random access to fasta subsequences", "text": "Read-ahead buffering can reduce runtime by 1/2 for sequential accesses to buffered regions.\n\n.. role:: red\n\nIf you want to modify the contents of your FASTA file in-place, you can use the `mutable` argument.\nAny portion of the FastaRecord can be replaced with an equivalent-length string.\n:red:`Warning`: *This will change the contents of your file immediately and permanently:*\n\n.. code:: python\n\nThe FastaVariant class provides a way to integrate single nucleotide variant calls to generate a consensus sequence.\n\n.. code:: python\n\nYou can also specify paths using ``pathlib.Path`` objects.\n\n.. code:: python\n\nAccessing fasta files from `filesystem_spec `_ filesystems:\n\n.. code:: python\n\n.. _faidx:\n\nIt also provides a command-line script:\n\ncli script: faidx\n~~~~~~~~~~~~~~~~~\n\n.. code:: bash\n\nExamples:\n\n.. code:: bash\n\nSimilar syntax as ``samtools faidx``\n\nA lower-level Faidx class is also available:\n\n.. code:: python\n\n-  If the FASTA file is not indexed, when ``Faidx`` is initialized the\n   ``build_index`` method will automatically run, and\n   the index will be written to \"filename.fa.fai\" with ``write_fai()``.\n   where \"filename.fa\" is the original FASTA file.\n-  Start and end coordinates are 1-based.\n\nSupport for compressed FASTA\n----------------------------\n\n``pyfaidx`` can create and read ``.fai`` indices for FASTA files that have\nbeen compressed using the `bgzip `_\ntool from `samtools `_. ``bgzip`` writes compressed\ndata in a ``BGZF`` format. ``BGZF`` is ``gzip`` compatible, consisting of\nmultiple concatenated ``gzip`` blocks, each with an additional ``gzip``\nheader making it possible to build an index for rapid random access. I.e.,\nfiles compressed with ``bgzip`` are valid ``gzip`` and so can be read by\n``gunzip``.  See `this description\n`_ for more details on\n``bgzip``.\n\nChangelog\n---------\n\nPlease see the `releases `_ for a\ncomprehensive list of version changes.\n\nKnown issues\n------------\n\nI try to fix as many bugs as possible, but most of this work is supported by a single developer. Please check the `known issues `_ for bugs relevant to your work. Pull requests are welcome.\n\nContributing\n------------\n\nCreate a new Pull Request with one feature. If you add a new feature, please\ncreate also the relevant test.\n\nTo get test running on your machine:\n - Create a new virtualenv and install the `dev-requirements.txt`.\n \n - Download the test data running:\n\n- Run the tests with\n\nAcknowledgements\n----------------\n\nThis project is freely licensed by the author, `Matthew\nShirley `_, and was completed under the\nmentorship and financial support of Drs. `Sarah\nWheelan `_ and `Vasan\nYegnasubramanian `_ at the Sidney Kimmel\nComprehensive Cancer Center in the Department of Oncology.\n\n.. |Travis| image:: https://travis-ci.com/mdshw5/pyfaidx.svg?branch=master\n\n.. |PyPI| image:: https://img.shields.io/pypi/v/pyfaidx.svg?branch=master\n\n.. |Landscape| image:: https://landscape.io/github/mdshw5/pyfaidx/master/landscape.svg\n   :target: https://landscape.io/github/mdshw5/pyfaidx/master\n   :alt: Code Health\n\n:target: https://codecov.io/gh/mdshw5/pyfaidx\n\n:target: http://depsy.org/package/python/pyfaidx\n\n.. |Appveyor| image:: https://ci.appveyor.com/api/projects/status/80ihlw30a003596w?svg=true\n   :target: https://ci.appveyor.com/project/mdshw5/pyfaidx\n\n:target: https://github.com/mdshw5/pyfaidx/actions/workflows/pypi.yml\n   \n.. |Downloads| image:: https://img.shields.io/pypi/dm/pyfaidx.svg\n   :target: https://pypi.python.org/pypi/pyfaidx/"}, {"name": "pyfaidx", "tags": ["math", "web"], "summary": "pyfaidx: efficient pythonic random access to fasta subsequences", "text": "This library is used to efficiently access and manipulate FASTA subsequences through random access, allowing for fast retrieval of specific sequences or regions. It also enables in-place modification of these files, making it a useful tool for complex manipulation of genomic data."}, {"name": "pyhmmer", "tags": ["cli", "math", "ui", "web"], "summary": "Cython bindings and Python interface to HMMER3.", "text": "\u2666\ufe0f PyHMMER (https://github.com/althonos/pyhmmer/stargazers)\n\n*[Cython](https://cython.org/) bindings and Python interface to [HMMER3](http://hmmer.org/).*\n\n(https://github.com/althonos/pyhmmer/actions)\n(https://codecov.io/gh/althonos/pyhmmer/)\n(https://pypi.org/project/pyhmmer)\n(https://anaconda.org/bioconda/pyhmmer)\n(https://aur.archlinux.org/packages/python-pyhmmer)\n(https://pypi.org/project/pyhmmer/#files)\n(https://pypi.org/project/pyhmmer/#files)\n(https://pypi.org/project/pyhmmer/#files)\n(https://choosealicense.com/licenses/mit/)\n(https://github.com/althonos/pyhmmer/)\n(https://git.embl.de/larralde/pyhmmer/)\n(https://github.com/althonos/pyhmmer/issues)\n(https://pyhmmer.readthedocs.io)\n(https://github.com/althonos/pyhmmer/blob/master/CHANGELOG.md)\n(https://pepy.tech/project/pyhmmer)\n(https://doi.org/10.1093/bioinformatics/btad214)\n\n\ufe0f Overview\n\nHMMER is a biological sequence analysis tool that uses profile hidden Markov\nmodels to search for sequence homologs. HMMER3 is developed and maintained by\nthe [Eddy/Rivas Laboratory](http://eddylab.org/) at Harvard University.\n\n`pyhmmer` is a Python package, implemented using the [Cython](https://cython.org/)\nlanguage, that provides bindings to HMMER3. It directly interacts with the\nHMMER internals, which has the following advantages over CLI wrappers\n(like [`hmmer-py`](https://pypi.org/project/hmmer/)):\n\n- **single dependency**: If your software or your analysis pipeline is\n  distributed as a Python package, you can add `pyhmmer` as a dependency to\n  your project, and stop worrying about the HMMER binaries being properly\n  setup on the end-user machine.\n- **no intermediate files**: Everything happens in memory, in Python objects\n  you have control on, making it easier to pass your inputs to HMMER without\n  needing to write them to a temporary file. Output retrieval is also done\n  in memory, via instances of the\n  [`pyhmmer.plan7.TopHits`](https://pyhmmer.readthedocs.io/en/stable/api/plan7/results.html#pyhmmer.plan7.TopHits)\n  class.\n- **no input formatting**: The Easel object model is exposed in the\n  [`pyhmmer.easel`](https://pyhmmer.readthedocs.io/en/stable/api/easel/index.html)\n  module, and you have the possibility to build a\n  [`DigitalSequence`](https://pyhmmer.readthedocs.io/en/stable/api/easel/seq.html#pyhmmer.easel.DigitalSequence)\n  object yourself to pass to the HMMER pipeline. This is useful if your sequences are already\n  loaded in memory, for instance because you obtained them from another\n  Python library (such as [Pyrodigal](https://github.com/althonos/pyrodigal)\n  or [Biopython](https://biopython.org/)).\n- **no output parsing**: HMMER3 is notorious for its numerous output files\n  and its fixed-width tabular output, which is hard to parse (even\n  [`Bio.SearchIO.HmmerIO`](https://biopython.org/docs/dev/api/Bio.SearchIO.HmmerIO.html)\n  is struggling on some sequences).\n- **efficient**: Using `pyhmmer` to launch `hmmsearch` on sequences\n  and HMMs in disk storage is typically as fast as directly using the\n  `hmmsearch` binary (see the [Benchmarks section](#%EF%B8%8F-benchmarks)).\n  [`pyhmmer.hmmer.hmmsearch`](https://pyhmmer.readthedocs.io/en/stable/api/hmmer/profile.html#pyhmmer.hmmer.hmmsearch)\n  uses a different parallelisation strategy compared to\n  the `hmmsearch` binary from HMMER, which can help getting the most of\n  multiple CPUs when annotating smaller sequence databases.\n\n*This library is still a work-in-progress. It follows [semantic-versioning](https://semver.org/),\nso API changes will be documented, but past `v0.10` the API has been more or\nless stable. It should already pack enough features to run biological analyses \nor workflows involving `hmmsearch`, `hmmscan`, `nhmmer`, `phmmer`, `hmmbuild`\nand `hmmalign`.*\n\nInstalling\n\n`pyhmmer` can be installed from [PyPI](https://pypi.org/project/pyhmmer/),\nwhich hosts some pre-built CPython wheels for Linux and MacOS on x86-64 and Arm64, as well as the code required to compile from source with Cython:\n\nCompilation for UNIX PowerPC is not tested in CI, but should work out of the\nbox. Note than non-UNIX operating systems (such as Windows) are not\nsupported by HMMER.\n\nA [Bioconda](https://bioconda.github.io/) package is also available:\n\nCitation\n\nPyHMMER is scientific software, with a\n[published paper](https://doi.org/10.1093/bioinformatics/btad214)\nin the [Bioinformatics](https://academic.oup.com/bioinformatics). Please\ncite both [PyHMMER](https://doi.org/10.21105/joss.04296)\nand [HMMER](http://hmmer.org) if you are using it in\nan academic work, for instance as:\n\n> PyHMMER (Larralde *et al.*, 2023), a Python library binding to HMMER (Eddy, 2011).\n\nDetailed references are available on the [Publications page](https://pyhmmer.readthedocs.io/en/stable/guide/publications.html) of the\n[online documentation](https://pyhmmer.readthedocs.io/).\n\nDocumentation"}, {"name": "pyhmmer", "tags": ["cli", "math", "ui", "web"], "summary": "Cython bindings and Python interface to HMMER3.", "text": "A complete [API reference](https://pyhmmer.readthedocs.io/en/stable/api/) can\nbe found in the [online documentation](https://pyhmmer.readthedocs.io/), or\ndirectly from the command line using\n[`pydoc`](https://docs.python.org/3/library/pydoc.html):\n\nExample\n\nUse `pyhmmer` to run `hmmsearch` to search for Type 2 PKS domains\n([`t2pks.hmm`](https://raw.githubusercontent.com/althonos/pyhmmer/v0.7.0/pyhmmer/tests/data/hmms/txt/t2pks.hmm))\ninside proteins extracted from the genome of *Anaerococcus provencensis*\n([`938293.PRJEB85.HG003687.faa`](https://raw.githubusercontent.com/althonos/pyhmmer/v0.7.0/pyhmmer/tests/data/seqs/938293.PRJEB85.HG003687.faa)).\nThis will produce an iterable over\n[`TopHits`] that can be used for further sorting/querying in Python.\nProcessing happens in parallel using Python threads, and a [`TopHits`]\nobject is yielded for every [`HMM`] passed in the input iterable.\n\nHave a look at more in-depth examples such as [building a HMM from an alignment](https://pyhmmer.readthedocs.io/en/stable/examples/msa_to_hmm.html),\n[analysing the active site of a hit](https://pyhmmer.readthedocs.io/en/stable/examples/active_site.html),\nor [fetching marker genes from a genome](https://pyhmmer.readthedocs.io/en/stable/examples/fetchmgs.html)\nin the [Examples](https://pyhmmer.readthedocs.io/en/stable/examples/index.html)\npage of the [online documentation](https://pyhmmer.readthedocs.io/).\n\nFeedback\n\n\u26a0\ufe0f Issue Tracker\n\nFound a bug ? Have an enhancement request ? Head over to the [GitHub issue\ntracker](https://github.com/althonos/pyhmmer/issues) if you need to report\nor ask something. If you are filing in on a bug, please include as much\ninformation as you can about the issue, and try to recreate the same bug\nin a simple, easily reproducible situation.\n\n\ufe0f Contributing\n\nContributions are more than welcome! See [`CONTRIBUTING.md`](https://github.com/althonos/pyhmmer/blob/master/CONTRIBUTING.md) for more details.\n\n\u23f1\ufe0f Benchmarks\n\nBenchmarks were run on a [i7-10710U CPU](https://ark.intel.com/content/www/us/en/ark/products/196448/intel-core-i7-10710u-processor-12m-cache-up-to-4-70-ghz.html) running @1.10GHz with 6 physical / 12\nlogical cores, using a FASTA file containing 4,489 protein sequences extracted\nfrom the genome of *Escherichia coli*\n([`562.PRJEB4685`](https://progenomes.embl.de/genome.cgi))\nand the version 33.1 of the [Pfam](https://pfam.xfam.org/) HMM library containing\n18,259 domains. Commands were run 3 times on a warm SSD. *Plain lines show\nthe times for pressed HMMs, and dashed-lines the times for HMMs in text format.*\n\nRaw numbers can be found in the [`benches` folder](https://github.com/althonos/pyhmmer/blob/master/benches/).\nThey suggest that `phmmer` should be run with the number of *logical* cores,\nwhile `hmmsearch` should be run with the number of *physical* cores (or less).\nA possible explanation for this observation would be that HMMER\nplatform-specific code requires too many [SIMD](https://en.wikipedia.org/wiki/SIMD)\nregisters per thread to benefit from [simultaneous multi-threading](https://en.wikipedia.org/wiki/Simultaneous_multithreading).\n\nTo read more about how PyHMMER achieves better parallelism than HMMER for\nmany-to-many searches, have a look at the [Performance page](https://pyhmmer.readthedocs.io/en/stable/guide/performance.html)\nof the documentation.\n\nSee Also\n\nBuilding a HMM from scratch? Then you may be interested in the [`pyfamsa`](https://pypi.org/project/pyfamsa/)\npackage, providing bindings to [FAMSA](https://github.com/refresh-bio/FAMSA),\na very fast multiple sequence aligner. In addition, you may want to trim alignments:\nin that case, consider [`pytrimal`](https://pypi.org/project/pytrimal), which\nwraps [trimAl 2.0](https://github.com/inab/trimal/tree/2.0_RC).\n\nIf despite of all the advantages listed earlier, you would rather use HMMER\nthrough its CLI, this package will not be of great help. You can instead check\nthe [`hmmer-py`](https://github.com/EBI-Metagenomics/hmmer-py) package developed\nby [Danilo Horta](https://github.com/horta) at the [EMBL-EBI](https://www.ebi.ac.uk).\n\n\u2696\ufe0f License\n\nThis library is provided under the [MIT License](https://choosealicense.com/licenses/mit/).\nThe HMMER3 and Easel code is available under the\n[BSD 3-clause](https://choosealicense.com/licenses/bsd-3-clause/) license.\nSee `vendor/hmmer/LICENSE` and `vendor/easel/LICENSE` for more information.\n\nThe PyHMMER logo and figures are available under the `Creative Commons Attribution 4.0 (CC-BY 4.0) `_\nlicense. The PyHMMER logo was derived from `Twemoji `_\nassets under the `Creative Commons Attribution 4.0 (CC-BY 4.0) `_\nlicense as well."}, {"name": "pyhmmer", "tags": ["cli", "math", "ui", "web"], "summary": "Cython bindings and Python interface to HMMER3.", "text": "*This project is in no way affiliated, sponsored, or otherwise endorsed by\nthe [original HMMER authors](http://hmmer.org/). It was developed by\n[Martin Larralde](https://github.com/althonos/pyhmmer) during his PhD project\nat the [European Molecular Biology Laboratory](https://www.embl.de/) in\nthe [Zeller team](https://zellerlab.org/).*"}, {"name": "pyhmmer", "tags": ["cli", "math", "ui", "web"], "summary": "Cython bindings and Python interface to HMMER3.", "text": "This library is used to provide a Python interface and Cython bindings for HMMER3, allowing developers to leverage the power of profile Hidden Markov Models in their applications. This library enables seamless integration with HMMER3's advanced sequence analysis capabilities, streamlining tasks such as multiple alignment, model search, and domain detection."}, {"name": "pyiqa", "tags": ["data", "dev", "math", "ml", "ui"], "summary": "PyTorch Toolbox for Image Quality Assessment", "text": "PyTorch Toolbox for Image Quality Assessment\n\nAn IQA toolbox with pure python and pytorch. Please refer to [Awesome-Image-Quality-Assessment](https://github.com/chaofengc/Awesome-Image-Quality-Assessment) for a comprehensive survey of IQA methods and download links for IQA datasets.\n\n(https://pypi.org/project/pyiqa/)\n(https://pepy.tech/project/pyiqa)\n\n(https://github.com/chaofengc/Awesome-Image-Quality-Assessment)\n(https://github.com/chaofengc/IQA-PyTorch/blob/main/README.md#bookmark_tabs-citation)\n(https://www.zhihu.com/column/c_1565424954811846656)\n\n:open_book: Introduction\n\nThis is a comprehensive image quality assessment (IQA) toolbox built with **pure Python and PyTorch**. We provide reimplementation of many widely used full reference (FR) and no reference (NR) metrics, **with results calibrated against official MATLAB scripts when available**. With GPU acceleration, **our implementations are much faster than their Matlab counterparts.** Please refer to the following documents for details:\n\n[Model Cards](docs/ModelCard.md)  |  \ufe0f [Dataset Cards](docs/Dataset_Preparation.md) |  [Datasets Download](https://huggingface.co/datasets/chaofengc/IQA-Toolbox-Datasets/tree/main) |  [Documentation](https://iqa-pytorch.readthedocs.io/en/latest/) | [Benchmark](https://github.com/chaofengc/IQA-PyTorch/tree/main?tab=readme-ov-file#performance-evaluation-protocol)\n\n---\n\n:triangular_flag_on_post: Updates/Changelog\n- **Jun, 2025**. Add `sfid`, a commonly used metric in generative models.\n- **Nov, 2024**. Add `pyiqa.load_dataset` for easy loading of several common datasets. \n- **Nov, 2024**. Add `compare2score` and `deepdc`. Thanks to [hanwei](https://github.com/h4nwei) for their great work , and please refer to their official papers for more details! \n- [**More**](docs/history_changelog.md)\n\n---\n\n:zap: Quick Start\n\nInstallation\n\nBasic Usage\n\nYou can simply use the package with commandline interface.\n\nAdvanced Usage with Codes\n\nTest metrics\n\nUse as loss functions\n\nNote that gradient propagation is disabled by default. Set `as_loss=True` to enable it as a loss function. **Not all metrics support backpropagation, please refer to [Model Cards](docs/ModelCard.md) and be sure that you are using it in a `lower_better` way.**\n\nUse custom settings and weights\n\nWe also provide a flexible way to use custom settings and weights in case you want to retrain or fine-tune the models.\n\nExample test script\n\nExample test script with input directory/images and reference directory/images.\n\nEasy load of popular datasets\n\nWe offer an easy way to load popular IQA datasets through the configuration file `pyiqa/default_dataset_configs.yml`. The specified datasets will automatically download from the [huggingface IQA-PyTorch-Dataset](https://huggingface.co/datasets/chaofengc/IQA-PyTorch-Datasets). See example code below:\n\n**Please refer to [Dataset Cards](docs/Dataset_Preparation.md) for more details about the `dataset_opts`.**\n\n:1st_place_medal: Benchmark Performances and Model Zoo\n\nResults Calibration\n\nPlease refer to the [results calibration](./ResultsCalibra/ResultsCalibra.md) to verify the correctness of the python implementations compared with official scripts in matlab or python.\n\n\u23ec Download Benchmark Datasets\n\nFor convenience, we upload all related datasets to [huggingface IQA-Toolbox-Dataset](https://huggingface.co/datasets/chaofengc/IQA-Toolbox-Datasets), and corresponding meta information files to [huggingface IQA-Toolbox-Dataset-metainfo](https://huggingface.co/datasets/chaofengc/IQA-Toolbox-Datasets-metainfo). \nHere are example codes to download them from huggingface:\n\n>[!CAUTION]\n> we only collect the datasets for academic, research, and educational purposes. It is important for the users to adhere to the usage guidelines, licensing terms, and conditions set forth by the original creators or owners of each dataset.\n\nDownload meta information from Huggingface with `git clone` or update with `git pull`:\n\nExamples to specific dataset options can be found in `./pyiqa/default_dataset_configs.yml`. Details of the dataloader interface and meta information files can be found in [Dataset Preparation](docs/Dataset_Preparation.md)\n\nPerformance Evaluation Protocol\n\n**We use official models for evaluation if available.** Otherwise, we use the following settings to train and evaluate different models for simplicity and consistency:\n\nMetric Type\n-------------\nFR\nNR\nAesthetic IQA\nFace IQA\nEfficiency"}, {"name": "pyiqa", "tags": ["data", "dev", "math", "ml", "ui"], "summary": "PyTorch Toolbox for Image Quality Assessment", "text": "Results are calculated with:\n- **PLCC without any correction**. Although test time value correction is common in IQA papers, we want to use the original value in our benchmark.\n- **Full image single input.** We **do not** use multi-patch testing unless necessary.\n\nBasically, we use the largest existing datasets for training, and cross dataset evaluation performance for fair comparison. The following models do not provide official weights, and are retrained by our scripts:\n\nMetric Type\n-------------\nFR\nNR\nAesthetic IQA\n\n>[!NOTE]\n>- Due to optimized training process, performance of some retrained approaches may be different with original paper.\n>- Results of all **retrained models by ours** are normalized to [0, 1] and change to higher better for convenience.\n>- Results of KonIQ-10k, AVA are both tested with official split.\n>- NIMA is only applicable to AVA dataset now. We use `inception_resnet_v2` for default `nima`.\n>- MUSIQ is not included in the IAA benchmark because we do not have train/split information of the official model.\n\nBenchmark Performance with Provided Script\n\nHere is an example script to get performance benchmark on different datasets:\n\n:hammer_and_wrench: Train\n\nExample Train Script\n\nExample to train DBCNN on LIVEChallenge dataset\n\nExample for distributed training\n\n:beers: Contribution\n\nAny contributions to this repository are greatly appreciated. Please follow the [contribution instructions](docs/Instruction.md) for contribution guidance.\n\n:scroll: License\n\nThis work is licensed under a [NTU S-Lab License](https://github.com/chaofengc/IQA-PyTorch/blob/main/LICENSE_NTU-S-Lab) and Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License.\n\n:bookmark_tabs: Citation\n\nIf you find our codes helpful to your research, please consider to use the following citation:\n\nPlease also consider to cite our works on image quality assessment if it is useful to you:\n\n:heart: Acknowledgement\n\nThe code architecture is borrowed from [BasicSR](https://github.com/xinntao/BasicSR). Several implementations are taken from: [IQA-optimization](https://github.com/dingkeyan93/IQA-optimization), [Image-Quality-Assessment-Toolbox](https://github.com/RyanXingQL/Image-Quality-Assessment-Toolbox), [piq](https://github.com/photosynthesis-team/piq), [piqa](https://github.com/francois-rozet/piqa), [clean-fid](https://github.com/GaParmar/clean-fid)\n\nWe also thanks the following public repositories: [MUSIQ](https://github.com/google-research/google-research/tree/master/musiq), [DBCNN](https://github.com/zwx8981/DBCNN-PyTorch), [NIMA](https://github.com/kentsyx/Neural-IMage-Assessment), [HyperIQA](https://github.com/SSL92/hyperIQA), [CNNIQA](https://github.com/lidq92/CNNIQA), [WaDIQaM](https://github.com/lidq92/WaDIQaM), [PieAPP](https://github.com/prashnani/PerceptualImageError), [paq2piq](https://github.com/baidut/paq2piq), [MANIQA](https://github.com/IIGROUP/MANIQA)\n\n:e-mail: Contact\n\nIf you have any questions, please email `chaofenghust@gmail.com`"}, {"name": "pyiqa", "tags": ["data", "dev", "math", "ml", "ui"], "summary": "PyTorch Toolbox for Image Quality Assessment", "text": "This library is used to evaluate the quality of images using various metrics, including both full-reference and no-reference methods, leveraging PyTorch for efficient computation. It provides a comprehensive set of image quality assessment tools that can be integrated into deep learning models or standalone applications."}, {"name": "pylatexenc", "tags": ["math"], "summary": "Simple LaTeX parser providing latex-to-unicode and unicode-to-latex conversion", "text": "pylatexenc\n==========\n\nSimple LaTeX parser providing latex-to-unicode and unicode-to-latex conversion\n\n.. image:: https://img.shields.io/github/license/phfaist/pylatexenc.svg?style=flat\n   :target: https://github.com/phfaist/pylatexenc/blob/master/LICENSE.txt\n\n.. image:: https://img.shields.io/travis/phfaist/pylatexenc.svg?style=flat\n   :target: https://travis-ci.org/phfaist/pylatexenc\n   \n.. image:: https://img.shields.io/pypi/v/pylatexenc.svg?style=flat\n   :target: https://pypi.org/project/pylatexenc/\n\n.. image:: https://img.shields.io/lgtm/alerts/g/phfaist/pylatexenc.svg?logo=lgtm&logoWidth=18&style=flat\n   :target: https://lgtm.com/projects/g/phfaist/pylatexenc/alerts/\n\nUnicode Text to LaTeX code\n--------------------------\n\nThe ``pylatexenc.latexencode`` module provides a function ``unicode_to_latex()``\nwhich converts a unicode string into LaTeX text and escape sequences. It should\nrecognize accented characters and most math symbols. A couple of switches allow\nyou to alter how this function behaves.\n\nYou can also run ``latexencode`` in command-line to convert plain unicode text\n(from the standard input or from files given on the command line) into LaTeX\ncode, written on to the standard output.\n\nA third party plug-in for Vim\n`vim-latexencode `_\nby `@Konfekt `_\nprovides a corresponding command to operate on a given range.\n\nParsing LaTeX code & converting to plain text (unicode)\n-------------------------------------------------------\n\nThe ``pylatexenc.latexwalker`` module provides a series of routines that parse\nthe LaTeX structure of given LaTeX code and returns a logical structure of\nobjects, which can then be used to produce output in another format such as\nplain text.  This is not a replacement for a full (La)TeX engine, rather, this\nmodule provides a way to parse a chunk of LaTeX code as mark-up code.\n\nThe ``pylatexenc.latex2text`` module builds up on top of\n``pylatexenc.latexwalker`` and provides functions to convert given LaTeX code to\nplain text with unicode characters.\n\nYou can also run ``latex2text`` in command-line to convert LaTeX input (either\nfrom the standard input, or from files given on the command line) into plain\ntext written on the standard output.\n\nDocumentation\n-------------\n\nFull documentation is available at https://pylatexenc.readthedocs.io/.\n\nLicense\n-------\n\nSee LICENSE.txt (MIT License).\n\nNOTE: See copyright notice and license information for file\n``tools/unicode.xml`` provided in ``tools/unicode.xml.LICENSE``.  (The file\n``tools/unicode.xml`` was downloaded from"}, {"name": "pylatexenc", "tags": ["math"], "summary": "Simple LaTeX parser providing latex-to-unicode and unicode-to-latex conversion", "text": "This library is used to convert text between Unicode and LaTeX formats, allowing developers to easily render LaTeX content as readable text. With this library, developers can also convert Unicode text into LaTeX code for rendering in various applications or environments."}, {"name": "pyloudnorm", "tags": ["math"], "summary": "Implementation of ITU-R BS.1770-4 loudness algorithm in Python.", "text": "pyloudnorm  (https://travis-ci.org/csteinmetz1/pyloudnorm) \nFlexible audio loudness meter in Python. \n\nImplementation of [ITU-R BS.1770-4](https://www.itu.int/dms_pubrec/itu-r/rec/bs/R-REC-BS.1770-4-201510-I!!PDF-E.pdf). \nAllows control over gating block size and frequency weighting filters for additional control. \n\nFor full details on the implementation see our [paper](https://csteinmetz1.github.io/pyloudnorm-eval/paper/pyloudnorm_preprint.pdf) with a summary in our [AES presentation video](https://www.youtube.com/watch?v=krSJpQ3d4gE).\n\nInstallation\nYou can install with pip as follows\n\nFor the latest releases always install from the GitHub repo\n\nUsage\n\nFind the loudness of an audio file\nIt's easy to measure the loudness of a wav file. \nHere we use PySoundFile to read a .wav file as an ndarray.\n\nLoudness normalize and peak normalize audio files\nMethods are included to normalize audio files to desired peak values or desired loudness.\n\nAdvanced operation\nA number of alternate weighting filters are available, as well as the ability to adjust the analysis block size. \nExamples are shown below.\n\nDependancies\n- **SciPy** ([https://www.scipy.org/](https://www.scipy.org/))\n- **NumPy** ([http://www.numpy.org/](http://www.numpy.org/))\n\nCitation\nIf you use pyloudnorm in your work please consider citing us.\n\nReferences\n\n> Ian Dash, Luis Miranda, and Densil Cabrera, \"[Multichannel Loudness Listening Test](http://www.aes.org/e-lib/browse.cfm?elib=14581),\"\n> 124th International Convention of the Audio Engineering Society, May 2008\n\n> Pedro D. Pestana and \u00c1lvaro Barbosa, \"[Accuracy of ITU-R BS.1770 Algorithm in Evaluating Multitrack Material](http://www.aes.org/e-lib/online/browse.cfm?elib=16608),\"\n> 133rd International Convention of the Audio Engineering Society, October 2012\n\n> Pedro D. Pestana, Josh D. Reiss, and \u00c1lvaro Barbosa, \"[Loudness Measurement of Multitrack Audio Content Using Modifications of ITU-R BS.1770](http://www.aes.org/e-lib/browse.cfm?elib=16714),\"\n> 134th International Convention of the Audio Engineering Society, May 2013\n\n> Steven Fenton and Hyunkook Lee, \"[Alternative Weighting Filters for Multi-Track Program Loudness Measurement](http://www.aes.org/e-lib/browse.cfm?elib=19215),\"\n> 143rd International Convention of the Audio Engineering Society, October 2017\n\n> Brecht De Man, \"[Evaluation of Implementations of the EBU R128 Loudness Measurement](http://www.aes.org/e-lib/browse.cfm?elib=19790),\" \n> 145th International Convention of the Audio Engineering Society, October 2018."}, {"name": "pyloudnorm", "tags": ["math"], "summary": "Implementation of ITU-R BS.1770-4 loudness algorithm in Python.", "text": "This library is used to accurately measure the loudness of audio files in compliance with ITU-R BS.1770-4 standards, allowing for customizable control over gating block size and frequency weighting filters. Developers can utilize this library to automate the process of measuring the loudness of audio files, enabling features such as dynamic range compression or content optimization."}, {"name": "pymap3d", "tags": ["math", "web"], "summary": "pure Python (no prereqs) coordinate conversions, following convention of several popular Matlab routines.", "text": "Python 3-D coordinate conversions\n\n(https://doi.org/10.5281/zenodo.213676)\n(https://doi.org/10.21105/joss.00580)\n(https://codecov.io/gh/geospace-code/pymap3d)\n\n(https://pypi.python.org/pypi/pymap3d)\n(http://pepy.tech/project/pymap3d)\n\nPure Python (no prerequistes beyond Python itself) 3-D geographic coordinate conversions and geodesy.\nFunction syntax is roughly similar to Matlab Mapping Toolbox.\nPyMap3D is intended for non-interactive use on massively parallel (HPC) and embedded systems.\n\n[API docs](https://geospace-code.github.io/pymap3d/)\n\nThanks to our [contributors](./.github/contributors.md).\n\nSimilar toolboxes in other code languages\n\n* [Matlab, GNU Octave](https://github.com/geospace-code/matmap3d)\n* [Fortran](https://github.com/geospace-code/maptran3d)\n* [Rust](https://github.com/gberrante/map_3d)\n* [C++](https://github.com/ClancyWalters/cppmap3d)\n\nPrerequisites\n\nNumpy and AstroPy are optional.\nAlgorithms from Vallado and Meeus are used if AstroPy is not present.\n\nInstall\n\nor for the latest development code:\n\nOne can verify Python functionality after installation by:\n\nUsage\n\nWhere consistent with the definition of the functions, all arguments may\nbe arbitrarily shaped (scalar, N-D array).\n\n[Python](https://www.python.org/dev/peps/pep-0448/)\n[argument unpacking](https://docs.python.org/3/tutorial/controlflow.html#unpacking-argument-lists)\ncan be used for compact function arguments with scalars or arbitrarily\nshaped N-D arrays:\n\nwhere tuple `lla` is comprised of scalar or N-D arrays `(lat,lon,alt)`.\n\nExample scripts are in the [examples](./Examples) directory.\n\nNative Python float is typically [64 bit](https://docs.python.org/3/library/stdtypes.html#typesnumeric).\nNumpy can select real precision bits: 32, 64, 128, etc.\n\nFunctions\n\nPopular mapping toolbox functions ported to Python include the\nfollowing, where the source coordinate system (before the \"2\") is\nconverted to the desired coordinate system:\n\nVincenty functions \"vincenty.vreckon\" and \"vincenty.vdist\" are accessed like:\n\nAdditional functions:\n\n* loxodrome_inverse: rhumb line distance and azimuth between ellipsoid points (lat,lon)  akin to Matlab `distance('rh', ...)` and `azimuth('rh', ...)`\n* loxodrome_direct\n* geodetic latitude transforms to/from: parametric, authalic, isometric, and more in pymap3d.latitude\n\nAbbreviations:\n\n* [AER: Azimuth, Elevation, Range](https://en.wikipedia.org/wiki/Spherical_coordinate_system)\n* [ECEF: Earth-centered, Earth-fixed](https://en.wikipedia.org/wiki/ECEF)\n* [ECI: Earth-centered Inertial using IERS](https://www.iers.org/IERS/EN/Home/home_node.html) via `astropy`\n* [ENU: East North Up](https://en.wikipedia.org/wiki/Axes_conventions#Ground_reference_frames:_ENU_and_NED)\n* [NED: North East Down](https://en.wikipedia.org/wiki/North_east_down)\n* [radec: right ascension, declination](https://en.wikipedia.org/wiki/Right_ascension)\n\nEllipsoid\n\nNumerous functions in pymap3d use an ellipsoid model.\nThe default is WGS84 Ellipsoid.\nNumerous other ellipsoids are available in pymap3d.Ellipsoid.\n\nPrint available ellipsoid models:\n\nSpecify GRS80 ellipsoid:\n\narray vs scalar\n\nUse of pymap3d on embedded systems or other streaming data applications often deal with scalar position data.\nThese data are handled efficiently with the Python math stdlib module.\nVector data can be handled via list comprehension.\n\nThose needing multidimensional data with SIMD and other Numpy and/or PyPy accelerated performance can do so automatically by installing Numpy.\npymap3d seamlessly falls back to Python's math module if Numpy isn't present.\nTo keep the code clean, only scalar data can be used without Numpy.\nAs noted above, use list comprehension if you need vector data without Numpy.\n\nCaveats\n\n* Atmospheric effects neglected in all functions not invoking AstroPy.\n  Would need to update code to add these input parameters (just start a GitHub Issue to request).\n* Planetary perturbations and nutation etc. not fully considered.\n\nCompare to Matlab Mapping and Aerospace Toolbox\n\nThe tests in files tests/test_matlab*.py selected by\n\nuse\n[Matlab Engine for Python](https://www.mathworks.com/help/matlab/matlab_external/install-the-matlab-engine-for-python.html)\nto compare Python PyMap3D output with Matlab output using Matlab functions.\n\nNotes\n\nAs compared to [PyProj](https://github.com/jswhit/pyproj):\n\n* PyMap3D does not require anything beyond pure Python for most transforms\n* Astronomical conversions are done using (optional) AstroPy for established accuracy\n* PyMap3D API is similar to Matlab Mapping Toolbox, while PyProj's interface is quite distinct\n* PyMap3D intrinsically handles local coordinate systems such as ENU,\n  while PyProj ENU requires some [additional effort](https://github.com/jswhit/pyproj/issues/105).\n* PyProj is oriented towards points on the planet surface, while PyMap3D handles points on or above the planet surface equally well, particularly important for airborne vehicles and remote sensing.\n\nAstroPy.Units.Quantity\n\nAt this time,\n[AstroPy.Units.Quantity](http://docs.astropy.org/en/stable/units/)\nis not supported.\nLet us know if this is of interest.\nImpacts on performance would have to be considered before making Quantity a first-class citizen.\nFor now, you can workaround by passing in the `.value` of the variable."}, {"name": "pymap3d", "tags": ["math", "web"], "summary": "pure Python (no prereqs) coordinate conversions, following convention of several popular Matlab routines.", "text": "This library is used to perform pure Python 3-D geographic coordinate conversions and geodesy operations, with a syntax similar to Matlab's Mapping Toolbox. This allows developers to easily convert and manipulate spatial coordinates in their applications without requiring any external dependencies beyond Python itself."}, {"name": "pymatgen", "tags": ["math"], "summary": "Python Materials Genomics is a robust materials analysis code that defines core object representations for structures", "text": "Why use `pymatgen`?\n\n1. **It is (fairly) robust.** Pymatgen is used by thousands of researchers and is the analysis code powering the\n   [Materials Project]. The analysis it produces survives rigorous scrutiny every single day. Bugs tend to be found\n   and corrected quickly. Pymatgen also uses Github Actions for continuous integration, which ensures that every\n   new code passes a comprehensive suite of unit tests.\n2. **It is well documented.** A fairly comprehensive documentation has been written to help you get to grips with\n   it quickly.\n3. **It is open.** You are free to use and contribute to `pymatgen`. It also means that `pymatgen` is continuously\n   being improved. We will attribute any code you contribute to any publication you specify. Contributing to\n   `pymatgen` means your research becomes more visible, which translates to greater impact.\n4. **It is fast.** Many of the core numerical methods in `pymatgen` have been optimized by vectorizing in\n   `numpy`/`scipy`. This means that coordinate manipulations are fast. Pymatgen also comes with a complete system\n   for handling periodic boundary conditions.\n5. **It will be around.** Pymatgen is not a pet research project. It is used in the well-established Materials\n   Project. It is also actively being developed and maintained by the [Materials Virtual Lab], the ABINIT group and\n   many other research groups.\n6. **A growing ecosystem of developers and add-ons**. Pymatgen has contributions from materials scientists all over\n   the world. We also now have an architecture to support add-ons that expand `pymatgen`'s functionality even\n   further. Check out the [contributing page](https://pymatgen.org/contributing) and [add-ons page](https://pymatgen.org/addons) for details and examples.\n\nInstallation\n\nThe version at the Python Package Index [PyPI] is always the latest stable release that is relatively bug-free and can be installed via `pip`:\n\nIf you'd like to use the latest unreleased changes on the main branch, you can install directly from GitHub:\n\nSome extra functionality (e.g., generation of POTCARs) does require additional setup (see the [`pymatgen` docs]).\n\nChange Log\n\nSee [GitHub releases](https://github.com/materialsproject/pymatgen/releases), [`docs/CHANGES.md`](docs/CHANGES.md) or [commit history](https://github.com/materialsproject/pymatgen/commits/master) in increasing order of details.\n\nUsing pymatgen\n\nPlease refer to the official [`pymatgen` docs] for tutorials and examples. Dr Anubhav Jain (@computron) has also created\na series of [tutorials](https://github.com/computron/pymatgen_tutorials) and [YouTube videos](https://www.youtube.com/playlist?list=PL7gkuUui8u7_M47KrV4tS4pLwhe7mDAjT), which is a good resource, especially for beginners.\n\nHow to cite pymatgen\n\nIf you use `pymatgen` in your research, please consider citing the following [work](https://doi.org/10.1016/j.commatsci.2012.10.028):\n\nIn addition, some of `pymatgen`'s functionality is based on scientific advances/principles developed by the\ncomputational materials scientists in our team. Please refer to the [`pymatgen` docs] on how to cite them.\n\nLicense\n\nPymatgen is released under the MIT License. The terms of the license are as follows:\n\nAbout the Pymatgen Development Team\n\nShyue Ping Ong (@shyuep) of the [Materials Virtual Lab] started Pymatgen in 2011 and is still the project lead.\nJanosh Riebesell (@janosh) and Matthew Horton (@mkhorton) are co-maintainers.\n\nThe [`pymatgen` development team] is the set of all contributors to the `pymatgen` project, including all subprojects.\n\nOur Copyright Policy\n\nPymatgen uses a shared copyright model. Each contributor maintains copyright over their contributions to `pymatgen`.\nBut, it is important to note that these contributions are typically only changes to the repositories. Thus, the\n`pymatgen` source code, in its entirety is not the copyright of any single person or institution. Instead, it is the\ncollective copyright of the entire [`pymatgen` Development Team]. If individual contributors want to maintain a\nrecord of what changes/contributions they have specific copyright on, they should indicate their copyright in the\ncommit message of the change, when they commit the change to one of the `pymatgen` repositories."}, {"name": "pymatgen", "tags": ["math"], "summary": "Python Materials Genomics is a robust materials analysis code that defines core object representations for structures", "text": "This library is used to define core object representations for structures in materials analysis, providing a robust and well-documented framework for researchers and developers. With pymatgen, users can perform comprehensive analyses of materials data with confidence, thanks to its rigorous testing and continuous integration processes."}, {"name": "pymc", "tags": ["math", "ui", "web"], "summary": "Probabilistic Programming in Python: Bayesian Modeling and Probabilistic Machine Learning with PyTensor", "text": ".. image:: https://cdn.rawgit.com/pymc-devs/pymc/main/docs/logos/svg/PyMC_banner.svg\n\nPyMC (formerly PyMC3) is a Python package for Bayesian statistical modeling\nfocusing on advanced Markov chain Monte Carlo (MCMC) and variational inference (VI)\nalgorithms. Its flexibility and extensibility make it applicable to a\nlarge suite of problems.\n\nCheck out the `PyMC overview `__,  or\none of `the many examples `__!\nFor questions on PyMC, head on over to our `PyMC Discourse `__ forum.\n\nFeatures\n========\n\n-  Intuitive model specification syntax, for example, ``x ~ N(0,1)``\n   translates to ``x = Normal('x',0,1)``\n-  **Powerful sampling algorithms**, such as the `No U-Turn\n   Sampler `__, allow complex models\n   with thousands of parameters with little specialized knowledge of\n   fitting algorithms.\n-  **Variational inference**: `ADVI `__\n   for fast approximate posterior estimation as well as mini-batch ADVI\n   for large data sets.\n-  Relies on `PyTensor `__ which provides:\n-  Transparent support for missing value imputation\n\nLinear Regression Example\n==========================\n\nPlant growth can be influenced by multiple factors, and understanding these relationships is crucial for optimizing agricultural practices.\n\nImagine we conduct an experiment to predict the growth of a plant based on different environmental variables.\n\n.. code-block:: python\n\nimport pymc as pm\n\n# Taking draws from a normal distribution\n   seed = 42\n   x_dist = pm.Normal.dist(shape=(100, 3))\n   x_data = pm.draw(x_dist, random_seed=seed)\n\n# Independent Variables:\n   # Sunlight Hours: Number of hours the plant is exposed to sunlight daily.\n   # Water Amount: Daily water amount given to the plant (in milliliters).\n   # Soil Nitrogen Content: Percentage of nitrogen content in the soil.\n\n# Dependent Variable:\n   # Plant Growth (y): Measured as the increase in plant height (in centimeters) over a certain period.\n\n# Define coordinate values for all dimensions of the data\n   coords={\n   }\n\n# Define generative model\n   with pm.Model(coords=coords) as generative_model:\n\n# Generating data from model by fixing parameters\n   fixed_parameters = {\n   }\n   with pm.do(generative_model, fixed_parameters) as synthetic_model:\n\n# Infer parameters conditioned on observed data\n   with pm.observe(generative_model, {\"plant growth\": synthetic_y}) as inference_model:\n\nFrom the summary, we can see that the mean of the inferred parameters are very close to the fixed parameters\n\n=====================  ======  =====  ========  =========  ===========  =========  ==========  ==========  =======\nParams                  mean     sd    hdi_3%    hdi_97%    mcse_mean    mcse_sd    ess_bulk    ess_tail    r_hat\n=====================  ======  =====  ========  =========  ===========  =========  ==========  ==========  =======\nbetas[sunlight hours]   4.972  0.054     4.866      5.066        0.001      0.001        3003        1257        1\nbetas[water amount]    19.963  0.051    19.872     20.062        0.001      0.001        3112        1658        1\nbetas[soil nitrogen]    1.994  0.055     1.899      2.107        0.001      0.001        3221        1559        1\nsigma                   0.511  0.037     0.438      0.575        0.001      0            2945        1522        1\n=====================  ======  =====  ========  =========  ===========  =========  ==========  ==========  =======\n\n.. code-block:: python\n\n# Simulate new data conditioned on inferred parameters\n   new_x_data = pm.draw(\n   )\n   new_coords = coords | {\"trial\": [0, 1, 2]}\n\nwith inference_model:\n\npm.stats.summary(idata.predictions, kind=\"stats\")\n\nThe new data conditioned on inferred parameters would look like:\n\n================ ======== ======= ======== =========\nOutput            mean     sd      hdi_3%   hdi_97%\n================ ======== ======= ======== =========\nplant growth[0]   14.229   0.515   13.325   15.272\nplant growth[1]   24.418   0.511   23.428   25.326\nplant growth[2]   -6.747   0.511   -7.740   -5.797\n================ ======== ======= ======== =========\n\n.. code-block:: python"}, {"name": "pymc", "tags": ["math", "ui", "web"], "summary": "Probabilistic Programming in Python: Bayesian Modeling and Probabilistic Machine Learning with PyTensor", "text": "# Simulate new data, under a scenario where the first beta is zero\n   with pm.do(\n   ) as plant_growth_model:\n\npm.stats.summary(new_predictions, kind=\"stats\")\n\nThe new data, under the above scenario would look like:\n\n================ ======== ======= ======== =========\nOutput            mean     sd      hdi_3%   hdi_97%\n================ ======== ======= ======== =========\nplant growth[0]   12.149   0.515   11.193   13.135\nplant growth[1]   29.809   0.508   28.832   30.717\nplant growth[2]   -0.131   0.507   -1.121    0.791\n================ ======== ======= ======== =========\n\nGetting started\n===============\n\nIf you already know about Bayesian statistics:\n----------------------------------------------\n\n-  `API quickstart guide `__\n-  The `PyMC tutorial `__\n-  `PyMC examples `__ and the `API reference `__\n\nLearn Bayesian statistics with a book together with PyMC\n--------------------------------------------------------\n\n-  `Bayesian Analysis with Python  `__ (third edition) by Osvaldo Martin: Great introductory book.\n-  `Probabilistic Programming and Bayesian Methods for Hackers `__: Fantastic book with many applied code examples.\n-  `PyMC port of the book \"Doing Bayesian Data Analysis\" by John Kruschke `__ as well as the `first edition `__.\n-  `PyMC port of the book \"Statistical Rethinking A Bayesian Course with Examples in R and Stan\" by Richard McElreath `__\n-  `PyMC port of the book \"Bayesian Cognitive Modeling\" by Michael Lee and EJ Wagenmakers `__: Focused on using Bayesian statistics in cognitive modeling.\n\nSee also the section on books using PyMC on `our website `__.\n\nAudio & Video\n-------------\n\n- Here is a `YouTube playlist `__ gathering several talks on PyMC.\n- You can also find all the talks given at **PyMCon 2020** `here `__.\n- The `\"Learning Bayesian Statistics\" podcast `__ helps you discover and stay up-to-date with the vast Bayesian community. Bonus: it's hosted by Alex Andorra, one of the PyMC core devs!\n\nInstallation\n============\n\nTo install PyMC on your system, follow the instructions on the `installation guide `__.\n\nCiting PyMC\n===========\nPlease choose from the following:\n\n- |DOIpaper| *PyMC: A Modern and Comprehensive Probabilistic Programming Framework in Python*, Abril-Pla O, Andreani V, Carroll C, Dong L, Fonnesbeck CJ, Kochurov M, Kumar R, Lao J, Luhmann CC, Martin OA, Osthege M, Vieira R, Wiecki T, Zinkov R. (2023)\n\n- BibTex version\n\n- |DOIzenodo| A DOI for all versions.\n- DOIs for specific versions are shown on Zenodo and under `Releases `_\n\n:target: https://doi.org/10.5281/zenodo.4603970\n\nContact\n=======\n\nWe are using `discourse.pymc.io `__ as our main communication channel.\n\nTo ask a question regarding modeling or usage of PyMC we encourage posting to our Discourse forum under the `\u201cQuestions\u201d Category `__. You can also suggest feature in the `\u201cDevelopment\u201d Category `__.\nRequests for non-technical information about the project are also welcome on Discourse,\nwe also use Discourse internally for general announcements or governance related processes.\n\nYou can also follow us on these social media platforms for updates and other announcements:\n\n- `LinkedIn @pymc `__\n- `YouTube @PyMCDevelopers `__\n- `X @pymc_devs `__\n- `Mastodon @pymc@bayes.club `__\n\nTo report an issue with PyMC please use the `issue tracker `__.\n\nLicense\n=======\n\n`Apache License, Version\n2.0 `__\n\nSoftware using PyMC\n===================\n\nGeneral purpose\n---------------"}, {"name": "pymc", "tags": ["math", "ui", "web"], "summary": "Probabilistic Programming in Python: Bayesian Modeling and Probabilistic Machine Learning with PyTensor", "text": "- `Bambi `__: BAyesian Model-Building Interface (BAMBI) in Python.\n- `calibr8 `__: A toolbox for constructing detailed observation models to be used as likelihoods in PyMC.\n- `gumbi `__: A high-level interface for building GP models.\n- `SunODE `__: Fast ODE solver, much faster than the one that comes with PyMC.\n- `pymc-learn `__: Custom PyMC models built on top of pymc3_models/scikit-learn API\n\nDomain specific\n---------------\n\n- `Exoplanet `__: a toolkit for modeling of transit and/or radial velocity observations of exoplanets and other astronomical time series.\n- `beat `__: Bayesian Earthquake Analysis Tool.\n- `CausalPy `__: A package focusing on causal inference in quasi-experimental settings.\n- `PyMC-Marketing `__: Bayesian marketing toolbox for marketing mix modeling, customer lifetime value, and more.\n\nSee also the `ecosystem page `__ on our website. Please contact us if your software is not listed here.\n\nPapers citing PyMC\n==================\n\nSee Google Scholar `here `__ and `here `__ for a continuously updated list.\n\nContributors\n============\n\nThe `GitHub contributor page `__ shows the people who have added content to this repo\nwhich includes a large portion of contributors to the PyMC project but not all of them. Other\ncontributors have added content to other repos of the ``pymc-devs`` GitHub organization or have contributed\nthrough other project spaces outside of GitHub like `our Discourse forum `__.\n\nIf you are interested in contributing yourself, read our `Code of Conduct `__\nand `Contributing guide `__.\n\nSupport\n=======\n\nPyMC is a non-profit project under NumFOCUS umbrella. If you want to support PyMC financially, you can donate `here `__.\n\nProfessional Consulting Support\n===============================\n\nYou can get professional consulting support from `PyMC Labs `__.\n\nSponsors\n========\n\nNumFOCUS\n\nPyMCLabs\n\nOpenWoundResearch\n\nThanks to our contributors\n==========================\n\ncontributors\n\n:target: https://mybinder.org/v2/gh/pymc-devs/pymc/main?filepath=%2Fdocs%2Fsource%2Fnotebooks\n\n:target: https://github.com/pymc-devs/pymc/actions?query=workflow%3Atests+branch%3Amain\n\n:target: https://codecov.io/gh/pymc-devs/pymc\n.. |Dockerhub| image:: https://img.shields.io/docker/automated/pymc/pymc.svg\n   :target: https://hub.docker.com/r/pymc/pymc\n\n:target: http://www.numfocus.org/\n.. |NumFOCUS| image:: https://github.com/pymc-devs/brand/blob/main/sponsors/sponsor_logos/sponsor_numfocus.png?raw=true\n   :target: http://www.numfocus.org/\n.. |PyMCLabs| image:: https://github.com/pymc-devs/brand/blob/main/sponsors/sponsor_logos/sponsor_pymc_labs.png?raw=true\n   :target: https://pymc-labs.com\n.. |OpenWoundResearch| image:: https://github.com/pymc-devs/brand/blob/main/sponsors/sponsor_logos/owr/sponsor_owr.png?raw=true\n   :target: https://www.openwoundresearch.com/\n.. |contributors| image:: https://contrib.rocks/image?repo=pymc-devs/pymc\n   :target: https://github.com/pymc-devs/pymc/graphs/contributors\n\n:target: https://anaconda.org/conda-forge/pymc"}, {"name": "pymc", "tags": ["math", "ui", "web"], "summary": "Probabilistic Programming in Python: Bayesian Modeling and Probabilistic Machine Learning with PyTensor", "text": "This library is used to create Bayesian statistical models and perform probabilistic machine learning tasks, leveraging advanced MCMC and variational inference algorithms. With PyMC, developers can build and analyze complex statistical models with ease, making it an essential tool for data scientists and statisticians."}, {"name": "pymeeus", "tags": ["dev", "math"], "summary": "Python implementation of Jean Meeus astronomical routines", "text": "PyMeeus\n=======\n\n**Library of astronomical algorithms in Python**.\n\nPyMeeus is a Python implementation of the astronomical algorithms\ndescribed in the classical book 'Astronomical Algorithms, 2nd Edition,\nWillmann-Bell Inc. (1998)' by Jean Meeus.\n\nThere are great astronomical libraries out there. For instance, if\nyou're looking for high precision and speed you should take a look at\n`libnova `__. For a set of python\nmodules aimed at professional astronomers, you should look at\n`Astropy `__. On the other hand, the advantages\nof PyMeeus are its simplicity, ease of use, ease of reading, ease of\ninstallation (it has the minimum amount of dependencies) and abundant\ndocumentation.\n\nInstallation\n------------\n\nThe easiest way of installing PyMeeus is using pip:\n\n.. code:: sh\n\nOr, for a per-user installation:\n\n.. code:: sh\n\nIf you prefer Python3, you can use:\n\n.. code:: sh\n\npip3 install --user pymeeus\n\nIf you have PyMeeus already installed, but want to upgrade to the latest\nversion:\n\n.. code:: sh\n\npip3 install -U pymeeus\n\nProperly Using PyMeeus\n----------------------\n\nIt is very common to try to run PyMeeus like this:\n\n.. code:: sh\n\nimport pymeeus\n\nmydate = pymeeus.Epoch(1992, 10, 13.0)\n\nBut if you do that, you'll get an error like this:\n\n.. code:: sh\n\nTraceback (most recent call last):\n   AttributeError: module 'pymeeus' has no attribute 'Epoch'\n\nThis issue points to a misunderstanding that is very common in the\nPython world. The keyword ``import`` is used to import **MODULES**\\ ...\nbut PyMeeus is **NOT** a module: It is a **LIBRARY** composed of\n**MULTIPLE** modules (``Angle``, ``Epoch``, ``Coordinates``, etc). As of\ntoday, the library Pymeeus has 19 different modules (if you look into\nthe directory where ``pip`` stores the library, you'll find one \".py\"\nfile per module).\n\nTherefore if you want to use, for example, the module ``Angle`` you\nshould use:\n\n.. code:: sh\n\nimport pymeeus.Angle\n\nI.e., your *module* is ``pymeeus.Angle``, and not just ``Angle``.\n\nBut there is more! When you use ``import`` to fetch a module, you must\nthen use the *dot* notation to access the components of the module\n(classes, functions, etc). For instance:\n\n.. code:: sh\n\nimport pymeeus.Angle\n\ni = pymeeus.Angle.Angle(11.94524)\n\nIn this case, you are telling the Python interpreter that you want to\nuse the class ``Angle`` (with parameter '11.94524') from the module\n``Angle`` belonging to the library ``pymeeus``.\n\nThere is, however, a more practical (and common) way to handle modules\nusing the statement ``from  import ``. For instance:\n\n.. code:: sh\n\nfrom pymeeus.Angle import Angle\n   from pymeeus.Epoch import Epoch, JDE2000\n   from math import sin, cos, tan, acos, atan2, sqrt, radians, log10\n\nThis way is preferred because, among other reasons, only the required\ncomponents are loaded into memory instead of the whole module. Also, now\nthe component is directly added to your execution environment, which\nmeans that you no longer need to use the *dot* notation.\n\nTherefore, the script at the beginning would become:\n\n.. code:: sh\n\nfrom pymeeus.Epoch import Epoch\n\nmydate = Epoch(1992, 10, 13.0)\n\nMeta\n----\n\nAuthor: Dagoberto Salazar"}, {"name": "pymeeus", "tags": ["dev", "math"], "summary": "Python implementation of Jean Meeus astronomical routines", "text": "Distributed under the GNU Lesser General Public License v3 (LGPLv3). See\n``LICENSE.txt`` and ``COPYING.LESSER`` for more information.\n\nDocumentation: https://pymeeus.readthedocs.io/en/latest/\n\nGitHub: https://github.com/architest/pymeeus\n\nIf you have Sphinx installed, you can generate your own, latest\ndocumentation going to directory 'docs' and issuing:\n\n.. code:: sh\n\nmake html\n\nThen the HTML documentation pages can be found in 'build/html'.\n\nContributing\n------------\n\nThe preferred method to contribute is through forking and pull requests:\n\n1. Fork it (https://github.com/architest/pymeeus/fork)\n2. Create your feature branch (``git checkout -b feature/fooBar``)\n3. Commit your changes (``git commit -am 'Add some fooBar'``)\n4. Push to the branch (``git push origin feature/fooBar``)\n5. Create a new Pull Request\n\nPlease bear in mind that PyMeeus follows the PEP8 style guide for Python\ncode `(PEP8) `__. We suggest\nyou install and use a linter like\n`Flake8 `__ before contributing.\n\nAdditionally, PyMeeus makes heavy use of automatic tests. As a general\nrule, every function or method added must have a corresponding test in\nthe proper place in ``tests`` directory.\n\nFinally, documentation is also a big thing here. Add proper and abundant\ndocumentation to your new code. This also includes in-line comments!!!.\n\nContributors\n------------\n\n-  `Neil Freeman `__ - Fixed undefined\n   variable in Epoch.tt2ut\n-  `molsen234 `__ - Fixed bug when using\n   fractional seconds, minutes, hours or days\n-  `Sebastian Veigl `__ - Added\n   functionality for Jupiter's moons\n-  Sophie Scholz - Added functionality for Jupiter's moons\n-  Vittorio Serra - Added functionality for Jupiter's moons\n-  Michael Lutz - Added functionality for Jupiter's moons\n-  `Ben Dilday `__ - Added ``__hash__()``\n   method to class Epoch\n-  `Zivoslav `__ - Bug report of winter\n   solstice\n-  `Devid `__, `Hugo van\n   Kemenade `__ - Test suggestions\n\nWhat's new\n----------\n\n-  0.5.12\n\n-  Fixed a bug in the computation of the winter solstice. Added new\n\n-  0.5.11\n\n-  Added parameter ``local`` to the ``Epoch`` class constructor and\n\n-  0.5.10\n\n-  Added methods ``moon_librations()`` and\n\n-  0.5.9\n\n-  Added method ``moon_maximum_declination()``.\n\n-  0.5.8\n\n-  Fixed several bugs in ``Epoch`` class, and added method ``doy()``.\n\n-  0.5.7\n\n-  Added method ``moon_passage_nodes()``.\n\n-  0.5.6\n\n-  Added method ``moon_perigee_apogee()``.\n\n-  0.5.5\n\n-  Added method ``moon_phase()``.\n\n-  0.5.4\n\n-  Added methods ``illuminated_fraction_disk()`` and\n\n-  0.5.3\n\n-  Fixed error in the return type of method\n\n-  0.5.2\n\n-  Added methods to compute the Moon's longitude of ascending node\n\n-  0.5.1\n\n-  Changes in the organization of the documentation.\n\n-  0.5.0\n\n-  Added ``Moon`` class and ``position()`` methods.\n\n-  0.4.3\n\n-  Added method ``ring_parameters()`` to Saturn class.\n\n-  0.4.2\n\n-  Added method ``__hash__()`` to Epoch. Now Epoch objects can be\n\n-  0.4.1\n\n-  Added funtionality to compute the positions of Jupiter's Galilean\n\n-  0.4.0\n\n-  Added methods to compute Saturn's ring inclination and longitude\n\n-  0.3.13\n\n-  Additional encoding changes.\n\n-  0.3.12\n\n-  Deleted ``encoding`` keyword from setup.py, which was giving\n\n-  0.3.11\n\n-  Added encoding specification to setup.py.\n\n-  0.3.10\n\n-  Fixed characters with the wrong encoding.\n\n-  0.3.9\n\n-  Relaxed requirements, added contributor molsen234, and fixed\n\n-  0.3.8\n\n-  Fixed undefined variable in ``Epoch.tt2ut``."}, {"name": "pymeeus", "tags": ["dev", "math"], "summary": "Python implementation of Jean Meeus astronomical routines", "text": "-  0.3.7\n\n-  Fix bug when using fractional seconds, minutes, hours or days,\n\n-  0.3.6\n\n-  Add method to compute rising and setting times of the Sun.\n\n-  0.3.5\n\n-  Add method ``magnitude()`` to planet classes.\n\n-  0.3.4\n\n-  Add method to compute the parallax correction to Earth class.\n\n-  0.3.3\n\n-  Add methods to compute the passage through the nodes.\n\n-  0.3.2\n\n-  Add methods to compute the perihelion and aphelion of all planets.\n\n-  0.3.1\n\n-  Fix errors in the elongation computation, add tests and examples\n\n-  0.3.0\n\n-  Added ``Pluto`` class."}, {"name": "pymeeus", "tags": ["dev", "math"], "summary": "Python implementation of Jean Meeus astronomical routines", "text": "This library is used to implement complex astronomical calculations in Python with simplicity and ease of use. With PyMeeus, developers can perform tasks such as planetary position calculations, moon phase determination, and other celestial navigation functions."}, {"name": "pymoo", "tags": ["math", "ml"], "summary": "Multi-Objective Optimization in Python", "text": ":alt: python 3.10\n\n   :alt: license apache\n   :target: https://www.apache.org/licenses/LICENSE-2.0\n\n.. |logo| image:: https://github.com/anyoptimization/pymoo-data/blob/main/logo.png?raw=true\n  :target: https://pymoo.org\n  :alt: pymoo\n\n.. |animation| image:: https://github.com/anyoptimization/pymoo-data/blob/main/animation.gif?raw=true\n  :target: https://pymoo.org\n  :alt: pymoo\n\n.. _Github: https://github.com/anyoptimization/pymoo\n.. _Documentation: https://www.pymoo.org/\n.. _Paper: https://ieeexplore.ieee.org/document/9078759\n\npython\n\nlogo\n\nDocumentation_ / Paper_ / Installation_ / Usage_ / Citation_ / Contact_\n\npymoo: Multi-objective Optimization in Python\n====================================================================\n\nOur open-source framework pymoo offers state of the art single- and multi-objective algorithms and many more features\nrelated to multi-objective optimization such as visualization and decision making.\n\n.. _Installation:\n\nInstallation\n********************************************************************************\n\nFirst, make sure you have a Python 3 environment installed. We recommend miniconda3 or anaconda3.\n\nThe official release is always available at PyPi:\n\n.. code:: bash\n\nFor the current developer version:\n\n.. code:: bash\n\nSince for speedup, some of the modules are also available compiled, you can double-check\nif the compilation worked. When executing the command, be sure not already being in the local pymoo\ndirectory because otherwise not the in site-packages installed version will be used.\n\n.. code:: bash\n\n.. _Usage:\n\nUsage\n********************************************************************************\n\nWe refer here to our documentation for all the details.\nHowever, for instance, executing NSGA2:\n\n.. code:: python\n\nA representative run of NSGA2 looks as follows:\n\nanimation\n\n.. _Citation:\n\nCitation\n********************************************************************************\n\nIf you have used our framework for research purposes, you can cite our publication by:\n\n`J. Blank and K. Deb, pymoo: Multi-Objective Optimization in Python, in IEEE Access, vol. 8, pp. 89497-89509, 2020, doi: 10.1109/ACCESS.2020.2990567 `_\nBibTex:\n\n::\n\n.. _Contact:\n\nContact\n********************************************************************************\n\nFeel free to contact me if you have any questions:\n\n`Julian Blank `_  (blankjul [at] outlook.com)\nMichigan State University\nComputational Optimization and Innovation Laboratory (COIN)\nEast Lansing, MI 48824, USA"}, {"name": "pymoo", "tags": ["math", "ml"], "summary": "Multi-Objective Optimization in Python", "text": "This library is used to perform multi-objective optimization tasks with ease, allowing developers to solve complex problems that involve multiple conflicting objectives. With pymoo, developers can efficiently and effectively optimize systems or processes by leveraging a range of advanced algorithms and techniques."}, {"name": "pymorphy3-dicts-ru", "tags": ["math"], "summary": "Russian dictionaries for pymorphy2", "text": "pymorphy3-dicts-ru\n==========================================================\n\nRussian dictionaries for `pymorphy3`_.\n\n.. _pymorphy3: https://github.com/no-plagiarism/pymorphy3\n\nInstallation\n------------\n\nInstall::\n\nUsage\n-----\n\nTo use these dictionaries with pymorphy2 create MorphAnalyzer\nwith ``lang='ru'`` parameter:\n\n>>> import pymorphy3\n>>> morph = pymorphy3.MorphAnalyzer(lang='ru')\n\nTo get a path to the installed dictionary data use\n``pymorphy3_dicts_ru.get_path()`` method.\n\nDevelopment\n-----------\n\nThe main repo is https://github.com/no-plagiarism/pymorphy3-dicts. The repository\ndoesn't contain the data itself: only package template and update\nscripts are stored in VCS.\n\nLicense for Python code in this package is MIT.\nThe data is licensed under\n`Creative Commons Attribution-Share Alike `_."}, {"name": "pymorphy3-dicts-ru", "tags": ["math"], "summary": "Russian dictionaries for pymorphy2", "text": "This library is used to enable Russian language support for pymorphy3, allowing developers to analyze and parse Russian words. It provides the necessary dictionaries to create a MorphAnalyzer instance with lang='ru' parameter for efficient Russian word processing."}, {"name": "pymorphy3", "tags": ["math"], "summary": "Morphological analyzer (POS tagger + inflection engine) for Russian language.", "text": "pymorphy3\n=========\n\npymorphy3 is the continuation of the unmaintained project [pymorphy2](https://github.com/kmike/pymorphy2) which is an morphological analyzer (POS tagger + inflection engine) for Russian and Ukrainian languages.\n\npymorphy3 officially supports Python 3.9 ~ 3.14.\n\n* Documentation: https://pymorphy2.readthedocs.io\n* Bug tracker: https://github.com/no-plagiarism/pymorphy3/issues\n* Changelog: https://github.com/no-plagiarism/pymorphy3/blob/master/CHANGES.rst\n* License: https://github.com/no-plagiarism/pymorphy3/blob/master/LICENSE.txt\n* Contributors: https://github.com/no-plagiarism/pymorphy3/blob/master/AUTHORS.rst"}, {"name": "pymorphy3", "tags": ["math"], "summary": "Morphological analyzer (POS tagger + inflection engine) for Russian language.", "text": "This library is used to analyze Russian and Ukrainian language texts by identifying parts of speech (POS tagging) and generating word inflections. Developers can leverage pymorphy3 to improve natural language processing tasks, such as text classification, sentiment analysis, or machine translation for these languages."}, {"name": "pynndescent", "tags": ["math", "ui", "web"], "summary": "Nearest Neighbor Descent", "text": ".. image:: doc/pynndescent_logo.png\n  :width: 600\n  :align: center\n  :alt: PyNNDescent Logo\n\n.. image:: https://dev.azure.com/TutteInstitute/build-pipelines/_apis/build/status%2Flmcinnes.pynndescent?branchName=master\n\n===========\nPyNNDescent\n===========\n\nPyNNDescent is a Python nearest neighbor descent for approximate nearest neighbors.\nIt provides a python implementation of Nearest Neighbor\nDescent for k-neighbor-graph construction and approximate nearest neighbor\nsearch, as per the paper:\n\nDong, Wei, Charikar Moses, and Kai Li.\n*\"Efficient k-nearest neighbor graph construction for generic similarity\nmeasures.\"*\nProceedings of the 20th international conference on World wide web. ACM, 2011.\n\nThis library supplements that approach with the use of random projection trees for\ninitialisation. This can be particularly useful for the metrics that are\namenable to such approaches (euclidean, minkowski, angular, cosine, etc.). Graph\ndiversification is also performed, pruning the longest edges of any triangles in the\ngraph.\n\nCurrently this library targets relatively high accuracy \n(80%-100% accuracy rate) approximate nearest neighbor searches.\n\n--------------------\nWhy use PyNNDescent?\n--------------------\n\nPyNNDescent provides fast approximate nearest neighbor queries. The\n`ann-benchmarks `_ system puts it\nsolidly in the mix of top performing ANN libraries:\n\n**SIFT-128 Euclidean**\n\n.. image:: https://pynndescent.readthedocs.io/en/latest/_images/sift.png\n\n**NYTimes-256 Angular**\n\n.. image:: https://pynndescent.readthedocs.io/en/latest/_images/nytimes.png\n\nWhile PyNNDescent is among fastest ANN library, it is also both easy to install (pip\nand conda installable) with no platform or compilation issues, and is very flexible,\nsupporting a wide variety of distance metrics by default:\n\n**Minkowski style metrics**\n\n- euclidean\n- manhattan\n- chebyshev\n- minkowski\n\n**Miscellaneous spatial metrics**\n\n- canberra\n- braycurtis\n- haversine\n\n**Normalized spatial metrics**\n\n- mahalanobis\n- wminkowski\n- seuclidean\n\n**Angular and correlation metrics**\n\n- cosine\n- dot\n- correlation\n- spearmanr\n- tsss\n- true_angular\n\n**Probability metrics**\n\n- hellinger\n- wasserstein\n\n**Metrics for binary data**\n\n- hamming\n- jaccard\n- dice\n- russelrao\n- kulsinski\n- rogerstanimoto\n- sokalmichener\n- sokalsneath\n- yule\n\nand also custom user defined distance metrics while still retaining performance.\n\nPyNNDescent also integrates well with Scikit-learn, including providing support\nfor the KNeighborTransformer as a drop in replacement for algorithms\nthat make use of nearest neighbor computations.\n\n----------------------\nHow to use PyNNDescent\n----------------------\n\nPyNNDescent aims to have a very simple interface. It is similar to (but more\nlimited than) KDTrees and BallTrees in ``sklearn``. In practice there are\nonly two operations -- index construction, and querying an index for nearest\nneighbors.\n\nTo build a new search index on some training data ``data`` you can do something\nlike\n\n.. code:: python\n\nYou can then use the index for searching (and can pickle it to disk if you\nwish). To search a pynndescent index for the 15 nearest neighbors of a test data\nset ``query_data`` you can do something like\n\n.. code:: python\n\nand that is pretty much all there is to it. You can find more details in the\n`documentation `_.\n\n----------\nInstalling\n----------\n\nPyNNDescent is designed to be easy to install being a pure python module with\nrelatively light requirements:\n\n* numpy\n* scipy\n* scikit-learn >= 0.22\n* numba >= 0.51\n\nall of which should be pip or conda installable. The easiest way to install should be\nvia conda:\n\n.. code:: bash\n\nor via pip:\n\n.. code:: bash\n\nTo manually install this package:\n\n.. code:: bash\n\n----------------\nHelp and Support\n----------------\n\nThis project is still young. The documentation is still growing. In the meantime please\n`open an issue `_\nand I will try to provide any help and guidance that I can. Please also check\nthe docstrings on the code, which provide some descriptions of the parameters.\n\n-------\nLicense\n-------\n\nThe pynndescent package is 2-clause BSD licensed. Enjoy.\n\n------------\nContributing\n------------\n\nContributions are more than welcome! There are lots of opportunities\nfor potential projects, so please get in touch if you would like to\nhelp out. Everything from code to notebooks to\nexamples and documentation are all *equally valuable* so please don't feel\nyou can't contribute. To contribute please `fork the project `_ make your changes and\nsubmit a pull request. We will do our best to work through any issues with\nyou and get your code merged into the main branch."}, {"name": "pynndescent", "tags": ["math", "ui", "web"], "summary": "Nearest Neighbor Descent", "text": "This library is used to efficiently construct k-neighbor graphs and perform approximate nearest neighbor search in high-dimensional spaces. It provides a Python implementation of Nearest Neighbor Descent with optimizations using random projection trees for initialization."}, {"name": "pyogrio", "tags": ["data", "math", "web"], "summary": "Vectorized spatial vector file format I/O using GDAL/OGR", "text": "pyogrio - bulk-oriented spatial vector file I/O using GDAL/OGR\n\nPyogrio provides fast, bulk-oriented read and write access to \n[GDAL/OGR](https://gdal.org/en/latest/drivers/vector/index.html) vector data\nsources, such as ESRI Shapefile, GeoPackage, GeoJSON, and several others.\nVector data sources typically have geometries, such as points, lines, or\npolygons, and associated records with potentially many columns worth of data.\n\nThe typical use is to read or write these data sources to/from\n[GeoPandas](https://github.com/geopandas/geopandas) `GeoDataFrames`. Because\nthe geometry column is optional, reading or writing only non-spatial data is\nalso possible. Hence, GeoPackage attribute tables, DBF files, or CSV files are\nalso supported.\n\nPyogrio is fast because it uses pre-compiled bindings for GDAL/OGR to read and\nwrite the data records in bulk. This approach avoids multiple steps of\nconverting to and from Python data types within Python, so performance becomes\nprimarily limited by the underlying I/O speed of data source drivers in\nGDAL/OGR.\n\nWe have seen \\>5-10x speedups reading files and \\>5-20x speedups writing files\ncompared to using row-per-row approaches (e.g. Fiona).\n\nRead the documentation for more information:\n[https://pyogrio.readthedocs.io](https://pyogrio.readthedocs.io/en/latest/).\n\nRequirements\n\nSupports Python 3.10 - 3.14 and GDAL 3.6.x - 3.11.x.\n\nReading to GeoDataFrames requires `geopandas>=0.12` with `shapely>=2`.\n\nAdditionally, installing `pyarrow` in combination with GDAL 3.6+ enables\na further speed-up when specifying `use_arrow=True`.\n\nInstallation\n\nPyogrio is currently available on\n[conda-forge](https://anaconda.org/conda-forge/pyogrio)\nand [PyPI](https://pypi.org/project/pyogrio/)\nfor Linux, MacOS, and Windows.\n\nPlease read the\n[installation documentation](https://pyogrio.readthedocs.io/en/latest/install.html)\nfor more information.\n\nSupported vector formats\n\nPyogrio supports most common vector data source formats (provided they are also\nsupported by GDAL/OGR), including ESRI Shapefile, GeoPackage, GeoJSON, and\nFlatGeobuf.\n\nPlease see the [list of supported formats](https://pyogrio.readthedocs.io/en/latest/supported_formats.html)\nfor more information.\n\nGetting started\n\nPlease read the [introduction](https://pyogrio.readthedocs.io/en/latest/supported_formats.html)\nfor more information and examples to get started using Pyogrio.\n\nYou can also check out the [API documentation](https://pyogrio.readthedocs.io/en/latest/api.html)\nfor full details on using the API.\n\nCredits\n\nThis project is made possible by the tremendous efforts of the GDAL, Fiona, and\nGeopandas communities."}, {"name": "pyogrio", "tags": ["data", "math", "web"], "summary": "Vectorized spatial vector file format I/O using GDAL/OGR", "text": "This library is used to perform fast and efficient bulk-oriented read and write operations on spatial vector file formats such as ESRI Shapefile and GeoPackage. With pyogrio, developers can easily interact with various vector data sources using GDAL/OGR."}, {"name": "pyomo", "tags": ["dev", "math"], "summary": "The Pyomo optimization modeling framework", "text": "Pyomo Overview\n\nPyomo is a Python-based open-source software package that supports a\ndiverse set of optimization capabilities for formulating and analyzing\noptimization models. Pyomo can be used to define symbolic problems,\ncreate concrete problem instances, and solve these instances with\nstandard solvers. Pyomo supports a wide range of problem types,\nincluding:\n\n -  Linear programming\n -  Quadratic programming\n -  Nonlinear programming\n -  Mixed-integer linear programming\n -  Mixed-integer quadratic programming\n -  Mixed-integer nonlinear programming\n -  Mixed-integer stochastic programming\n -  Generalized disjunctive programming\n -  Differential algebraic equations\n -  Mathematical programming with equilibrium constraints\n -  Constraint programming\n\nPyomo supports analysis and scripting within a full-featured programming\nlanguage. Further, Pyomo has also proven an effective framework for\ndeveloping high-level optimization and analysis tools.  For example, the\n[`mpi-sppy`](https://github.com/Pyomo/mpi-sppy) package provides generic\nsolvers for stochastic programming. `mpi-sppy` leverages the fact that\nPyomo's modeling objects are embedded within a full-featured high-level\nprogramming language, which allows for transparent parallelization of\nsubproblems using Python parallel communication libraries.\n\n* [Pyomo Home](https://www.pyomo.org)\n* [About Pyomo](https://www.pyomo.org/about)\n* [Download](https://www.pyomo.org/installation/)\n* [Documentation](https://www.pyomo.org/documentation/)\n* [Performance Plots](https://pyomo.github.io/performance/)\n\nPyomo was formerly released as the Coopr software library.\n\nPyomo is available under the BSD License - see the \n[`LICENSE.md`](https://github.com/Pyomo/pyomo/blob/main/LICENSE.md) file.\n\nPyomo is currently tested with the following Python implementations:\n\n* CPython: 3.9, 3.10, 3.11, 3.12, 3.13, 3.14\n* PyPy: 3.11\n\n_Testing and support policy_:\n\nAt the time of the first Pyomo release after the end-of-life of a minor Python\nversion, we will remove testing for that Python version.\n\nInstallation\n\nPyPI (https://pypi.org/project/Pyomo/) (https://pypistats.org/packages/pyomo)\n\nAnaconda (https://anaconda.org/conda-forge/pyomo) (https://anaconda.org/conda-forge/pyomo)\n\nTutorials and Examples\n\n* [Pyomo \u2014 Optimization Modeling in Python](https://link.springer.com/book/10.1007/978-3-030-68928-5)\n* [Pyomo Workshop Slides](https://github.com/Pyomo/pyomo-tutorials/blob/main/Pyomo-Workshop-December-2023.pdf)\n* [Prof. Jeffrey Kantor's Pyomo Cookbook](https://jckantor.github.io/ND-Pyomo-Cookbook/)\n* The [companion notebooks](https://mobook.github.io/MO-book/intro.html)\n  for *Hands-On Mathematical Optimization with Python*\n* [Pyomo Gallery](https://github.com/Pyomo/PyomoGallery)\n\nGetting Help\n\nTo get help from the Pyomo community ask a question on one of the following:\n* [Use the #pyomo tag on StackOverflow](https://stackoverflow.com/questions/ask?tags=pyomo)\n* [Pyomo Forum](https://groups.google.com/forum/?hl=en#!forum/pyomo-forum)\n\nDevelopers\n\nPyomo development moved to this repository in June 2016 from\nSandia National Laboratories. Developer discussions are hosted by\n[Google Groups](https://groups.google.com/forum/#!forum/pyomo-developers).\n\nThe Pyomo Development team holds weekly coordination meetings on\nTuesdays 12:30 - 14:00 (MT).  Please contact wg-pyomo@sandia.gov to\nrequest call-in information.\n\nBy contributing to this software project, you are agreeing to the\nfollowing terms and conditions for your contributions:\n\n1. You agree your contributions are submitted under the BSD license. \n2. You represent you are authorized to make the contributions and grant\n   the license. If your employer has rights to intellectual property that\n   includes your contributions, you represent that you have received\n   permission to make contributions and grant the required license on\n   behalf of that employer.\n\nRelated Packages\n\nSee https://pyomo.readthedocs.io/en/latest/related_packages.html."}, {"name": "pyomo", "tags": ["dev", "math"], "summary": "The Pyomo optimization modeling framework", "text": "This library is used to define, solve, and analyze complex optimization models across various problem types, including linear, nonlinear, mixed-integer, and stochastic programming. With Pyomo, developers can create and optimize mathematical models using Python, facilitating the solution of diverse optimization problems in fields such as operations research, engineering, and economics."}, {"name": "pyqtgraph", "tags": ["math", "visualization"], "summary": "Scientific Graphics and GUI Library for Python", "text": "PyQtGraph is a pure-python graphics and GUI library built on PyQt5/PySide2 and\nnumpy. \n\nIt is intended for use in mathematics / scientific / engineering applications.\nDespite being written entirely in python, the library is very fast due to its\nheavy leverage of numpy for number crunching, Qt's GraphicsView framework for\n2D display, and OpenGL for 3D display."}, {"name": "pyqtgraph", "tags": ["math", "visualization"], "summary": "Scientific Graphics and GUI Library for Python", "text": "This library is used to create high-performance scientific graphics and GUIs for mathematical, scientific, or engineering applications. With PyQtGraph, developers can build complex, interactive visualizations that leverage the power of numpy and Qt's GraphicsView framework."}, {"name": "pysbd", "tags": ["math", "ml", "web"], "summary": "pysbd (Python Sentence Boundary Disambiguation) is a rule-based sentence boundary detection that works out-of-the-box across many languages.", "text": "pySBD: Python Sentence Boundary Disambiguation (SBD)\n\n (https://codecov.io/gh/nipunsadvilkar/pySBD) (https://github.com/nipunsadvilkar/pySBD/blob/master/LICENSE) (https://pypi.python.org/pypi/pysbd) (https://github.com/nipunsadvilkar/pySBD)\n\npySBD - python Sentence Boundary Disambiguation (SBD) - is a rule-based sentence boundary detection module that works out-of-the-box.\n\nThis project is a direct port of ruby gem - [Pragmatic Segmenter](https://github.com/diasks2/pragmatic_segmenter) which provides rule-based sentence boundary detection.\n\nHighlights\n**'PySBD: Pragmatic Sentence Boundary Disambiguation'** a short research paper got accepted into 2nd Workshop for Natural Language Processing Open Source Software (NLP-OSS) at EMNLP 2020. \n\n**Research Paper:**\n\n**[Recorded Talk:](https://slideslive.com/38939754)**\n\n(https://slideslive.com/38939754)\n\n**Poster:**\n\n(artifacts/pysbd_poster.png)\n\nInstall\n\n**Python**\n\nUsage\n\n-   Currently pySBD supports only English language. Support for more languages will be released soon.\n\n-   Use `pysbd` as a [spaCy](https://spacy.io/usage/processing-pipelines) pipeline component. (recommended)Please refer to example [pysbd\\_as\\_spacy\\_component.py](https://github.com/nipunsadvilkar/pySBD/blob/master/examples/pysbd_as_spacy_component.py)\n- Use pysbd through [entrypoints](https://spacy.io/usage/saving-loading#entry-points-components)\n\nContributing\n\nIf you want to contribute new feature/language support or found a text that is incorrectly segmented using pySBD, then please head to [CONTRIBUTING.md](https://github.com/nipunsadvilkar/pySBD/blob/master/CONTRIBUTING.md) to know more and follow these steps.\n\n1.  Fork it ( https://github.com/nipunsadvilkar/pySBD/fork )\n2.  Create your feature branch (`git checkout -b my-new-feature`)\n3.  Commit your changes (`git commit -am 'Add some feature'`)\n4.  Push to the branch (`git push origin my-new-feature`)\n5.  Create a new Pull Request\n\nCitation\nIf you use `pysbd` package in your projects or research, please cite [PySBD: Pragmatic Sentence Boundary Disambiguation](https://www.aclweb.org/anthology/2020.nlposs-1.15).\n\nCredit\n\nThis project wouldn't be possible without the great work done by [Pragmatic Segmenter](https://github.com/diasks2/pragmatic_segmenter) team."}, {"name": "pysbd", "tags": ["math", "ml", "web"], "summary": "pysbd (Python Sentence Boundary Disambiguation) is a rule-based sentence boundary detection that works out-of-the-box across many languages.", "text": "This library is used to detect sentence boundaries in text across multiple languages using a rule-based approach, allowing developers to accurately identify sentences and use this information for various natural language processing tasks. By leveraging pysbd, developers can improve the accuracy of their NLP applications and enable features such as automatic summarization, question answering, and more."}, {"name": "pyshp", "tags": ["cli", "data", "dev", "math", "ui", "web"], "summary": "Pure Python read/write support for ESRI Shapefile format", "text": "PyShp\n\nThe Python Shapefile Library (PyShp) reads and writes ESRI Shapefiles in pure Python.\n\n- **Author**: [Joel Lawhead](https://github.com/GeospatialPython)\n- **Maintainers**: [Karim Bahgat](https://github.com/karimbahgat)\n- **Version**: 3.0.3.dev0\n- **Date**: 10th October, 2025\n- **License**: [MIT](https://github.com/GeospatialPython/pyshp/blob/master/LICENSE.TXT)\n\nContents\n\n- [Overview](#overview)\n- [Version Changes](#version-changes)\n- [The Basics](#the-basics)\n\t- [Reading Shapefiles](#reading-shapefiles)\n\t\t- [The Reader Class](#the-reader-class)\n\t\t\t- [Reading Shapefiles from Local Files](#reading-shapefiles-from-local-files)\n\t\t\t- [Reading Shapefiles from Zip Files](#reading-shapefiles-from-zip-files)\n\t\t\t- [Reading Shapefiles from URLs](#reading-shapefiles-from-urls)\n\t\t\t- [Reading Shapefiles from File-Like Objects](#reading-shapefiles-from-file-like-objects)\n\t\t\t- [Reading Shapefiles Using the Context Manager](#reading-shapefiles-using-the-context-manager)\n\t\t\t- [Reading Shapefile Meta-Data](#reading-shapefile-meta-data)\n\t\t- [Reading Geometry](#reading-geometry)\n\t\t- [Reading Records](#reading-records)\n\t\t- [Reading Geometry and Records Simultaneously](#reading-geometry-and-records-simultaneously)\n\t- [Writing Shapefiles](#writing-shapefiles)\n\t\t- [The Writer Class](#the-writer-class)\n\t\t\t- [Writing Shapefiles to Local Files](#writing-shapefiles-to-local-files)\n\t\t\t- [Writing Shapefiles to File-Like Objects](#writing-shapefiles-to-file-like-objects)\n\t\t\t- [Writing Shapefiles Using the Context Manager](#writing-shapefiles-using-the-context-manager)\n\t\t\t- [Setting the Shape Type](#setting-the-shape-type)\n\t\t- [Adding Records](#adding-records)\n\t\t- [Adding Geometry](#adding-geometry)\n\t\t- [Geometry and Record Balancing](#geometry-and-record-balancing)\n- [Advanced Use](#advanced-use)\n\t- [Reading Large Shapefiles](#reading-large-shapefiles)\n\t\t- [Iterating through a shapefile](#iterating-through-a-shapefile)\n\t\t- [Limiting which fields to read](#limiting-which-fields-to-read)\n\t\t- [Attribute filtering](#attribute-filtering)\n\t\t- [Spatial filtering](#spatial-filtering)\n\t- [Writing large shapefiles](#writing-large-shapefiles)\n\t\t- [Merging multiple shapefiles](#merging-multiple-shapefiles)\n\t\t- [Editing shapefiles](#editing-shapefiles)\n\t- [3D and Other Geometry Types](#3d-and-other-geometry-types)\n\t\t- [Shapefiles with elevation (Z) values](#shapefiles-with-elevation-z-values)\n\t\t- [3D MultiPatch Shapefiles](#3d-multipatch-shapefiles)\n- [Testing](#testing)\n- [Contributors](#contributors)\n\nOverview\n\nThe Python Shapefile Library (PyShp) provides read and write support for the\nEsri Shapefile format. The Shapefile format is a popular Geographic\nInformation System vector data format created by Esri. For more information\nabout this format please read the well-written \"ESRI Shapefile Technical\nDescription - July 1998\" located at [http://www.esri.com/library/whitepapers/p\ndfs/shapefile.pdf](http://www.esri.com/library/whitepapers/pdfs/shapefile.pdf)\n. The Esri document describes the shp and shx file formats. However a third\nfile format called dbf is also required. This format is documented on the web\nas the \"XBase File Format Description\" and is a simple file-based database\nformat created in the 1960's. For more on this specification see: [http://www.clicketyclick.dk/databases/xbase/format/index.html](http://www.clicketyclick.dk/databases/xbase/format/index.html)\n\nBoth the Esri and XBase file-formats are very simple in design and memory\nefficient which is part of the reason the shapefile format remains popular\ndespite the numerous ways to store and exchange GIS data available today.\n\nThis document provides examples for using PyShp to read and write shapefiles. However\nmany more examples are continually added to the blog [http://GeospatialPython.com](http://GeospatialPython.com),\nand by searching for PyShp on [https://gis.stackexchange.com](https://gis.stackexchange.com).\n\nCurrently the sample census blockgroup shapefile referenced in the examples is available on the GitHub project site at\n[https://github.com/GeospatialPython/pyshp](https://github.com/GeospatialPython/pyshp). These\nexamples are straight-forward and you can also easily run them against your\nown shapefiles with minimal modification.\n\nImportant: If you are new to GIS you should read about map projections.\nPlease visit: [https://github.com/GeospatialPython/pyshp/wiki/Map-Projections](https://github.com/GeospatialPython/pyshp/wiki/Map-Projections)\n\nI sincerely hope this library eliminates the mundane distraction of simply\nreading and writing data, and allows you to focus on the challenging and FUN\npart of your geospatial project.\n\nVersion Changes\n\n3.0.3\n\nType checking\n- Add optional dependency, \"stubs\", containing the package \"pyshp-stubs\" generated with stubgen (to avoid\nincluding py.typed, and no longer being a single file project).\n\nBug fix\n- Prevented UnboundLocalError when reading non-single point M and Z type Shapefiles (@ekawas-vrify)."}, {"name": "pyshp", "tags": ["cli", "data", "dev", "math", "ui", "web"], "summary": "Pure Python read/write support for ESRI Shapefile format", "text": "Testing.\n- Test PyShp on the Python 3.14 official release (officially released this week, no longer in beta).\n\n3.0.2\n\n.post1\n- Update version at the top of this Readme file (to make what PyPi users see consistent with the changelog and __version__).\n\n.post0\n- Re-release to trigger the deploy job to publish PyShp to Pypi\n\nBug fix\n- Deleted py.typed (probably temporarily).  Including the py.typed marker file with a single file package caused type checkers to type check all other libraries installed in the same directory (whether they're typed or not, enforcing type checking for all the adjacent dirs libraries, regardless of the user's intentions for the type checker) (pointed out by @dl1jbe - thanks Thomas).  Discussions will be started about a longer term fix, possibly e.g. possibly refactoring to a package, and restoring py.typed.\n\nCode quality:\n- Ruff check's UP rule added (mimicks PyUpgrade) (@mwtoews).\n\n3.0.1\n\nImprovements\n- Reader(shp=, dbf=, shx=) now support pathlib.Paths, and any pathlike object (@mwtoews).\n\nBug fixes\n- PyShp 3 no longer modifies the global doctest module (@JamesParrott).\n\nCode quality\n- isort replaced by Ruff check's I rule (@mwtoews).\n- mypy --strict used in CI (@JamesParrott).\n- LICENSE.TXT re-encoded in UTF-8 (@musicinmybrain).\n\n3.0.0\n\nBreaking Changes:\n- Python 2 and Python 3.8 support dropped.\n- Field info tuple is now a namedtuple (Field) instead of a list.\n- Field type codes are now FieldType 'enum' members.\n- bbox, mbox and zbox attributes are all new Namedtuples.\n- Writer does not mutate Shapes.\n- New custom subclasses for each shape type: Null, Multipatch, Point, Polyline,\n  Multipoint, and Polygon, plus the latter 4's M and Z variants (Reader and\n  Writer are still compatible with their base class, Shape, as before).\n- Shape sub classes are creatable from, and serializable to bytes streams,\n  as per the shapefile spec.\n\nImprovements:\n- Speeded up writing shapefiles by up to another ~27% (on top of the recent ~39% improvement in 2.4.1).\n\nCode quality\n- Statically typed, and checked with Mypy\n- Checked with Ruff.\n- f-strings\n- Remove Python 2 specific functions.\n- Run doctests against wheels.\n- Testing of wheels before publishing them\n- pyproject.toml src layout\n- Slow test marked.\n\n2.4.1\n\nImprovements:\n- Speed up writing shapefiles by up to ~39%.  Combined for loops of calls to f.write(pack(...)), into single calls.\n\nBreaking Change.  Support for Python 2 and Pythons = 3.9).\n- This will not break any projects, as pip and other package managers should not install PyShp 3.0.0\n(after its release) in unsupported Pythons.  But we no longer promise such projects will get PyShp's latest\nbug fixes and features.\n- If this negatively impacts your project, all feedback about this decision is welcome\non our [the discussion page](https://github.com/GeospatialPython/pyshp/discussions/290).\n\n2.4.0\n\nNew Features:\n- Reader.iterRecords now allows start and stop to be specified, to lookup smaller ranges of records.\n- Equality comparisons between Records now also require the fields to be the same (and in the same order)."}, {"name": "pyshp", "tags": ["cli", "data", "dev", "math", "ui", "web"], "summary": "Pure Python read/write support for ESRI Shapefile format", "text": "Development:\n- Code quality tools (Ruff format) run on PyShp\n- Network, non-network, or all doctests selectable via command line args\n- Network tests made runnable on localhost.\n\n2.3.1\n\nBug fixes:\n\n- Fix recently introduced issue where Reader/Writer closes file-like objects provided by user (#244)\n\n2.3.0\n\nNew Features:\n\n- Added support for pathlib and path-like shapefile filepaths (@mwtoews).\n- Allow reading individual file extensions via filepaths.\n\nImprovements:\n\n- Simplified setup and deployment (@mwtoews)\n- Faster shape access when missing shx file\n- Switch to named logger (see #240)\n\nBug fixes:\n\n- More robust handling of corrupt shapefiles (fixes #235)\n- Fix errors when writing to individual file-handles (fixes #237)\n- Revert previous decision to enforce geojson output ring orientation (detailed explanation at https://github.com/SciTools/cartopy/issues/2012)\n- Fix test issues in environments without network access (@sebastic, @musicinmybrain).\n\n2.2.0\n\nNew Features:\n\n- Read shapefiles directly from zipfiles.\n- Read shapefiles directly from urls.\n- Allow fast extraction of only a subset of dbf fields through a `fields` arg.\n- Allow fast filtering which shapes to read from the file through a `bbox` arg.\n\nImprovements:\n\n- More examples and restructuring of README.\n- More informative Shape to geojson warnings (see #219).\n- Add shapefile.VERBOSE flag to control warnings verbosity (default True).\n- Shape object information when calling repr().\n- Faster ring orientation checks, enforce geojson output ring orientation.\n\nBug fixes:\n\n- Remove null-padding at end of some record character fields.\n- Fix dbf writing error when the number of record list or dict entries didn't match the number of fields.\n- Handle rare garbage collection issue after deepcopy (https://github.com/mattijn/topojson/issues/120)\n- Fix bug where records and shapes would be assigned incorrect record number (@karanrn)\n- Fix typos in docs (@timgates)\n\n2.1.3\n\nBug fixes:\n\n- Fix recent bug in geojson hole-in-polygon checking (see #205)\n- Misc fixes to allow geo interface dump to json (eg dates as strings)\n- Handle additional dbf date null values, and return faulty dates as unicode (see #187)\n- Add writer target typecheck\n- Fix bugs to allow reading shp/shx/dbf separately\n- Allow delayed shapefile loading by passing no args\n- Fix error with writing empty z/m shapefile (@mcuprjak)\n- Fix signed_area() so ignores z/m coords\n- Enforce writing the 11th field name character as null-terminator (only first 10 are used)\n- Minor README fixes\n- Added more tests\n\n2.1.2\n\nBug fixes:\n\n- Fix issue where warnings.simplefilter('always') changes global warning behavior [see #203]\n\n2.1.1\n\nImprovements:\n\n- Handle shapes with no coords and represent as geojson with no coords (GeoJSON null-equivalent)\n- Expand testing to Python 3.6, 3.7, 3.8 and PyPy; drop 3.3 and 3.4 [@mwtoews]\n- Added pytest testing [@jmoujaes]\n\nBug fixes:"}, {"name": "pyshp", "tags": ["cli", "data", "dev", "math", "ui", "web"], "summary": "Pure Python read/write support for ESRI Shapefile format", "text": "- Fix incorrect geo interface handling of multipolygons with complex exterior-hole relations [see #202]\n- Enforce shapefile requirement of at least one field, to avoid writing invalid shapefiles [@Jonty]\n- Fix Reader geo interface including DeletionFlag field in feature properties [@nnseva]\n- Fix polygons not being auto closed, which was accidentally dropped\n- Fix error for null geometries in feature geojson\n- Misc docstring cleanup [@fiveham]\n\n2.1.0\n\nNew Features:\n\n- Added back read/write support for unicode field names.\n- Improved Record representation\n- More support for geojson on Reader, ShapeRecord, ShapeRecords, and shapes()\n\nBug fixes:\n\n- Fixed error when reading optional m-values\n- Fixed Record attribute autocomplete in Python 3\n- Misc readme cleanup\n\n2.0.0\n\nThe newest version of PyShp, version 2.0 introduced some major new improvements.\nA great thanks to all who have contributed code and raised issues, and for everyone's\npatience and understanding during the transition period.\nSome of the new changes are incompatible with previous versions.\nUsers of the previous version 1.x should therefore take note of the following changes\n(Note: Some contributor attributions may be missing):\n\nMajor Changes:\n\n- Full support for unicode text, with custom encoding, and exception handling.\n  - Means that the Reader returns unicode, and the Writer accepts unicode.\n- PyShp has been simplified to a pure input-output library using the Reader and Writer classes, dropping the Editor class.\n- Switched to a new streaming approach when writing files, keeping memory-usage at a minimum:\n  - Specify filepath/destination and text encoding when creating the Writer.\n  - The file is written incrementally with each call to shape/record.\n  - Adding shapes is now done using dedicated methods for each shapetype.\n- Reading shapefiles is now more convenient:\n  - Shapefiles can be opened using the context manager, and files are properly closed.\n  - Shapefiles can be iterated, have a length, and supports the geo interface.\n  - New ways of inspecting shapefile metadata by printing. [@megies]\n  - More convenient accessing of Record values as attributes. [@philippkraft]\n  - More convenient shape type name checking. [@megies]\n- Add more support and documentation for MultiPatch 3D shapes.\n- The Reader \"elevation\" and \"measure\" attributes now renamed \"zbox\" and \"mbox\", to make it clear they refer to the min/max values.\n- Better documentation of previously unclear aspects, such as field types.\n\nImportant Fixes:\n\n- More reliable/robust:\n  - Fixed shapefile bbox error for empty or point type shapefiles. [@mcuprjak]\n  - Reading and writing Z and M type shapes is now more robust, fixing many errors, and has been added to the documentation. [@ShinNoNoir]\n  - Improved parsing of field value types, fixed errors and made more flexible.\n  - Fixed bug when writing shapefiles with datefield and date values earlier than 1900 [@megies]\n- Fix some geo interface errors, including checking polygon directions.\n- Bug fixes for reading from case sensitive file names, individual files separately, and from file-like objects. [@gastoneb, @kb003308, @erickskb]\n- Enforce maximum field limit. [@mwtoews]\n\nThe Basics\n\nBefore doing anything you must import the library.\n\n>>> import shapefile"}, {"name": "pyshp", "tags": ["cli", "data", "dev", "math", "ui", "web"], "summary": "Pure Python read/write support for ESRI Shapefile format", "text": "The examples below will use a shapefile created from the U.S. Census Bureau\nBlockgroups data set near San Francisco, CA and available in the git\nrepository of the PyShp GitHub site.\n\nReading Shapefiles\n\nThe Reader Class\n\nReading Shapefiles from Local Files\n\nTo read a shapefile create a new \"Reader\" object and pass it the name of an\nexisting shapefile. The shapefile format is actually a collection of three\nfiles. You specify the base filename of the shapefile or the complete filename\nof any of the shapefile component files.\n\n>>> sf = shapefile.Reader(\"shapefiles/blockgroups\")\n\nOR\n\n>>> sf = shapefile.Reader(\"shapefiles/blockgroups.shp\")\n\nOR\n\n>>> sf = shapefile.Reader(\"shapefiles/blockgroups.dbf\")\n\nOR any of the other 5+ formats which are potentially part of a shapefile. The\nlibrary does not care about file extensions. You can also specify that you only\nwant to read some of the file extensions through the use of keyword arguments:\n\n>>> sf = shapefile.Reader(dbf=\"shapefiles/blockgroups.dbf\")\n\nReading Shapefiles from Zip Files\n\nIf your shapefile is wrapped inside a zip file, the library is able to handle that too, meaning you don't have to worry about unzipping the contents:\n\n>>> sf = shapefile.Reader(\"shapefiles/blockgroups.zip\")\n\nIf the zip file contains multiple shapefiles, just specify which shapefile to read by additionally specifying the relative path after the \".zip\" part:\n\n>>> sf = shapefile.Reader(\"shapefiles/blockgroups_multishapefile.zip/blockgroups2.shp\")\n\nReading Shapefiles from URLs\n\nFinally, you can use all of the above methods to read shapefiles directly from the internet, by giving a url instead of a local path, e.g.:\n\n>>> # from a zipped shapefile on website\n\t>>> sf = shapefile.Reader(\"https://github.com/JamesParrott/PyShp_test_shapefile/raw/main/gis_osm_natural_a_free_1.zip\")\n\n>>> # from a shapefile collection of files in a github repository\n\t>>> sf = shapefile.Reader(\"https://github.com/nvkelso/natural-earth-vector/blob/master/110m_cultural/ne_110m_admin_0_tiny_countries.shp?raw=true\")\n\nThis will automatically download the file(s) to a temporary location before reading, saving you a lot of time and repetitive boilerplate code when you just want quick access to some external data.\n\nReading Shapefiles from File-Like Objects\n\nYou can also load shapefiles from any Python file-like object using keyword\narguments to specify any of the three files. This feature is very powerful and\nallows you to custom load shapefiles from arbitrary storage formats, such as a protected url or zip file, a serialized object, or in some cases a database.\n\n>>> myshp = open(\"shapefiles/blockgroups.shp\", \"rb\")\n\t>>> mydbf = open(\"shapefiles/blockgroups.dbf\", \"rb\")\n\t>>> r = shapefile.Reader(shp=myshp, dbf=mydbf)\n\nNotice in the examples above the shx file is never used. The shx file is a\nvery simple fixed-record index for the variable-length records in the shp\nfile. This file is optional for reading. If it's available PyShp will use the\nshx file to access shape records a little faster but will do just fine without\nit.\n\nReading Shapefiles Using the Context Manager\n\nThe \"Reader\" class can be used as a context manager, to ensure open file\nobjects are properly closed when done reading the data:\n\nReading Shapefile Meta-Data\n\nShapefiles have a number of attributes for inspecting the file contents.\nA shapefile is a container for a specific type of geometry, and this can be checked using the\nshapeType attribute."}, {"name": "pyshp", "tags": ["cli", "data", "dev", "math", "ui", "web"], "summary": "Pure Python read/write support for ESRI Shapefile format", "text": ">>> sf = shapefile.Reader(\"shapefiles/blockgroups.dbf\")\n\t>>> sf.shapeType\n\t5\n\nShape types are represented by numbers between 0 and 31 as defined by the\nshapefile specification and listed below. It is important to note that the numbering system has\nseveral reserved numbers that have not been used yet, therefore the numbers of\nthe existing shape types are not sequential:\n\n- NULL = 0\n- POINT = 1\n- POLYLINE = 3\n- POLYGON = 5\n- MULTIPOINT = 8\n- POINTZ = 11\n- POLYLINEZ = 13\n- POLYGONZ = 15\n- MULTIPOINTZ = 18\n- POINTM = 21\n- POLYLINEM = 23\n- POLYGONM = 25\n- MULTIPOINTM = 28\n- MULTIPATCH = 31\n\nBased on this we can see that our blockgroups shapefile contains\nPolygon type shapes. The shape types are also defined as constants in\nthe shapefile module, so that we can compare types more intuitively:\n\n>>> sf.shapeType == shapefile.POLYGON\n\tTrue\n\nFor convenience, you can also get the name of the shape type as a string:\n\n>>> sf.shapeTypeName == 'POLYGON'\n\tTrue\n\nOther pieces of meta-data that we can check include the number of features\nand the bounding box area the shapefile covers:\n\n>>> len(sf)\n\t663\n\t>>> sf.bbox\n\t(-122.515048, 37.652916, -122.327622, 37.863433)\n\nFinally, if you would prefer to work with the entire shapefile in a different\nformat, you can convert all of it to a GeoJSON dictionary, although you may lose\nsome information in the process, such as z- and m-values:\n\n>>> sf.__geo_interface__['type']\n\t'FeatureCollection'\n\nReading Geometry\n\nA shapefile's geometry is the collection of points or shapes made from\nvertices and implied arcs representing physical locations. All types of\nshapefiles just store points. The metadata about the points determine how they\nare handled by software.\n\nYou can get a list of the shapefile's geometry by calling the shapes()\nmethod.\n\n>>> shapes = sf.shapes()\n\nThe shapes method returns a list of Shape objects describing the geometry of\neach shape record.\n\n>>> len(shapes)\n\t663\n\nTo read a single shape by calling its index use the shape() method. The index\nis the shape's count from 0. So to read the 8th shape record you would use its\nindex which is 7.\n\n>>> s = sf.shape(7)\n\t>>> s\n\tPolygon #7\n\n>>> # Read the bbox of the 8th shape to verify\n\t>>> # Round coordinates to 3 decimal places\n\t>>> ['%.3f' % coord for coord in s.bbox]\n\t['-122.450', '37.801', '-122.442', '37.808']\n\nEach shape record (except Points) contains the following attributes. Records of\nshapeType Point do not have a bounding box 'bbox'.\n\nTODO!!  Fix attributes\n\n>>> for name in dir(shapes[3]):\n\t...     if not name.startswith('_'):\n\t...         name\n\t'bbox'\n\t'from_byte_stream'\n\t'oid'\n\t'parts'\n\t'points'\n\t'shapeType'\n\t'shapeTypeName'\n\t'write_to_byte_stream'\n\n* `oid`: The shape's index position in the original shapefile.\n\n>>> shapes[3].oid\n\t\t3\n\n* `shapeType`: an integer representing the type of shape as defined by the\n\t  shapefile specification.\n\n>>> shapes[3].shapeType\n\t\t5\n\n* `shapeTypeName`: a string representation of the type of shape as defined by shapeType. Read-only.\n\n>>> shapes[3].shapeTypeName\n\t\t'POLYGON'"}, {"name": "pyshp", "tags": ["cli", "data", "dev", "math", "ui", "web"], "summary": "Pure Python read/write support for ESRI Shapefile format", "text": "* `bbox`: If the shape type contains multiple points this tuple describes the\n\t  lower left (x,y) coordinate and upper right corner coordinate creating a\n\t  complete box around the points. If the shapeType is a\n\t  Null (shapeType == 0) then an AttributeError is raised.\n\n>>> # Get the bounding box of the 4th shape.\n\t\t>>> # Round coordinates to 3 decimal places\n\t\t>>> bbox = shapes[3].bbox\n\t\t>>> ['%.3f' % coord for coord in bbox]\n\t\t['-122.486', '37.787', '-122.446', '37.811']\n\n* `parts`: Parts simply group collections of points into shapes. If the shape\n\t  record has multiple parts this attribute contains the index of the first\n\t  point of each part. If there is only one part then a list containing 0 is\n\t  returned.\n\n>>> shapes[3].parts\n\t\t[0]\n\n* `points`: The points attribute contains a list of tuples containing an\n\t  (x,y) coordinate for each point in the shape.\n\n>>> len(shapes[3].points)\n\t\t173\n\t\t>>> # Get the 8th point of the fourth shape\n\t\t>>> # Truncate coordinates to 3 decimal places\n\t\t>>> shape = shapes[3].points[7]\n\t\t>>> ['%.3f' % coord for coord in shape]\n\t\t['-122.471', '37.787']\n\nIn most cases, however, if you need to do more than just type or bounds checking, you may want\nto convert the geometry to the more human-readable [GeoJSON format](http://geojson.org),\nwhere lines and polygons are grouped for you:\n\n>>> s = sf.shape(0)\n\t>>> geoj = s.__geo_interface__\n\t>>> geoj[\"type\"]\n\t'MultiPolygon'\n\nThe results from the shapes() method similarly supports converting to GeoJSON:\n\n>>> shapes.__geo_interface__['type']\n\t'GeometryCollection'\n\nNote: In some cases, if the conversion from shapefile geometry to GeoJSON encountered any problems\nor potential issues, a warning message will be displayed with information about the affected\ngeometry. To ignore or suppress these warnings, you can disable this behavior by setting the\nmodule constant VERBOSE to False:\n\n>>> shapefile.VERBOSE = False\n\nReading Records\n\nA record in a shapefile contains the attributes for each shape in the\ncollection of geometries. Records are stored in the dbf file. The link between\ngeometry and attributes is the foundation of all geographic information systems.\nThis critical link is implied by the order of shapes and corresponding records\nin the shp geometry file and the dbf attribute file.\n\nThe field names of a shapefile are available as soon as you read a shapefile.\nYou can call the \"fields\" attribute of the shapefile as a Python list. Each\nfield is a Python namedtuple (Field) with the following information:"}, {"name": "pyshp", "tags": ["cli", "data", "dev", "math", "ui", "web"], "summary": "Pure Python read/write support for ESRI Shapefile format", "text": "* name: the name describing the data at this column index (a string).\n  * field_type: a FieldType enum member determining the type of data at this column index. Names can be:\n\t   * \"N\": Numbers, with or without decimals.\n\t   * \"F\": Floats (same as \"N\").\n\t   * \"L\": Logical, for boolean True/False values.\n\t   * \"D\": Dates.\n\t   * \"M\": Memo, has no meaning within a GIS and is part of the xbase spec instead.\n  * size: Field length: the length of the data found at this column index. Older GIS\n\t   software may truncate this length to 8 or 11 characters for \"Character\"\n\t   fields.\n  * deci: Decimal length. The number of decimal places found in \"Number\" fields.\n\nA new field can be created directly from the type enum member etc., or as follows:\n\n>>> shapefile.Field.from_unchecked(\"Population\", \"N\", 10,0)\n\tField(name=\"Population\", field_type=FieldType.N, size=10, decimal=0)\n\nUsing this method the conversion from string to enum is done automatically.\n\nTo see the fields for the Reader object above (sf) call the \"fields\"\nattribute:\n\n>>> sf.fields\n\t[Field(name=\"DeletionFlag\", field_type=FieldType.C, size=1, decimal=0), Field(name=\"AREA\", field_type=FieldType.N, size=18, decimal=5), Field(name=\"BKG_KEY\", field_type=FieldType.C, size=12, decimal=0), Field(name=\"POP1990\", field_type=FieldType.N, size=9, decimal=0), Field(name=\"POP90_SQMI\", field_type=FieldType.N, size=10, decimal=1), Field(name=\"HOUSEHOLDS\", field_type=FieldType.N, size=9, decimal=0), Field(name=\"MALES\", field_type=FieldType.N, size=9, decimal=0), Field(name=\"FEMALES\", field_type=FieldType.N, size=9, decimal=0), Field(name=\"WHITE\", field_type=FieldType.N, size=9, decimal=0), Field(name=\"BLACK\", field_type=FieldType.N, size=8, decimal=0), Field(name=\"AMERI_ES\", field_type=FieldType.N, size=7, decimal=0), Field(name=\"ASIAN_PI\", field_type=FieldType.N, size=8, decimal=0), Field(name=\"OTHER\", field_type=FieldType.N, size=8, decimal=0), Field(name=\"HISPANIC\", field_type=FieldType.N, size=8, decimal=0), Field(name=\"AGE_UNDER5\", field_type=FieldType.N, size=8, decimal=0), Field(name=\"AGE_5_17\", field_type=FieldType.N, size=8, decimal=0), Field(name=\"AGE_18_29\", field_type=FieldType.N, size=8, decimal=0), Field(name=\"AGE_30_49\", field_type=FieldType.N, size=8, decimal=0), Field(name=\"AGE_50_64\", field_type=FieldType.N, size=8, decimal=0), Field(name=\"AGE_65_UP\", field_type=FieldType.N, size=8, decimal=0), Field(name=\"NEVERMARRY\", field_type=FieldType.N, size=8, decimal=0), Field(name=\"MARRIED\", field_type=FieldType.N, size=9, decimal=0), Field(name=\"SEPARATED\", field_type=FieldType.N, size=7, decimal=0), Field(name=\"WIDOWED\", field_type=FieldType.N, size=8, decimal=0), Field(name=\"DIVORCED\", field_type=FieldType.N, size=8, decimal=0), Field(name=\"HSEHLD_1_M\", field_type=FieldType.N, size=8, decimal=0), Field(name=\"HSEHLD_1_F\", field_type=FieldType.N, size=8, decimal=0), Field(name=\"MARHH_CHD\", field_type=FieldType.N, size=8, decimal=0), Field(name=\"MARHH_NO_C\", field_type=FieldType.N, size=8, decimal=0), Field(name=\"MHH_CHILD\", field_type=FieldType.N, size=7, decimal=0), Field(name=\"FHH_CHILD\", field_type=FieldType.N, size=7, decimal=0), Field(name=\"HSE_UNITS\", field_type=FieldType.N, size=9, decimal=0), Field(name=\"VACANT\", field_type=FieldType.N, size=7, decimal=0), Field(name=\"OWNER_OCC\", field_type=FieldType.N, size=8, decimal=0), Field(name=\"RENTER_OCC\", field_type=FieldType.N, size=8, decimal=0), Field(name=\"MEDIAN_VAL\", field_type=FieldType.N, size=7, decimal=0), Field(name=\"MEDIANRENT\", field_type=FieldType.N, size=4, decimal=0), Field(name=\"UNITS_1DET\", field_type=FieldType.N, size=8, decimal=0), Field(name=\"UNITS_1ATT\", field_type=FieldType.N, size=7, decimal=0), Field(name=\"UNITS2\", field_type=FieldType.N, size=7, decimal=0), Field(name=\"UNITS3_9\", field_type=FieldType.N, size=8, decimal=0), Field(name=\"UNITS10_49\", field_type=FieldType.N, size=8, decimal=0), Field(name=\"UNITS50_UP\", field_type=FieldType.N, size=8, decimal=0), Field(name=\"MOBILEHOME\", field_type=FieldType.N, size=7, decimal=0)]\n\nThe first field of a dbf file is always a 1-byte field called \"DeletionFlag\",\nwhich indicates records that have been deleted but not removed. However,\nsince this flag is very rarely used, PyShp currently will return all records\nregardless of their deletion flag, and the flag is also not included in the list of\nrecord values. In other words, the DeletionFlag field has no real purpose, and\nshould in most cases be ignored. For instance, to get a list of all fieldnames:\n\n>>> fieldnames = [f[0] for f in sf.fields[1:]]\n\nYou can get a list of the shapefile's records by calling the records() method:\n\n>>> records = sf.records()\n\n>>> len(records)\n\t663\n\nTo read a single record call the record() method with the record's index:\n\n>>> rec = sf.record(3)"}, {"name": "pyshp", "tags": ["cli", "data", "dev", "math", "ui", "web"], "summary": "Pure Python read/write support for ESRI Shapefile format", "text": "Each record is a list-like Record object containing the values corresponding to each field in\nthe field list (except the DeletionFlag). A record's values can be accessed by positional indexing or slicing.\nFor example in the blockgroups shapefile the 2nd and 3rd fields are the blockgroup id\nand the 1990 population count of that San Francisco blockgroup:\n\n>>> rec[1:3]\n\t['060750601001', 4715]\n\nFor simpler access, the fields of a record can also accessed via the name of the field,\neither as a key or as an attribute name. The blockgroup id (BKG_KEY) of the blockgroups shapefile\ncan also be retrieved as:\n\nThe record values can be easily integrated with other programs by converting it to a field-value dictionary:\n\n>>> dct = rec.as_dict()\n\t>>> sorted(dct.items())\n\t[('AGE_18_29', 1467), ('AGE_30_49', 1681), ('AGE_50_64', 92), ('AGE_5_17', 848), ('AGE_65_UP', 30), ('AGE_UNDER5', 597), ('AMERI_ES', 6), ('AREA', 2.34385), ('ASIAN_PI', 452), ('BKG_KEY', '060750601001'), ('BLACK', 1007), ('DIVORCED', 149), ('FEMALES', 2095), ('FHH_CHILD', 16), ('HISPANIC', 416), ('HOUSEHOLDS', 1195), ('HSEHLD_1_F', 40), ('HSEHLD_1_M', 22), ('HSE_UNITS', 1258), ('MALES', 2620), ('MARHH_CHD', 79), ('MARHH_NO_C', 958), ('MARRIED', 2021), ('MEDIANRENT', 739), ('MEDIAN_VAL', 337500), ('MHH_CHILD', 0), ('MOBILEHOME', 0), ('NEVERMARRY', 703), ('OTHER', 288), ('OWNER_OCC', 66), ('POP1990', 4715), ('POP90_SQMI', 2011.6), ('RENTER_OCC', 3733), ('SEPARATED', 49), ('UNITS10_49', 49), ('UNITS2', 160), ('UNITS3_9', 672), ('UNITS50_UP', 0), ('UNITS_1ATT', 302), ('UNITS_1DET', 43), ('VACANT', 93), ('WHITE', 2962), ('WIDOWED', 37)]\n\nIf at a later point you need to check the record's index position in the original\nshapefile, you can do this through the \"oid\" attribute:\n\n>>> rec.oid\n\t3\n\nReading Geometry and Records Simultaneously\n\nYou may want to examine both the geometry and the attributes for a record at\nthe same time. The shapeRecord() and shapeRecords() method let you do just\nthat.\n\nCalling the shapeRecords() method will return the geometry and attributes for\nall shapes as a list of ShapeRecord objects. Each ShapeRecord instance has a\n\"shape\" and \"record\" attribute. The shape attribute is a Shape object as\ndiscussed in the first section \"Reading Geometry\". The record attribute is a\nlist-like object containing field values as demonstrated in the \"Reading Records\" section.\n\n>>> shapeRecs = sf.shapeRecords()\n\nLet's read the blockgroup key and the population for the 4th blockgroup:\n\n>>> shapeRecs[3].record[1:3]\n\t['060750601001', 4715]\n\nThe results from the shapeRecords() method is a list-like object that can be easily converted\nto GeoJSON through the _\\_geo_interface\\_\\_:\n\n>>> shapeRecs.__geo_interface__['type']\n\t'FeatureCollection'\n\nThe shapeRecord() method reads a single shape/record pair at the specified index.\nTo get the 4th shape record from the blockgroups shapefile use the third index:\n\n>>> shapeRec = sf.shapeRecord(3)\n\t>>> shapeRec.record[1:3]\n\t['060750601001', 4715]\n\nEach individual shape record also supports the _\\_geo_interface\\_\\_ to convert it to a GeoJSON feature:\n\n>>> shapeRec.__geo_interface__['type']\n\t'Feature'\n\nWriting Shapefiles\n\nThe Writer Class\n\nPyShp tries to be as flexible as possible when writing shapefiles while\nmaintaining some degree of automatic validation to make sure you don't\naccidentally write an invalid file."}, {"name": "pyshp", "tags": ["cli", "data", "dev", "math", "ui", "web"], "summary": "Pure Python read/write support for ESRI Shapefile format", "text": "PyShp can write just one of the component files such as the shp or dbf file\nwithout writing the others. So in addition to being a complete shapefile\nlibrary, it can also be used as a basic dbf (xbase) library. Dbf files are a\ncommon database format which are often useful as a standalone simple database\nformat. And even shp files occasionally have uses as a standalone format. Some\nweb-based GIS systems use an user-uploaded shp file to specify an area of\ninterest. Many precision agriculture chemical field sprayers also use the shp\nformat as a control file for the sprayer system (usually in combination with\ncustom database file formats).\n\nWriting Shapefiles to Local Files\n\nTo create a shapefile you begin by initiating a new Writer instance, passing it\nthe file path and name to save to:\n\n>>> w = shapefile.Writer('shapefiles/test/testfile')\n\t>>> w.field('field1', 'C')\n\nFile extensions are optional when reading or writing shapefiles. If you specify\nthem PyShp ignores them anyway. When you save files you can specify a base\nfile name that is used for all three file types. Or you can specify a name for\none or more file types:\n\n>>> w = shapefile.Writer(dbf='shapefiles/test/onlydbf.dbf')\n\t>>> w.field('field1', 'C')\n\nIn that case, any file types not assigned will not\nsave and only file types with file names will be saved.\n\nWriting Shapefiles to File-Like Objects\n\nJust as you can read shapefiles from python file-like objects you can also\nwrite to them:\n\n>>> try:\n\t...     from StringIO import StringIO\n\t... except ImportError:\n\t...     from io import BytesIO as StringIO\n\t>>> shp = StringIO()\n\t>>> shx = StringIO()\n\t>>> dbf = StringIO()\n\t>>> w = shapefile.Writer(shp=shp, shx=shx, dbf=dbf)\n\t>>> w.field('field1', 'C')\n\t>>> w.record()\n\t>>> w.null()\n\t>>> w.close()\n\n>>> # To read back the files you could call the \"StringIO.getvalue()\" method later.\n\t>>> assert shp.getvalue()\n\t>>> assert shx.getvalue()\n\t>>> assert dbf.getvalue()\n\n>>> # In fact, you can read directly from them using the Reader\n\t>>> r = shapefile.Reader(shp=shp, shx=shx, dbf=dbf)\n\t>>> len(r)\n\t1\n\nWriting Shapefiles Using the Context Manager\n\nThe \"Writer\" class automatically closes the open files and writes the final headers once it is garbage collected.\nIn case of a crash and to make the code more readable, it is nevertheless recommended\nyou do this manually by calling the \"close()\" method:\n\n>>> w.close()\n\nAlternatively, you can also use the \"Writer\" class as a context manager, to ensure open file\nobjects are properly closed and final headers written once you exit the with-clause:\n\n>>> with shapefile.Writer(\"shapefiles/test/contextwriter\") as w:\n\t... \tw.field('field1', 'C')\n\t... \tpass\n\nSetting the Shape Type\n\nThe shape type defines the type of geometry contained in the shapefile. All of\nthe shapes must match the shape type setting.\n\nThere are three ways to set the shape type:\n  * Set it when creating the class instance.\n  * Set it by assigning a value to an existing class instance.\n  * Set it automatically to the type of the first non-null shape by saving the shapefile."}, {"name": "pyshp", "tags": ["cli", "data", "dev", "math", "ui", "web"], "summary": "Pure Python read/write support for ESRI Shapefile format", "text": "To manually set the shape type for a Writer object when creating the Writer:\n\n>>> w = shapefile.Writer('shapefiles/test/shapetype', shapeType=3)\n\t>>> w.field('field1', 'C')\n\n>>> w.shapeType\n\t3\n\nOR you can set it after the Writer is created:\n\n>>> w.shapeType = 1\n\n>>> w.shapeType\n\t1\n\nAdding Records\n\nBefore you can add records you must first create the fields that define what types of\nvalues will go into each attribute.\n\nThere are several different field types, all of which support storing None values as NULL.\n\nText fields are created using the 'C' type, and the third 'size' argument can be customized to the expected\nlength of text values to save space:\n\n>>> w = shapefile.Writer('shapefiles/test/dtype')\n\t>>> w.field('TEXT', 'C')\n\t>>> w.field('SHORT_TEXT', 'C', size=5)\n\t>>> w.field('LONG_TEXT', 'C', size=250)\n\t>>> w.null()\n\t>>> w.record('Hello', 'World', 'World'*50)\n\t>>> w.close()\n\n>>> r = shapefile.Reader('shapefiles/test/dtype')\n\t>>> assert r.record(0) == ['Hello', 'World', 'World'*50]\n\nDate fields are created using the 'D' type, and can be created using either\ndate objects, lists, or a YYYYMMDD formatted string.\nField length or decimal have no impact on this type:\n\n>>> from datetime import date\n\t>>> w = shapefile.Writer('shapefiles/test/dtype')\n\t>>> w.field('DATE', 'D')\n\t>>> w.null()\n\t>>> w.null()\n\t>>> w.null()\n\t>>> w.null()\n\t>>> w.record(date(1898,1,30))\n\t>>> w.record([1998,1,30])\n\t>>> w.record('19980130')\n\t>>> w.record(None)\n\t>>> w.close()\n\n>>> r = shapefile.Reader('shapefiles/test/dtype')\n\t>>> assert r.record(0) == [date(1898,1,30)]\n\t>>> assert r.record(1) == [date(1998,1,30)]\n\t>>> assert r.record(2) == [date(1998,1,30)]\n\t>>> assert r.record(3) == [None]\n\nNumeric fields are created using the 'N' type (or the 'F' type, which is exactly the same).\nBy default the fourth decimal argument is set to zero, essentially creating an integer field.\nTo store floats you must set the decimal argument to the precision of your choice.\nTo store very large numbers you must increase the field length size to the total number of digits\n(including comma and minus).\n\n>>> w = shapefile.Writer('shapefiles/test/dtype')\n\t>>> w.field('INT', 'N')\n\t>>> w.field('LOWPREC', 'N', decimal=2)\n\t>>> w.field('MEDPREC', 'N', decimal=10)\n\t>>> w.field('HIGHPREC', 'N', decimal=30)\n\t>>> w.field('FTYPE', 'F', decimal=10)\n\t>>> w.field('LARGENR', 'N', 101)\n\t>>> nr = 1.3217328\n\t>>> w.null()\n\t>>> w.null()\n\t>>> w.record(INT=nr, LOWPREC=nr, MEDPREC=nr, HIGHPREC=-3.2302e-25, FTYPE=nr, LARGENR=int(nr)*10**100)\n\t>>> w.record(None, None, None, None, None, None)\n\t>>> w.close()\n\n>>> r = shapefile.Reader('shapefiles/test/dtype')\n\t>>> assert r.record(0) == [1, 1.32, 1.3217328, -3.2302e-25, 1.3217328, 10000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000]\n\t>>> assert r.record(1) == [None, None, None, None, None, None]\n\nFinally, we can create boolean fields by setting the type to 'L'.\nThis field can take True or False values, or 1 (True) or 0 (False).\nNone is interpreted as missing.\n\n>>> w = shapefile.Writer('shapefiles/test/dtype')\n\t>>> w.field('BOOLEAN', 'L')\n\t>>> w.null()\n\t>>> w.null()\n\t>>> w.null()\n\t>>> w.null()\n\t>>> w.null()\n\t>>> w.null()\n\t>>> w.record(True)\n\t>>> w.record(1)\n\t>>> w.record(False)\n\t>>> w.record(0)\n\t>>> w.record(None)\n\t>>> w.record(\"Nonsense\")\n\t>>> w.close()\n\n>>> r = shapefile.Reader('shapefiles/test/dtype')\n\t>>> r.record(0)\n\tRecord #0: [True]\n\t>>> r.record(1)\n\tRecord #1: [True]\n\t>>> r.record(2)\n\tRecord #2: [False]\n\t>>> r.record(3)\n\tRecord #3: [False]\n\t>>> r.record(4)\n\tRecord #4: [None]\n\t>>> r.record(5)\n\tRecord #5: [None]\n\nYou can also add attributes using keyword arguments where the keys are field names."}, {"name": "pyshp", "tags": ["cli", "data", "dev", "math", "ui", "web"], "summary": "Pure Python read/write support for ESRI Shapefile format", "text": ">>> w = shapefile.Writer('shapefiles/test/dtype')\n\t>>> w.field('FIRST_FLD','C', 40)\n\t>>> w.field('SECOND_FLD','C', 40)\n\t>>> w.null()\n\t>>> w.null()\n\t>>> w.record('First', 'Line')\n\t>>> w.record(FIRST_FLD='First', SECOND_FLD='Line')\n\t>>> w.close()\n\nAdding Geometry\n\nGeometry is added using one of several convenience methods. The \"null\" method is used\nfor null shapes, \"point\" is used for point shapes, \"multipoint\" is used for multipoint shapes, \"line\" for lines,\n\"poly\" for polygons.\n\n**Adding a Null shape**\n\nA shapefile may contain some records for which geometry is not available, and may be set using the \"null\" method.\nBecause Null shape types (shape type 0) have no geometry the \"null\" method is called without any arguments.\n\n>>> w = shapefile.Writer('shapefiles/test/null')\n\t>>> w.field('name', 'C')\n\n>>> w.null()\n\t>>> w.record('nullgeom')\n\n>>> w.close()\n\n**Adding a Point shape**\n\nPoint shapes are added using the \"point\" method. A point is specified by an x and\ny value.\n\n>>> w = shapefile.Writer('shapefiles/test/point')\n\t>>> w.field('name', 'C')\n\n>>> w.point(122, 37)\n\t>>> w.record('point1')\n\n>>> w.close()\n\n**Adding a MultiPoint shape**\n\nIf your point data allows for the possibility of multiple points per feature, use \"multipoint\" instead.\nThese are specified as a list of xy point coordinates.\n\n>>> w = shapefile.Writer('shapefiles/test/multipoint')\n\t>>> w.field('name', 'C')\n\n>>> w.multipoint([[122,37], [124,32]])\n\t>>> w.record('multipoint1')\n\n>>> w.close()\n\n**Adding a LineString shape**\n\nFor LineString shapefiles, each shape is given as a list of one or more linear features.\nEach of the linear features must have at least two points.\n\n>>> w = shapefile.Writer('shapefiles/test/line')\n\t>>> w.field('name', 'C')\n\n>>> w.line([\n\t...\t\t\t[[1,5],[5,5],[5,1],[3,3],[1,1]], # line 1\n\t...\t\t\t[[3,2],[2,6]] # line 2\n\t...\t\t\t])\n\n>>> w.record('linestring1')\n\n>>> w.close()\n\n**Adding a Polygon shape**\n\nSimilarly to LineString, Polygon shapes consist of multiple polygons, and must be given as a list of polygons.\nThe main difference is that polygons must have at least 4 points and the last point must be the same as the first.\nIt's also okay if you forget to repeat the first point at the end; PyShp automatically checks and closes the polygons\nif you don't.\n\nIt's important to note that for Polygon shapefiles, your polygon coordinates must be ordered in a clockwise direction.\nIf any of the polygons have holes, then the hole polygon coordinates must be ordered in a counterclockwise direction.\nThe direction of your polygons determines how shapefile readers will distinguish between polygon outlines and holes.\n\n>>> w = shapefile.Writer('shapefiles/test/polygon')\n\t>>> w.field('name', 'C')\n\n>>> w.poly([\n\t...\t        [[113,24], [112,32], [117,36], [122,37], [118,20]], # poly 1\n\t...\t        [[116,29],[116,26],[119,29],[119,32]], # hole 1\n\t...         [[15,2], [17,6], [22,7]]  # poly 2\n\t...        ])\n\t>>> w.record('polygon1')\n\n>>> w.close()\n\n**Adding from an existing Shape object**\n\nFinally, geometry can be added by passing an existing \"Shape\" object to the \"shape\" method.\nYou can also pass it any GeoJSON dictionary or _\\_geo_interface\\_\\_ compatible object.\nThis can be particularly useful for copying from one file to another:\n\n>>> r = shapefile.Reader('shapefiles/test/polygon')\n\n>>> w = shapefile.Writer('shapefiles/test/copy')\n\t>>> w.fields = r.fields[1:] # skip first deletion field\n\n>>> # adding existing Shape objects\n\t>>> for shaperec in r.iterShapeRecords():\n\t...     w.record(*shaperec.record)\n\t...     w.shape(shaperec.shape)\n\n>>> # or GeoJSON dicts\n\t>>> for shaperec in r.iterShapeRecords():\n\t...     w.record(*shaperec.record)\n\t...     w.shape(shaperec.shape.__geo_interface__)"}, {"name": "pyshp", "tags": ["cli", "data", "dev", "math", "ui", "web"], "summary": "Pure Python read/write support for ESRI Shapefile format", "text": ">>> w.close()\n\nGeometry and Record Balancing\n\nBecause every shape must have a corresponding record it is critical that the\nnumber of records equals the number of shapes to create a valid shapefile. You\nmust take care to add records and shapes in the same order so that the record\ndata lines up with the geometry data. For example:\n\n>>> w = shapefile.Writer('shapefiles/test/balancing', shapeType=shapefile.POINT)\n\t>>> w.field(\"field1\", \"C\")\n\t>>> w.field(\"field2\", \"C\")\n\n>>> w.record(\"row\", \"one\")\n\t>>> w.point(1, 1)\n\n>>> w.record(\"row\", \"two\")\n\t>>> w.point(2, 2)\n\nTo help prevent accidental misalignment PyShp has an \"auto balance\" feature to\nmake sure when you add either a shape or a record the two sides of the\nequation line up. This way if you forget to update an entry the\nshapefile will still be valid and handled correctly by most shapefile\nsoftware. Autobalancing is NOT turned on by default. To activate it set\nthe attribute autoBalance to 1 or True:\n\n>>> w.record(\"row\", \"three\")\n\t>>> w.record(\"row\", \"four\")\n\t>>> w.point(4, 4)\n\n>>> w.recNum == w.shpNum\n\tTrue\n\nYou also have the option of manually calling the balance() method at any time\nto ensure the other side is up to date. When balancing is used\nnull shapes are created on the geometry side or records\nwith a value of \"NULL\" for each field is created on the attribute side.\nThis gives you flexibility in how you build the shapefile.\nYou can create all of the shapes and then create all of the records or vice versa.\n\n>>> w.record(\"row\", \"five\")\n\t>>> w.record(\"row\", \"six\")\n\t>>> w.record(\"row\", \"seven\")\n\t>>> w.point(5, 5)\n\t>>> w.point(6, 6)\n\t>>> w.balance()\n\n>>> w.recNum == w.shpNum\n\tTrue\n\nIf you do not use the autoBalance() or balance() method and forget to manually\nbalance the geometry and attributes the shapefile will be viewed as corrupt by\nmost shapefile software.\n\nWriting .prj files\nA .prj file, or projection file, is a simple text file that stores a shapefile's map projection and coordinate reference system to help mapping software properly locate the geometry on a map. If you don't have one, you may get confusing errors when you try and use the shapefile you created. The GIS software may complain that it doesn't know the shapefile's projection and refuse to accept it, it may assume the shapefile is the same projection as the rest of your GIS project and put it in the wrong place, or it might assume the coordinates are an offset in meters from latitude and longitude 0,0 which will put your data in the middle of the ocean near Africa. The text in the .prj file is a [Well-Known-Text (WKT) projection string](https://en.wikipedia.org/wiki/Well-known_text_representation_of_coordinate_reference_systems). Projection strings can be quite long so they are often referenced using numeric codes call EPSG codes. The .prj file must have the same base name as your shapefile. So for example if you have a shapefile named \"myPoints.shp\", the .prj file must be named \"myPoints.prj\"."}, {"name": "pyshp", "tags": ["cli", "data", "dev", "math", "ui", "web"], "summary": "Pure Python read/write support for ESRI Shapefile format", "text": "If you're using the same projection over and over, the following is a simple way to create the .prj file assuming your base filename is stored in a variable called \"filename\":\n\nIf you need to dynamically fetch WKT projection strings, you can use the pure Python [PyCRS](https://github.com/karimbahgat/PyCRS) module which has a number of useful features.\n\nAdvanced Use\n\nCommon Errors and Fixes\n\nBelow we list some commonly encountered errors and ways to fix them.\n\nWarnings and Logging\n\nBy default, PyShp chooses to be transparent and provide the user with logging information and warnings about non-critical issues when reading or writing shapefiles. This behavior is controlled by the module constant `VERBOSE` (which defaults to True). If you would rather suppress this information, you can simply set this to False:\n\n>>> shapefile.VERBOSE = False\n\nAll logging happens under the namespace `shapefile`. So another way to suppress all PyShp warnings would be to alter the logging behavior for that namespace:\n\n>>> import logging\n\t>>> logging.getLogger('shapefile').setLevel(logging.ERROR)\n\nShapefile Encoding Errors\n\nPyShp supports reading and writing shapefiles in any language or character encoding, and provides several options for decoding and encoding text.\nMost shapefiles are written in UTF-8 encoding, PyShp's default encoding, so in most cases you don't have to specify the encoding.\nIf you encounter an encoding error when reading a shapefile, this means the shapefile was likely written in a non-utf8 encoding.\nFor instance, when working with English language shapefiles, a common reason for encoding errors is that the shapefile was written in Latin-1 encoding.\nFor reading shapefiles in any non-utf8 encoding, such as Latin-1, just\nsupply the encoding option when creating the Reader class.\n\n>>> r = shapefile.Reader(\"shapefiles/test/latin1.shp\", encoding=\"latin1\")\n\t>>> r.record(0) == [2, u'\u00d1and\u00fa']\n\tTrue\n\nOnce you have loaded the shapefile, you may choose to save it using another more supportive encoding such\nas UTF-8. Assuming the new encoding supports the characters you are trying to write, reading it back in\nshould give you the same unicode string you started with.\n\n>>> w = shapefile.Writer(\"shapefiles/test/latin_as_utf8.shp\", encoding=\"utf8\")\n\t>>> w.fields = r.fields[1:]\n\t>>> w.record(*r.record(0))\n\t>>> w.null()\n\t>>> w.close()\n\n>>> r = shapefile.Reader(\"shapefiles/test/latin_as_utf8.shp\", encoding=\"utf8\")\n\t>>> r.record(0) == [2, u'\u00d1and\u00fa']\n\tTrue\n\nIf you supply the wrong encoding and the string is unable to be decoded, PyShp will by default raise an\nexception. If however, on rare occasion, you are unable to find the correct encoding and want to ignore\nor replace encoding errors, you can specify the \"encodingErrors\" to be used by the decode method. This\napplies to both reading and writing.\n\n>>> r = shapefile.Reader(\"shapefiles/test/latin1.shp\", encoding=\"ascii\", encodingErrors=\"replace\")\n\t>>> r.record(0) == [2, u'\ufffdand\ufffd']\n\tTrue\n\nReading Large Shapefiles\n\nDespite being a lightweight library, PyShp is designed to be able to read shapefiles of any size, allowing you to work with hundreds of thousands or even millions\nof records and complex geometries.\n\nIterating through a shapefile\n\nAs an example, let's load this Natural Earth shapefile of more than 4000 global administrative boundary polygons:\n\n>>> sf = shapefile.Reader(\"https://github.com/nvkelso/natural-earth-vector/blob/master/10m_cultural/ne_10m_admin_1_states_provinces?raw=true\")"}, {"name": "pyshp", "tags": ["cli", "data", "dev", "math", "ui", "web"], "summary": "Pure Python read/write support for ESRI Shapefile format", "text": "When first creating the Reader class, the library only reads the header information\nand leaves the rest of the file contents alone. Once you call the records() and shapes()\nmethods however, it will attempt to read the entire file into memory at once.\nFor very large files this can result in MemoryError. So when working with large files\nit is recommended to use instead the iterShapes(), iterRecords(), or iterShapeRecords()\nmethods instead. These iterate through the file contents one at a time, enabling you to loop\nthrough them while keeping memory usage at a minimum.\n\n>>> for shape in sf.iterShapes():\n\t...     # do something here\n\t...     pass\n\n>>> for rec in sf.iterRecords():\n\t...     # do something here\n\t...     pass\n\n>>> for shapeRec in sf.iterShapeRecords():\n\t...     # do something here\n\t...     pass\n\n>>> for shapeRec in sf: # same as iterShapeRecords()\n\t...     # do something here\n\t...     pass\n\nLimiting which fields to read\n\nBy default when reading the attribute records of a shapefile, pyshp unpacks and returns the data for all of the dbf fields, regardless of whether you actually need that data or not. To limit which field data is unpacked when reading each record and speed up processing time, you can specify the `fields` argument to any of the methods involving record data. Note that the order of the specified fields does not matter, the resulting records will list the specified field values in the order that they appear in the original dbf file. For instance, if we are only interested in the country and name of each admin unit, the following is a more efficient way of iterating through the file:\n\n>>> fields = [\"geonunit\", \"name\"]\n\t>>> for rec in sf.iterRecords(fields=fields):\n\t... \t# do something\n\t... \tpass\n\t>>> rec\n\tRecord #4595: ['Birgu', 'Malta']\n\nAttribute filtering\n\nIn many cases, we aren't interested in all entries of a shapefile, but rather only want to retrieve a small subset of records by filtering on some attribute. To avoid wasting time reading records and shapes that we don't need, we can start by iterating only the records and fields of interest, check if the record matches some condition as a way to filter the data, and finally load the full record and shape geometry for those that meet the condition:\n\n>>> filter_field = \"geonunit\"\n\t>>> filter_value = \"Eritrea\"\n\t>>> for rec in sf.iterRecords(fields=[filter_field]):\n\t...     if rec[filter_field] == filter_value:\n\t... \t\t# load full record and shape\n\t... \t\tshapeRec = sf.shapeRecord(rec.oid)\n\t... \t\tshapeRec.record[\"name\"]\n\t'Debubawi Keyih Bahri'\n\t'Debub'\n\t'Semenawi Keyih Bahri'\n\t'Gash Barka'\n\t'Maekel'\n\t'Anseba'\n\nSelectively reading only the necessary data in this way is particularly useful for efficiently processing a limited subset of data from very large files or when looping through a large number of files, especially if they contain large attribute tables or complex shape geometries.\n\nSpatial filtering"}, {"name": "pyshp", "tags": ["cli", "data", "dev", "math", "ui", "web"], "summary": "Pure Python read/write support for ESRI Shapefile format", "text": "Another common use-case is that we only want to read those records that are located in some region of interest. Because the shapefile stores the bounding box of each shape separately from the geometry data, it's possible to quickly retrieve all shapes that might overlap a given bounding box region without having to load the full shape geometry data for every shape. This can be done by specifying the `bbox` argument to the shapes, iterShapes, or iterShapeRecords methods:\n\n>>> bbox = [36.423, 12.360, 43.123, 18.004] # ca bbox of Eritrea\n\t>>> fields = [\"geonunit\",\"name\"]\n\t>>> for shapeRec in sf.iterShapeRecords(bbox=bbox, fields=fields):\n\t... \tshapeRec.record\n\tRecord #368: ['Afar', 'Ethiopia']\n\tRecord #369: ['Tadjourah', 'Djibouti']\n\tRecord #375: ['Obock', 'Djibouti']\n\tRecord #376: ['Debubawi Keyih Bahri', 'Eritrea']\n\tRecord #1106: ['Amhara', 'Ethiopia']\n\tRecord #1107: ['Gedarif', 'Sudan']\n\tRecord #1108: ['Tigray', 'Ethiopia']\n\tRecord #1414: ['Sa`dah', 'Yemen']\n\tRecord #1415: ['`Asir', 'Saudi Arabia']\n\tRecord #1416: ['Hajjah', 'Yemen']\n\tRecord #1417: ['Jizan', 'Saudi Arabia']\n\tRecord #1598: ['Debub', 'Eritrea']\n\tRecord #1599: ['Red Sea', 'Sudan']\n\tRecord #1600: ['Semenawi Keyih Bahri', 'Eritrea']\n\tRecord #1601: ['Gash Barka', 'Eritrea']\n\tRecord #1602: ['Kassala', 'Sudan']\n\tRecord #1603: ['Maekel', 'Eritrea']\n\tRecord #2037: ['Al Hudaydah', 'Yemen']\n\tRecord #3741: ['Anseba', 'Eritrea']\n\nThis functionality means that shapefiles can be used as a bare-bones spatially indexed database, with very fast bounding box queries for even the largest of shapefiles. Note that, as with all spatial indexing, this method does not guarantee that the *geometries* of the resulting matches overlap the queried region, only that their *bounding boxes* overlap.\n\nWriting large shapefiles\n\nSimilar to the Reader class, the shapefile Writer class uses a streaming approach to keep memory\nusage at a minimum and allow writing shapefiles of arbitrarily large sizes. The library takes care of this under-the-hood by immediately\nwriting each geometry and record to disk the moment they\nare added using shape() or record(). Once the writer is closed, exited, or garbage\ncollected, the final header information is calculated and written to the beginning of\nthe file.\n\nMerging multiple shapefiles\n\nThis means that it's possible to merge hundreds or thousands of shapefiles, as\nlong as you iterate through the source files to avoid loading everything into\nmemory. The following example copies the contents of a shapefile to a new file 10 times:\n\n>>> # create writer\n\t>>> w = shapefile.Writer('shapefiles/test/merge')\n\n>>> # copy over fields from the reader\n\t>>> r = shapefile.Reader(\"shapefiles/blockgroups\")\n\t>>> for field in r.fields[1:]:\n\t...     w.field(*field)\n\n>>> # copy the shapefile to writer 10 times\n\t>>> repeat = 10\n\t>>> for i in range(repeat):\n\t...     r = shapefile.Reader(\"shapefiles/blockgroups\")\n\t...     for shapeRec in r.iterShapeRecords():\n\t...         w.record(*shapeRec.record)\n\t...         w.shape(shapeRec.shape)\n\n>>> # check that the written file is 10 times longer\n\t>>> len(w) == len(r) * 10\n\tTrue\n\n>>> # close the writer\n\t>>> w.close()\n\nIn this trivial example, we knew that all files had the exact same field names, ordering, and types. In other scenarios, you will have to additionally make sure that all shapefiles have the exact same fields in the same order, and that they all contain the same geometry type.\n\nEditing shapefiles"}, {"name": "pyshp", "tags": ["cli", "data", "dev", "math", "ui", "web"], "summary": "Pure Python read/write support for ESRI Shapefile format", "text": "If you need to edit a shapefile you would have to read the\nfile one record at a time, modify or filter the contents, and write it back out. For instance, to create a copy of a shapefile that only keeps a subset of relevant fields:\n\n>>> # create writer\n\t>>> w = shapefile.Writer('shapefiles/test/edit')\n\n>>> # define which fields to keep\n\t>>> keep_fields = ['BKG_KEY', 'MEDIANRENT']\n\n>>> # copy over the relevant fields from the reader\n\t>>> r = shapefile.Reader(\"shapefiles/blockgroups\")\n\t>>> for field in r.fields[1:]:\n\t...     if field[0] in keep_fields:\n\t...         w.field(*field)\n\n>>> # write only the relevant attribute values\n\t>>> for shapeRec in r.iterShapeRecords(fields=keep_fields):\n\t...     w.record(*shapeRec.record)\n\t...     w.shape(shapeRec.shape)\n\n>>> # close writer\n\t>>> w.close()\n\n3D and Other Geometry Types\n\nMost shapefiles store conventional 2D points, lines, or polygons. But the shapefile format is also capable\nof storing various other types of geometries as well, including complex 3D surfaces and objects.\n\nShapefiles with measurement (M) values\n\nMeasured shape types are shapes that include a measurement value at each vertex, for instance\nspeed measurements from a GPS device. Shapes with measurement (M) values are added with the following\nmethods: \"pointm\", \"multipointm\", \"linem\", and \"polygonm\". The M-values are specified by adding a\nthird M value to each XY coordinate. Missing or unobserved M-values are specified with a None value,\nor by simply omitting the third M-coordinate.\n\n>>> w = shapefile.Writer('shapefiles/test/linem')\n\t>>> w.field('name', 'C')\n\n>>> w.linem([\n\t...\t\t\t[[1,5,0],[5,5],[5,1,3],[3,3,None],[1,1,0]], # line with one omitted and one missing M-value\n\t...\t\t\t[[3,2],[2,6]] # line without any M-values\n\t...\t\t\t])\n\n>>> w.record('linem1')\n\n>>> w.close()\n\nShapefiles containing M-values can be examined in several ways:\n\n>>> r = shapefile.Reader('shapefiles/test/linem')\n\n>>> r.mbox # the lower and upper bound of M-values in the shapefile\n\t(0.0, 3.0)\n\n>>> r.shape(0).m # flat list of M-values\n\t[0.0, None, 3.0, None, 0.0, None, None]\n\nShapefiles with elevation (Z) values\n\nElevation shape types are shapes that include an elevation value at each vertex, for instance elevation from a GPS device.\nShapes with elevation (Z) values are added with the following methods: \"pointz\", \"multipointz\", \"linez\", and \"polyz\".\nThe Z-values are specified by adding a third Z value to each XY coordinate. Z-values do not support the concept of missing data,\nbut if you omit the third Z-coordinate it will default to 0. Note that Z-type shapes also support measurement (M) values added\nas a fourth M-coordinate. This too is optional.\n\n>>> w = shapefile.Writer('shapefiles/test/linez')\n\t>>> w.field('name', 'C')\n\n>>> w.linez([\n\t...\t\t\t[[1,5,18],[5,5,20],[5,1,22],[3,3],[1,1]], # line with some omitted Z-values\n\t...\t\t\t[[3,2],[2,6]], # line without any Z-values\n\t...\t\t\t[[3,2,15,0],[2,6,13,3],[1,9,14,2]] # line with both Z- and M-values\n\t...\t\t\t])\n\n>>> w.record('linez1')\n\n>>> w.close()\n\nTo examine a Z-type shapefile you can do:\n\n>>> r = shapefile.Reader('shapefiles/test/linez')\n\n>>> r.zbox # the lower and upper bound of Z-values in the shapefile\n\t(0.0, 22.0)\n\n>>> r.shape(0).z # flat list of Z-values\n\t[18.0, 20.0, 22.0, 0.0, 0.0, 0.0, 0.0, 15.0, 13.0, 14.0]\n\n3D MultiPatch Shapefiles"}, {"name": "pyshp", "tags": ["cli", "data", "dev", "math", "ui", "web"], "summary": "Pure Python read/write support for ESRI Shapefile format", "text": "Multipatch shapes are useful for storing composite 3-Dimensional objects.\nA MultiPatch shape represents a 3D object made up of one or more surface parts.\nEach surface in \"parts\" is defined by a list of XYZM values (Z and M values optional), and its corresponding type is\ngiven in the \"partTypes\" argument. The part type decides how the coordinate sequence is to be interpreted, and can be one\nof the following module constants: TRIANGLE_STRIP, TRIANGLE_FAN, OUTER_RING, INNER_RING, FIRST_RING, or RING.\nFor instance, a TRIANGLE_STRIP may be used to represent the walls of a building, combined with a TRIANGLE_FAN to represent\nits roof:\n\n>>> from shapefile import TRIANGLE_STRIP, TRIANGLE_FAN\n\n>>> w = shapefile.Writer('shapefiles/test/multipatch')\n\t>>> w.field('name', 'C')\n\n>>> w.multipatch([\n\t...\t\t\t\t [[0,0,0],[0,0,3],[5,0,0],[5,0,3],[5,5,0],[5,5,3],[0,5,0],[0,5,3],[0,0,0],[0,0,3]], # TRIANGLE_STRIP for house walls\n\t...\t\t\t\t [[2.5,2.5,5],[0,0,3],[5,0,3],[5,5,3],[0,5,3],[0,0,3]], # TRIANGLE_FAN for pointed house roof\n\t...\t\t\t\t ],\n\t...\t\t\t\t partTypes=[TRIANGLE_STRIP, TRIANGLE_FAN]) # one type for each part\n\n>>> w.record('house1')\n\n>>> w.close()\n\nFor an introduction to the various multipatch part types and examples of how to create 3D MultiPatch objects see [this\nESRI White Paper](http://downloads.esri.com/support/whitepapers/ao_/J9749_MultiPatch_Geometry_Type.pdf).\n\nTesting\n\nThe testing framework is pytest, and the tests are located in test_shapefile.py.\nThis includes an extensive set of unit tests of the various pyshp features,\nand tests against various input data.\nIn the same folder as README.md and shapefile.py, from the command line run:\n\nAdditionally, all the code and examples located in this file, README.md,\nis tested and verified with the builtin doctest framework.\nA special routine for invoking the doctest is run when calling directly on shapefile.py.\nIn the same folder as README.md and shapefile.py, from the command line run:\n\nThis tests the code inside shapefile.py itself.  To test an installed PyShp wheel against\nthe doctests, the same special routine can be invoked (in an env with the wheel and pytest\ninstalled) from the test file:\n\nLinux/Mac and similar platforms may need to run `$ dos2unix README.md` in order\nto correct line endings in README.md, if Git has not automatically changed them.\n\nNetwork tests\n\nSome of the tests and doctests, are intended to test reading shapefiles from\nremote servers, which requires internet connectivity.  The pytest tests are marked \"network\".\nFor rapid iteration, in CI, or when developing in offline testing environments, these\ntests can be dealt with in two ways:\n i) by skipping the network tests via :\n\nor the doctests via:\n\nor ii) by cloning a repo of the files they download, serving these on localhost in a separate process,\nand running the network tests with the environment variable REPLACE_REMOTE_URLS_WITH_LOCALHOST to `yes`:\nSetup a local file server (*):\n\nand then:\n\nor the doctests via:\n\nThe network tests alone can also be run (without also running all the tests that don't\nmake network requests) using: `pytest -m network` (or the doctests using: `python shapefile.py -m network`).\n\nContributors"}, {"name": "pyshp", "tags": ["cli", "data", "dev", "math", "ui", "web"], "summary": "Pure Python read/write support for ESRI Shapefile format", "text": "This library is used to read and write ESRI Shapefile format in pure Python, providing developers with a straightforward way to access and manipulate spatial data. With pyshp, developers can easily import and export shapefiles from local files, zip archives, or URLs, making it a versatile tool for geospatial applications."}, {"name": "pystac-client", "tags": ["math", "web"], "summary": "Python library for searching SpatioTemporal Asset Catalog (STAC) APIs.", "text": "pystac-client\n\n(https://github.com/stac-utils/pystac-client/actions/workflows/continuous-integration.yml)\n(https://github.com/stac-utils/pystac-client/actions/workflows/release.yml)\n\n(https://pystac-client.readthedocs.io)\n(https://codecov.io/gh/stac-utils/pystac-client)\n\nA Python client for working with [STAC](https://stacspec.org/) APIs.\n\nInstallation\n\nPySTAC Client is published to PyPi as [pystac-client](https://pypi.org/project/pystac-client/).\n\nThe only direct Python dependencies of **pystac-client** are [PySTAC](https://pystac.readthedocs.io),\n[requests](https://docs.python-requests.org), and [dateutil](https://dateutil.readthedocs.io).\n\nTo install with pip, run:\n\nDocumentation\n\nSee the [documentation page](https://pystac-client.readthedocs.io/en/latest/) for the latest docs.\n\nDevelopment\n\nSee the [contributing page](https://pystac-client.readthedocs.io/en/latest/contributing.html) for the latest development instructions."}, {"name": "pystac-client", "tags": ["math", "web"], "summary": "Python library for searching SpatioTemporal Asset Catalog (STAC) APIs.", "text": "This library is used to enable developers to easily search and interact with SpatioTemporal Asset Catalog (STAC) APIs in their Python applications. With pystac-client, developers can efficiently retrieve and process geospatial data from STAC-compliant APIs."}, {"name": "pytensor", "tags": ["math", "web"], "summary": "Optimizing compiler for evaluating mathematical expressions on CPUs and GPUs.", "text": ".. image:: https://cdn.rawgit.com/pymc-devs/pytensor/main/doc/images/PyTensor_RGB.svg\n\nTests Status\n\nProject Name\nefficiently evaluate mathematical expressions involving multi-dimensional arrays.\nIt provides the computational backend for `PyMC `__.\n\nFeatures\n========\n\n- A hackable, pure-Python codebase\n- Extensible graph framework suitable for rapid development of custom operators and symbolic optimizations\n- Implements an extensible graph transpilation framework that currently provides\n  compilation via C, `JAX `__, and `Numba `__\n- Contrary to PyTorch and TensorFlow, PyTensor maintains a static graph which can be modified in-place to\n  allow for advanced optimizations\n\nGetting started\n===============\n\n.. code-block:: python\n\nSee `the PyTensor documentation `__ for in-depth tutorials.\n\nInstallation\n============\n\nThe latest release of |Project Name| can be installed from PyPI using ``pip``:\n\n::\n\nOr via conda-forge:\n\n::\n\nThe current development branch of |Project Name| can be installed from GitHub, also using ``pip``:\n\n::\n\nBackground\n==========\n\nPyTensor is a fork of `Aesara `__, which is a fork of `Theano `__.\n\nContributing\n============\n\nWe welcome bug reports and fixes and improvements to the documentation.\n\nFor more information on contributing, please see the\n`contributing guide `__.\n\nA good place to start contributing is by looking through the issues\n`here `__.\n\n.. |Project Name| replace:: PyTensor\n\n  :target: https://github.com/pymc-devs/pytensor/actions?query=workflow%3ATests+branch%3Amain\n\n  :target: https://codecov.io/gh/pymc-devs/pytensor"}, {"name": "pytensor", "tags": ["math", "web"], "summary": "Optimizing compiler for evaluating mathematical expressions on CPUs and GPUs.", "text": "This library is used to efficiently evaluate mathematical expressions involving multi-dimensional arrays, providing a hackable and extensible framework for rapid development of custom operators and symbolic optimizations. It enables the compilation of these expressions on both CPUs and GPUs through its graph transpilation framework."}, {"name": "pytest", "tags": ["dev"], "summary": "pytest: simple powerful testing with Python", "text": ".. image:: https://github.com/pytest-dev/pytest/raw/main/doc/en/img/pytest_logo_curves.svg\n   :target: https://docs.pytest.org/en/stable/\n   :align: center\n   :height: 200\n   :alt: pytest\n\n------\n\n.. image:: https://img.shields.io/pypi/v/pytest.svg\n\n.. image:: https://img.shields.io/conda/vn/conda-forge/pytest.svg\n\n.. image:: https://img.shields.io/pypi/pyversions/pytest.svg\n\n   :target: https://results.pre-commit.ci/latest/github/pytest-dev/pytest/main\n   :alt: pre-commit.ci status\n\nThe ``pytest`` framework makes it easy to write small tests, yet\nscales to support complex functional testing for applications and libraries.\n\nAn example of a simple test:\n\n.. code-block:: python\n\nTo execute it::\n\nThanks to ``pytest``'s detailed assertion introspection, you can simply use plain ``assert`` statements. See `getting-started `_ for more examples.\n\nFeatures\n--------\n\n- Detailed info on failing `assert statements `_ (no need to remember ``self.assert*`` names)\n\n- `Auto-discovery\n  `_\n  of test modules and functions\n\n- `Modular fixtures `_ for\n  managing small or parametrized long-lived test resources\n\n- Can run `unittest `_ (or trial)\n  test suites out of the box\n\n- Python 3.10+ or PyPy3\n\n- Rich plugin architecture, with over 1300+ `external plugins `_ and thriving community\n\nDocumentation\n-------------\n\nFor full documentation, including installation, tutorials and PDF documents, please see https://docs.pytest.org/en/stable/.\n\nBugs/Requests\n-------------\n\nPlease use the `GitHub issue tracker `_ to submit bugs or request features.\n\nChangelog\n---------\n\nConsult the `Changelog `__ page for fixes and enhancements of each version.\n\nSupport pytest\n--------------\n\n`Open Collective`_ is an online funding platform for open and transparent communities.\nIt provides tools to raise money and share your finances in full transparency.\n\nIt is the platform of choice for individuals and companies that want to make one-time or\nmonthly donations directly to the project.\n\nSee more details in the `pytest collective`_.\n\n.. _Open Collective: https://opencollective.com\n.. _pytest collective: https://opencollective.com/pytest\n\npytest for enterprise\n---------------------\n\nAvailable as part of the Tidelift Subscription.\n\nThe maintainers of pytest and thousands of other packages are working with Tidelift to deliver commercial support and\nmaintenance for the open source dependencies you use to build your applications.\nSave time, reduce risk, and improve code health, while paying the maintainers of the exact dependencies you use.\n\n`Learn more. `_\n\nSecurity\n^^^^^^^^\n\npytest has never been associated with a security vulnerability, but in any case, to report a\nsecurity vulnerability please use the `Tidelift security contact `_.\nTidelift will coordinate the fix and disclosure.\n\nLicense\n-------\n\nCopyright Holger Krekel and others, 2004.\n\nDistributed under the terms of the `MIT`_ license, pytest is free and open source software.\n\n.. _`MIT`: https://github.com/pytest-dev/pytest/blob/main/LICENSE"}, {"name": "pytest", "tags": ["dev"], "summary": "pytest: simple powerful testing with Python", "text": "This library is used to write and run tests for Python applications and libraries with ease. With pytest, developers can create small, efficient tests that scale to support complex functional testing needs."}, {"name": "pythainlp", "tags": ["math", "ml"], "summary": "Thai Natural Language Processing library", "text": "Install\n\nFor stable version:\n\nFor development version:\n\nSome functionalities, like named-entity recognition, require extra packages.\nSee https://github.com/PyThaiNLP/pythainlp for installation options."}, {"name": "pythainlp", "tags": ["math", "ml"], "summary": "Thai Natural Language Processing library", "text": "This library is used to enable natural language processing tasks in the Thai language, including advanced capabilities such as named-entity recognition. With pythainlp, developers can build applications that understand and extract meaningful information from Thai text data."}, {"name": "python-crfsuite", "tags": ["math", "web"], "summary": "Python binding for CRFsuite", "text": "===============\npython-crfsuite\n===============\n\n.. image:: https://img.shields.io/pypi/v/python-crfsuite.svg?style=flat-square\n\npython-crfsuite is a python binding to CRFsuite_.\n\nInstallation\n============\n\nUsing ``pip``::\n\nUsing ``conda``::\n\nUsage\n=====\n\nSee docs_ and an example_.\n\n.. _docs: http://python-crfsuite.rtfd.org/\n.. _example: https://github.com/scrapinghub/python-crfsuite/blob/master/examples/CoNLL%202002.ipynb\n\nSee Also\n========\n\nsklearn-crfsuite_ is a python-crfsuite wrapper which provides\nAPI similar to scikit-learn.\n\n.. _sklearn-crfsuite: https://github.com/TeamHG-Memex/sklearn-crfsuite\n\nContributing\n============\n\n* Source code: https://github.com/scrapinghub/python-crfsuite\n* Issue tracker: https://github.com/scrapinghub/python-crfsuite/issues\n\nFeel free to submit ideas, bugs reports, pull requests or regular patches.\n\nPlease don't commit generated cpp files in the same commit as other files.\n\n.. _Cython: http://cython.org/\n.. _tox: http://tox.testrun.org\n\nAuthors and Contributors\n========================\n\nOriginal authors are Terry Peng  and\nMikhail Korobov . Many other people contributed;\nsome of them can be found at github Contributors_ page.\n\nBundled CRFSuite_ C/C++ library is by Naoaki Okazaki & contributors.\n\n.. _Contributors: https://github.com/scrapinghub/python-crfsuite/graphs/contributors\n\nLicense\n=======\n\npython-crfsuite is licensed under MIT license.\nCRFsuite_ library is licensed under BSD license.\n\n.. _CRFsuite: https://github.com/chokkan/crfsuite\n\nAlternatives\n============\n\n* https://github.com/chokkan/crfsuite/tree/master/swig/python - official\n  Python wrapper, exposes C++ API using SWIG.\n* https://github.com/jakevdp/pyCRFsuite - uses C API instead of C++ API;\n  allows to use scipy sparse matrices as an input. At the time of writing\n  it is unmaintained.\n* https://github.com/bosondata/crfsuite-rs - uses a Rust wrapper with CFFI instead of C++ API;\n  allows to tag with GIL released for better performance.\n\nThis package (python-crfsuite) wraps CRFsuite C++ API using Cython.\nIt is faster than official SWIG wrapper and has a simpler codebase than\na more advanced pyCRFsuite. python-crfsuite works in Python 2 and Python 3,\ndoesn't have external dependencies (CRFsuite is bundled, numpy/scipy stack\nis not needed) and workarounds some of the issues with C++ CRFsuite library."}, {"name": "python-crfsuite", "tags": ["math", "web"], "summary": "Python binding for CRFsuite", "text": "This library is used to train and use Conditional Random Field (CRF) models for natural language processing tasks, enabling developers to perform sequence labeling and classification. With python-crfsuite, developers can efficiently implement CRFs in their Python projects using the optimized CRFsuite engine."}, {"name": "python-doctr", "tags": ["data", "dev", "math", "ml", "web"], "summary": "Document Text Recognition (docTR): deep Learning for high-performance OCR on documents.", "text": "Quick Tour\n\nGetting your pretrained model\n\nEnd-to-End OCR is achieved in docTR using a two-stage approach: text detection (localizing words), then text recognition (identify all characters in the word).\nAs such, you can select the architecture used for [text detection](https://mindee.github.io/doctr/latest/modules/models.html#doctr-models-detection), and the one for [text recognition](https://mindee.github.io/doctr/latest//modules/models.html#doctr-models-recognition) from the list of available implementations.\n\nReading files\n\nDocuments can be interpreted from PDF or images:\n\nPutting it together\n\nLet's use the default pretrained model for an example:\n\nDealing with rotated documents\n\nShould you use docTR on documents that include rotated pages, or pages with multiple box orientations,\nyou have multiple options to handle it:\n\n- If you only use straight document pages with straight words (horizontal, same reading direction),\nconsider passing `assume_straight_boxes=True` to the ocr_predictor. It will directly fit straight boxes\non your page and return straight boxes, which makes it the fastest option.\n\n- If you want the predictor to output straight boxes (no matter the orientation of your pages, the final localizations\nwill be converted to straight boxes), you need to pass `export_as_straight_boxes=True` in the predictor. Otherwise, if `assume_straight_pages=False`, it will return rotated bounding boxes (potentially with an angle of 0\u00b0).\n\nIf both options are set to False, the predictor will always fit and return rotated boxes.\n\nTo interpret your model's predictions, you can visualize them interactively as follows:\n\nOr even rebuild the original document from its predictions:\n\nThe `ocr_predictor` returns a `Document` object with a nested structure (with `Page`, `Block`, `Line`, `Word`, `Artefact`).\nTo get a better understanding of our document model, check our [documentation](https://mindee.github.io/doctr/modules/io.html#document-structure):\n\nYou can also export them as a nested dict, more appropriate for JSON format:\n\nUse the KIE predictor\n\nThe KIE predictor is a more flexible predictor compared to OCR as your detection model can detect multiple classes in a document. For example, you can have a detection model to detect just dates and addresses in a document.\n\nThe KIE predictor makes it possible to use detector with multiple classes with a recognition model and to have the whole pipeline already setup for you.\n\nThe KIE predictor results per page are in a dictionary format with each key representing a class name and it's value are the predictions for that class.\n\nIf you are looking for support from the Mindee team\n\n(https://mindee.com/product/doctr)\n\nInstallation\n\nPrerequisites\n\nPython 3.10 (or higher) and [pip](https://pip.pypa.io/en/stable/) are required to install docTR.\n\nLatest release\n\nYou can then install the latest release of the package using [pypi](https://pypi.org/project/python-doctr/) as follows:\n\nWe try to keep extra dependencies to a minimum. You can install specific builds as follows:\n\nDeveloper mode\n\nAlternatively, you can install it from source, which will require you to install [Git](https://git-scm.com/book/en/v2/Getting-Started-Installing-Git).\nFirst clone the project repository:\n\nAgain, if you prefer to avoid the risk of missing dependencies, you can install the build:\n\nModels architectures\n\nCredits where it's due: this repository is implementing, among others, architectures from published research papers.\n\nText Detection\n\nText Recognition\n\nMore goodies\n\nDocumentation\n\nThe full package documentation is available [here](https://mindee.github.io/doctr/) for detailed specifications.\n\nDemo app"}, {"name": "python-doctr", "tags": ["data", "dev", "math", "ml", "web"], "summary": "Document Text Recognition (docTR): deep Learning for high-performance OCR on documents.", "text": "A minimal demo app is provided for you to play with our end-to-end OCR models!\n\nLive demo\n\nCourtesy of :hugs: [Hugging Face](https://huggingface.co/) :hugs:, docTR has now a fully deployed version available on [Spaces](https://huggingface.co/spaces)!\nCheck it out (https://huggingface.co/spaces/mindee/doctr)\n\nRunning it locally\n\nIf you prefer to use it locally, there is an extra dependency ([Streamlit](https://streamlit.io/)) that is required.\n\nThen run your app in your default browser with:\n\nDocker container\n\nWe offer Docker container support for easy testing and deployment. [Here are the available docker tags.](https://github.com/mindee/doctr/pkgs/container/doctr).\n\nUsing GPU with docTR Docker Images\n\nThe docTR Docker images are GPU-ready and based on CUDA `12.2`. Make sure your host is **at least `12.2`**, otherwise Torch won't be able to initialize the GPU.\nPlease ensure that Docker is configured to use your GPU.\n\nTo verify and configure GPU support for Docker, please follow the instructions provided in the [NVIDIA Container Toolkit Installation Guide](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html).\n\nOnce Docker is configured to use GPUs, you can run docTR Docker containers with GPU support:\n\nAvailable Tags\n\nThe Docker images for docTR follow a specific tag nomenclature: `-py-`. Here's a breakdown of the tag structure:\n\n- ``: `torch`, `torch-viz-html-contrib`.\n- ``: `3.9.18`, `3.10.13` or `3.11.8`.\n- ``: a tag >= `v0.11.0`\n- ``: e.g. `2014-10`\n\nHere are examples of different image tags:\n\nTag\n----------------------------\n`torch-viz-html-contrib-py3.11.8-2024-10`\n`torch-py3.11.8-2024-10`\n\nBuilding Docker Images Locally\n\nYou can also build docTR Docker images locally on your computer.\n\nYou can specify custom Python versions and docTR versions using build arguments. For example, to build a docTR image with PyTorch, Python version `3.9.10`, and docTR version `v0.7.0`, run the following command:\n\nExample script\n\nAn example script is provided for a simple documentation analysis of a PDF or image file:\n\nAll script arguments can be checked using `python scripts/analyze.py --help`\n\nMinimal API integration\n\nLooking to integrate docTR into your API? Here is a template to get you started with a fully working API using the wonderful [FastAPI](https://github.com/tiangolo/fastapi) framework.\n\nDeploy your API locally\n\nSpecific dependencies are required to run the API template, which you can install as follows:\n\nYou can now run your API locally:\n\nAlternatively, you can run the same server on a docker container if you prefer using:\n\nWhat you have deployed\n\nYour API should now be running locally on your port 8002. Access your automatically-built documentation at [http://localhost:8002/redoc](http://localhost:8002/redoc) and enjoy your three functional routes (\"/detection\", \"/recognition\", \"/ocr\", \"/kie\"). Here is an example with Python to send a request to the OCR route:\n\nExample notebooks\n\nLooking for more illustrations of docTR features? You might want to check the [Jupyter notebooks](https://github.com/mindee/doctr/tree/main/notebooks) designed to give you a broader overview.\n\nCitation\n\nIf you wish to cite this project, feel free to use this [BibTeX](http://www.bibtex.org/) reference:\n\nContributing\n\nIf you scrolled down to this section, you most likely appreciate open source. Do you feel like extending the range of our supported characters? Or perhaps submitting a paper implementation? Or contributing in any other way?\n\nYou're in luck, we compiled a short guide (cf. [`CONTRIBUTING`](https://mindee.github.io/doctr/contributing/contributing.html)) for you to easily do so!\n\nLicense"}, {"name": "python-doctr", "tags": ["data", "dev", "math", "ml", "web"], "summary": "Document Text Recognition (docTR): deep Learning for high-performance OCR on documents.", "text": "Distributed under the Apache 2.0 License. See [`LICENSE`](https://github.com/mindee/doctr?tab=Apache-2.0-1-ov-file#readme) for more information."}, {"name": "python-doctr", "tags": ["data", "dev", "math", "ml", "web"], "summary": "Document Text Recognition (docTR): deep Learning for high-performance OCR on documents.", "text": "This library is used to perform high-performance OCR (Optical Character Recognition) on documents using a two-stage approach, consisting of text detection and recognition. Developers can use docTR to accurately extract text from PDFs and images, even when dealing with rotated or complex document layouts."}, {"name": "pytorch-lightning", "tags": ["dev", "math", "ml", "ui"], "summary": "PyTorch Lightning is the lightweight PyTorch wrapper for ML researchers. Scale your models. Write less boilerplate.", "text": "\\*Codecov is > 90%+ but build delays may show less\n\n______________________________________________________________________\n\nPyTorch Lightning is just organized PyTorch\n\nLightning disentangles PyTorch code to decouple the science from the engineering.\n\n______________________________________________________________________\n\nLightning Design Philosophy\n\nLightning structures PyTorch code with these principles:\n\n  \n\nLightning forces the following structure to your code which makes it reusable and shareable:\n\n- Research code (the LightningModule).\n- Engineering code (you delete, and is handled by the Trainer).\n- Non-essential research code (logging, etc... this goes in Callbacks).\n- Data (use PyTorch DataLoaders or organize them into a LightningDataModule).\n\nOnce you do this, you can train on multiple-GPUs, TPUs, CPUs, HPUs and even in 16-bit precision without changing your code!\n\n[Get started in just 15 minutes](https://lightning.ai/docs/pytorch/latest/starter/introduction.html)\n\n______________________________________________________________________\n\nContinuous Integration\n\nLightning is rigorously tested across multiple CPUs, GPUs and TPUs and against major Python and PyTorch versions.\n\n  Current build statuses\n\nSystem / PyTorch ver.\n:--------------------------------:\nLinux py3.9 [GPUs]\nLinux (multiple Python versions)\nOSX (multiple Python versions)\nWindows (multiple Python versions)\n\n______________________________________________________________________\n\nHow To Use\n\nStep 0: Install\n\nSimple installation from PyPI\n\nStep 1: Add these imports\n\nStep 2: Define a LightningModule (nn.Module subclass)\n\nA LightningModule defines a full *system* (ie: a GAN, autoencoder, BERT or a simple Image Classifier).\n\n**Note: Training_step defines the training loop. Forward defines how the LightningModule behaves during inference/prediction.**\n\nStep 3: Train!\n\nAdvanced features\n\nLightning has over [40+ advanced features](https://lightning.ai/docs/pytorch/stable/common/trainer.html#trainer-flags) designed for professional AI research at scale.\n\nHere are some examples:\n\n  \n\n  Highlighted feature code snippets\n\nTrain on TPUs without code changes\n\n16-bit precision\n\nExperiment managers\n\nEarlyStopping\n\nCheckpointing\n\nExport to torchscript (JIT) (production use)\n\nExport to ONNX (production use)\n\nPro-level control of optimization (advanced users)\n\nFor complex/professional level work, you have optional full control of the optimizers.\n\n______________________________________________________________________\n\nAdvantages over unstructured PyTorch\n\n- Models become hardware agnostic\n- Code is clear to read because engineering code is abstracted away\n- Easier to reproduce\n- Make fewer mistakes because lightning handles the tricky engineering\n- Keeps all the flexibility (LightningModules are still PyTorch modules), but removes a ton of boilerplate\n- Lightning has dozens of integrations with popular machine learning tools.\n- [Tested rigorously with every new PR](https://github.com/Lightning-AI/lightning/tree/master/tests). We test every combination of PyTorch and Python supported versions, every OS, multi GPUs and even TPUs.\n- Minimal running speed overhead (about 300 ms per epoch compared with pure PyTorch).\n\n______________________________________________________________________\n\nExamples\n\nSelf-supervised Learning\n\nConvolutional Architectures\n\n- [GPT-2](https://lightning-bolts.readthedocs.io/en/stable/models/convolutional.html#gpt-2)\n- [UNet](https://lightning-bolts.readthedocs.io/en/stable/models/convolutional.html#unet)\n\nReinforcement Learning\n\nGANs\n\n- [Basic GAN](https://lightning-bolts.readthedocs.io/en/stable/models/gans.html#basic-gan)\n- [DCGAN](https://lightning-bolts.readthedocs.io/en/stable/models/gans.html#dcgan)\n\nClassic ML\n\n- [Logistic Regression](https://lightning-bolts.readthedocs.io/en/stable/models/classic_ml.html#logistic-regression)\n- [Linear Regression](https://lightning-bolts.readthedocs.io/en/stable/models/classic_ml.html#linear-regression)\n\n______________________________________________________________________\n\nCommunity\n\nThe PyTorch Lightning community is maintained by\n\n- [10+ core contributors](https://lightning.ai/docs/pytorch/stable/community/governance.html) who are all a mix of professional engineers, Research Scientists, and Ph.D. students from top AI labs.\n- 680+ active community contributors.\n\nWant to help us build Lightning and reduce boilerplate for thousands of researchers? [Learn how to make your first contribution here](https://medium.com/pytorch-lightning/quick-contribution-guide-86d977171b3a)\n\nPyTorch Lightning is also part of the [PyTorch ecosystem](https://pytorch.org/ecosystem/) which requires projects to have solid testing, documentation and support.\n\nAsking for help\n\nIf you have any questions please:\n\n1. [Read the docs](https://lightning.ai/docs/pytorch/stable).\n1. [Search through existing Discussions](https://github.com/Lightning-AI/lightning/discussions), or [add a new question](https://github.com/Lightning-AI/lightning/discussions/new)\n1. [Join our Discord community](https://discord.gg/VptPCZkGNa)."}, {"name": "pytorch-lightning", "tags": ["dev", "math", "ml", "ui"], "summary": "PyTorch Lightning is the lightweight PyTorch wrapper for ML researchers. Scale your models. Write less boilerplate.", "text": "This library is used to simplify and scale PyTorch model development by disentangling research code from engineering tasks. With PyTorch Lightning, developers can write less boilerplate code while still achieving robust and reusable models for machine learning research."}, {"name": "pyusb", "tags": ["math"], "summary": "Easy USB access for Python", "text": "PyUSB offers easy USB devices communication in Python.\nIt should work without additional code in any environment with\nPython >= 3.9, ctypes and a pre-built USB backend library\n(currently: libusb 1.x, libusb 0.1.x or OpenUSB)."}, {"name": "pyusb", "tags": ["math"], "summary": "Easy USB access for Python", "text": "This library is used to enable easy communication with USB devices from Python applications, allowing developers to access and control various types of USB peripherals in their code. With this library, developers can easily interact with USB devices without needing to write additional low-level code for device communication."}, {"name": "pyvisa", "tags": ["math", "web"], "summary": "Python VISA bindings for GPIB, RS232, TCPIP and USB instruments", "text": "PyVISA\n======\n\n.. image:: https://dev.azure.com/pyvisa/pyvisa/_apis/build/status/pyvisa.keysight-assisted?branchName=main\n\n.. image:: https://img.shields.io/pypi/l/PyVISA\n.. image:: https://img.shields.io/pypi/v/PyVISA\n.. image:: https://joss.theoj.org/papers/10.21105/joss.05304/status.svg\n   :target: https://doi.org/10.21105/joss.05304\n\nA Python package for support of the \"Virtual Instrument Software\nArchitecture\" (VISA), in order to control measurement devices and\ntest equipment via GPIB, RS232, Ethernet or USB.\n\nDescription\n-----------\n\nThe programming of measurement instruments can be real pain. There are many\ndifferent protocols, sent over many different interfaces and bus systems\n(GPIB, RS232, USB). For every programming language you want to use, you have to\nfind libraries that support both your device and its bus system.\n\nIn order to ease this unfortunate situation, the Virtual Instrument Software\nArchitecture (VISA_) specification was defined in the middle of the 90's. Today\nVISA is implemented on all significant operating systems. A couple of vendors\noffer VISA libraries, partly with free download. These libraries work together\nwith arbitrary peripheral devices, although they may be limited to certain\ninterface devices, such as the vendor\u2019s GPIB card.\n\nThe VISA specification has explicit bindings to Visual Basic, C, and G\n(LabVIEW\u2019s graphical language). Python can be used to call functions from a\nVISA shared library (`.dll`, `.so`, `.dylib`) allowing to directly leverage the\nstandard implementations. In addition, Python can be used to directly access\nmost bus systems used by instruments which is why one can envision to implement\nthe VISA standard directly in Python (see the `PyVISA-Py`_ project for more\ndetails). PyVISA is both a Python wrapper for VISA shared libraries but\ncan also serve as a front-end for other VISA implementation such as\n`PyVISA-Py`_.\n\n.. _VISA: http://www.ivifoundation.org/specifications/default.aspx\n.. _`PyVISA-Py`: http://pyvisa-py.readthedocs.io/en/latest/\n\nVISA and Python\n---------------\n\nPython has a couple of features that make it very interesting for controlling\ninstruments:\n\n- Python is an easy-to-learn scripting language with short development cycles.\n- It represents a high abstraction level [2], which perfectly blends with the\n  abstraction level of measurement programs.\n- It has a rich set of native libraries, including numerical and plotting\n  modules for data analysis and visualisation.\n- A large set of books (in many languages) and on-line publications is\n  available.\n\nRequirements\n------------\n\n- Python (tested with 3.10+)\n- VISA (tested with NI-VISA 17.5, Win7, from www.ni.com/visa and Keysight-VISA )\n\nInstallation\n--------------\n\nUsing pip:\n\nor easy_install:\n\nor download and unzip the source distribution file and:\n\nDocumentation\n--------------\n\nThe documentation can be read online at https://pyvisa.readthedocs.org\n\nCiting\n------\n\nIf you are using this package, you can cite the `PyVISA publication`_\n\nGrecco et al., (2023). PyVISA: the Python instrumentation package. Journal of Open Source\nSoftware, 8(84), 5304, https://doi.org/10.21105/joss.05304\n\n.. _`PyVISA publication`: https://joss.theoj.org/papers/10.21105/joss.05304#"}, {"name": "pyvisa", "tags": ["math", "web"], "summary": "Python VISA bindings for GPIB, RS232, TCPIP and USB instruments", "text": "This library is used to control measurement devices and test equipment via GPIB, RS232, Ethernet or USB using the VISA protocol. This allows developers to write Python programs that interact with a wide range of scientific instruments and laboratory equipment."}, {"name": "pyvista", "tags": ["data", "dev", "math", "ui", "visualization", "web"], "summary": "Easier Pythonic interface to VTK", "text": "#######\nPyVista\n#######\n\n.. image:: https://github.com/pyvista/pyvista/raw/main/doc/source/_static/pyvista_banner_small.png\n   :target: https://docs.pyvista.org/examples/index.html\n   :alt: pyvista\n\nPyVista is:\n\n* *Pythonic VTK*: a high-level API to the `Visualization Toolkit`_ (VTK)\n* mesh data structures and filtering methods for spatial datasets\n* 3D plotting made simple and built for large/complex data geometries\n\n.. _Visualization Toolkit: https://vtk.org\n\n.. image:: https://github.com/pyvista/pyvista/raw/main/assets/pyvista_ipython_demo.gif\n   :alt: pyvista ipython demo\n\nPyVista is a helper module for the Visualization Toolkit (VTK) that wraps the VTK library\nthrough NumPy and direct array access through a variety of methods and classes.\nThis package provides a Pythonic, well-documented interface exposing\nVTK's powerful visualization backend to facilitate rapid prototyping, analysis,\nand visual integration of spatially referenced datasets.\n\nThis module can be used for scientific plotting for presentations and research\npapers as well as a supporting module for other mesh 3D rendering dependent\nPython modules; see Connections for a list of projects that leverage\nPyVista.\n\nPyVista is a NumFOCUS affiliated project\n\n.. image:: https://raw.githubusercontent.com/numfocus/templates/master/images/numfocus-logo.png\n   :target: https://numfocus.org/sponsored-projects/affiliated-projects\n   :alt: NumFOCUS affiliated projects\n   :height: 60px\n\n=============\n\n:target: https://zenodo.org/records/8415866\n\n.. |joss| image:: http://joss.theoj.org/papers/10.21105/joss.01450/status.svg\n   :target: https://doi.org/10.21105/joss.01450\n\n.. |pypi| image:: https://img.shields.io/pypi/v/pyvista.svg?logo=python&logoColor=white\n   :target: https://pypi.org/project/pyvista/\n\n.. |conda| image:: https://img.shields.io/conda/vn/conda-forge/pyvista.svg?logo=conda-forge&logoColor=white\n   :target: https://anaconda.org/conda-forge/pyvista\n\n:target: https://github.com/pyvista/pyvista/actions/workflows/testing-and-deployment.yml\n\n:target: https://app.codecov.io/gh/pyvista/pyvista\n\n:target: https://app.codacy.com/gh/pyvista/pyvista/dashboard\n\n:target: https://opensource.org/license/mit/\n\n:target: https://communityinviter.com/apps/pyvista/pyvista\n\n.. |PyPIact| image:: https://img.shields.io/pypi/dm/pyvista.svg?label=PyPI%20downloads\n   :target: https://pypi.org/project/pyvista/\n\n.. |condaact| image:: https://img.shields.io/conda/dn/conda-forge/pyvista.svg?label=Conda%20downloads\n   :target: https://anaconda.org/conda-forge/pyvista\n\n:target: https://github.com/pyvista/pyvista/discussions\n\n:target: https://github.com/prettier/prettier\n  :alt: prettier\n\n:target: https://www.python.org/downloads/\n\n:target: https://numfocus.org/sponsored-projects/affiliated-projects\n\n:target: https://results.pre-commit.ci/latest/github/pyvista/pyvista/main\n\n:target: https://github.com/astral-sh/ruff\n   :alt: Ruff\n\n:target: https://github.com/nschloe/awesome-scientific-computing\n\n:target: https://repology.org/project/python:pyvista/versions\n\n.. |Good first issue| image:: https://img.shields.io/github/issues/pyvista/pyvista/good%20first%20issue\n   :target: https://github.com/pyvista/pyvista/issues?q=is%3Aissue+is%3Aopen+label%3A%22good+first+issue%22\n\n.. |GitHub Repo stars| image:: https://img.shields.io/github/stars/pyvista/pyvista\n   :target: https://github.com/pyvista/pyvista/stargazers\n\n.. |pyversions| image:: https://img.shields.io/pypi/pyversions/pyvista.svg?color=orange&logo=python&label=python&logoColor=white\n\n+----------------------+------------------------------------------------+\nDeployment\n+----------------------+------------------------------------------------+\nBuild Status\n+----------------------+------------------------------------------------+\nMetrics\n+----------------------+------------------------------------------------+\nActivity\n+----------------------+------------------------------------------------+\nCitation\n+----------------------+------------------------------------------------+\nLicense\n+----------------------+------------------------------------------------+\nCommunity\nGitHub Repo stars\n+----------------------+------------------------------------------------+\nFormatter\n+----------------------+------------------------------------------------+\nLinter\n+----------------------+------------------------------------------------+\nAffiliated\n+----------------------+------------------------------------------------+\nMentioned\n+----------------------+------------------------------------------------+\n\nHighlights\n==========\n\n:target: https://mybinder.org/v2/gh/pyvista/pyvista-examples/master\n   :alt: Launch on Binder\n\nHead over to the `Quick Examples`_ page in the docs to explore our gallery of\nexamples showcasing what PyVista can do. Want to test-drive PyVista?\nAll of the examples from the gallery are live on MyBinder for you to test\ndrive without installing anything locally: |binder|\n\n.. _Quick Examples: http://docs.pyvista.org/examples/index.html\n\nOverview of Features\n--------------------\n\n* Extensive gallery of examples (see `Quick Examples`_)\n* Interactive plotting in Jupyter Notebooks with server-side and client-side\n  rendering with `trame`_.\n* Filtering/plotting tools built for interactivity (see `Widgets`_)\n* Direct access to mesh analysis and transformation routines (see Filters_)\n* Intuitive plotting routines with ``matplotlib`` similar syntax (see Plotting_)\n* Import meshes from many common formats (use ``pyvista.read()``). Support for all formats handled by `meshio`_ is built-in.\n* Export meshes as VTK, STL, OBJ, or PLY (``mesh.save()``) file types or any formats supported by meshio_ (``pyvista.save_meshio()``)\n\n.. _trame: https://github.com/Kitware/trame\n.. _Widgets: https://docs.pyvista.org/api/plotting/index.html#widget-api\n.. _Filters: https://docs.pyvista.org/api/core/filters.html\n.. _Plotting: https://docs.pyvista.org/api/plotting/index.html\n.. _meshio: https://github.com/nschloe/meshio\n\nDocumentation\n=============\n\nRefer to the `documentation `_ for detailed\ninstallation and usage details.\n\nFor general questions about the project, its applications, or about software\nusage, please create a discussion in `pyvista/discussions`_\nwhere the community can collectively address your questions. You are also\nwelcome to join us on Slack_.\n\n.. _pyvista/discussions: https://github.com/pyvista/pyvista/discussions\n.. _Slack: https://communityinviter.com/apps/pyvista/pyvista\n\nInstallation\n============"}, {"name": "pyvista", "tags": ["data", "dev", "math", "ui", "visualization", "web"], "summary": "Easier Pythonic interface to VTK", "text": "PyVista can be installed from `PyPI `_\nusing ``pip`` on Python >= 3.9::\n\nYou can also visit `PyPI `_,\n`Anaconda `_, or\n`GitHub `_ to download the source.\n\nSee the `Installation `_\nfor more details regarding optional dependencies or if the installation through pip doesn't work out.\n\nConnections\n===========\n\nPyVista is a powerful tool that researchers can harness to create compelling,\nintegrated visualizations of large datasets in an intuitive, Pythonic manner.\n\nLearn more about how PyVista is used across science and engineering disciplines\nby a diverse community of users on our `Connections page`_.\n\n.. _Connections page: https://docs.pyvista.org/getting-started/connections.html\n\nAuthors\n=======\n\n.. |contrib.rocks| image:: https://contrib.rocks/image?repo=pyvista/pyvista\n   :target: https://github.com/pyvista/pyvista/graphs/contributors\n   :alt: contrib.rocks\n\nPlease take a look at the `contributors page`_ and the active `list of authors`_\nto learn more about the developers of PyVista.\n\ncontrib.rocks\n\nMade with `contrib rocks`_.\n\n.. _contributors page: https://github.com/pyvista/pyvista/graphs/contributors/\n.. _list of authors: https://docs.pyvista.org/getting-started/authors.html#authors\n.. _contrib rocks: https://contrib.rocks\n\nContributing\n============\n\n:target: CODE_OF_CONDUCT.md\n\n:target: https://www.codetriage.com/pyvista/pyvista\n   :alt: Code Triage\n\n:target: https://codespaces.new/pyvista/pyvista\n   :alt: Open in GitHub Codespaces\n\nContributor Covenant\ncodetriage\nOpen in GitHub Codespaces\n\nWe absolutely welcome contributions and we hope that our `Contributing Guide`_\nwill facilitate your ability to make PyVista better. PyVista is mostly\nmaintained on a volunteer basis and thus we need to foster a community that can\nsupport user questions and develop new features to make this software a useful\ntool for all users while encouraging every member of the community to share\ntheir ideas. To learn more about contributing to PyVista, please see the\n`Contributing Guide`_ and our `Code of Conduct`_.\n\n.. _Contributing Guide: https://github.com/pyvista/pyvista/blob/main/CONTRIBUTING.rst\n.. _Code of Conduct: https://github.com/pyvista/pyvista/blob/main/CODE_OF_CONDUCT.md\n\nStar History\n============\n\n.. image:: https://api.star-history.com/svg?repos=pyvista/pyvista&type=Date\n   :alt: Star History Chart\n   :target: https://star-history.com/#pyvista/pyvista&Date\n\nCiting PyVista\n==============\n\nThere is a `paper about PyVista `_.\n\nIf you are using PyVista in your scientific research, please help our scientific\nvisibility by citing our work.\n\nBibTex:\n\n.. code:: latex\n\nProfessional Support\n====================\n\nWhile PyVista is an Open Source project with a big community, you might be looking for professional support.\nThis section aims to list companies with VTK/PyVista expertise who can help you with your software project.\n\n+---------------+-----------------------------------------+\nCompany Name\n+---------------+-----------------------------------------+\nDescription\nfor our customers based on our\nwell-established open source platforms.\n+---------------+-----------------------------------------+\nExpertise\n+---------------+-----------------------------------------+\nContact\n+---------------+-----------------------------------------+"}, {"name": "pyvista", "tags": ["data", "dev", "math", "ui", "visualization", "web"], "summary": "Easier Pythonic interface to VTK", "text": "This library is used to create 3D plots with ease and simplify complex data geometries using a high-level API for the Visualization Toolkit (VTK). It enables developers to work with mesh data structures and filtering methods through a Pythonic interface."}, {"name": "pyviz-comms", "tags": ["math"], "summary": "A JupyterLab extension for rendering HoloViz content.", "text": "pyviz_comms\n\nOffers a simple bidirectional communication architecture between Python and JavaScript,\nwith support for Jupyter comms in both the classic notebook and Jupyterlab.\nAvailable for use by any [PyViz](https://pyviz.org) tool, but currently primarily used by\n[HoloViz](https://holoviz.org) tools.\n\nThere are two installable components in this repository: a Python\ncomponent used by various HoloViz tools and an extension to enable\nJupyterlab support. For JupyterLab 3.0 and above the extension is automatically\nbundled with the `pyviz_comms` Python package.\n\nInstalling the Jupyterlab extension\n\nJupyterlab users will need to install the Jupyterlab pyviz extension. Starting with JupyterLab 3.0 and above the extension will be automatically installed when installing `pyviz_comms` with `pip` using:\n\nor using `conda` with:\n\nFor older versions of JupyterLab you must separately install:\n\nCompatibility\n\nThe [Holoviz](https://github.com/holoviz/holoviz) libraries are generally version independent of\n[JupyterLab](https://github.com/jupyterlab/jupyterlab) and the `jupyterlab_pyviz` extension\nhas been supported since holoviews 1.10.0 and the first release of `pyviz_comms`.\n\nOur goal is that `jupyterlab_pyviz` minor releases (using the [SemVer](https://semver.org/) pattern) are\nmade to follow JupyterLab minor release bumps and micro releases are for new `jupyterlab_pyviz` features\nor bug fix releases. We've been previously inconsistent with having the extension release minor version bumps\ntrack that of JupyterLab, so users seeking to find extension releases that are compatible with their JupyterLab\ninstallation may refer to the below table.\n\nCompatible JupyterLab and jupyterlab_pyviz versions\n\nJupyterLab\n----------\n0.33.x\n0.34.x\n0.35.x\n1.0.x\n2.0.x\n3.x\n4.x\n\nDeveloping the Jupyterlab extension\n\nNote: You will need NodeJS to build the extension package.\n\nThe `jlpm` command is JupyterLab's pinned version of\n[yarn](https://yarnpkg.com/) that is installed with JupyterLab. You may use\n`yarn` or `npm` in lieu of `jlpm` below.\n\nYou can watch the source directory and run JupyterLab at the same time in different terminals to watch for changes in the extension's source and automatically rebuild the extension.\n\nWith the watch command running, every saved change will immediately be built locally and available in your running JupyterLab. Refresh JupyterLab to load the change in your browser (you may need to wait several seconds for the extension to be rebuilt).\n\nBy default, the `jlpm run build` command generates the source maps for this extension to make it easier to debug using the browser dev tools. To also generate source maps for the JupyterLab core extensions, you can run the following command:"}, {"name": "pyviz-comms", "tags": ["math"], "summary": "A JupyterLab extension for rendering HoloViz content.", "text": "This library is used to enable seamless communication between Python and JavaScript in a JupyterLab environment, allowing for dynamic rendering of HoloViz content. It simplifies the process of integrating interactive visualizations from PyViz tools into Jupyter notebooks and lab interfaces."}, {"name": "qiskit-aer", "tags": ["math", "web"], "summary": "Aer - High performance simulators for Qiskit", "text": "Aer - high performance quantum circuit simulation for Qiskit\n\n(https://opensource.org/licenses/Apache-2.0)\n(https://github.com/Qiskit/qiskit-aer/actions/workflows/build.yml)\n(https://github.com/Qiskit/qiskit-aer/actions/workflows/tests.yml)\n(https://github.com/Qiskit/qiskit-aer/releases)\n(https://pypi.org/project/qiskit-aer/)\n\n**Aer** is a high performance simulator for quantum circuits written in Qiskit, that includes realistic noise models.\n\nInstallation\n\nWe encourage installing Aer via the pip tool (a python package manager):\n\nPip will handle all dependencies automatically for us, and you will always install the latest (and well-tested) version.\n\nTo install from source, follow the instructions in the [contribution guidelines](CONTRIBUTING.md).\n\nInstalling GPU support\n\nIn order to install and run the GPU supported simulators on Linux, you need CUDA&reg; 11.2 or newer previously installed.\nCUDA&reg; itself would require a set of specific GPU drivers. Please follow CUDA&reg; installation procedure in the NVIDIA&reg; [web](https://www.nvidia.com/drivers).\n\nIf you want to install our GPU supported simulators, you have to install this other package:\n\nThe package above is for CUDA&reg 12, so if your system has CUDA&reg; 11 installed, install separate package:\n\nThis will overwrite your current `qiskit-aer` package installation giving you\nthe same functionality found in the canonical `qiskit-aer` package, plus the\nability to run the GPU supported simulators: statevector, density matrix, and unitary.\n\n**Note**: This package is only available on x86_64 Linux. For other platforms\nthat have CUDA support, you will have to build from source. You can refer to\nthe [contributing guide](CONTRIBUTING.md#building-with-gpu-support)\nfor instructions on doing this.\n\nSimulating your first Qiskit circuit with Aer\nNow that you have Aer installed, you can start simulating quantum circuits using primitives and noise models. Here is a basic example:\n\nContribution Guidelines\n\nIf you'd like to contribute to Aer, please take a look at our\n[contribution guidelines](CONTRIBUTING.md). This project adheres to Qiskit's [code of conduct](CODE_OF_CONDUCT.md). By participating, you are expected to uphold this code.\n\nWe use [GitHub issues](https://github.com/Qiskit/qiskit-aer/issues) for tracking requests and bugs. Please use our [slack](https://qiskit.slack.com) for discussion and simple questions. To join our Slack community use the [link](https://qiskit.slack.com/join/shared_invite/zt-fybmq791-hYRopcSH6YetxycNPXgv~A#/). For questions that are more suited for a forum, we use the Qiskit tag in the [Stack Exchange](https://quantumcomputing.stackexchange.com/questions/tagged/qiskit).\n\nNext Steps\n\nNow you're set up and ready to check out some of the other examples from the [Aer documentation](https://qiskit.github.io/qiskit-aer/).\n\nAuthors and Citation\n\nAer is the work of [many people](https://github.com/Qiskit/qiskit-aer/graphs/contributors) who contribute to the project at different levels.\nIf you use Qiskit, please cite as per the included [BibTeX file](https://github.com/Qiskit/qiskit/blob/main/CITATION.bib).\n\nLicense\n\n[Apache License 2.0](LICENSE.txt)"}, {"name": "qiskit-aer", "tags": ["math", "web"], "summary": "Aer - High performance simulators for Qiskit", "text": "This library is used to enable high-performance quantum circuit simulation within Qiskit applications, providing realistic noise models for accurate and efficient simulations. Developers can leverage Aer's high performance capabilities to run complex quantum circuits with reliable and accurate results."}, {"name": "qiskit", "tags": ["math", "ui", "web"], "summary": "An open-source SDK for working with quantum computers at the level of extended quantum circuits, operators, and primitives.", "text": "Qiskit\n\n(https://opensource.org/licenses/Apache-2.0) \n(https://github.com/Qiskit/qiskit/releases)\n(https://github.com/Qiskit/qiskit/releases?q=tag%3A1)\n(https://pypi.org/project/qiskit/)\n(https://coveralls.io/github/Qiskit/qiskit?branch=main)\n\n(https://rust-lang.github.io/rfcs/2495-min-rust-version.html)\n(https://pepy.tech/project/qiskit)\n(https://doi.org/10.5281/zenodo.2583252)\n\n**Qiskit**  is an open-source SDK for working with quantum computers at the level of extended quantum circuits, operators, and primitives.\n\nThis library is the core component of Qiskit, which contains the building blocks for creating and working with quantum circuits, quantum operators, and primitive functions (Sampler and Estimator).\nIt also contains a transpiler that supports optimizing quantum circuits, and a quantum information toolbox for creating advanced operators.\n\nFor more details on how to use Qiskit, refer to the documentation located here:\n\nInstallation\n\nWe encourage installing Qiskit via ``pip``:\n\nPip will handle all dependencies automatically and you will always install the latest (and well-tested) version.\n\nTo install from source, follow the instructions in the [documentation](https://quantum.cloud.ibm.com/docs/guides/install-qiskit-source).\n\nCreate your first quantum program in Qiskit\n\nNow that Qiskit is installed, it's time to begin working with Qiskit. The essential parts of a quantum program are:\n1. Define and build a quantum circuit that represents the quantum state\n2. Define the classical output by measurements or a set of observable operators\n3. Depending on the output, use the Sampler primitive to sample outcomes or the Estimator primitive to estimate expectation values.\n\nCreate an example quantum circuit using the `QuantumCircuit` class:\n\nThis simple example creates an entangled state known as a [GHZ state](https://en.wikipedia.org/wiki/Greenberger%E2%80%93Horne%E2%80%93Zeilinger_state) $(|000\\rangle + i|111\\rangle)/\\sqrt{2}$. It uses the standard quantum gates: Hadamard gate (`h`), Phase gate (`p`), and CNOT gate (`cx`).\n\nOnce you've made your first quantum circuit, choose which primitive you will use. Starting with the Sampler,\nwe use `measure_all(inplace=False)` to get a copy of the circuit in which all the qubits are measured:\n\nRunning this will give an outcome similar to `{'000': 497, '111': 503}` which is `000` 50% of the time and `111` 50% of the time up to statistical fluctuations.\nTo illustrate the power of the Estimator, we now use the quantum information toolbox to create the operator $XXY+XYX+YXX-YYY$ and pass it to the `run()` function, along with our quantum circuit. Note that the Estimator requires a circuit _**without**_ measurements, so we use the `qc` circuit we created earlier.\n\nRunning this will give the outcome `4`. For fun, try to assign a value of +/- 1 to each single-qubit operator X and Y \nand see if you can achieve this outcome. (Spoiler alert: this is not possible!)"}, {"name": "qiskit", "tags": ["math", "ui", "web"], "summary": "An open-source SDK for working with quantum computers at the level of extended quantum circuits, operators, and primitives.", "text": "Using the Qiskit-provided `qiskit.primitives.StatevectorSampler` and `qiskit.primitives.StatevectorEstimator` will not take you very far.\nThe power of quantum computing cannot be simulated on classical computers and you need to use real quantum hardware to scale to larger quantum circuits.\nHowever, running a quantum circuit on hardware requires rewriting to the basis gates and connectivity of the quantum hardware.\nThe tool that does this is the [transpiler](https://quantum.cloud.ibm.com/docs/api/qiskit/transpiler), and Qiskit includes transpiler passes for synthesis, optimization, mapping, and scheduling.\nHowever, it also includes a default compiler, which works very well in most examples.\nThe following code will map the example circuit to the `basis_gates = [\"cz\", \"sx\", \"rz\"]` and a linear chain of qubits $0 \\rightarrow 1 \\rightarrow 2$ with the `coupling_map = [[0, 1], [1, 2]]`.\n\nExecuting your code on real quantum hardware\n\nQiskit provides an abstraction layer that lets users run quantum circuits on hardware from any vendor that provides a compatible interface. \nThe best way to use Qiskit is with a runtime environment that provides optimized implementations of Sampler and Estimator for a given hardware platform. This runtime may involve using pre- and post-processing, such as optimized transpiler passes with error suppression, error mitigation, and, eventually, error correction built in. A runtime implements `qiskit.primitives.BaseSamplerV2` and `qiskit.primitives.BaseEstimatorV2` interfaces. For example,\nsome packages that provide implementations of a runtime primitive implementation are:\n\n* https://github.com/Qiskit/qiskit-ibm-runtime\n\nQiskit also provides a lower-level abstract interface for describing quantum backends. This interface, located in\n``qiskit.providers``, defines an abstract `BackendV2` class that providers can implement to represent their\nhardware or simulators to Qiskit. The backend class includes a common interface for executing circuits on the backends; however, in this interface each provider may perform different types of pre- and post-processing and return outcomes that are vendor-defined. Some examples of published provider packages that interface with real hardware are:\n\n* https://github.com/qiskit-community/qiskit-ionq\n* https://github.com/qiskit-community/qiskit-aqt-provider\n* https://github.com/qiskit-community/qiskit-braket-provider\n* https://github.com/qiskit-community/qiskit-quantinuum-provider\n* https://github.com/rigetti/qiskit-rigetti\n\nYou can refer to the documentation of these packages for further instructions\non how to get access and use these systems.\n\nContribution Guidelines\n\nIf you'd like to contribute to Qiskit, please take a look at our\n[contribution guidelines](CONTRIBUTING.md). By participating, you are expected to uphold our [code of conduct](CODE_OF_CONDUCT.md).\n\nWe use [GitHub issues](https://github.com/Qiskit/qiskit/issues) for tracking requests and bugs. Please\n[join the Qiskit Slack community](https://qisk.it/join-slack) for discussion, comments, and questions.\nFor questions related to running or using Qiskit, [Stack Overflow has a `qiskit`](https://stackoverflow.com/questions/tagged/qiskit).\nFor questions on quantum computing with Qiskit, use the `qiskit` tag in the [Quantum Computing Stack Exchange](https://quantumcomputing.stackexchange.com/questions/tagged/qiskit) (please, read first the [guidelines on how to ask](https://quantumcomputing.stackexchange.com/help/how-to-ask) in that forum).\n\nAuthors and Citation\n\nQiskit is the work of [many people](https://github.com/Qiskit/qiskit/graphs/contributors) who contribute\nto the project at different levels. If you use Qiskit, please cite as per the included [BibTeX file](CITATION.bib).\n\nChangelog and Release Notes\n\nThe changelog for a particular release is dynamically generated and gets\nwritten to the release page on Github for each release. For example, you can\nfind the page for the `1.2.0` release here:"}, {"name": "qiskit", "tags": ["math", "ui", "web"], "summary": "An open-source SDK for working with quantum computers at the level of extended quantum circuits, operators, and primitives.", "text": "The changelog for the current release can be found in the releases tab:\n(https://github.com/Qiskit/qiskit/releases)\nThe changelog provides a quick overview of notable changes for a given\nrelease.\n\nAdditionally, as part of each release, detailed release notes are written to\ndocument in detail what has changed as part of a release. This includes any\ndocumentation on potential breaking changes on upgrade and new features. See [all release notes here](https://quantum.cloud.ibm.com/docs/api/qiskit/release-notes).\n\nAcknowledgements\n\nWe acknowledge partial support for Qiskit development from the DOE Office of Science National Quantum Information Science Research Centers, Co-design Center for Quantum Advantage (C2QA) under contract number DE-SC0012704.\n\nLicense\n\n[Apache License 2.0](LICENSE.txt)"}, {"name": "qiskit", "tags": ["math", "ui", "web"], "summary": "An open-source SDK for working with quantum computers at the level of extended quantum circuits, operators, and primitives.", "text": "This library is used to create, optimize, and execute complex quantum circuits, as well as work with quantum operators and primitives. With this library, developers can build applications that leverage the power of quantum computing at the level of extended quantum circuits, operators, and primitives."}, {"name": "rasterio", "tags": ["cli", "data", "dev", "math", "ui", "web"], "summary": "Fast and direct raster I/O for use with Numpy and SciPy", "text": "========\nRasterio\n========\n\nRasterio reads and writes geospatial raster data.\n\n   :target: https://github.com/rasterio/rasterio/actions/workflows/tests.yaml\n\n   :target: https://github.com/rasterio/rasterio/actions/workflows/test_gdal_latest.yaml\n\n   :target: https://github.com/rasterio/rasterio/actions/workflows/test_gdal_tags.yaml\n\n.. image:: https://img.shields.io/pypi/v/rasterio\n   :target: https://pypi.org/project/rasterio/\n\nGeographic information systems use GeoTIFF and other formats to organize and\nstore gridded, or raster, datasets. Rasterio reads and writes these formats and\nprovides a Python API based on N-D arrays.\n\nRasterio 1.4.4+ works with Python >= 3.10, Numpy >= 1.24, and GDAL >= 3.6. Official\nbinary packages for Linux, macOS, and Windows with most built-in format drivers\nplus HDF5, netCDF, and OpenJPEG2000 are available on PyPI.\n\nRead the documentation for more details: https://rasterio.readthedocs.io/.\n\nExample\n=======\n\nHere's an example of some basic features that Rasterio provides. Three bands\nare read from an image and averaged to produce something like a panchromatic\nband.  This new band is then written to a new single band TIFF.\n\n.. code-block:: python\n\nThe output:\n\n.. image:: http://farm6.staticflickr.com/5501/11393054644_74f54484d9_z_d.jpg\n   :width: 640\n   :height: 581\n\nAPI Overview\n============\n\nRasterio gives access to properties of a geospatial raster file.\n\n.. code-block:: python\n\nA rasterio dataset also provides methods for getting read/write windows (like\nextended array slices) given georeferenced coordinates.\n\n.. code-block:: python\n\nRasterio CLI\n============\n\nRasterio's command line interface, named \"rio\", is documented at `cli.rst\n`__. Its ``rio\ninsp`` command opens the hood of any raster dataset so you can poke around\nusing Python.\n\n.. code-block:: pycon\n\nRio Plugins\n-----------\n\nRio provides the ability to create subcommands using plugins.  See\n`cli.rst `__\nfor more information on building plugins.\n\nSee the\n`plugin registry `__\nfor a list of available plugins.\n\nInstallation\n============\n\nSee `docs/installation.rst `__\n\nSupport\n=======\n\nThe primary forum for questions about installation and usage of Rasterio is\nquestions when they have expertise to share and time to explain. Please take\nthe time to craft a clear question and be patient about responses.\n\nPlease do not bring these questions to Rasterio's issue tracker, which we want\nto reserve for bug reports and other actionable issues.\n\nDevelopment and Testing\n=======================\n\nSee `CONTRIBUTING.rst `__.\n\nDocumentation\n=============\n\nSee `docs/ `__.\n\nLicense\n=======\n\nSee `LICENSE.txt `__.\n\nAuthors\n=======\n\nThe `rasterio` project was begun at Mapbox and was transferred to the `rasterio` Github organization in October 2021.\n\nSee `AUTHORS.txt `__.\n\nChanges\n=======\n\nSee `CHANGES.txt `__.\n\nWho is Using Rasterio?\n======================\n\nSee `here `__."}, {"name": "rasterio", "tags": ["cli", "data", "dev", "math", "ui", "web"], "summary": "Fast and direct raster I/O for use with Numpy and SciPy", "text": "This library is used to read and write geospatial raster data in various formats, such as GeoTIFF, using a Python API based on N-D arrays. With Rasterio, developers can efficiently handle large gridded datasets in their applications."}, {"name": "reedsolo", "tags": ["math", "web"], "summary": "Pure-Python Reed Solomon encoder/decoder", "text": "Reed Solomon\n============\n\nPyPI-Status\n\nBuild-Status\n\nConda-Forge-Status\n\nA pythonic `universal errors-and-erasures Reed-Solomon Codec `_ to protect your data from errors and bitrot. It includes a pure python implementation and an optional speed-optimized Cython/C extension.\n\nThis is a burst-type implementation, so that it supports any Galois field higher than 2^3, but not binary streams. Burst errors are non-random errors that more often happen on data storage mediums such as hard drives, hence this library is better suited for data storage protection, and less for streams noise correction, although it also works for this purpose but with a bit of overhead (since it works with bytes only, instead of bits).\n\nBased on the wonderful tutorial at `Wikiversity `_, written by \"Bobmath\" and \"LRQ3000\". If you are just starting with Reed-Solomon error correction codes, the Wikiversity article is a good beginner's introduction.\n\n------------------------------------\n\n.. contents:: Table of contents\n   :backlinks: top\n   :local:\n\nInstallation\n------------\n\nFor the latest stable release, install with:\n\n.. code:: sh\n\nFor the latest development release (do not use in production!), use:\n\n.. code:: sh\n\nIf you have some issues installing through pip, maybe this command may help:\n\n.. code:: sh\n\nBy default, only a pure-python implementation is installed. If you have Cython and a C++ compiler, a faster cythonized binary can be optionally built with:\n.. code:: sh\nor locally with:\n\n.. code:: sh\n\nThe setup.py will then try to build the Cython optimized module ``creedsolo.pyx`` if Cython is installed, which can then be imported as `import creedsolo` instead of `import reedsolo`, with the same features between both modules.\n\nAs an alternative, use `conda `_ to install a compiled version for various platforms:\n\n.. code:: sh\n\nUsage\n-----\n\nBasic usage with high-level RSCodec class\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n.. code:: python\n\n**Important upgrade notice for pre-1.0 users:** Note that ``RSCodec.decode()`` returns 3 variables:\n\nHere is how to use these outputs:\n\n.. code:: python\n\nSince we failed to decode with 6 errors with a codec set with 10 error correction code (ecc) symbols, let's try to use a bigger codec, with 12 ecc symbols.\n\n.. code:: python\n\nThis shows that we can decode twice as many erasures (where we provide the location of errors ourselves) than errors (with unknown locations). This is the cost of error correction compared to erasure correction.\n\nTo get the maximum number of errors *or* erasures that can be independently corrected (ie, not simultaneously):\n\n.. code:: python\n\nTo get the maximum number of errors *and* erasures that can be simultaneously corrected, you need to specify the number of errors or erasures you expect:\n\n.. code:: python"}, {"name": "reedsolo", "tags": ["math", "web"], "summary": "Pure-Python Reed Solomon encoder/decoder", "text": "Note that if a chunk has more errors and erasures than the Singleton Bound as calculated by the ``maxerrata()`` method, the codec will try to raise a ``ReedSolomonError`` exception,\nbut may very well not detect any error either (this is a theoretical limitation of error correction codes). In other words, error correction codes are unreliable to detect if a chunk of a message\nis corrupted beyond the Singleton Bound. If you want more reliability in errata detection, use a checksum or hash such as SHA or MD5 on your message, these are much more reliable and have no bounds\non the number of errata (the only potential issue is with collision but the probability is very very low).\n\nNote: to catch a ``ReedSolomonError`` exception, do not forget to import it first with: ``from reedsolo import ReedSolomonError``\n\nTo check if a message is tampered given its error correction symbols, without decoding, use the ``check()`` method:\n\n.. code:: python\n\nBy default, most Reed-Solomon codecs are limited to characters that can be encoded in 256 bits and with a length of maximum 256 characters. But this codec is universal, you can reduce or increase the length and maximum character value by increasing the Galois Field:\n\n.. code:: python\n\nNote that the ``RSCodec`` class supports transparent chunking, so you don't need to increase the Galois Field to support longer messages, but characters will still be limited to 256 bits (or\nwhatever field you set with ``c_exp``).\n\nLow-level usage via direct access to math functions\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nIf you want full control, you can skip the API and directly use the library as-is. Here's how:\n\nFirst you need to init the precomputed tables:\n\n.. code:: python\n\nPro tip: if you get the error: ValueError: byte must be in range(0, 256), please check that your prime polynomial is correct for your field.\nPro tip2: by default, you can only encode messages of max length and max symbol value = 256. If you want to encode bigger messages,\nplease use the following (where c_exp is the exponent of your Galois Field, eg, 12 = max length 2^12 = 4096):\n\n.. code:: python\n\nLet's define our RS message and ecc size:\n\n.. code:: python\n\nTo optimize, you can precompute the generator polynomial:\n\n.. code:: python\n\nThen to encode:\n\n.. code:: python\n\nLet's tamper our message:\n\n.. code:: python\n\nTo decode:\n\n.. code:: python\n\nNote that both the message and the ecc are corrected (if possible of course).\nPro tip: if you know a few erasures positions, you can specify them in a list ``erase_pos`` to double the repair power. But you can also just specify an empty list.\n\nYou can check how many errors and/or erasures were corrected, which can be useful to design adaptive bitrate algorithms:\n\n.. code:: python\n\nIf the decoding fails, it will normally automatically check and raise a ReedSolomonError exception that you can handle.\nHowever if you want to manually check if the repaired message is correct, you can do so:\n\n.. code:: python"}, {"name": "reedsolo", "tags": ["math", "web"], "summary": "Pure-Python Reed Solomon encoder/decoder", "text": "Note: if you want to use multiple reedsolomon with different parameters, you need to backup the globals and restore them before calling reedsolo functions:\n\n.. code:: python\n\nThen at anytime, you can do:\n\n.. code:: python\n\nThe globals backup is not necessary if you use RSCodec, it will be automatically managed.\n\nRead the sourcecode's comments for more info about how it works, and for the various parameters you can setup if\nyou need to interface with other RS codecs.\n\nExtended description\n--------------------\nThe code of wikiversity is here consolidated into a nice API with exceptions handling.\nThe algorithm can correct up to 2*e+v `_. Then you can simply cd to the root of the folder where creedsolo.pyx is, and type ``python setup.py build_ext --inplace --cythonize``. Alternatively, you can generate just the C++ code by typing `cython -3 creedsolo.pyx`. When building a distributable egg or installing the module from source, the Cython module can be transpiled and compiled if both Cython and a C compiler are installed and the ``--cythonize`` flag is supplied to the setup.py, otherwise by default only the pure-python implementation and the `.pyx` cython source code will be included, but the binary won't be in the wheel.\n\nThen, use `import RSCodec from creedsolo` instead of importing from the `reedsolo` module, and finally only feed `bytearray()` objects to the `RSCodec` object. Exclusively using bytearrays is one of the reasons creedsolo is faster than reedsolo. You can convert any string by specifying the encoding: `bytearray(\"Hello World\", \"UTF-8\")`.\n\nNote that there is an inherent limitation of the C implementation which cannot work with higher galois fields than 8 (= characters of max 255 value) because the C implementation only works with bytearrays, and bytearrays only support characters up to 255. If you want to use higher galois fields, you need to use the pure python version, which includes a fake `_bytearray` function that overloads the standard bytearray in case galois fields higher than 8 are used to `init_tables()`, or rewrite the C implementation to use lists instead of bytearrays (which will be MUCH slower so this defeats the purpose and you are better off simply using the pure python version under PyPy - an older version of the C implementation was doing just that, and without bytearrays, all performance gains were lost, hence why the bytearrays were kept despite the limitations).\n\nEdge cases\n-------------\n\nAlthough sanity checks are implemented whenever possible and when they are not too much resource consuming, there are a few cases where messages will not be decoded correctly without raising an exception:\n\n* If an incorrect erasure location is provided, the decoding algorithm will just trust the provided locations and create a syndrome that will be wrong, resulting in an incorrect decoded message. In case reliability is critical, always use the check() method after decoding to check the decoding did not go wrong."}, {"name": "reedsolo", "tags": ["math", "web"], "summary": "Pure-Python Reed Solomon encoder/decoder", "text": "* Reed-Solomon algorithm is limited by the Singleton Bound, which limits not only its capacity to correct errors and erasures relatively to the number of error correction symbols, but also its ability to check if the message can be decoded or not. Indeed, if the number of errors and erasures are greater than the Singleton Bound, the decoder has no way to mathematically know for sure whether there is an error at all, it may very well be a valid message (although not the message you expect, but mathematically valid nevertheless). Hence, when the message is tampered beyond the Singleton Bound, the decoder may raise an exception, but it may also return a mathematically valid but still tampered message. Using the check() method cannot fix that either. To work around this issue, a solution is to use parity or hashing functions in parallel to the Reed-Solomon codec: use the Reed-Solomon codec to repair messages, use the parity or hashing function to check if there is any error. Due to how parity and hashing functions work, they are much less likely to produce a false negative than the Reed-Solomon algorithm. This is a general rule: error correction codes are efficient at correcting messages but not at detecting errors, hashing and parity functions are the adequate tool for this purpose.\n\nRecommended reading\n-------------------\n\n* \"`Reed-Solomon codes for coders `_\", free practical beginner's tutorial with Python code examples on WikiVersity. Partially written by one of the authors of the present software.\n* \"Algebraic codes for data transmission\", Blahut, Richard E., 2003, Cambridge university press. `Readable online on Google Books `_. This book was pivotal in helping to understand the intricacies of the universal Berlekamp-Massey algorithm (see figures 7.5 and 7.10).\n\nAuthors\n-------\n\nThis module was conceived and developed by Tomer Filiba in 2012.\n\nIt was further extended and is currently maintained by Stephen Karl Larroque since 2015.\n\nAnd several other contributors helped improve and make it more robust:\n\nContributors\n\nFor a list of all contributors, please see `the GitHub Contributors graph `_ and the `commits history `_.\n\nLicense\n-------\n\nThis software is released under your choice of the Unlicense or the MIT-0 (MIT No Attribution) License. Both licenses are `public-domain-equivalent licenses `_, as intended by the original author Tomer Filiba.\n\n.. |PyPI-Status| image:: https://img.shields.io/pypi/v/reedsolo.svg\n   :target: https://pypi.org/project/reedsolo\n.. |PyPI-Versions| image:: https://img.shields.io/pypi/pyversions/reedsolo.svg?logo=python&logoColor=white\n   :target: https://pypi.org/project/reedsolo\n.. |PyPI-Downloads| image:: https://img.shields.io/pypi/dm/reedsolo.svg?label=pypi%20downloads&logo=python&logoColor=white\n   :target: https://pypi.org/project/reedsolo\n\n:target: https://coveralls.io/github/tomerfiliba/reedsolomon?branch=master\n.. |Conda-Forge-Status| image:: https://img.shields.io/conda/vn/conda-forge/reedsolo.svg\n   :target: https://anaconda.org/conda-forge/reedsolo\n\n:target: https://anaconda.org/conda-forge/reedsolo\n\n:target: https://anaconda.org/conda-forge/reedsolo\n.. |Contributors| image:: https://contrib.rocks/image?repo=tomerfiliba/reedsolomon\n   :target: https://github.com/tomerfiliba/reedsolomon/graphs/contributors"}, {"name": "reedsolo", "tags": ["math", "web"], "summary": "Pure-Python Reed Solomon encoder/decoder", "text": "This library is used to protect data from errors and bitrot during storage using Reed-Solomon encoding. It allows developers to safeguard their data against burst-type errors that commonly occur in data storage mediums."}, {"name": "requests", "tags": ["data", "web"], "summary": "Python HTTP for Humans.", "text": "Requests\n\n**Requests** is a simple, yet elegant, HTTP library.\n\nRequests allows you to send HTTP/1.1 requests extremely easily. There\u2019s no need to manually add query strings to your URLs, or to form-encode your `PUT` & `POST` data \u2014 but nowadays, just use the `json` method!\n\nRequests is one of the most downloaded Python packages today, pulling in around `30M downloads / week`\u2014 according to GitHub, Requests is currently [depended upon](https://github.com/psf/requests/network/dependents?package_id=UGFja2FnZS01NzA4OTExNg%3D%3D) by `1,000,000+` repositories. You may certainly put your trust in this code.\n\n(https://pepy.tech/project/requests)\n(https://pypi.org/project/requests)\n(https://github.com/psf/requests/graphs/contributors)\n\nInstalling Requests and Supported Versions\n\nRequests is available on PyPI:\n\nRequests officially supports Python 3.9+.\n\nSupported Features & Best\u2013Practices\n\nRequests is ready for the demands of building robust and reliable HTTP\u2013speaking applications, for the needs of today.\n\n- Keep-Alive & Connection Pooling\n- International Domains and URLs\n- Sessions with Cookie Persistence\n- Browser-style TLS/SSL Verification\n- Basic & Digest Authentication\n- Familiar `dict`\u2013like Cookies\n- Automatic Content Decompression and Decoding\n- Multi-part File Uploads\n- SOCKS Proxy Support\n- Connection Timeouts\n- Streaming Downloads\n- Automatic honoring of `.netrc`\n- Chunked HTTP Requests\n\nAPI Reference and User Guide available on [Read the Docs](https://requests.readthedocs.io)\n\n(https://requests.readthedocs.io)\n\nCloning the repository\n\nWhen cloning the Requests repository, you may need to add the `-c\nfetch.fsck.badTimezone=ignore` flag to avoid an error about a bad commit timestamp (see\n[this issue](https://github.com/psf/requests/issues/2690) for more background):\n\nYou can also apply this setting to your global Git config:\n\n---\n\n(https://kennethreitz.org) (https://www.python.org/psf)"}, {"name": "requests", "tags": ["data", "web"], "summary": "Python HTTP for Humans.", "text": "This library is used to send HTTP/1.1 requests easily, eliminating the need for manual query string additions or form-encoding of POST data. With requests, developers can streamline their API interactions with a simple and elegant interface."}, {"name": "rerun-sdk", "tags": ["math", "ml", "visualization", "web"], "summary": "The Rerun Logging SDK", "text": "The Rerun Python SDK\n\nUse the Rerun SDK to record data like images, tensors, point clouds, and text. Data is streamed to the Rerun Viewer for live visualization or to file for later use.\n\nInstall\n\n\u2139\ufe0f Note:\nThe Python module is called `rerun`, while the package published on PyPI is `rerun-sdk`.\n\nFor other SDK languages see [Installing Rerun](https://www.rerun.io/docs/getting-started/installing-viewer).\n\nWe also provide a [Jupyter widget](https://pypi.org/project/rerun-notebook/) for interactive data visualization in Jupyter notebooks:\n\nExample\n\nResources\n* [Examples](https://www.rerun.io/examples)\n* [Python API docs](https://ref.rerun.io/docs/python)\n* [Quick start](https://www.rerun.io/docs/getting-started/quick-start/python)\n* [Tutorial](https://www.rerun.io/docs/getting-started/data-in/python)\n* [Troubleshooting](https://www.rerun.io/docs/getting-started/troubleshooting)\n* [Discord Server](https://discord.com/invite/Gcm8BbTaAj)\n\nLogging and viewing in different processes\n\nYou can run the Viewer and logger in different processes.\n\nIn one terminal, start up a Viewer with a server that the SDK can connect to:\n\nIn a second terminal, run the example with the `--connect` option:\n\nNote that SDK and Viewer can run on different machines!\n\nBuilding Rerun from source\n\nWe use the [`pixi`](https://pixi.sh/) for managing dev-tool versioning, download and task running. See [here](https://github.com/casey/just#installation) for installation instructions.\n\nTo build SDK & Viewer for Python (or `pixi run py-build` for a debug build) and install it in the Pixi environment.\n\nYou can then run examples from the repository, either by making the Pixi shell active with `pixi shell -e py` and then running Python or by using `pixi run -e py`, e.g. `pixi run -e py python examples/python/minimal/minimal.py`.\n\nRespectively, to build a wheel instead for manual install use:\n\nRefer to [BUILD.md](../BUILD.md) for details on the various different build options of the Rerun Viewer and SDKs for all target languages.\n\nInstalling a pre-release\n\nPrebuilt dev wheels from head of main are available at .\n\nWhile we try to keep the main branch usable at all times, it may be unstable occasionally. Use at your own risk.\n\nRunning Python unit tests\n\nIf you run into a problem, run `rm -rf .pixi` and try again.\n\nRunning specific Python unit tests"}, {"name": "rerun-sdk", "tags": ["math", "ml", "visualization", "web"], "summary": "The Rerun Logging SDK", "text": "This library is used to record and stream various types of data, such as images, tensors, point clouds, and text, for live visualization or later use. With this SDK, developers can easily integrate data logging capabilities into their applications using a variety of programming languages."}, {"name": "rfc3987-syntax", "tags": ["dev", "math"], "summary": "Helper functions to syntactically validate strings according to RFC 3987.", "text": "rfc3987-syntax\n\nHelper functions to parse and validate the **syntax** of terms defined in **[RFC 3987](https://www.rfc-editor.org/info/rfc3987)** \u2014 the IETF standard for Internationalized Resource Identifiers (IRIs).\n\nPurpose\n\nThe goal of `rfc3987-syntax` is to provide a **lightweight, permissively licensed Python module** for validating that strings conform to the **ABNF grammar defined in RFC 3987**. These helpers are:\n\n-  Strictly aligned with the **syntax rules of RFC 3987**\n-  Built using a **permissive MIT license**\n-  Designed for both **open source and proprietary use**\n-  Powered by [Lark](https://github.com/lark-parser/lark), a fast, EBNF-based parser\n\n>  **Note:** This project focuses on **syntax validation only**. RFC 3987 specifies **additional semantic rules** (e.g., Unicode normalization, BiDi constraints, percent-encoding requirements) that must be enforced separately.\n\nLicense, Attribution, and Citation\n\n**`rfc3987-syntax`** is licensed under the [MIT License](LICENSE), which allows reuse in both open source and commercial software.\n\nThis project:\n\n-  Does **not** depend on the `rfc3987` Python package (GPL-licensed)\n-  Uses [`lark`](https://github.com/lark-parser/lark), licensed under MIT\n-  Implements grammar from **[RFC 3987](https://datatracker.ietf.org/doc/html/rfc3987)**, using **[RFC 3986](https://datatracker.ietf.org/doc/html/rfc3986)** where RFC 3987 delegates syntax\n\n> \u26a0\ufe0f This project is **not affiliated with or endorsed by** the authors of RFC 3987 or the `rfc3987` Python package.\n\nPlease cite this software in accordance with the enclosed CITATION.cff file.\n\n\u26a0\ufe0f Limitations\n\nThe grammar and parser enforce **only the ABNF syntax** defined in RFC 3987. The following are **not validated** and must be handled separately for full compliance:\n\n-  Unicode **Normalization Form C (NFC)**\n-  Bidirectional text (**BiDi**) constraints (RFC 3987 \u00a74.1)\n-  **Port number ranges** (must be 0\u201365535)\n-  Valid **IPv6 compression** (only one `::`, max segments)\n-  Context-aware **percent-encoding** requirements\n\nChatGPT 40 was used during the original development process. Errors may exist due to this assistance. Additional review, testing, and bug fixes by human experts is welcome.\n\nInstallation\n\nUsage\n\nList all supported \"terms\" (i.e., non-terminals and terminals within ABNF production rules) used to validate the syntax of an IRI according to RFC 3987\n\nSyntactically validate a string using the general-purpose validator\n\nAlternatively, use term-specific helpers to validate RFC 3987 syntax.\n\nGet the Lark parse tree for a syntax validation (useful for additional semantic validation)\n\nSources\n\nThis grammar was derived from:\n\n- **[RFC 3987 \u2013 Internationalized Resource Identifiers (IRIs)]**  \n  \u2192 Defines IRI syntax and extensions to URI (e.g. Unicode characters, `ucschar`)  \n  \u2192 https://datatracker.ietf.org/doc/html/rfc3987\n\n- **[RFC 3986 \u2013 Uniform Resource Identifier (URI): Generic Syntax)]**  \n  \u2192 Provides reusable components like `scheme`, `authority`, `ipv4address`, etc.  \n  \u2192 https://datatracker.ietf.org/doc/html/rfc3986\n\n>  When `RFC 3986` is listed as the source, it is **used in accordance with RFC 3987**, which explicitly references it for foundational elements.\n\nRule-to-Source Mapping\n\nRule/Component\n----------------------\n`iri`\n`iri_reference`\n`absolute_iri`\n`scheme`\n`ihier_part`\n`irelative_ref`\n`irelative_part`\n`iauthority`\n`ipath_abempty`\n`ipath_absolute`\n`ipath_noscheme`\n`ipath_rootless`\n`iquery`\n`ifragment`\n`ipchar`, `isegment`\n`isegment_nz_nc`\n`iunreserved`\n`ucschar`, `iprivate`\n`sub_delims`\n`ip_literal`\n`ipv6address`\n`ipvfuture`\n`ipv4address`\n`ls32`\n`h16`, `dec_octet`\n`port`\n`pct_encoded`\n`alpha`, `digit`, `hexdig`"}, {"name": "rfc3987-syntax", "tags": ["dev", "math"], "summary": "Helper functions to syntactically validate strings according to RFC 3987.", "text": "This library is used to syntactically validate strings according to the ABNF grammar defined in RFC 3987, ensuring conformance to the Internationalized Resource Identifiers standard. It provides a lightweight and permissively licensed module for developers to verify the syntax of IRI-related strings in their applications."}, {"name": "rich", "tags": ["cli", "data", "ui", "web"], "summary": "Render rich text, tables, progress bars, syntax highlighting, markdown and more to the terminal", "text": "Compatibility\n\nRich works with Linux, macOS and Windows. True color / emoji works with new Windows Terminal, classic terminal is limited to 16 colors. Rich requires Python 3.8 or later.\n\nRich works with [Jupyter notebooks](https://jupyter.org/) with no additional configuration required.\n\nInstalling\n\nInstall with `pip` or your favorite PyPI package manager.\n\nRun the following to test Rich output on your terminal:\n\nRich Print\n\nTo effortlessly add rich output to your application, you can import the [rich print](https://rich.readthedocs.io/en/latest/introduction.html#quick-start) method, which has the same signature as the builtin Python function. Try this:\n\nRich REPL\n\nRich can be installed in the Python REPL, so that any data structures will be pretty printed and highlighted.\n\nUsing the Console\n\nFor more control over rich terminal content, import and construct a [Console](https://rich.readthedocs.io/en/latest/reference/console.html#rich.console.Console) object.\n\nThe Console object has a `print` method which has an intentionally similar interface to the builtin `print` function. Here's an example of use:\n\nAs you might expect, this will print `\"Hello World!\"` to the terminal. Note that unlike the builtin `print` function, Rich will word-wrap your text to fit within the terminal width.\n\nThere are a few ways of adding color and style to your output. You can set a style for the entire output by adding a `style` keyword argument. Here's an example:\n\nThe output will be something like the following:\n\nThat's fine for styling a line of text at a time. For more finely grained styling, Rich renders a special markup which is similar in syntax to [bbcode](https://en.wikipedia.org/wiki/BBCode). Here's an example:\n\nYou can use a Console object to generate sophisticated output with minimal effort. See the [Console API](https://rich.readthedocs.io/en/latest/console.html) docs for details.\n\nRich Inspect\n\nRich has an [inspect](https://rich.readthedocs.io/en/latest/reference/init.html?highlight=inspect#rich.inspect) function which can produce a report on any Python object, such as class, instance, or builtin.\n\nSee the [inspect docs](https://rich.readthedocs.io/en/latest/reference/init.html#rich.inspect) for details.\n\nRich Library\n\nRich contains a number of builtin _renderables_ you can use to create elegant output in your CLI and help you debug your code.\n\nClick the following headings for details:\n\nLog\n\nThe Console object has a `log()` method which has a similar interface to `print()`, but also renders a column for the current time and the file and line which made the call. By default Rich will do syntax highlighting for Python structures and for repr strings. If you log a collection (i.e. a dict or a list) Rich will pretty print it so that it fits in the available space. Here's an example of some of these features.\n\nThe above produces the following output:\n\nNote the `log_locals` argument, which outputs a table containing the local variables where the log method was called.\n\nThe log method could be used for logging to the terminal for long running applications such as servers, but is also a very nice debugging aid.\n\nLogging Handler\n\nYou can also use the builtin [Handler class](https://rich.readthedocs.io/en/latest/logging.html) to format and colorize output from Python's logging module. Here's an example of the output:\n\nEmoji\n\nTo insert an emoji in to console output place the name between two colons. Here's an example:"}, {"name": "rich", "tags": ["cli", "data", "ui", "web"], "summary": "Render rich text, tables, progress bars, syntax highlighting, markdown and more to the terminal", "text": "Please use this feature wisely.\n\nTables\n\nRich can render flexible [tables](https://rich.readthedocs.io/en/latest/tables.html) with unicode box characters. There is a large variety of formatting options for borders, styles, cell alignment etc.\n\nThe animation above was generated with [table_movie.py](https://github.com/textualize/rich/blob/master/examples/table_movie.py) in the examples directory.\n\nHere's a simpler table example:\n\nThis produces the following output:\n\nNote that console markup is rendered in the same way as `print()` and `log()`. In fact, anything that is renderable by Rich may be included in the headers / rows (even other tables).\n\nThe `Table` class is smart enough to resize columns to fit the available width of the terminal, wrapping text as required. Here's the same example, with the terminal made smaller than the table above:\n\nProgress Bars\n\nRich can render multiple flicker-free [progress](https://rich.readthedocs.io/en/latest/progress.html) bars to track long-running tasks.\n\nFor basic usage, wrap any sequence in the `track` function and iterate over the result. Here's an example:\n\nIt's not much harder to add multiple progress bars. Here's an example taken from the docs:\n\nThe columns may be configured to show any details you want. Built-in columns include percentage complete, file size, file speed, and time remaining. Here's another example showing a download in progress:\n\nTo try this out yourself, see [examples/downloader.py](https://github.com/textualize/rich/blob/master/examples/downloader.py) which can download multiple URLs simultaneously while displaying progress.\n\nStatus\n\nFor situations where it is hard to calculate progress, you can use the [status](https://rich.readthedocs.io/en/latest/reference/console.html#rich.console.Console.status) method which will display a 'spinner' animation and message. The animation won't prevent you from using the console as normal. Here's an example:\n\nThis generates the following output in the terminal.\n\nThe spinner animations were borrowed from [cli-spinners](https://www.npmjs.com/package/cli-spinners). You can select a spinner by specifying the `spinner` parameter. Run the following command to see the available values:\n\nThe above command generates the following output in the terminal:\n\nTree\n\nRich can render a [tree](https://rich.readthedocs.io/en/latest/tree.html) with guide lines. A tree is ideal for displaying a file structure, or any other hierarchical data.\n\nThe labels of the tree can be simple text or anything else Rich can render. Run the following for a demonstration:\n\nThis generates the following output:\n\nSee the [tree.py](https://github.com/textualize/rich/blob/master/examples/tree.py) example for a script that displays a tree view of any directory, similar to the linux `tree` command.\n\nColumns\n\nRich can render content in neat [columns](https://rich.readthedocs.io/en/latest/columns.html) with equal or optimal width. Here's a very basic clone of the (MacOS / Linux) `ls` command which displays a directory listing in columns:\n\nThe following screenshot is the output from the [columns example](https://github.com/textualize/rich/blob/master/examples/columns.py) which displays data pulled from an API in columns:\n\nMarkdown\n\nRich can render [markdown](https://rich.readthedocs.io/en/latest/markdown.html) and does a reasonable job of translating the formatting to the terminal.\n\nTo render markdown import the `Markdown` class and construct it with a string containing markdown code. Then print it to the console. Here's an example:\n\nThis will produce output something like the following:\n\nSyntax Highlighting\n\nRich uses the [pygments](https://pygments.org/) library to implement [syntax highlighting](https://rich.readthedocs.io/en/latest/syntax.html). Usage is similar to rendering markdown; construct a `Syntax` object and print it to the console. Here's an example:\n\nThis will produce the following output:"}, {"name": "rich", "tags": ["cli", "data", "ui", "web"], "summary": "Render rich text, tables, progress bars, syntax highlighting, markdown and more to the terminal", "text": "Tracebacks\n\nRich can render [beautiful tracebacks](https://rich.readthedocs.io/en/latest/traceback.html) which are easier to read and show more code than standard Python tracebacks. You can set Rich as the default traceback handler so all uncaught exceptions will be rendered by Rich.\n\nHere's what it looks like on OSX (similar on Linux):\n\nAll Rich renderables make use of the [Console Protocol](https://rich.readthedocs.io/en/latest/protocol.html), which you can also use to implement your own Rich content.\n\nRich CLI\n\nSee also [Rich CLI](https://github.com/textualize/rich-cli) for a command line application powered by Rich. Syntax highlight code, render markdown, display CSVs in tables, and more, directly from the command prompt.\n\nTextual\n\nSee also Rich's sister project, [Textual](https://github.com/Textualize/textual), which you can use to build sophisticated User Interfaces in the terminal."}, {"name": "rich", "tags": ["cli", "data", "ui", "web"], "summary": "Render rich text, tables, progress bars, syntax highlighting, markdown and more to the terminal", "text": "This library is used to render visually appealing text and tables in the terminal with features like syntax highlighting, markdown support, and true color. Developers can add rich output to their applications using the `rich print` method or by installing Rich in a Jupyter notebook for enhanced code presentation."}, {"name": "rio-cogeo", "tags": ["math", "web"], "summary": "Cloud Optimized GeoTIFF (COGEO) creation plugin for rasterio", "text": "rio-cogeo\n\n  \n\n  Cloud Optimized GeoTIFF (COG) creation and validation plugin for Rasterio.\n\n  \n\n  \n  \n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n---\n\n**Documentation**: https://cogeotiff.github.io/rio-cogeo/\n\n**Source Code**: https://github.com/cogeotiff/rio-cogeo\n\n---\n\nCloud Optimized GeoTIFF\n\nThis plugin aims to facilitate the creation and validation of Cloud Optimized\nGeoTIFF (COG or COGEO). While it respects the\n[COG specifications](https://github.com/cogeotiff/cog-spec/blob/master/spec.md), this plugin also\nenforces several features:\n\n- **Internal overviews** (User can remove overview with option `--overview-level 0`)\n- **Internal tiles** (default profiles have 512x512 internal tiles)\n\n**Important**: in GDAL 3.1 a new COG driver has been added ([doc](https://gdal.org/drivers/raster/cog.html), [discussion](https://lists.osgeo.org/pipermail/gdal-dev/2019-May/050169.html)), starting with `rio-cogeo` version 2.2, `--use-cog-driver` option was added to create COG using the `COG` driver.\n\nInstall\n\nOr install from source:\n\nGDAL Version\n\nIt is recommended to use GDAL > 2.3.2. Previous versions might not be able to\ncreate proper COGs (ref: https://github.com/OSGeo/gdal/issues/754).\n\nMore info in https://github.com/cogeotiff/rio-cogeo/issues/55\n\nMore\n\nBlog post on good and bad COG formats: https://medium.com/@_VincentS_/do-you-really-want-people-using-your-data-ec94cd94dc3f\n\nCheckout [rio-glui](https://github.com/mapbox/rio-glui/) or [rio-viz](https://github.com/developmentseed/rio-viz) rasterio plugins to explore COG locally in your web browser.\n\nContribution & Development\n\nSee [CONTRIBUTING.md](https://github.com/cogeotiff/rio-cogeo/blob/main/CONTRIBUTING.md)\n\nChanges\n\nSee [CHANGES.md](https://github.com/cogeotiff/rio-cogeo/blob/main/CHANGES.md).\n\nLicense\n\nSee [LICENSE](https://github.com/cogeotiff/rio-cogeo/blob/main/LICENSE)"}, {"name": "rio-cogeo", "tags": ["math", "web"], "summary": "Cloud Optimized GeoTIFF (COGEO) creation plugin for rasterio", "text": "This library is used to create and validate Cloud Optimized GeoTIFF (COGEO) files for efficient cloud-based geospatial data storage, with features such as internal overviews and tiles. It integrates seamlessly with Rasterio to provide a streamlined workflow for working with COGEO files."}, {"name": "rio-tiler", "tags": ["data", "math", "web"], "summary": "User friendly Rasterio plugin to read raster datasets.", "text": "rio-tiler\n\n  \n\n  User friendly Rasterio plugin to read raster datasets.\n\n  \n\n  \n  \n\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n  \n\n---\n\n**Documentation**: https://cogeotiff.github.io/rio-tiler/\n\n**Source Code**: https://github.com/cogeotiff/rio-tiler\n\n---\n\nDescription\n\n`rio-tiler` was initially designed to create [slippy map\ntiles](https://en.wikipedia.org/wiki/Tiled_web_map) from large raster data\nsources and render these tiles dynamically on a web map. Since `rio-tiler` v2.0, we added many more helper methods to read\ndata and metadata from any raster source supported by Rasterio/GDAL.\nThis includes local and remote files via HTTP, AWS S3, Google Cloud Storage,\netc.\n\nAt the low level, `rio-tiler` is *just* a wrapper around the [rasterio](https://github.com/rasterio/rasterio) and [GDAL](https://github.com/osgeo/gdal) libraries.\n\nFeatures\n\n- Read any dataset supported by GDAL/Rasterio\n\n- User friendly `tile`, `part`, `feature`, `point` reading methods\n\n- Enable property assignment (e.g nodata) on data reading\n\n- [STAC](https://github.com/radiantearth/stac-spec) support\n\n- [Xarray](https://xarray.dev) support **(>=4.0)**\n\n- Non-Geo Image support **(>=4.0)**\n\n- [Mosaic](https://cogeotiff.github.io/rio-tiler/mosaic/) (merging or stacking)\n\n- Native support for multiple TileMatrixSet via [morecantile](https://developmentseed.org/morecantile/)\n\nInstall\n\nYou can install `rio-tiler` using pip\n\nor install from source:\n\nPlugins\n\n[**rio-tiler-pds**][rio-tiler-pds]\n\n`rio-tiler` v1 included several helpers for reading popular public datasets (e.g. Sentinel 2, Sentinel 1, Landsat 8, CBERS) from cloud providers. This functionality is now in a [separate plugin][rio-tiler-pds], enabling easier access to more public datasets.\n\n[**rio-tiler-mvt**][rio-tiler-mvt]\n\nCreate Mapbox Vector Tiles from raster sources\n\nImplementations\n\n[**titiler**][titiler]: A lightweight Cloud Optimized GeoTIFF dynamic tile server.\n\n[**cogeo-mosaic**][cogeo-mosaic]: Create mosaics of Cloud Optimized GeoTIFF based on the [mosaicJSON][mosaicjson_spec] specification.\n\nContribution & Development\n\nSee [CONTRIBUTING.md](https://github.com/cogeotiff/rio-tiler/blob/main/CONTRIBUTING.md)\n\nAuthors\n\nThe `rio-tiler` project was begun at Mapbox and was transferred to the `cogeotiff` Github organization in January 2019.\n\nSee [AUTHORS.txt](https://github.com/cogeotiff/rio-tiler/blob/main/AUTHORS.txt) for a listing of individual contributors.\n\nChanges\n\nSee [CHANGES.md](https://github.com/cogeotiff/rio-tiler/blob/main/CHANGES.md).\n\nLicense\n\nSee [LICENSE](https://github.com/cogeotiff/rio-tiler/blob/main/LICENSE)"}, {"name": "rio-tiler", "tags": ["data", "math", "web"], "summary": "User friendly Rasterio plugin to read raster datasets.", "text": "This library is used to read raster datasets from various sources, including local files, HTTP, AWS S3, and Google Cloud Storage. It provides user-friendly helper methods for accessing data and metadata from these raster sources, supported by Rasterio/GDAL."}, {"name": "rioxarray", "tags": ["math", "web"], "summary": "geospatial xarray extension powered by rasterio", "text": "================\nrioxarray README\n================\n\nrasterio xarray extension.\n\n   :alt: Join the chat at https://gitter.im/rioxarray/community\n\n.. image:: https://img.shields.io/pypi/v/rioxarray.svg\n\n.. image:: https://img.shields.io/conda/vn/conda-forge/rioxarray.svg\n\n.. image:: https://ci.appveyor.com/api/projects/status/e6sr22mkpen261c1/branch/master?svg=true\n\nDocumentation\n-------------\n\n- Stable: https://corteva.github.io/rioxarray/stable/\n- Latest: https://corteva.github.io/rioxarray/latest/\n\nBugs/Questions\n--------------\n\nCredits\n-------\n\nThe *reproject* functionality was adopted from https://github.com/opendatacube/datacube-core\n  - Source file: `geo_xarray.py `_\n  - `datacube is licensed `_ under the Apache License, Version 2.0.\n\nAdoptions from https://github.com/pydata/xarray:\n  - *open_rasterio*: `rasterio_.py `_\n  - *set_options*: `options.py `_\n  - `xarray is licensed `_ under the Apache License, Version 2.0.\n\nRasterioWriter dask write functionality was adopted from https://github.com/dymaxionlabs/dask-rasterio\n  - Source file: `write.py `_\n\nThis package was originally templated with with Cookiecutter_.\n\n.. _Cookiecutter: https://github.com/audreyr/cookiecutter"}, {"name": "rioxarray", "tags": ["math", "web"], "summary": "geospatial xarray extension powered by rasterio", "text": "This library is used to integrate geospatial raster data with xarray for efficient and scalable analysis and processing. This integration enables developers to work seamlessly with spatial data using a powerful and flexible array-based interface."}, {"name": "rq", "tags": ["math"], "summary": "RQ is a simple, lightweight, library for creating background jobs, and processing them.", "text": "Support RQ\n\nIf you find RQ useful, please consider supporting this project via [Tidelift](https://tidelift.com/subscription/pkg/pypi-rq?utm_source=pypi-rq&utm_medium=referral&utm_campaign=readme).\n\nGetting started\n\nFirst, run a Redis server, of course:\n\nTo put jobs on queues, you don't have to do anything special, just define\nyour typically lengthy or blocking function:\n\nThen, create an RQ queue:\n\nAnd enqueue the function call:\n\nJob Prioritization\n\nBy default, jobs are added to the end of a single queue. RQ offers two ways to give certain jobs higher priority:\n\n1. Enqueue at the front\n\nYou can enqueue a job at the front of its queue so it\u2019s picked up before other jobs:\n\n2. Use multiple queues\nYou can create multiple queues and enqueue jobs into different queues based on their priority:\n\nThen start workers with a prioritized queue list:\n\nThis command starts a worker that listens to both `high` and `low` queues. The worker will process\njobs from the `high` queue first, followed by the `low` queue. You can also run different workers\nfor different queues, allowing you to scale your workers based on the number of jobs in each queue.\n\nScheduling Jobs\n\nScheduling jobs is also easy:\n\nRepeating Jobs\n\nTo execute a `Job` multiple times, use the `Repeat` class:\n\nRetrying Failed Jobs\n\nRetrying failed jobs is also supported:\n\nFor a more complete example, refer to the [docs][d].  But this is the essence.\n\nInterval and Cron Job Scheduling\n\nRQ >= 2.5 provides built-in job scheduling functionality that supports both simple interval-based scheduling and flexible cron syntax.\n\nFirst, create a configuration file (e.g., `cron_config.py`) that defines the jobs you want to run periodically.\n\nAnd then start the `rq cron` command to enqueue these jobs at specified intervals:\n\nYou can also use standard cron syntax for more flexible scheduling:\n\npython\n\nMore details on functionality can be found in the [docs](https://python-rq.org/docs/cron/).\n\nThe Worker\n\nTo start executing enqueued function calls in the background, start a worker\nfrom your project's directory:\n\nTo run multiple workers in production, use process managers like `systemd`. RQ also ships with a `worker-pool` that lets you run multiple worker processes with a single command.\n\nMore options are documented on [python-rq.org](https://python-rq.org/docs/workers/).\n\nInstallation\n\nSimply use the following command to install the latest released version:\n\nNotes on Performance\n\n**TL;DR \u2014 run `Worker` or `SpawnWorker` in production.**\n\nIn a simple hello world [microbenchmark](docs/benchmark.md), `SimpleWorker` processed 1,000 jobs in just 1.02 seconds vs. 6.64 seconds with the default `Worker`), more than 6x faster.\n\n`SimpleWorker` is faster because it skips `fork()` or `spawn()` and runs jobs in process. `Worker` and `SpawnWorker` run each job in a separate process, acting as a sandbox that isolates crashes, memory leaks and enforce hard time-outs.\n\nAlthough `SimpleWorker` is faster in benchmarks, this overhead is negligible in most real world applications like sending emails, generating reports, processing images, etc. In production systems, the time spent performing jobs usually dwarfs any queueing/worker overhead.\n\nUse `SimpleWorker` in production only if:\n* Your jobs are extremely short-lived (single digit milliseconds).\n* The `fork()` or `spawn()` latency is a proven bottleneck at your traffic levels.\n* Your job code is 100% trusted and known to be free of resource leaks and the possibility of crashing/segfaults.\n\nDocs\n\nTo build and run the docs, install [jekyll](https://jekyllrb.com/docs/) and run:\n\nRelated Projects\n\nIf you use RQ, Check out these below repos which might be useful in your rq based project.\n\nProject history\n\nThis project has been inspired by the good parts of [Celery][1], [Resque][2]\nand [this snippet][3], and has been created as a lightweight alternative to the\nheaviness of Celery or other AMQP-based queueing implementations.\n\nRQ is maintained by [Stamps](https://stamps.id), an Indonesian based company that provides enterprise grade CRM and order management systems."}, {"name": "rq", "tags": ["math"], "summary": "RQ is a simple, lightweight, library for creating background jobs, and processing them.", "text": "This library is used to create background jobs that can be processed asynchronously, allowing developers to offload lengthy or blocking operations from their main application flow. With RQ, developers can easily prioritize and manage these background tasks using features like job prioritization and multiple queue support."}, {"name": "rtree", "tags": ["data", "math", "web"], "summary": "R-Tree spatial index for Python GIS", "text": "Rtree: Spatial indexing for Python\n\n(https://github.com/Toblerity/rtree/actions/workflows/test.yml)\n\nRtree is a [ctypes](https://docs.python.org/3/library/ctypes.html) Python wrapper of [libspatialindex](https://libspatialindex.org/) that provides a\nnumber of advanced spatial indexing features for the spatially curious Python\nuser.  These features include:\n\n* Nearest neighbor search\n* Intersection search\n* Multi-dimensional indexes\n* Clustered indexes (store Python pickles directly with index entries)\n* Bulk loading\n* Deletion\n* Disk serialization\n* Custom storage implementation (to implement spatial indexing in ZODB, for example)\n\nWheels are available for most major platforms, and `rtree` with bundled `libspatialindex` can be installed via pip:\n\nSee [changes](https://rtree.readthedocs.io/en/latest/changes.html) for all versions."}, {"name": "rtree", "tags": ["data", "math", "web"], "summary": "R-Tree spatial index for Python GIS", "text": "This library is used to efficiently store and query large spatial datasets in Python, enabling features like nearest neighbor search, intersection detection, and bulk loading. With rtree, developers can optimize their GIS applications for faster performance and better scalability."}, {"name": "ruptures", "tags": ["math"], "summary": "Change point detection for signals in Python.", "text": "Welcome to ruptures\n\n(https://GitHub.com/deepcharles/ruptures/graphs/commit-activity)\n(https://github.com/deepcharles/ruptures/actions/workflows/run-test.yml)\n\n(https://anaconda.org/conda-forge/ruptures)\n(https://github.com/deepcharles/ruptures/actions/workflows/check-docs.yml)\n\n(https://pepy.tech/project/ruptures)\n\n(https://mybinder.org/v2/gh/deepcharles/ruptures/master)\n(https://app.codecov.io/gh/deepcharles/ruptures/branch/master)\n\n`ruptures` is a Python library for off-line change point detection.\nThis package provides methods for the analysis and segmentation of non-stationary signals.  Implemented algorithms include exact and approximate detection for various parametric and non-parametric models.\n`ruptures` focuses on ease of use by providing a well-documented and consistent interface.\nIn addition, thanks to its modular structure, different algorithms and models can be connected and extended within this package.\n\n**How to cite.** If you use `ruptures` in a scientific publication, we would appreciate citations to the following paper:\n\n- C. Truong, L. Oudre, N. Vayatis. Selective review of offline change point detection methods. _Signal Processing_, 167:107299, 2020. [[journal]](https://doi.org/10.1016/j.sigpro.2019.107299) [[pdf]](http://www.laurentoudre.fr/publis/TOG-SP-19.pdf)\n\nLatest news\n\n- Welcome to our new PhD student, [Nicolas Cecchi](https://fr.linkedin.com/in/nicolascecchi/fr)! He will integrate new algorithms in ruptures and create tutorials and illustrative examples.\n\n- NASA uses ruptures! They monitor crops in California and Iran. We submitted an article, let's hope for the best!!\n  - Jalilvand, E., Kumar, S.V., Haacker, E., Truong, C., Mahanama, S., 2024, Characterizing spatiotemporal variability in irrigation extent and timing through thermal remote sensing, submitted to Remote Sensing of Environment.\n\n- We have been contacted by CHELSEA FC to monitor players using ruptures. Stay tuned...\n\n- We use ruptures to understand the regulation of acetylcholine, an important neurotransmitter that plays a role in muscle contraction (involved in myasthenia gravis). Check out our work at the [Journal of Physiology](https://doi.org/10.1113/JP287243)\n\n- They use ruptures to detect changes in classroom engagement and student participation in Japan. [Check out their work](https://doi.org/10.1186/s40561-024-00317-6)\n\n- ruptures is part of a larger pipeline to observe Earth, in particular marine biodiversity. The authors use it to find change in the phytoplankton diversity. [Check out their work](https://doi.org/10.1007/s10712-024-09859-3)\n\n- We work with Croatian crystallographers (hi [Zoran \u0160tefani\u0107](https://www.irb.hr/eng/About-RBI/People/Zoran-Stefanic)!) to [understand protein motions using angular diagrams](https://pubs.acs.org/doi/10.1021/acs.jcim.4c00650). We will keep you posted for our next joint publications.\n\n- [Charles Truong](https://charles.doffy.net) presented ruptures at [PyConDE & PyData Berlin 2024](https://pretalx.com/pyconde-pydata-2024/speaker/BFRLAK/). [Check out the video.](https://kiwi.cmla.ens-cachan.fr/index.php/s/ss3rZwNSKwGtyQW)\n\nBasic usage\n\n(Please refer to the [documentation](https://centre-borelli.github.io/ruptures-docs/ \"Link to documentation\") for more advanced use.)\n\nThe following snippet creates a noisy piecewise constant signal, performs a penalized kernel change point detection and displays the results (alternating colors mark true regimes and dashed lines mark estimated change points).\n\nGeneral information\n\nContact\n\nConcerning this package, its use and bugs, use the [issue page](https://github.com/deepcharles/ruptures/issues) of the [ruptures repository](https://github.com/deepcharles/ruptures). For other inquiries, you can contact me [here](https://charles.doffy.net/contact/).\n\nImportant links\n\n- [Documentation](https://centre-borelli.github.io/ruptures-docs)\n- [Pypi package index](https://pypi.python.org/pypi/ruptures)\n\nDependencies and install\n\nInstallation instructions can be found [here](https://centre-borelli.github.io/ruptures-docs/install/).\n\nChangelog\n\nSee the [changelog](https://github.com/deepcharles/ruptures/blob/master/CHANGELOG.md) for a history of notable changes to `ruptures`.\n\nThanks to all our contributors\n\nLicense\n\nThis project is under BSD license."}, {"name": "ruptures", "tags": ["math"], "summary": "Change point detection for signals in Python.", "text": "This library is used to identify changes or breaks in signal data, allowing developers to analyze and segment non-stationary signals. With ruptures, developers can implement exact and approximate change point detection algorithms for various parametric and non-parametric models."}, {"name": "sbvirtualdisplay", "tags": ["math", "web"], "summary": "A customized pyvirtualdisplay for SeleniumBase.", "text": "sbVirtualDisplay () (https://pypi.python.org/pypi/sbvirtualdisplay)\n\nA customized [pyvirtualdisplay](https://github.com/ponty/PyVirtualDisplay) for use with [SeleniumBase](https://github.com/seleniumbase/SeleniumBase) automation.\n\nUsage example:\n\nOr as a context manager:\n\nWhen to use:\n\nIf you need to run browser tests on a headless machine (such as a Linux backend), and you can't use a browser's headless mode (such as Chrome's headless mode), then this may help. For example, Chrome does not allow extensions in headless mode, so if you need to run automated tests on a headless Linux machine and you need to use Chrome extensions, then this will let you run those tests using a virtual display.\n\nMore info:\n\n* [Xvfb](https://en.wikipedia.org/wiki/Xvfb) is required for this to work."}, {"name": "sbvirtualdisplay", "tags": ["math", "web"], "summary": "A customized pyvirtualdisplay for SeleniumBase.", "text": "This library is used to provide a customized virtual display solution for SeleniumBase automation on headless machines, allowing developers to run browser tests that require extensions or cannot use headless mode. With sbvirtualdisplay, developers can successfully execute automated tests on Linux backends without relying on a browser's native headless capabilities."}, {"name": "scanpy", "tags": ["data", "dev", "math", "visualization", "web"], "summary": "Single-Cell Analysis in Python.", "text": "Scanpy \u2013 Single-Cell Analysis in Python\n\nScanpy is a scalable toolkit for analyzing single-cell gene expression data\nbuilt jointly with [anndata][].  It includes\npreprocessing, visualization, clustering, trajectory inference and differential\nexpression testing.  The Python-based implementation efficiently deals with\ndatasets of more than one million cells.\n\nDiscuss usage on the scverse [Discourse][]. Read the [documentation][].\nIf you'd like to contribute by opening an issue or creating a pull request, please take a look at our [contribution guide][].\n\n[//]: # (numfocus-fiscal-sponsor-attribution)\n\nscanpy is part of the scverse project ([website](https://scverse.org), [governance](https://scverse.org/about/roles)) and is fiscally sponsored by [NumFOCUS](https://numfocus.org/).\nIf you like scverse and want to support our mission, please consider making a [donation](https://numfocus.org/donate-to-scverse) to support our efforts.\n\nCitation\n\nIf you use `scanpy` in your work, please cite the `scanpy` publication as follows:\n\n> **SCANPY: large-scale single-cell gene expression data analysis**\n>\n> F. Alexander Wolf, Philipp Angerer, Fabian J. Theis\n>\n> _Genome Biology_ 2018 Feb 06. doi: [10.1186/s13059-017-1382-0](https://doi.org/10.1186/s13059-017-1382-0).\n\nYou can cite the scverse publication as follows:\n\n> **The scverse project provides a computational ecosystem for single-cell omics data analysis**\n>\n> Isaac Virshup, Danila Bredikhin, Lukas Heumos, Giovanni Palla, Gregor Sturm, Adam Gayoso, Ilia Kats, Mikaela Koutrouli, Scverse Community, Bonnie Berger, Dana Pe\u2019er, Aviv Regev, Sarah A. Teichmann, Francesca Finotello, F. Alexander Wolf, Nir Yosef, Oliver Stegle & Fabian J. Theis\n>\n> _Nat Biotechnol._ 2023 Apr 10. doi: [10.1038/s41587-023-01733-8](https://doi.org/10.1038/s41587-023-01733-8).\n\n[contribution guide]: CONTRIBUTING.md"}, {"name": "scanpy", "tags": ["data", "dev", "math", "visualization", "web"], "summary": "Single-Cell Analysis in Python.", "text": "This library is used to efficiently analyze single-cell gene expression data with features including preprocessing, visualization, clustering, trajectory inference, and differential expression testing. With scanpy, developers can easily handle large datasets of over one million cells in Python."}, {"name": "scikit-base", "tags": ["math"], "summary": "Base classes for sklearn-like parametric objects", "text": "Welcome to skbase\n\n> A framework factory for scikit-learn-like and sktime-like parametric objects\n\n`skbase` provides base classes for creating scikit-learn-like parametric objects,\nalong with tools to make it easier to build your own packages that follow these design patterns.\n\n:rocket: Version 0.13.0 is now available. Check out our\n[release notes](https://skbase.readthedocs.io/en/latest/changelog.html).\n\nOverview\n---\n\n**Code**\n**Downloads**\n**Citation**\n\n(#contributors)\n\nDocumentation and Tutorials\n\nTo learn more about the package check out:\n\n* our [documentation](https://skbase.readthedocs.io/en/latest/)\n* our [introductory tutorial](https://github.com/sktime/sktime-tutorial-pydata-seattle-2023) (jupyter notebooks and video presentation)\n\n:hourglass_flowing_sand: Install skbase\nFor trouble shooting or more information, see our\n[detailed installation instructions](https://skbase.readthedocs.io/en/latest/user_documentation/installation.html).\n\n- **Operating system**: macOS \u00b7 Linux \u00b7 Windows 8.1 or higher\n- **Python version**: Python 3.10, 3.11, 3.12, 3.13, and 3.14\n- **Package managers**: [pip]\n\npip\nskbase releases are available as source packages and binary wheels via PyPI\nand can be installed using pip. Checkout the full list of pre-compiled [wheels on PyPi](https://pypi.org/simple/skbase/).\n\nTo install the core package use:\n\nor, if you want to install with the maximum set of dependencies, use:\n\nContributors \n\nThis project follows the\n[all-contributors](https://github.com/all-contributors/all-contributors) specification.\nContributions of any kind welcome!\n\nThanks go to these wonderful people:\n\n[skbase contributors](https://github.com/sktime/skbase/graphs/contributors)"}, {"name": "scikit-base", "tags": ["math"], "summary": "Base classes for sklearn-like parametric objects", "text": "This library is used to create scikit-learn-like parametric objects with a consistent design pattern, enabling developers to build their own packages that follow these standards. With skbase, developers can easily construct and manage complex parametric objects for machine learning tasks."}, {"name": "scikit-image", "tags": ["math"], "summary": "Image processing in Python", "text": "scikit-image: Image processing in Python\n\n(https://forum.image.sc/tags/scikit-image)\n(https://stackoverflow.com/questions/tagged/scikit-image)\n(https://skimage.zulipchat.com)\n(https://scientific-python.org/specs/)\n(https://insights.linuxfoundation.org/project/scikit-image-scikit-image)\n\nInstallation\n\n- **pip:** `pip install scikit-image`\n- **conda:** `conda install -c conda-forge scikit-image`\n\nAlso see [installing `scikit-image`](https://github.com/scikit-image/scikit-image/blob/main/INSTALL.rst).\n\nLicense\n\nSee [LICENSE.txt](https://github.com/scikit-image/scikit-image/blob/main/LICENSE.txt).\n\nCitation\n\nIf you find this project useful, please cite:\n\n> St\u00e9fan van der Walt, Johannes L. Sch\u00f6nberger, Juan Nunez-Iglesias,\n> Fran\u00e7ois Boulogne, Joshua D. Warner, Neil Yager, Emmanuelle\n> Gouillart, Tony Yu, and the scikit-image contributors.\n> _scikit-image: Image processing in Python_. PeerJ 2:e453 (2014)\n> https://doi.org/10.7717/peerj.453"}, {"name": "scikit-image", "tags": ["math"], "summary": "Image processing in Python", "text": "This library is used to perform a wide range of image processing tasks in Python, including filtering, thresholding, and feature extraction. Developers can leverage scikit-image to build robust image analysis applications with ease."}, {"name": "scikit-learn", "tags": ["dev", "math", "web"], "summary": "A set of python modules for machine learning and data mining", "text": ".. -*- mode: rst -*-\n\nAzure\n\n.. |Azure| image:: https://dev.azure.com/scikit-learn/scikit-learn/_apis/build/status/scikit-learn.scikit-learn?branchName=main\n   :target: https://dev.azure.com/scikit-learn/scikit-learn/_build/latest?definitionId=1&branchName=main\n\n.. |CircleCI| image:: https://circleci.com/gh/scikit-learn/scikit-learn/tree/main.svg?style=shield\n   :target: https://circleci.com/gh/scikit-learn/scikit-learn\n\n   :target: https://codecov.io/gh/scikit-learn/scikit-learn\n\n   :target: https://github.com/scikit-learn/scikit-learn/actions?query=workflow%3A%22Wheel+builder%22+event%3Aschedule\n\n   :target: https://github.com/astral-sh/ruff\n\n.. |PythonVersion| image:: https://img.shields.io/pypi/pyversions/scikit-learn.svg\n   :target: https://pypi.org/project/scikit-learn/\n\n.. |PyPI| image:: https://img.shields.io/pypi/v/scikit-learn\n   :target: https://pypi.org/project/scikit-learn\n\n   :target: https://scikit-learn.org/scikit-learn-benchmarks\n\n.. |PythonMinVersion| replace:: 3.11\n.. |NumPyMinVersion| replace:: 1.24.1\n.. |SciPyMinVersion| replace:: 1.10.0\n.. |JoblibMinVersion| replace:: 1.3.0\n.. |ThreadpoolctlMinVersion| replace:: 3.2.0\n.. |MatplotlibMinVersion| replace:: 3.6.1\n.. |Scikit-ImageMinVersion| replace:: 0.22.0\n.. |PandasMinVersion| replace:: 1.5.0\n.. |SeabornMinVersion| replace:: 0.13.0\n.. |PytestMinVersion| replace:: 7.1.2\n.. |PlotlyMinVersion| replace:: 5.18.0\n\n.. image:: https://raw.githubusercontent.com/scikit-learn/scikit-learn/main/doc/logos/scikit-learn-logo.png\n  :target: https://scikit-learn.org/\n\n**scikit-learn** is a Python module for machine learning built on top of\nSciPy and is distributed under the 3-Clause BSD license.\n\nThe project was started in 2007 by David Cournapeau as a Google Summer\nof Code project, and since then many volunteers have contributed. See\nthe `About us `__ page\nfor a list of core contributors.\n\nIt is currently maintained by a team of volunteers.\n\nWebsite: https://scikit-learn.org\n\nInstallation\n------------\n\nDependencies\n~~~~~~~~~~~~\n\nscikit-learn requires:\n\n- Python (>= |PythonMinVersion|)\n- NumPy (>= |NumPyMinVersion|)\n- SciPy (>= |SciPyMinVersion|)\n- joblib (>= |JoblibMinVersion|)\n- threadpoolctl (>= |ThreadpoolctlMinVersion|)\n\n=======\n\nScikit-learn plotting capabilities (i.e., functions start with ``plot_`` and\nclasses end with ``Display``) require Matplotlib (>= |MatplotlibMinVersion|).\nFor running the examples Matplotlib >= |MatplotlibMinVersion| is required.\nA few examples require scikit-image >= |Scikit-ImageMinVersion|, a few examples\nrequire pandas >= |PandasMinVersion|, some examples require seaborn >=\nSeabornMinVersion\n\nUser installation\n~~~~~~~~~~~~~~~~~\n\nIf you already have a working installation of NumPy and SciPy,\nthe easiest way to install scikit-learn is using ``pip``::\n\nor ``conda``::\n\nThe documentation includes more detailed `installation instructions `_.\n\nChangelog\n---------\n\nSee the `changelog `__\nfor a history of notable changes to scikit-learn.\n\nDevelopment\n-----------\n\nWe welcome new contributors of all experience levels. The scikit-learn\ncommunity goals are to be helpful, welcoming, and effective. The\n`Development Guide `_\nhas detailed information about contributing code, documentation, tests, and\nmore. We've included some basic information in this README.\n\nImportant links\n~~~~~~~~~~~~~~~\n\nSource code\n~~~~~~~~~~~\n\nYou can check the latest sources with the command::\n\nContributing\n~~~~~~~~~~~~\n\nTo learn more about making a contribution to scikit-learn, please see our\n`Contributing guide\n`_.\n\nTesting\n~~~~~~~\n\nAfter installation, you can launch the test suite from outside the source\ndirectory (you will need to have ``pytest`` >= |PytestMinVersion| installed)::\n\nSee the web page https://scikit-learn.org/dev/developers/contributing.html#testing-and-improving-test-coverage\nfor more information.\n\nSubmitting a Pull Request\n~~~~~~~~~~~~~~~~~~~~~~~~~\n\nBefore opening a Pull Request, have a look at the\nfull Contributing page to make sure your code complies\nwith our guidelines: https://scikit-learn.org/stable/developers/index.html\n\nProject History\n---------------\n\nThe project was started in 2007 by David Cournapeau as a Google Summer\nof Code project, and since then many volunteers have contributed. See\nthe `About us `__ page\nfor a list of core contributors.\n\nThe project is currently maintained by a team of volunteers.\n\n**Note**: `scikit-learn` was previously referred to as `scikits.learn`.\n\nHelp and Support\n----------------\n\nDocumentation\n~~~~~~~~~~~~~\n\nCommunication\n~~~~~~~~~~~~~\n\nMain Channels\n^^^^^^^^^^^^^\n\nDeveloper & Support\n^^^^^^^^^^^^^^^^^^^^^^\n\nSocial Media Platforms\n^^^^^^^^^^^^^^^^^^^^^^\n\nResources\n^^^^^^^^^\n\n- **Calendar**: https://blog.scikit-learn.org/calendar/\n- **Logos & Branding**: https://github.com/scikit-learn/scikit-learn/tree/main/doc/logos\n\nCitation\n~~~~~~~~\n\nIf you use scikit-learn in a scientific publication, we would appreciate citations: https://scikit-learn.org/stable/about.html#citing-scikit-learn"}, {"name": "scikit-learn", "tags": ["dev", "math", "web"], "summary": "A set of python modules for machine learning and data mining", "text": "This library is used to implement various machine learning algorithms and data mining techniques in Python, enabling developers to build complex models for classification, regression, clustering, and more. With scikit-learn, developers can efficiently train and deploy predictive models on large datasets, streamlining the process of extracting insights from data."}, {"name": "scikit-optimize", "tags": ["dev", "math", "web"], "summary": "Sequential model-based optimization toolbox.", "text": "Logo\n\npypi\n\nScikit-Optimize\n===============\n\nScikit-Optimize, or ``skopt``, is a simple and efficient library for\noptimizing (very) expensive and noisy black-box functions. It implements\nseveral methods for sequential model-based optimization. ``skopt`` aims\nto be accessible and easy to use in many contexts.\n\nThe library is built on top of NumPy, SciPy, and Scikit-Learn.\n\nWe do not perform gradient-based optimization. For gradient-based\noptimization algorithms look at\n``scipy.optimize``\n`here `_.\n\n.. figure:: https://raw.githubusercontent.com/holgern/scikit-optimize/main/media/bo-objective.png\n   :alt: Approximated objective\n\nApproximated objective function after 50 iterations of ``gp_minimize``.\nPlot made using ``skopt.plots.plot_objective``.\n\nMaintaining the codebase\n------------------------\nThis repo is a copy of the original repositoy at https://github.com/scikit-optimize/scikit-optimize/.\nAs the original repo is now in read-only mode, i decided to continue the development on it on my own.\nI still have credentials for pypi, so I will publish new releases at https://pypi.org/project/scikit-optimize/.\nI did my best to include all open PR since 2021 in the new release of scikit-optimize 0.10.\n\nImportant links\n---------------\n\n-  Project website https://scikit-optimize.readthedocs.io/\n-  Example notebooks - can be found in examples_.\n-  `Discussion forum\n   `__\n-  Issue tracker -\n-  Releases - https://pypi.python.org/pypi/scikit-optimize\n-  Conda feedstock - https://github.com/conda-forge/scikit-optimize-feedstock\n\nInstall\n-------\n\nscikit-optimize requires\n\n* Python >= 3.8\n* NumPy (>= 1.20.3)\n* SciPy (>= 0.19.1)\n* joblib (>= 0.11)\n* scikit-learn >= 1.0.0\n* matplotlib >= 2.0.0\n\nYou can install the latest release with:\n::\n\nThis installs the essentials. To install plotting functionality,\nyou can instead do:\n::\n\nThis will additionally install Matplotlib.\n\nIf you're using Anaconda platform, there is a `conda-forge `_\npackage of scikit-optimize:\n::\n\nUsing conda-forge is probably the easiest way to install scikit-optimize on\nWindows.\n\nGetting started\n---------------\n\nFind the minimum of the noisy function ``f(x)`` over the range\n``-2 `__\nand the other examples_.\n\nDevelopment\n-----------\n\nThe library is still experimental and under development. Checkout\nthe `next\nmilestone `__\nfor the plans for the next release or look at some `easy\nissues `__\nto get started contributing.\n\nThe development version can be installed through:\n\n::\n\nRun all tests by executing ``pytest`` in the top level directory.\n\nTo only run the subset of tests with short run time, you can use ``pytest -m 'fast_test'`` (``pytest -m 'slow_test'`` is also possible). To exclude all slow running tests try ``pytest -m 'not slow_test'``.\n\nThis is implemented using pytest `attributes `__. If a tests runs longer than 1 second, it is marked as slow, else as fast.\n\nAll contributors are welcome!\n\nPre-commit-config\n-----------------\n\nInstallation\n~~~~~~~~~~~~\n\n::\n\nUsing homebrew\n~~~~~~~~~~~~~~\n::\n\nInstall the git hook scripts\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n::\n\nRun against all the files\n~~~~~~~~~~~~~~~~~~~~~~~~~\n::\n\nUpdate package rev in pre-commit yaml\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n::\n\nMaking a Release\n~~~~~~~~~~~~~~~~\n\nThe release procedure is almost completely automated. By tagging a new release,\nCI will build all required packages and push them to PyPI. To make a release,\ncreate a new issue and work through the following checklist:\n\n* [ ] check if the dependencies in `setup.py` are valid or need unpinning,\n* [ ] check that the `doc/whats_new/v0.X.rst` is up-to-date,\n* [ ] did the last build of master succeed?\n* [ ] create a [new release](https://github.com/holgern/scikit-optimize/releases),\n* [ ] ping [conda-forge](https://github.com/conda-forge/scikit-optimize-feedstock).\n\nBefore making a release, we usually create a release candidate. If the next\nrelease is v0.X, then the release candidate should be tagged v0.Xrc1.\nMark the release candidate as a \"pre-release\" on GitHub when you tag it.\n\nMade possible by\n----------------\n\nThe scikit-optimize project was made possible with the support of\n\n.. image:: https://avatars1.githubusercontent.com/u/18165687?v=4&s=128\n   :alt: Wild Tree Tech\n   :target: https://wildtreetech.com\n\n.. image:: https://i.imgur.com/lgxboT5.jpg\n\n.. image:: https://i.imgur.com/V1VSIvj.jpg\n\n.. image:: https://i.imgur.com/3enQ6S8.jpg\n\nIf your employer allows you to work on scikit-optimize during the day and would like\nrecognition, feel free to add them to the \"Made possible by\" list.\n\n.. |pypi| image:: https://img.shields.io/pypi/v/scikit-optimize.svg\n   :target: https://pypi.python.org/pypi/scikit-optimize\n\n   :target: https://anaconda.org/conda-forge/scikit-optimize\n\n   :target: https://github.com/holgern/scikit-optimize/actions/workflows/ci.yml?query=branch%3Amain\n.. |Logo| image:: https://avatars2.githubusercontent.com/u/18578550?v=4&s=80\n\n   :target: https://mybinder.org/v2/gh/holgern/scikit-optimize/main?filepath=examples\n\n   :target: https://zenodo.org/doi/10.5281/zenodo.10804382\n.. |scipy.optimize| replace:: ``scipy.optimize``\n.. _scipy.optimize: https://docs.scipy.org/doc/scipy/reference/optimize.html\n.. _examples: https://scikit-optimize.readthedocs.io/en/latest/auto_examples/index.html\n\n   :target: https://codecov.io/gh/holgern/scikit-optimize"}, {"name": "scikit-optimize", "tags": ["dev", "math", "web"], "summary": "Sequential model-based optimization toolbox.", "text": "This library is used to efficiently optimize expensive and noisy black-box functions through various sequential model-based optimization methods. Developers can use scikit-optimize to find optimal solutions with minimal computational resources, making it suitable for complex tasks that require multiple iterations of function evaluation."}, {"name": "scipy", "tags": ["math", "web"], "summary": "Fundamental algorithms for scientific computing in Python", "text": ".. image:: https://raw.githubusercontent.com/scipy/scipy/main/doc/source/_static/logo.svg\n  :target: https://scipy.org\n  :width: 110\n  :height: 110\n  :align: left \n\n  :target: https://numfocus.org\n\n.. image:: https://img.shields.io/pypi/dm/scipy.svg?label=Pypi%20downloads\n  :target: https://pypi.org/project/scipy/\n\n.. image:: https://img.shields.io/conda/dn/conda-forge/scipy.svg?label=Conda%20downloads\n  :target: https://anaconda.org/conda-forge/scipy\n\n  :target: https://stackoverflow.com/questions/tagged/scipy\n\n  :target: https://www.nature.com/articles/s41592-019-0686-2\n\nSciPy (pronounced \"Sigh Pie\") is an open-source software for mathematics,\nscience, and engineering. It includes modules for statistics, optimization,\nintegration, linear algebra, Fourier transforms, signal and image processing,\nODE solvers, and more.\n\nSciPy is built to work with\nNumPy arrays, and provides many user-friendly and efficient numerical routines,\nsuch as routines for numerical integration and optimization. Together, they\nrun on all popular operating systems, are quick to install, and are free of\ncharge. NumPy and SciPy are easy to use, but powerful enough to be depended\nupon by some of the world's leading scientists and engineers. If you need to\nmanipulate numbers on a computer and display or publish the results, give\nSciPy a try!\n\nFor the installation instructions, see `our install\nguide `__.\n\nCall for Contributions\n----------------------\n\nWe appreciate and welcome contributions. Small improvements or fixes are always appreciated; issues labeled as \"good\nfirst issue\" may be a good starting point. Have a look at `our contributing\nguide `__.\n\nWriting code isn\u2019t the only way to contribute to SciPy. You can also:\n\n- review pull requests\n- triage issues\n- develop tutorials, presentations, and other educational materials\n- maintain and improve `our website `__\n- develop graphic design for our brand assets and promotional materials\n- help with outreach and onboard new contributors\n- write grant proposals and help with other fundraising efforts\n\nIf you\u2019re unsure where to start or how your skills fit in, reach out! You can\nask on the `forum `__\nor here, on GitHub, by leaving a comment on a relevant issue that is already\nopen.\n\nIf you are new to contributing to open source, `this\nguide `__ helps explain why, what,\nand how to get involved."}, {"name": "scipy", "tags": ["math", "web"], "summary": "Fundamental algorithms for scientific computing in Python", "text": "This library is used to accelerate scientific computing tasks with robust algorithms for statistical analysis, numerical integration, optimization, and data processing in Python. With SciPy, developers can efficiently tackle complex mathematical problems and data manipulation tasks in various fields of science, engineering, and research."}, {"name": "seaborn", "tags": ["dev", "math", "visualization", "web"], "summary": "Statistical data visualization", "text": "--------------------------------------\n\nseaborn: statistical data visualization\n=======================================\n\n(https://pypi.org/project/seaborn/)\n(https://github.com/mwaskom/seaborn/blob/master/LICENSE.md)\n(https://doi.org/10.21105/joss.03021)\n(https://github.com/mwaskom/seaborn/actions)\n(https://codecov.io/gh/mwaskom/seaborn)\n\nSeaborn is a Python visualization library based on matplotlib. It provides a high-level interface for drawing attractive statistical graphics.\n\nDocumentation\n-------------\n\nOnline documentation is available at [seaborn.pydata.org](https://seaborn.pydata.org).\n\nThe docs include a [tutorial](https://seaborn.pydata.org/tutorial.html), [example gallery](https://seaborn.pydata.org/examples/index.html), [API reference](https://seaborn.pydata.org/api.html), [FAQ](https://seaborn.pydata.org/faq), and other useful information.\n\nTo build the documentation locally, please refer to [`doc/README.md`](doc/README.md).\n\nDependencies\n------------\n\nSeaborn supports Python 3.8+.\n\nInstallation requires [numpy](https://numpy.org/), [pandas](https://pandas.pydata.org/), and [matplotlib](https://matplotlib.org/). Some advanced statistical functionality requires [scipy](https://www.scipy.org/) and/or [statsmodels](https://www.statsmodels.org/).\n\nInstallation\n------------\n\nThe latest stable release (and required dependencies) can be installed from PyPI:\n\nIt is also possible to include optional statistical dependencies:\n\nSeaborn can also be installed with conda:\n\nNote that the main anaconda repository lags PyPI in adding new releases, but conda-forge (`-c conda-forge`) typically updates quickly.\n\nCiting\n------\n\nA paper describing seaborn has been published in the [Journal of Open Source Software](https://joss.theoj.org/papers/10.21105/joss.03021). The paper provides an introduction to the key features of the library, and it can be used as a citation if seaborn proves integral to a scientific publication.\n\nTesting\n-------\n\nTesting seaborn requires installing additional dependencies; they can be installed with the `dev` extra (e.g., `pip install .[dev]`).\n\nTo test the code, run `make test` in the source directory. This will exercise the unit tests (using [pytest](https://docs.pytest.org/)) and generate a coverage report.\n\nCode style is enforced with `flake8` using the settings in the [`setup.cfg`](./setup.cfg) file. Run `make lint` to check. Alternately, you can use `pre-commit` to automatically run lint checks on any files you are committing: just run `pre-commit install` to set it up, and then commit as usual going forward.\n\nDevelopment\n-----------\n\nSeaborn development takes place on Github: https://github.com/mwaskom/seaborn\n\nPlease submit bugs that you encounter to the [issue tracker](https://github.com/mwaskom/seaborn/issues) with a reproducible example demonstrating the problem. Questions about usage are more at home on StackOverflow, where there is a [seaborn tag](https://stackoverflow.com/questions/tagged/seaborn)."}, {"name": "seaborn", "tags": ["dev", "math", "visualization", "web"], "summary": "Statistical data visualization", "text": "This library is used to create attractive and informative statistical graphics, providing a high-level interface for drawing visualizations based on matplotlib. With seaborn, developers can easily produce publication-quality plots to effectively communicate insights from their data."}, {"name": "seleniumbase", "tags": ["cli", "dev", "math", "ml", "ui", "visualization", "web"], "summary": "A complete web automation framework for end-to-end testing.", "text": "SeleniumBase\n\nAutomate, test, and scrape the web \u2014 on your own terms.\n\nStart |\n Features |\n\ufe0f Options |\n Examples |\n Scripts |\n Mobile\n\nThe API |\n  SyntaxFormats |\n Recorder |\n Dashboard |\n Locale\n\n\ufe0f GUI |\n TestPage |\n UC Mode |\n CDP Mode |\n Charts  |\n\ufe0f Farm\n\n\ufe0f How |\n Migration |\n Stealthy Playwright |\n MasterQA |\n Tours\n\nCI/CD |\n\ufe0f JSMgr |\n Translator |\n\ufe0f Presenter |\n\ufe0f Visual |\n\ufe0f CPlans\n\nSeleniumBase is a browser automation framework that empowers software teams to innovate faster and handle modern web challenges with ease. With stealth options like CDP Mode, you'll avoid the usual restrictions imposed by websites deploying bot-detection services.\n\n--------\n\nLearn from [**over 200 examples** in the **SeleniumBase/examples/** folder](https://github.com/seleniumbase/SeleniumBase/tree/master/examples).\n\nStealth modes: UC Mode and CDP Mode can bypass bot-detection, solve CAPTCHAs, and call advanced methods from the Chrome Devtools Protocol.\n\n\u2139\ufe0f Many examples run with raw python, although some use Syntax Formats that expect pytest (a Python unit-testing framework included with SeleniumBase that can discover, collect, and run tests automatically).\n\n--------\n\nThis script performs a Google Search using SeleniumBase UC Mode + CDP Mode:SeleniumBase/examples/raw_google.py (Results are saved as PDF, HTML, and PNG)\n\n> `python raw_google.py`\n\n--------\n\nHere's a script that bypasses Cloudflare's challenge page with UC Mode + CDP Mode: SeleniumBase/examples/cdp_mode/raw_gitlab.py\n\nThere's also SeleniumBase's \"Pure CDP Mode\", which doesn't use WebDriver or Selenium at all: SeleniumBase/examples/cdp_mode/raw_cdp_gitlab.py\n\n--------\n\nHere's SeleniumBase/examples/test_get_swag.py, which tests an e-commerce site:\n\n> `pytest test_get_swag.py`\n\n> (The default browser is ``--chrome`` if not set.)\n\n--------\n\nHere's SeleniumBase/examples/test_coffee_cart.py, which verifies an e-commerce site:\n\n> (--demo mode slows down tests and highlights actions)\n\n--------\n\nHere's SeleniumBase/examples/test_demo_site.py, which covers several actions:\n\n> Easy to type, click, select, toggle, drag & drop, and more.\n\n(For more examples, see the SeleniumBase/examples/ folder.)\n\n--------\n\nExplore the README:\n\nGet Started / Installation\nBasic Example / Usage\nCommon Test Methods\nFun Facts / Learn More\nDemo Mode / Debugging\nCommand-line Options\nDirectory Configuration\nSeleniumBase Dashboard\nGenerating Test Reports\n\n--------\n\n\u25b6\ufe0f How is SeleniumBase different from raw Selenium? (click to expand)\n\nSeleniumBase is a Python framework for browser automation and testing. SeleniumBase uses Selenium/WebDriver APIs and incorporates test-runners such as pytest, pynose, and behave to provide organized structure, test discovery, test execution, test state (eg. passed, failed, or skipped), and command-line options for changing default settings (eg. browser selection). With raw Selenium, you would need to set up your own options-parser for configuring tests from the command-line.\n\nSeleniumBase's driver manager gives you more control over automatic driver downloads. (Use --driver-version=VER with your pytest run command to specify the version.) By default, SeleniumBase will download a driver version that matches your major browser version if not set.\n\nSeleniumBase automatically detects between CSS Selectors and XPath, which means you don't need to specify the type of selector in your commands (but optionally you could)."}, {"name": "seleniumbase", "tags": ["cli", "dev", "math", "ml", "ui", "visualization", "web"], "summary": "A complete web automation framework for end-to-end testing.", "text": "SeleniumBase methods often perform multiple actions in a single method call. For example, self.type(selector, text) does the following:1. Waits for the element to be visible.2. Waits for the element to be interactive.3. Clears the text field.4. Types in the new text.5. Presses Enter/Submit if the text ends in \"\\n\".With raw Selenium, those actions require multiple method calls.\n\nSeleniumBase uses default timeout values when not set:\n self.click(\"button\")\nWith raw Selenium, methods would fail instantly (by default) if an element needed more time to load:\n self.driver.find_element(by=\"css selector\", value=\"button\").click()\n(Reliable code is better than unreliable code.)\n\nSeleniumBase lets you change the explicit timeout values of methods:\n self.click(\"button\", timeout=10)\nWith raw Selenium, that requires more code:\n WebDriverWait(driver, 10).until(EC.element_to_be_clickable(\"css selector\", \"button\")).click()\n(Simple code is better than complex code.)\n\nSeleniumBase gives you clean error output when a test fails. With raw Selenium, error messages can get very messy.\n\nSeleniumBase gives you the option to generate a dashboard and reports for tests. It also saves screenshots from failing tests to the ./latest_logs/ folder. Raw Selenium does not have these options out-of-the-box.\n\nSeleniumBase includes desktop GUI apps for running tests, such as SeleniumBase Commander for pytest and SeleniumBase Behave GUI for behave.\n\nSeleniumBase has its own Recorder / Test Generator for creating tests from manual browser actions.\n\nSeleniumBase comes with test case management software, (\"CasePlans\"), for organizing tests and step descriptions.\n\nSeleniumBase includes tools for building data apps, (\"ChartMaker\"), which can generate JavaScript from Python.\n\n--------\n\nLearn about different ways of writing tests:\n\nHere's test_simple_login.py, which uses BaseCase class inheritance, and runs with pytest or pynose. (Use self.driver to access Selenium's raw driver.)\n\nHere's raw_login_sb.py, which uses the SB Context Manager. Runs with pure python. (Use sb.driver to access Selenium's raw driver.)\n\nHere's raw_login_driver.py, which uses the Driver Manager. Runs with pure python. (The driver is an improved version of Selenium's raw driver, with more methods.)\n\n--------\n\nSet up Python & Git:\n\nAdd Python and Git to your System PATH.\n\nUsing a Python virtual env is recommended.\n\nInstall SeleniumBase:\n\n**You can install ``seleniumbase`` from [PyPI](https://pypi.org/project/seleniumbase/) or [GitHub](https://github.com/seleniumbase/SeleniumBase):**\n\n**How to install ``seleniumbase`` from PyPI:**\n\n* (Add ``--upgrade`` OR ``-U`` to upgrade SeleniumBase.)\n* (Add ``--force-reinstall`` to upgrade indirect packages.)\n\n**How to install ``seleniumbase`` from a GitHub clone:**\n\n**How to upgrade an existing install from a GitHub clone:**\n\n**Type ``seleniumbase`` or ``sbase`` to verify that SeleniumBase was installed successfully:**\n\nDownloading webdrivers:\n\nSeleniumBase automatically downloads webdrivers as needed, such as ``chromedriver``.\n\n\u25b6\ufe0f Here's sample output from a chromedriver download. (click to expand)\n\nBasic Example / Usage:\n\nIf you've cloned SeleniumBase, you can run tests from the [examples/](https://github.com/seleniumbase/SeleniumBase/tree/master/examples) folder.\n\nHere's my_first_test.py:\n\nHere's the full code for my_first_test.py:\n\n* By default, **[CSS Selectors](https://www.w3schools.com/cssref/css_selectors.asp)** are used for finding page elements.\n* If you're new to CSS Selectors, games like [CSS Diner](http://flukeout.github.io/) can help you learn.\n* For more reading, [here's an advanced guide on CSS attribute selectors](https://developer.mozilla.org/en-US/docs/Web/CSS/Attribute_selectors).\n\nHere are some common SeleniumBase methods:\n\nFor the complete list of SeleniumBase methods, see: Method Summary\n\nFun Facts / Learn More:"}, {"name": "seleniumbase", "tags": ["cli", "dev", "math", "ml", "ui", "visualization", "web"], "summary": "A complete web automation framework for end-to-end testing.", "text": "SeleniumBase automatically handles common WebDriver actions such as launching web browsers before tests, saving screenshots during failures, and closing web browsers after tests.\n\nSeleniumBase lets you customize tests via command-line options.\n\nSeleniumBase uses simple syntax for commands. Example:\n\nMost SeleniumBase scripts can be run with pytest, pynose, or pure python. Not all test runners can run all test formats. For example, tests that use the ``sb`` pytest fixture can only be run with ``pytest``. (See Syntax Formats) There's also a Gherkin test format that runs with behave.\n\npytest includes automatic test discovery. If you don't specify a specific file or folder to run, pytest will automatically search through all subdirectories for tests to run based on the following criteria:\n\n* Python files that start with ``test_`` or end with ``_test.py``.\n* Python methods that start with ``test_``.\n\nWith a SeleniumBase [pytest.ini](https://github.com/seleniumbase/SeleniumBase/blob/master/examples/pytest.ini) file present, you can modify default discovery settings. The Python class name can be anything because ``seleniumbase.BaseCase`` inherits ``unittest.TestCase`` to trigger autodiscovery.\n\nYou can do a pre-flight check to see which tests would get discovered by pytest before the actual run:\n\nYou can be more specific when calling pytest or pynose on a file:\n\nNo More Flaky Tests! SeleniumBase methods automatically wait for page elements to finish loading before interacting with them (up to a timeout limit). This means you no longer need random time.sleep() statements in your scripts.\n\nSeleniumBase supports all major browsers and operating systems:\nBrowsers: Chrome, Edge, Firefox, and Safari.\nSystems: Linux/Ubuntu, macOS, and Windows.\n\nSeleniumBase works on all popular CI/CD platforms:\n\nSeleniumBase includes an automated/manual hybrid solution called MasterQA to speed up manual testing with automation while manual testers handle validation.\n\nSeleniumBase supports running tests while offline (assuming webdrivers have previously been downloaded when online).\n\nFor a full list of SeleniumBase features, Click Here.\n\nDemo Mode / Debugging:\n\nDemo Mode helps you see what a test is doing. If a test is moving too fast for your eyes, run it in Demo Mode to pause the browser briefly between actions, highlight page elements being acted on, and display assertions:\n\n``time.sleep(seconds)`` can be used to make a test wait at a specific spot:\n\n**Debug Mode** with Python's built-in **[pdb](https://docs.python.org/3/library/pdb.html)** library helps you debug tests:\n\n> (**``pdb``** commands: ``n``, ``c``, ``s``, ``u``, ``d`` => ``next``, ``continue``, ``step``, ``up``, ``down``)\n\nTo pause an active test that throws an exception or error, (*and keep the browser window open while **Debug Mode** begins in the console*), add **``--pdb``** as a ``pytest`` option:\n\nTo start tests in Debug Mode, add **``--trace``** as a ``pytest`` option:\n\nCommand-line Options:\n\nHere are some useful command-line options that come with pytest:\n\nSeleniumBase provides additional pytest command-line options for tests:\n\n(See the full list of command-line option definitions **[here](https://github.com/seleniumbase/SeleniumBase/blob/master/seleniumbase/plugins/pytest_plugin.py)**. For detailed examples of command-line options, see **[customizing_test_runs.md](https://github.com/seleniumbase/SeleniumBase/blob/master/help_docs/customizing_test_runs.md)**)\n\n--------"}, {"name": "seleniumbase", "tags": ["cli", "dev", "math", "ml", "ui", "visualization", "web"], "summary": "A complete web automation framework for end-to-end testing.", "text": "During test failures, logs and screenshots from the most recent test run will get saved to the ``latest_logs/`` folder. Those logs will get moved to ``archived_logs/`` if you add --archive_logs to command-line options, or have ``ARCHIVE_EXISTING_LOGS`` set to True in [settings.py](https://github.com/seleniumbase/SeleniumBase/blob/master/seleniumbase/config/settings.py), otherwise log files with be cleaned up at the start of the next test run. The ``test_suite.py`` collection contains tests that fail on purpose so that you can see how logging works.\n\nAn easy way to override seleniumbase/config/settings.py is by using a custom settings file.\nHere's the command-line option to add to tests: (See [examples/custom_settings.py](https://github.com/seleniumbase/SeleniumBase/blob/master/examples/custom_settings.py))\n``--settings_file=custom_settings.py``\n(Settings include default timeout values, a two-factor auth key, DB credentials, S3 credentials, and other important settings used by tests.)\n\nTo pass additional data from the command-line to tests, add ``--data=\"ANY STRING\"``.\nInside your tests, you can use ``self.data`` to access that.\n\nDirectory Configuration:\n\nWhen running tests with **``pytest``**, you'll want a copy of **[pytest.ini](https://github.com/seleniumbase/SeleniumBase/blob/master/pytest.ini)** in your root folders. When running tests with **``pynose``**, you'll want a copy of **[setup.cfg](https://github.com/seleniumbase/SeleniumBase/blob/master/setup.cfg)** in your root folders. These files specify default configuration details for tests. Test folders should also include a blank **[__init__.py](https://github.com/seleniumbase/SeleniumBase/blob/master/examples/offline_examples/__init__.py)** file to allow your test files to import other files from that folder.\n\n``sbase mkdir DIR`` creates a folder with config files and sample tests:\n\n> That new folder will have these files:\n\nProTip\u2122: You can also create a boilerplate folder without any sample tests in it by adding ``-b`` or ``--basic`` to the ``sbase mkdir`` command:\n\n> That new folder will have these files:\n\nOf those files, the ``pytest.ini`` config file is the most important, followed by a blank ``__init__.py`` file. There's also a ``setup.cfg`` file (for pynose). Finally, the ``requirements.txt`` file can be used to help you install seleniumbase into your environments (if it's not already installed).\n\n--------\n\nLog files from failed tests:\n\nLet's try an example of a test that fails:\n\nYou can run it from the ``examples/`` folder like this:\n\nYou'll notice that a logs folder, ``./latest_logs/``, was created to hold information (and screenshots) about the failing test. During test runs, past results get moved to the archived_logs folder if you have ARCHIVE_EXISTING_LOGS set to True in [settings.py](https://github.com/seleniumbase/SeleniumBase/blob/master/seleniumbase/config/settings.py), or if your run tests with ``--archive-logs``. If you choose not to archive existing logs, they will be deleted and replaced by the logs of the latest test run.\n\n--------\n\nSeleniumBase Dashboard:\n\nThe ``--dashboard`` option for pytest generates a SeleniumBase Dashboard located at ``dashboard.html``, which updates automatically as tests run and produce results. Example:\n\nAdditionally, you can host your own SeleniumBase Dashboard Server on a port of your choice. Here's an example of that using Python's ``http.server``:\n\nNow you can navigate to ``http://localhost:1948/dashboard.html`` in order to view the dashboard as a web app. This requires two different terminal windows: one for running the server, and another for running the tests, which should be run from the same directory. (Use Ctrl+C to stop the http server.)\n\nHere's a full example of what the SeleniumBase Dashboard may look like:\n\n--------\n\nGenerating Test Reports:"}, {"name": "seleniumbase", "tags": ["cli", "dev", "math", "ml", "ui", "visualization", "web"], "summary": "A complete web automation framework for end-to-end testing.", "text": "pytest HTML Reports:\n\nUsing ``--html=report.html`` gives you a fancy report of the name specified after your test suite completes.\n\nWhen combining pytest html reports with SeleniumBase Dashboard usage, the pie chart from the Dashboard will get added to the html report. Additionally, if you set the html report URL to be the same as the Dashboard URL when also using the dashboard, (example: ``--dashboard --html=dashboard.html``), then the Dashboard will become an advanced html report when all the tests complete.\n\nHere's an example of an upgraded html report:\n\nIf viewing pytest html reports in [Jenkins](https://www.jenkins.io/), you may need to [configure Jenkins settings](https://stackoverflow.com/a/46197356/7058266) for the html to render correctly. This is due to [Jenkins CSP changes](https://www.jenkins.io/doc/book/system-administration/security/configuring-content-security-policy/).\n\nYou can also use ``--junit-xml=report.xml`` to get an xml report instead. Jenkins can use this file to display better reporting for your tests.\n\npynose Reports:\n\nThe ``--report`` option gives you a fancy report after your test suite completes.\n\n(NOTE: You can add ``--show-report`` to immediately display pynose reports after the test suite completes. Only use ``--show-report`` when running tests locally because it pauses the test run.)\n\nbehave Dashboard & Reports:\n\n(The [behave_bdd/](https://github.com/seleniumbase/SeleniumBase/tree/master/examples/behave_bdd) folder can be found in the [examples/](https://github.com/seleniumbase/SeleniumBase/tree/master/examples) folder.)\n\nYou can also use ``--junit`` to get ``.xml`` reports for each behave feature. Jenkins can use these files to display better reporting for your tests.\n\nAllure Reports:\n\nSee: [https://allurereport.org/docs/pytest/](https://allurereport.org/docs/pytest/)\n\nSeleniumBase no longer includes ``allure-pytest`` as part of installed dependencies. If you want to use it, install it first:\n\nNow your tests can create Allure results files, which can be processed by Allure Reports.\n\n--------\n\nUsing a Proxy Server:\n\nIf you wish to use a proxy server for your browser tests (Chromium or Firefox), you can add ``--proxy=IP_ADDRESS:PORT`` as an argument on the command line.\n\nIf the proxy server that you wish to use requires authentication, you can do the following (Chromium only):\n\nSeleniumBase also supports SOCKS4 and SOCKS5 proxies:\n\nTo make things easier, you can add your frequently-used proxies to PROXY_LIST in [proxy_list.py](https://github.com/seleniumbase/SeleniumBase/blob/master/seleniumbase/config/proxy_list.py), and then use ``--proxy=KEY_FROM_PROXY_LIST`` to use the IP_ADDRESS:PORT of that key.\n\nChanging the User-Agent:\n\nIf you wish to change the User-Agent for your browser tests (Chromium and Firefox only), you can add ``--agent=\"USER AGENT STRING\"`` as an argument on the command-line.\n\nHandling Pop-Up Alerts:\n\nself.accept_alert() automatically waits for and accepts alert pop-ups. self.dismiss_alert() automatically waits for and dismisses alert pop-ups. On occasion, some methods like self.click(SELECTOR) might dismiss a pop-up on its own because they call JavaScript to make sure that the readyState of the page is complete before advancing. If you're trying to accept a pop-up that got dismissed this way, use this workaround: Call self.find_element(SELECTOR).click() instead, (which will let the pop-up remain on the screen), and then use self.accept_alert() to accept the pop-up (more on that here). If pop-ups are intermittent, wrap code in a try/except block.\n\nBuilding Guided Tours for Websites:\n\nLearn about SeleniumBase Interactive Walkthroughs (in the ``examples/tour_examples/`` folder). It's great for prototyping a website onboarding experience.\n\n--------\n\nProduction Environments & Integrations:"}, {"name": "seleniumbase", "tags": ["cli", "dev", "math", "ml", "ui", "visualization", "web"], "summary": "A complete web automation framework for end-to-end testing.", "text": "\u25b6\ufe0f Here are some things you can do to set up a production environment for your testing. (click to expand)\n\nYou can set up a Jenkins build server for running tests at regular intervals. For a real-world Jenkins example of headless browser automation in action, check out the SeleniumBase Jenkins example on Azure or the SeleniumBase Jenkins example on Google Cloud.\n\nYou can use the Selenium Grid to scale your testing by distributing tests on several machines with parallel execution. To do this, check out the SeleniumBase selenium_grid folder, which should have everything you need, including the Selenium Grid ReadMe to help you get started.\n\nIf you're using the SeleniumBase MySQL feature to save results from tests running on a server machine, you can install MySQL Workbench to help you read & write from your DB more easily.\n\nIf you're using AWS, you can set up an Amazon S3 account for saving log files and screenshots from your tests. To activate this feature, modify settings.py with connection details in the S3 section, and add --with-s3-logging on the command-line when running your tests.\n\nHere's an example of running tests with some additional features enabled:\n\nDetailed Method Specifications and Examples:\n\n**Navigating to a web page: (and related commands)**\n\nProTip\u2122: You can use the self.get_page_source() method with Python's find() command to parse through HTML to find something specific. (For more advanced parsing, see the BeautifulSoup example.)\n\n**Clicking:**\n\nTo click an element on the page:\n\n**ProTip\u2122:** In most web browsers, you can right-click on a page and select ``Inspect Element`` to see the CSS selector details that you'll need to create your own scripts.\n\n**Typing Text:**\n\nself.type(selector, text)  # updates the text from the specified element with the specified value. An exception is raised if the element is missing or if the text field is not editable. Example:\n\nYou can also use self.add_text() or the WebDriver .send_keys() command, but those won't clear the text box first if there's already text inside.\n\n**Getting the text from an element on a page:**\n\n**Getting the attribute value from an element on a page:**\n\n**Asserting existence of an element on a page within some number of seconds:**\n\n(NOTE: You can also use: ``self.assert_element_present(ELEMENT)``)\n\n**Asserting visibility of an element on a page within some number of seconds:**\n\n(NOTE: The short versions of that are ``self.find_element(ELEMENT)`` and ``self.assert_element(ELEMENT)``. The ``find_element()`` version returns the element.)\n\nSince the line above returns the element, you can combine that with ``.click()`` as shown below:\n\n**ProTip\u2122:** You can use dots to signify class names (Ex: ``div.class_name``) as a simplified version of ``div[class=\"class_name\"]`` within a CSS selector.\n\nYou can also use ``*=`` to search for any partial value in a CSS selector as shown below:\n\n**Asserting visibility of text inside an element on a page within some number of seconds:**\n\n(NOTE: ``self.find_text(TEXT, ELEMENT)`` and ``self.wait_for_text(TEXT, ELEMENT)`` also do this. For backwards compatibility, older method names were kept, but the default timeout may be different.)\n\n**Asserting Anything:**\n\n**Useful Conditional Statements: (with creative examples)**"}, {"name": "seleniumbase", "tags": ["cli", "dev", "math", "ml", "ui", "visualization", "web"], "summary": "A complete web automation framework for end-to-end testing.", "text": "``is_element_visible(selector):``  (visible on the page)\n\n``is_element_present(selector):``  (present in the HTML)\n\n``is_text_visible(text, selector):``  (text visible on element)\n\n\u25b6\ufe0f Click for a longer example of is_text_visible():\n\n``is_link_text_visible(link_text):``\n\nSwitching Tabs:\n\nIf your test opens up a new tab/window, you can switch to it. (SeleniumBase automatically switches to new tabs that don't open to about:blank URLs.)\n\nHow to handle iframes:\n\niframes follow the same principle as new windows: You must first switch to the iframe if you want to perform actions in there:\n\nTo exit from multiple iframes, use ``self.switch_to_default_content()``. (If inside a single iframe, this has the same effect as ``self.switch_to_parent_frame()``.)\n\nYou can also use a context manager to act inside iframes:\n\nThis also works with nested iframes:\n\nHow to execute custom jQuery scripts:\n\njQuery is a powerful JavaScript library that allows you to perform advanced actions in a web browser.\nIf the web page you're on already has jQuery loaded, you can start executing jQuery scripts immediately.\nYou'd know this because the web page would contain something like the following in the HTML:\n\nIt's OK if you want to use jQuery on a page that doesn't have it loaded yet. To do so, run the following command first:\n\n\u25b6\ufe0f Here are some examples of using jQuery in your scripts. (click to expand)\n\n(Most of the above commands can be done directly with built-in SeleniumBase methods.)\n\nHow to handle a restrictive CSP:\n\nSome websites have a restrictive [Content Security Policy](https://developer.mozilla.org/en-US/docs/Web/HTTP/CSP) to prevent users from loading jQuery and other external libraries onto their websites. If you need to use jQuery or another JS library on those websites, add ``--disable-csp`` as a ``pytest`` command-line option to load a Chromium extension that bypasses the CSP.\n\nMore JavaScript fun:\n\n\u25b6\ufe0f In this example, JavaScript creates a referral button on a page, which is then clicked. (click to expand)\n\n(Due to popular demand, this traffic generation example has been included in SeleniumBase with the self.generate_referral(start_page, end_page) and the self.generate_traffic(start_page, end_page, loops) methods.)\n\nHow to use deferred asserts:\n\nLet's say you want to verify multiple different elements on a web page in a single test, but you don't want the test to fail until you verified several elements at once so that you don't have to rerun the test to find more missing elements on the same page. That's where deferred asserts come in. Here's an example:\n\ndeferred_assert_element() and deferred_assert_text() will save any exceptions that would be raised.\nTo flush out all the failed deferred asserts into a single exception, make sure to call self.process_deferred_asserts() at the end of your test method. If your test hits multiple pages, you can call self.process_deferred_asserts() before navigating to a new page so that the screenshot from your log files matches the URL where the deferred asserts were made.\n\nHow to access raw WebDriver:\n\nIf you need access to any commands that come with standard WebDriver, you can call them directly like this:\n\n(In general, you'll want to use the SeleniumBase versions of methods when available.)\n\nHow to retry failing tests automatically:"}, {"name": "seleniumbase", "tags": ["cli", "dev", "math", "ml", "ui", "visualization", "web"], "summary": "A complete web automation framework for end-to-end testing.", "text": "You can use pytest --reruns=NUM to retry failing tests that many times. Add --reruns-delay=SECONDS to wait that many seconds between retries. Example:\n\nYou can use the @retry_on_exception() decorator to retry failing methods. (First import: from seleniumbase import decorators). To learn more about SeleniumBase decorators, click here.\n\n--------\n\n> \"Catch bugs in QA before deploying code to Production!\"\n\n--------\n\nWrap-Up\n\nIf you see something, say something!"}, {"name": "seleniumbase", "tags": ["cli", "dev", "math", "ml", "ui", "visualization", "web"], "summary": "A complete web automation framework for end-to-end testing.", "text": "This library is used to automate web interactions, end-to-end testing, and web scraping with ease. With SeleniumBase, developers can handle modern web challenges and innovate faster by leveraging its stealth options and comprehensive API."}, {"name": "semchunk", "tags": ["math", "ml", "web"], "summary": "A fast, lightweight and easy-to-use Python library for splitting text into semantically meaningful chunks.", "text": "semchunk\n\n**`semchunk`** by [**Isaacus**](https://isaacus.com/) is a fast, lightweight and easy-to-use Python library for splitting text into semantically meaningful chunks.\n\nIt has built-in support for tokenizers from OpenAI's `tiktoken` and Hugging Face's `transformers` and `tokenizers` libraries, in addition to supporting custom tokenizers and token counters. It can also overlap chunks as well as return their offsets.\n\nPowered by an efficient yet highly accurate chunking algorithm ([How It Works ](https://github.com/isaacus-dev/semchunk#how-it-works-)), `semchunk` produces chunks that are more semantically meaningful than regular token and recursive character chunkers like `langchain`'s `RecursiveCharacterTextSplitter`, while also being 85% faster than its closest alternative, `semantic-text-splitter` ([Benchmarks ](https://github.com/isaacus-dev/semchunk#benchmarks-)).\n\n`semchunk` is production ready, being used every day in the [Isaacus API](https://docs.isaacus.com) to split extremely long legal documents into more manageable chunks for our [Kanon legal AI models](https://docs.isaacus.com/models).\n\nInstallation \n`semchunk` can be installed with `pip`:\n\n`semchunk` is also available on `conda-forge`:\n\nIn addition, [@dominictarro](https://github.com/dominictarro) maintains a Rust port of `semchunk` named [`semchunk-rs`](https://crates.io/crates/semchunk-rs).\n\nQuickstart \u200d\nThe code snippet below demonstrates how to chunk text with `semchunk`:\n\nUsage \ufe0f\n\n`chunkerify()`\n\n`chunkerify()` constructs a chunker that splits one or more texts into semantically meaningful chunks of a specified size as determined by the provided tokenizer or token counter.\n\n`tokenizer_or_token_counter` is either: the name of a `tiktoken` or `transformers` tokenizer (with priority given to the former); a tokenizer that possesses an `encode` attribute (e.g., a `tiktoken`, `transformers` or `tokenizers` tokenizer); or a token counter that returns the number of tokens in an input.\n\n`chunk_size` is the maximum number of tokens a chunk may contain. It defaults to `None` in which case it will be set to the same value as the tokenizer's `model_max_length` attribute (deducted by the number of tokens returned by attempting to tokenize an empty string) if possible, otherwise a `ValueError` will be raised.\n\n`max_token_chars` is the maximum number of characters a token may contain. It is used to significantly speed up the token counting of long inputs. It defaults to `None` in which case it will either not be used or will, if possible, be set to the number of characters in the longest token in the tokenizer's vocabulary as determined by the `token_byte_values` or `get_vocab` methods.\n\n`memoize` flags whether to memoize the token counter. It defaults to `True`.\n\n`cache_maxsize` is the maximum number of text-token count pairs that can be stored in the token counter's cache. It defaults to `None`, which makes the cache unbounded. This argument is only used if `memoize` is `True`."}, {"name": "semchunk", "tags": ["math", "ml", "web"], "summary": "A fast, lightweight and easy-to-use Python library for splitting text into semantically meaningful chunks.", "text": "This function returns a chunker that takes either a single text or a sequence of texts and returns, depending on whether multiple texts have been provided, a list or list of lists of chunks up to `chunk_size`-tokens-long with any whitespace used to split the text removed, and, if the optional `offsets` argument to the chunker is `True`, a list or lists of tuples of the form `(start, end)` where `start` is the index of the first character of a chunk in a text and `end` is the index of the character succeeding the last character of the chunk such that `chunks[i] == text[offsets[i][0]:offsets[i][1]]`.\n\nThe resulting chunker can be passed a `processes` argument that specifies the number of processes to be used when chunking multiple texts.\n\nIt is also possible to pass a `progress` argument which, if set to `True` and multiple texts are passed, will display a progress bar.\n\nAs described above, the `offsets` argument, if set to `True`, will cause the chunker to return the start and end offsets of each chunk.\n\nThe chunker accepts an `overlap` argument that specifies the proportion of the chunk size, or, if >=1, the number of tokens, by which chunks should overlap. It defaults to `None`, in which case no overlapping occurs.\n\n`chunk()`\n\n`chunk()` splits a text into semantically meaningful chunks of a specified size as determined by the provided token counter.\n\n`text` is the text to be chunked.\n\n`chunk_size` is the maximum number of tokens a chunk may contain.\n\n`token_counter` is a callable that takes a string and returns the number of tokens in it.\n\n`memoize` flags whether to memoize the token counter. It defaults to `True`.\n\n`offsets` flags whether to return the start and end offsets of each chunk. It defaults to `False`.\n\n`overlap` specifies the proportion of the chunk size, or, if >=1, the number of tokens, by which chunks should overlap. It defaults to `None`, in which case no overlapping occurs.\n\n`cache_maxsize` is the maximum number of text-token count pairs that can be stored in the token counter's cache. It defaults to `None`, which makes the cache unbounded. This argument is only used if `memoize` is `True`.\n\nThis function returns a list of chunks up to `chunk_size`-tokens-long, with any whitespace used to split the text removed, and, if `offsets` is `True`, a list of tuples of the form `(start, end)` where `start` is the index of the first character of the chunk in the original text and `end` is the index of the character after the last character of the chunk such that `chunks[i] == text[offsets[i][0]:offsets[i][1]]`."}, {"name": "semchunk", "tags": ["math", "ml", "web"], "summary": "A fast, lightweight and easy-to-use Python library for splitting text into semantically meaningful chunks.", "text": "How It Works \n`semchunk` works by recursively splitting texts until all resulting chunks are equal to or less than a specified chunk size. In particular, it:\n1. Splits text using the most semantically meaningful splitter possible;\n1. Recursively splits the resulting chunks until a set of chunks equal to or less than the specified chunk size is produced;\n1. Merges any chunks that are under the chunk size back together until the chunk size is reached;\n1. Reattaches any non-whitespace splitters back to the ends of chunks barring the final chunk if doing so does not bring chunks over the chunk size, otherwise adds non-whitespace splitters as their own chunks; and\n1. Since version 3.0.0, excludes chunks consisting entirely of whitespace characters.\n\nTo ensure that chunks are as semantically meaningful as possible, `semchunk` uses the following splitters, in order of precedence:\n1. The largest sequence of newlines (`\\n`) and/or carriage returns (`\\r`);\n1. The largest sequence of tabs;\n1. The largest sequence of whitespace characters (as defined by regex's `\\s` character class) or, since version 3.2.0, if the largest sequence of whitespace characters is only a single character and there exist whitespace characters preceded by any of the semantically meaningful non-whitespace characters listed below (in the same order of precedence), then only those specific whitespace characters;\n1. Sentence terminators (`.`, `?`, `!` and `*`);\n1. Clause separators (`;`, `,`, `(`, `)`, `[`, `]`, `\u201c`, `\u201d`, `\u2018`, `\u2019`, `'`, `\"` and `` ` ``);\n1. Sentence interrupters (`:`, `\u2014` and `\u2026`);\n1. Word joiners (`/`, `\\`, `\u2013`, `&` and `-`); and\n1. All other characters.\n\nIf overlapping chunks have been requested, `semchunk` also:\n1. Internally reduces the chunk size to `min(overlap, chunk_size - overlap)` (`overlap` being computed as `floor(chunk_size * overlap)` for relative overlaps and `min(overlap, chunk_size - 1)` for absolute overlaps); and\n1. Merges every `floor(original_chunk_size / reduced_chunk_size)` chunks starting from the first chunk and then jumping by `floor((original_chunk_size - overlap) / reduced_chunk_size)` chunks until the last chunk is reached.\n\nBenchmarks \nOn a desktop with a Ryzen 9 7900X, 96 GB of DDR5 5600MHz CL40 RAM, Windows 11 and Python 3.12.4, it takes `semchunk` 3.04 seconds to split every sample in [NLTK's Gutenberg Corpus](https://www.nltk.org/howto/corpus.html#plaintext-corpora) into 512-token-long chunks with GPT-4's tokenizer (for context, the Corpus contains 18 texts and 3,001,260 tokens). By comparison, it takes [`semantic-text-splitter`](https://pypi.org/project/semantic-text-splitter/) (with multiprocessing) 24.84 seconds to chunk the same texts into 512-token-long chunks \u2014 a difference of 87.76%.\n\nThe code used to benchmark `semchunk` and `semantic-text-splitter` is available [here](https://github.com/isaacus-dev/semchunk/blob/main/tests/bench.py).\n\nLicence \nThis library is licensed under the [MIT License](https://github.com/isaacus-dev/semchunk/blob/main/LICENCE)."}, {"name": "semchunk", "tags": ["math", "ml", "web"], "summary": "A fast, lightweight and easy-to-use Python library for splitting text into semantically meaningful chunks.", "text": "This library is used to split text into semantically meaningful chunks efficiently and accurately. It enables developers to extract relevant information from text data by segmenting it into coherent units that preserve meaning."}, {"name": "sentence-transformers", "tags": ["math", "ml"], "summary": "Embeddings, Retrieval, and Reranking", "text": "Sentence Transformers: Embeddings, Retrieval, and Reranking\n\nThis framework provides an easy method to compute embeddings for accessing, using, and training state-of-the-art embedding and reranker models. It can be used to compute embeddings using Sentence Transformer models ([quickstart](https://sbert.net/docs/quickstart.html#sentence-transformer)), to calculate similarity scores using Cross-Encoder (a.k.a. reranker) models ([quickstart](https://sbert.net/docs/quickstart.html#cross-encoder)) or to generate sparse embeddings using Sparse Encoder models ([quickstart](https://sbert.net/docs/quickstart.html#sparse-encoder)). This unlocks a wide range of applications, including [semantic search](https://sbert.net/examples/applications/semantic-search/README.html), [semantic textual similarity](https://sbert.net/docs/sentence_transformer/usage/semantic_textual_similarity.html), and [paraphrase mining](https://sbert.net/examples/applications/paraphrase-mining/README.html).\n\nA wide selection of over [15,000 pre-trained Sentence Transformers models](https://huggingface.co/models?library=sentence-transformers) are available for immediate use on  Hugging Face, including many of the state-of-the-art models from the [Massive Text Embeddings Benchmark (MTEB) leaderboard](https://huggingface.co/spaces/mteb/leaderboard). Additionally, it is easy to train or finetune your own [embedding models](https://sbert.net/docs/sentence_transformer/training_overview.html), [reranker models](https://sbert.net/docs/cross_encoder/training_overview.html) or [sparse encoder models](https://sbert.net/docs/sparse_encoder/training_overview.html) using Sentence Transformers, enabling you to create custom models for your specific use cases.\n\nFor the **full documentation**, see **[www.SBERT.net](https://www.sbert.net)**.\n\nInstallation\n\nWe recommend **Python 3.10+**, **[PyTorch 1.11.0+](https://pytorch.org/get-started/locally/)**, and **[transformers v4.34.0+](https://github.com/huggingface/transformers)**.\n\n**Install with pip**\n\n**Install with conda**\n\n**Install from sources**\n\nAlternatively, you can also clone the latest version from the [repository](https://github.com/huggingface/sentence-transformers) and install it directly from the source code:\n\n**PyTorch with CUDA**\n\nIf you want to use a GPU / CUDA, you must install PyTorch with the matching CUDA Version. Follow\n[PyTorch - Get Started](https://pytorch.org/get-started/locally/) for further details how to install PyTorch.\n\nGetting Started\n\nSee [Quickstart](https://www.sbert.net/docs/quickstart.html) in our documentation.\n\nEmbedding Models\n\nFirst download a pretrained embedding a.k.a. Sentence Transformer model.\n\nThen provide some texts to the model.\n\nAnd that's already it. We now have numpy arrays with the embeddings, one for each text. We can use these to compute similarities.\n\nReranker Models\n\nFirst download a pretrained reranker a.k.a. Cross Encoder model.\n\nThen provide some texts to the model.\n\nAnd we're good to go. You can also use [`model.rank`](https://sbert.net/docs/package_reference/cross_encoder/cross_encoder.html#sentence_transformers.cross_encoder.CrossEncoder.rank) to avoid having to perform the reranking manually:\n\nSparse Encoder Models\n\nFirst download a pretrained sparse embedding a.k.a. Sparse Encoder model.\n\nPre-Trained Models\n\nWe provide a large list of pretrained models for more than 100 languages. Some models are general purpose models, while others produce embeddings for specific use cases.\n\nTraining\n\nThis framework allows you to fine-tune your own sentence embedding methods, so that you get task-specific sentence embeddings. You have various options to choose from in order to get perfect sentence embeddings for your specific task.\n\n- Embedding Models\n  - [Sentence Transformer > Training Overview](https://www.sbert.net/docs/sentence_transformer/training_overview.html)\n  - [Sentence Transformer > Training Examples](https://www.sbert.net/docs/sentence_transformer/training/examples.html) or [training examples on GitHub](https://github.com/huggingface/sentence-transformers/tree/main/examples/sentence_transformer/training).\n- Reranker Models\n  - [Cross Encoder > Training Overview](https://www.sbert.net/docs/cross_encoder/training_overview.html)\n  - [Cross Encoder > Training Examples](https://www.sbert.net/docs/cross_encoder/training/examples.html) or [training examples on GitHub](https://github.com/huggingface/sentence-transformers/tree/main/examples/cross_encoder/training).\n- Sparse Embedding Models\n  - [Sparse Encoder > Training Overview](https://www.sbert.net/docs/sparse_encoder/training_overview.html)\n  - [Sparse Encoder > Training Examples](https://www.sbert.net/docs/sparse_encoder/training/examples.html) or [training examples on GitHub](https://github.com/huggingface/sentence-transformers/tree/main/examples/sparse_encoder/training).\n\nSome highlights across the different types of training are:\n\n- Support of various transformer networks including BERT, RoBERTa, XLM-R, DistilBERT, Electra, BART, ...\n- Multi-Lingual and multi-task learning\n- Evaluation during training to find optimal model\n- [20+ loss functions](https://www.sbert.net/docs/package_reference/sentence_transformer/losses.html) for embedding models, [10+ loss functions](https://www.sbert.net/docs/package_reference/cross_encoder/losses.html) for reranker models and [10+ loss functions](https://www.sbert.net/docs/package_reference/sparse_encoder/losses.html) for sparse embedding models, allowing you to tune models specifically for semantic search, paraphrase mining, semantic similarity comparison, clustering, triplet loss, contrastive loss, etc.\n\nApplication Examples\n\nYou can use this framework for:\n\n- **Computing Sentence Embeddings**\n\n  - [Dense Embeddings](https://www.sbert.net/examples/sentence_transformer/applications/computing-embeddings/README.html)\n  - [Sparse Embeddings](https://www.sbert.net/examples/sparse_encoder/applications/computing_embeddings/README.html)\n\n- **Semantic Textual Similarity**\n\n  - [Dense STS](https://www.sbert.net/docs/sentence_transformer/usage/semantic_textual_similarity.html)\n  - [Sparse STS](https://www.sbert.net/examples/sparse_encoder/applications/semantic_textual_similarity/README.html)\n\n- **Semantic Search**\n\n  - [Dense Search](https://www.sbert.net/examples/sentence_transformer/applications/semantic-search/README.html)\n  - [Sparse Search](https://www.sbert.net/examples/sparse_encoder/applications/semantic_search/README.html)\n\n- **Retrieve & Re-Rank**\n\n  - [Dense only Retrieval](https://www.sbert.net/examples/sentence_transformer/applications/retrieve_rerank/README.html)\n  - [Sparse/Dense/Hybrid Retrieval](https://www.sbert.net/examples/sentence_transformer/applications/retrieve_rerank/README.html)\n\n- [Clustering](https://www.sbert.net/examples/sentence_transformer/applications/clustering/README.html)\n\n- [Paraphrase Mining](https://www.sbert.net/examples/sentence_transformer/applications/paraphrase-mining/README.html)\n\n- [Translated Sentence Mining](https://www.sbert.net/examples/sentence_transformer/applications/parallel-sentence-mining/README.html)\n\n- [Multilingual Image Search, Clustering & Duplicate Detection](https://www.sbert.net/examples/sentence_transformer/applications/image-search/README.html)\n\nand many more use-cases.\n\nFor all examples, see [examples/sentence_transformer/applications](https://github.com/huggingface/sentence-transformers/tree/main/examples/sentence_transformer/applications).\n\nDevelopment setup\n\nAfter cloning the repo (or a fork) to your machine, in a virtual environment, run:\n\nTo test your changes, run:\n\nCiting & Authors\n\nIf you find this repository helpful, feel free to cite our publication [Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks](https://huggingface.co/papers/1908.10084):\n\nIf you use one of the multilingual models, feel free to cite our publication [Making Monolingual Sentence Embeddings Multilingual using Knowledge Distillation](https://huggingface.co/papers/2004.09813):\n\nPlease have a look at [Publications](https://www.sbert.net/docs/publications.html) for our different publications that are integrated into SentenceTransformers.\n\nMaintainers\n\nMaintainer: [Tom Aarsen](https://github.com/tomaarsen),  Hugging Face\n\nDon't hesitate to open an issue if something is broken (and it shouldn't be) or if you have further questions.\n\n---\n\nThis project was originally developed by the [Ubiquitous Knowledge Processing (UKP) Lab](https://www.ukp.tu-darmstadt.de/) at TU Darmstadt. We're grateful for their foundational work and continued contributions to the field.\n\n> This repository contains experimental software and is published for the sole purpose of giving additional background details on the respective publication."}, {"name": "sentence-transformers", "tags": ["math", "ml"], "summary": "Embeddings, Retrieval, and Reranking", "text": "This library is used to compute embeddings for state-of-the-art models and calculate similarity scores using various encoder types. With this library, developers can easily implement applications such as semantic search and reranking in their projects."}, {"name": "shap", "tags": ["data", "math", "ml", "visualization", "web"], "summary": "A unified approach to explain the output of any machine learning model.", "text": "Install\n\nSHAP can be installed from either [PyPI](https://pypi.org/project/shap) or [conda-forge](https://anaconda.org/conda-forge/shap):\n\nor\n\nTree ensemble example (XGBoost/LightGBM/CatBoost/scikit-learn/pyspark models)\n\nWhile SHAP can explain the output of any machine learning model, we have developed a high-speed exact algorithm for tree ensemble methods (see our [Nature MI paper](https://rdcu.be/b0z70)). Fast C++ implementations are supported for *XGBoost*, *LightGBM*, *CatBoost*, *scikit-learn* and *pyspark* tree models:\n\nThe above explanation shows features each contributing to push the model output from the base value (the average model output over the training dataset we passed) to the model output. Features pushing the prediction higher are shown in red, those pushing the prediction lower are in blue. Another way to visualize the same explanation is to use a force plot (these are introduced in our [Nature BME paper](https://rdcu.be/baVbR)):\n\nIf we take many force plot explanations such as the one shown above, rotate them 90 degrees, and then stack them horizontally, we can see explanations for an entire dataset (in the notebook this plot is interactive):\n\nTo understand how a single feature effects the output of the model we can plot the SHAP value of that feature vs. the value of the feature for all the examples in a dataset. Since SHAP values represent a feature's responsibility for a change in the model output, the plot below represents the change in predicted house price as the latitude changes. Vertical dispersion at a single value of latitude represents interaction effects with other features. To help reveal these interactions we can color by another feature. If we pass the whole explanation tensor to the `color` argument the scatter plot will pick the best feature to color by. In this case it picks longitude.\n\nTo get an overview of which features are most important for a model we can plot the SHAP values of every feature for every sample. The plot below sorts features by the sum of SHAP value magnitudes over all samples, and uses SHAP values to show the distribution of the impacts each feature has on the model output. The color represents the feature value (red high, blue low). This reveals for example that higher median incomes increases the predicted home price.\n\nWe can also just take the mean absolute value of the SHAP values for each feature to get a standard bar plot (produces stacked bars for multi-class outputs):\n\nNatural language example (transformers)\n\nSHAP has specific support for natural language models like those in the Hugging Face transformers library. By adding coalitional rules to traditional Shapley values we can form games that explain large modern NLP model using very few function evaluations. Using this functionality is as simple as passing a supported transformers pipeline to SHAP:\n\nDeep learning example with DeepExplainer (TensorFlow/Keras models)"}, {"name": "shap", "tags": ["data", "math", "ml", "visualization", "web"], "summary": "A unified approach to explain the output of any machine learning model.", "text": "Deep SHAP is a high-speed approximation algorithm for SHAP values in deep learning models that builds on a connection with [DeepLIFT](https://arxiv.org/abs/1704.02685) described in the SHAP NIPS paper. The implementation here differs from the original DeepLIFT by using a distribution of background samples instead of a single reference value, and using Shapley equations to linearize components such as max, softmax, products, divisions, etc. Note that some of these enhancements have also been since integrated into DeepLIFT. TensorFlow models and Keras models using the TensorFlow backend are supported (there is also preliminary support for PyTorch):\n\nThe plot above explains ten outputs (digits 0-9) for four different images. Red pixels increase the model's output while blue pixels decrease the output. The input images are shown on the left, and as nearly transparent grayscale backings behind each of the explanations. The sum of the SHAP values equals the difference between the expected model output (averaged over the background dataset) and the current model output. Note that for the 'zero' image the blank middle is important, while for the 'four' image the lack of a connection on top makes it a four instead of a nine.\n\nDeep learning example with GradientExplainer (TensorFlow/Keras/PyTorch models)\n\nExpected gradients combines ideas from [Integrated Gradients](https://arxiv.org/abs/1703.01365), SHAP, and [SmoothGrad](https://arxiv.org/abs/1706.03825) into a single expected value equation. This allows an entire dataset to be used as the background distribution (as opposed to a single reference value) and allows local smoothing. If we approximate the model with a linear function between each background data sample and the current input to be explained, and we assume the input features are independent then expected gradients will compute approximate SHAP values. In the example below we have explained how the 7th intermediate layer of the VGG16 ImageNet model impacts the output probabilities.\n\nPredictions for two input images are explained in the plot above. Red pixels represent positive SHAP values that increase the probability of the class, while blue pixels represent negative SHAP values the reduce the probability of the class. By using `ranked_outputs=2` we explain only the two most likely classes for each input (this spares us from explaining all 1,000 classes).\n\nModel agnostic example with KernelExplainer (explains any function)\n\nKernel SHAP uses a specially-weighted local linear regression to estimate SHAP values for any model. Below is a simple example for explaining a multi-class SVM on the classic iris dataset.\n\nThe above explanation shows four features each contributing to push the model output from the base value (the average model output over the training dataset we passed) towards zero. If there were any features pushing the class label higher they would be shown in red.\n\nIf we take many explanations such as the one shown above, rotate them 90 degrees, and then stack them horizontally, we can see explanations for an entire dataset. This is exactly what we do below for all the examples in the iris test set:\n\nSHAP Interaction Values"}, {"name": "shap", "tags": ["data", "math", "ml", "visualization", "web"], "summary": "A unified approach to explain the output of any machine learning model.", "text": "SHAP interaction values are a generalization of SHAP values to higher order interactions. Fast exact computation of pairwise interactions are implemented for tree models with `shap.TreeExplainer(model).shap_interaction_values(X)`. This returns a matrix for every prediction, where the main effects are on the diagonal and the interaction effects are off-diagonal. These values often reveal interesting hidden relationships, such as how the increased risk of death peaks for men at age 60 (see the NHANES notebook for details):\n\nSample notebooks\n\nThe notebooks below demonstrate different use cases for SHAP. Look inside the notebooks directory of the repository if you want to try playing with the original notebooks yourself.\n\nTreeExplainer\n\nAn implementation of Tree SHAP, a fast and exact algorithm to compute SHAP values for trees and ensembles of trees.\n\n- [**NHANES survival model with XGBoost and SHAP interaction values**](https://shap.github.io/shap/notebooks/NHANES%20I%20Survival%20Model.html) - Using mortality data from 20 years of followup this notebook demonstrates how to use XGBoost and `shap` to uncover complex risk factor relationships.\n\n- [**Census income classification with LightGBM**](https://shap.github.io/shap/notebooks/tree_explainer/Census%20income%20classification%20with%20LightGBM.html) - Using the standard adult census income dataset, this notebook trains a gradient boosting tree model with LightGBM and then explains predictions using `shap`.\n\n- [**League of Legends Win Prediction with XGBoost**](https://shap.github.io/shap/notebooks/League%20of%20Legends%20Win%20Prediction%20with%20XGBoost.html) - Using a Kaggle dataset of 180,000 ranked matches from League of Legends we train and explain a gradient boosting tree model with XGBoost to predict if a player will win their match.\n\nDeepExplainer\n\nAn implementation of Deep SHAP, a faster (but only approximate) algorithm to compute SHAP values for deep learning models that is based on connections between SHAP and the DeepLIFT algorithm.\n\n- [**MNIST Digit classification with Keras**](https://shap.github.io/shap/notebooks/deep_explainer/Front%20Page%20DeepExplainer%20MNIST%20Example.html) - Using the MNIST handwriting recognition dataset, this notebook trains a neural network with Keras and then explains predictions using `shap`.\n\n- [**Keras LSTM for IMDB Sentiment Classification**](https://shap.github.io/shap/notebooks/deep_explainer/Keras%20LSTM%20for%20IMDB%20Sentiment%20Classification.html) - This notebook trains an LSTM with Keras on the IMDB text sentiment analysis dataset and then explains predictions using `shap`.\n\nGradientExplainer\n\nAn implementation of expected gradients to approximate SHAP values for deep learning models. It is based on connections between SHAP and the Integrated Gradients algorithm. GradientExplainer is slower than DeepExplainer and makes different approximation assumptions.\n\n- [**Explain an Intermediate Layer of VGG16 on ImageNet**](https://shap.github.io/shap/notebooks/gradient_explainer/Explain%20an%20Intermediate%20Layer%20of%20VGG16%20on%20ImageNet.html) - This notebook demonstrates how to explain the output of a pre-trained VGG16 ImageNet model using an internal convolutional layer.\n\nLinearExplainer\n\nFor a linear model with independent features we can analytically compute the exact SHAP values. We can also account for feature correlation if we are willing to estimate the feature covariance matrix. LinearExplainer supports both of these options.\n\n- [**Sentiment Analysis with Logistic Regression**](https://shap.github.io/shap/notebooks/linear_explainer/Sentiment%20Analysis%20with%20Logistic%20Regression.html) - This notebook demonstrates how to explain a linear logistic regression sentiment analysis model.\n\nKernelExplainer\n\nAn implementation of Kernel SHAP, a model agnostic method to estimate SHAP values for any model. Because it makes no assumptions about the model type, KernelExplainer is slower than the other model type specific algorithms."}, {"name": "shap", "tags": ["data", "math", "ml", "visualization", "web"], "summary": "A unified approach to explain the output of any machine learning model.", "text": "- [**Census income classification with scikit-learn**](https://shap.github.io/shap/notebooks/Census%20income%20classification%20with%20scikit-learn.html) - Using the standard adult census income dataset, this notebook trains a k-nearest neighbors classifier using scikit-learn and then explains predictions using `shap`.\n\n- [**ImageNet VGG16 Model with Keras**](https://shap.github.io/shap/notebooks/ImageNet%20VGG16%20Model%20with%20Keras.html) - Explain the classic VGG16 convolutional neural network's predictions for an image. This works by applying the model agnostic Kernel SHAP method to a super-pixel segmented image.\n\n- [**Iris classification**](https://shap.github.io/shap/notebooks/Iris%20classification%20with%20scikit-learn.html) - A basic demonstration using the popular iris species dataset. It explains predictions from six different models in scikit-learn using `shap`.\n\nDocumentation notebooks\n\nThese notebooks comprehensively demonstrate how to use specific functions and objects.\n\n- [`shap.decision_plot` and `shap.multioutput_decision_plot`](https://shap.github.io/shap/notebooks/plots/decision_plot.html)\n\n- [`shap.dependence_plot`](https://shap.github.io/shap/notebooks/plots/dependence_plot.html)\n\nMethods Unified by SHAP\n\n1. *LIME:* Ribeiro, Marco Tulio, Sameer Singh, and Carlos Guestrin. \"Why should i trust you?: Explaining the predictions of any classifier.\" Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. ACM, 2016.\n\n2. *Shapley sampling values:* Strumbelj, Erik, and Igor Kononenko. \"Explaining prediction models and individual predictions with feature contributions.\" Knowledge and information systems 41.3 (2014): 647-665.\n\n3. *DeepLIFT:* Shrikumar, Avanti, Peyton Greenside, and Anshul Kundaje. \"Learning important features through propagating activation differences.\" arXiv preprint arXiv:1704.02685 (2017).\n\n4. *QII:* Datta, Anupam, Shayak Sen, and Yair Zick. \"Algorithmic transparency via quantitative input influence: Theory and experiments with learning systems.\" Security and Privacy (SP), 2016 IEEE Symposium on. IEEE, 2016.\n\n5. *Layer-wise relevance propagation:* Bach, Sebastian, et al. \"On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation.\" PloS one 10.7 (2015): e0130140.\n\n6. *Shapley regression values:* Lipovetsky, Stan, and Michael Conklin. \"Analysis of regression in game theory approach.\" Applied Stochastic Models in Business and Industry 17.4 (2001): 319-330.\n\n7. *Tree interpreter:* Saabas, Ando. Interpreting random forests. http://blog.datadive.net/interpreting-random-forests/\n\nCitations\n\nThe algorithms and visualizations used in this package came primarily out of research in [Su-In Lee's lab](https://suinlee.cs.washington.edu) at the University of Washington, and Microsoft Research. If you use SHAP in your research we would appreciate a citation to the appropriate paper(s):"}, {"name": "shap", "tags": ["data", "math", "ml", "visualization", "web"], "summary": "A unified approach to explain the output of any machine learning model.", "text": "This library is used to explain the output of any machine learning model by attributing contribution of each feature to the predicted outcome. This enables developers to gain insights into how their models make predictions and identify key factors influencing the results."}, {"name": "shapely", "tags": ["data", "math", "ui"], "summary": "Manipulation and analysis of geometric objects", "text": "=======\nShapely\n=======\n\n.. Documentation at RTD \u2014 https://readthedocs.org\n\n   :alt: Documentation Status\n   :target: https://shapely.readthedocs.io/en/stable/\n\n.. Github Actions status \u2014 https://github.com/shapely/shapely/actions\n\n   :alt: Github Actions status\n   :target: https://github.com/shapely/shapely/actions?query=branch%3Amain\n\n.. PyPI\n\n.. image:: https://img.shields.io/pypi/v/shapely.svg\n   :alt: PyPI\n   :target: https://pypi.org/project/shapely/\n\n.. Anaconda\n\n.. image:: https://img.shields.io/conda/vn/conda-forge/shapely\n   :alt: Anaconda\n   :target: https://anaconda.org/conda-forge/shapely\n\n.. Coverage\n\n   :target: https://coveralls.io/github/shapely/shapely?branch=main\n\n.. Zenodo\n\n..   :alt: Zenodo\n\nManipulation and analysis of geometric objects in the Cartesian plane.\n\n.. image:: https://c2.staticflickr.com/6/5560/31301790086_b3472ea4e9_c.jpg\n   :width: 800\n   :height: 378\n\nShapely is a BSD-licensed Python package for manipulation and analysis of\nplanar geometric objects. It is using the widely deployed open-source\ngeometry library `GEOS `__ (the engine of `PostGIS\n`__, and a port of `JTS `__).\nShapely wraps GEOS geometries and operations to provide both a feature rich\n`Geometry` interface for singular (scalar) geometries and higher-performance\nNumPy ufuncs for operations using arrays of geometries.\nShapely is not primarily focused on data serialization formats or coordinate\nsystems, but can be readily integrated with packages that are.\n\nWhat is a ufunc?\n----------------\n\nA universal function (or ufunc for short) is a function that operates on\n*n*-dimensional arrays on an element-by-element fashion and supports array\nbroadcasting. The underlying ``for`` loops are implemented in C to reduce the\noverhead of the Python interpreter.\n\nMultithreading\n--------------\n\nShapely functions generally support multithreading by releasing the Global\nInterpreter Lock (GIL) during execution. Normally in Python, the GIL prevents\nmultiple threads from computing at the same time. Shapely functions\ninternally release this constraint so that the heavy lifting done by GEOS can\nbe done in parallel, from a single Python process.\n\nUsage\n=====\n\nHere is the canonical example of building an approximately circular patch by\nbuffering a point, using the scalar Geometry interface:\n\n.. code-block:: pycon\n\nUsing the vectorized ufunc interface (instead of using a manual for loop),\ncompare an array of points with a polygon:\n\n.. code:: python\n\nSee the documentation for more examples and guidance: https://shapely.readthedocs.io\n\nRequirements\n============\n\nShapely 2.1 requires\n\n* Python >=3.10\n* GEOS >=3.9\n* NumPy >=1.21\n\nInstalling Shapely\n==================\n\nWe recommend installing Shapely using one of the available built\ndistributions, for example using ``pip`` or ``conda``:\n\n.. code-block:: console\n\nSee the `installation documentation `__\nfor more details and advanced installation instructions.\n\nIntegration\n===========\n\nShapely does not read or write data files, but it can serialize and deserialize\nusing several well known formats and protocols. The shapely.wkb and shapely.wkt\nmodules provide dumpers and loaders inspired by Python's pickle module.\n\n.. code-block:: pycon\n\nShapely can also integrate with other Python GIS packages using GeoJSON-like\ndicts.\n\n.. code-block:: pycon\n\nSupport\n=======\n\nQuestions about using Shapely may be asked on the `GIS StackExchange\n`__ using the \"shapely\"\ntag.\n\nBugs may be reported at https://github.com/shapely/shapely/issues.\n\nCopyright & License\n===================\n\nShapely is licensed under BSD 3-Clause license.\nGEOS is available under the terms of GNU Lesser General Public License (LGPL) 2.1 at https://libgeos.org."}, {"name": "shapely", "tags": ["data", "math", "ui"], "summary": "Manipulation and analysis of geometric objects", "text": "This library is used to perform manipulation and analysis of geometric objects, such as points, lines, and polygons. With Shapely, developers can create and manage spatial relationships between complex shapes and boundaries."}, {"name": "simsimd", "tags": ["dev", "math", "ml", "web"], "summary": "Portable mixed-precision BLAS-like vector math library for x86 and ARM", "text": "Features\n\n__SimSIMD__ (Arabic: \"\u0633\u064a\u0645\u0633\u064a\u0645 \u062f\u064a\") is a mixed-precision math library of __over 350 SIMD-optimized kernels__ extensively used in AI, Search, and DBMS workloads.\nNamed after the iconic [\"Open Sesame\"](https://en.wikipedia.org/wiki/Open_sesame) command that opened doors to treasure in _Ali Baba and the Forty Thieves_, SimSIMD can help you 10x the cost-efficiency of your computational pipelines.\nImplemented distance functions include:\n\n- Euclidean (L2) and Cosine (Angular) spatial distances for Vector Search. _[docs][docs-spatial]_\n- Dot-Products for real & complex vectors for DSP & Quantum computing. _[docs][docs-dot]_\n- Hamming (~ Manhattan) and Jaccard (~ Tanimoto) bit-level distances. _[docs][docs-binary]_\n- Set Intersections for Sparse Vectors and Text Analysis. _[docs][docs-sparse]_\n- Mahalanobis distance and Quadratic forms for Scientific Computing. _[docs][docs-curved]_\n- Kullback-Leibler and Jensen\u2013Shannon divergences for probability distributions. _[docs][docs-probability]_\n- Fused-Multiply-Add (FMA) and Weighted Sums to replace BLAS level 1 functions. _[docs][docs-fma]_\n- For Levenshtein, Needleman\u2013Wunsch, and Smith-Waterman, check [StringZilla][stringzilla].\n-  Haversine and Vincenty's formulae for Geospatial Analysis.\n\n[docs-spatial]: #cosine-similarity-reciprocal-square-root-and-newton-raphson-iteration\n[docs-curved]: #curved-spaces-mahalanobis-distance-and-bilinear-quadratic-forms\n[docs-sparse]: #set-intersection-galloping-and-binary-search\n[docs-dot]: #complex-dot-products-conjugate-dot-products-and-complex-numbers\n[docs-probability]: #logarithms-in-kullback-leibler--jensenshannon-divergences\n[docs-fma]: #mixed-precision-in-fused-multiply-add-and-weighted-sums\n\nMoreover, SimSIMD...\n\n- handles `float64`, `float32`, `float16`, and `bfloat16` real & complex vectors.\n- handles `int8` integral, `int4` sub-byte, and `b8` binary vectors.\n- handles sparse `uint32` and `uint16` sets, and weighted sparse vectors.\n- is a zero-dependency [header-only C 99](#using-simsimd-in-c) library.\n- has [Python](#using-simsimd-in-python), [Rust](#using-simsimd-in-rust), [JS](#using-simsimd-in-javascript), and [Swift](#using-simsimd-in-swift) bindings.\n- has Arm backends for NEON, Scalable Vector Extensions (SVE), and SVE2.\n- has x86 backends for Haswell, Skylake, Ice Lake, Genoa, and Sapphire Rapids.\n- with both compile-time and runtime CPU feature detection easily integrates anywhere!\n\nDue to the high-level of fragmentation of SIMD support in different x86 CPUs, SimSIMD generally uses the names of select Intel CPU generations for its backends.\nThey, however, also work on AMD CPUs.\nIntel Haswell is compatible with AMD Zen 1/2/3, while AMD Genoa Zen 4 covers AVX-512 instructions added to Intel Skylake and Ice Lake.\nYou can learn more about the technical implementation details in the following blog-posts:\n\nBenchmarks\n\n> For benchmarks we mostly use 1536-dimensional vectors, like the embeddings produced by the OpenAI Ada API.\n> The code was compiled with GCC 12, using glibc v2.35.\n> The benchmarks performed on Arm-based Graviton3 AWS `c7g` instances and `r7iz` Intel Sapphire Rapids.\n> Most modern Arm-based 64-bit CPUs will have similar relative speedups.\n> Variance within x86 CPUs will be larger.\n\nSimilar speedups are often observed even when compared to BLAS and LAPACK libraries underlying most numerical computing libraries, including NumPy and SciPy in Python.\nBroader benchmarking results:\n\nUsing SimSIMD in Python\n\nThe package is intended to replace the usage of `numpy.inner`, `numpy.dot`, and `scipy.spatial.distance`.\nAside from drastic performance improvements, SimSIMD significantly improves accuracy in mixed precision setups.\nNumPy and SciPy, processing `int8`, `uint8` or `float16` vectors, will use the same types for accumulators, while SimSIMD can combine `int8` enumeration, `int16` multiplication, and `int32` accumulation to avoid overflows entirely.\nThe same applies to processing `float16` and `bfloat16` values with `float32` precision.\n\nInstallation\n\nUse the following snippet to install SimSIMD and list hardware acceleration options available on your machine:"}, {"name": "simsimd", "tags": ["dev", "math", "ml", "web"], "summary": "Portable mixed-precision BLAS-like vector math library for x86 and ARM", "text": "With precompiled binaries, SimSIMD ships `.pyi` interface files for type hinting and static analysis.\nYou can check all the available functions in [`python/annotations/__init__.pyi`](https://github.com/ashvardanian/SimSIMD/blob/main/python/annotations/__init__.pyi).\n\nOne-to-One Distance\n\nSupported functions include `cosine`, `inner`, `sqeuclidean`, `hamming`, `jaccard`, `kullbackleibler`, `jensenshannon`, and `intersect`.\nDot products are supported for both real and complex numbers:\n\nUnlike SciPy, SimSIMD allows explicitly stating the precision of the input vectors, which is especially useful for mixed-precision setups.\nThe `dtype` argument can be passed both by name and as a positional argument:\n\nBinary distance functions are computed at a bit-level.\nMeaning a vector of 10x 8-bit integers will be treated as a sequence of 80 individual bits or dimensions.\nThis differs from NumPy, that can't handle smaller-than-byte types, but you can still avoid the `bin8` argument by reinterpreting the vector as booleans:\n\nWith other frameworks, like PyTorch, one can get a richer type-system than NumPy, but the lack of good CPython interoperability makes it hard to pass data without copies.\nHere is an example of using SimSIMD with PyTorch to compute the cosine similarity between two `bfloat16` vectors:\n\nIt also allows using SimSIMD for half-precision complex numbers, which NumPy does not support.\nFor that, view data as continuous even-length `np.float16` vectors and override type-resolution with `complex32` string.\n\nWhen dealing with sparse representations and integer sets, you can apply the `intersect` function to two 1-dimensional arrays of `uint16` or `uint32` integers:\n\nOne-to-Many Distances\n\nEvery distance function can be used not only for one-to-one but also one-to-many and many-to-many distance calculations.\nFor one-to-many:\n\nMany-to-Many Distances\n\nAll distance functions in SimSIMD can be used to compute many-to-many distances.\nFor two batches of 100 vectors to compute 100 distances, one would call it like this:\n\nInput matrices must have identical shapes.\nThis functionality isn't natively present in NumPy or SciPy, and generally requires creating intermediate arrays, which is inefficient and memory-consuming.\n\nMany-to-Many All-Pairs Distances\n\nOne can use SimSIMD to compute distances between all possible pairs of rows across two matrices (akin to [`scipy.spatial.distance.cdist`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.cdist.html)).\nThe resulting object will have a type `DistancesTensor`, zero-copy compatible with NumPy and other libraries.\nFor two arrays of 10 and 1,000 entries, the resulting tensor will have 10,000 cells:\n\nElement-wise Kernels\n\nSimSIMD also provides mixed-precision element-wise kernels, where the input vectors and the output have the same numeric type, but the intermediate accumulators are of a higher precision.\n\nSimilarly, the `fma` takes three arguments and computes the fused multiply-add operation.\nIn applications like Machine Learning you may also benefit from using the \"brain-float\" format not natively supported by NumPy.\nIn 3D Graphics, for example, we can use FMA to compute the [Phong shading model](https://en.wikipedia.org/wiki/Phong_shading):\n\nMultithreading and Memory Usage\n\nBy default, computations use a single CPU core.\nTo override this behavior, use the `threads` argument.\nSet it to `0` to use all available CPU cores and let the underlying C library manage the thread pool.\nHere is an example of dealing with large sets of binary vectors:"}, {"name": "simsimd", "tags": ["dev", "math", "ml", "web"], "summary": "Portable mixed-precision BLAS-like vector math library for x86 and ARM", "text": "Alternatively, when using free-threading Python 3.13t builds, one can combine single-threaded SimSIMD operations with Python's `concurrent.futures.ThreadPoolExecutor` to parallelize the computations.\nBy default, the output distances will be stored in double-precision `float64` floating-point numbers.\nThat behavior may not be space-efficient, especially if you are computing the hamming distance between short binary vectors, that will generally fit into 8x smaller `uint8` or `uint16` types.\nTo override this behavior, use the `out_dtype` argument, or consider pre-allocating the output array and passing it to the `out` argument.\nA more complete example may look like this:\n\nUsing Python API with USearch\n\nWant to use it in Python with [USearch](https://github.com/unum-cloud/usearch)?\nYou can wrap the raw C function pointers SimSIMD backends into a `CompiledMetric` and pass it to USearch, similar to how it handles Numba's JIT-compiled code.\n\nUsing SimSIMD in Rust\n\nTo install, add the following to your `Cargo.toml`:\n\nBefore using the SimSIMD library, ensure you have imported the necessary traits and types into your Rust source file.\nThe library provides several traits for different distance/similarity kinds - `SpatialSimilarity`, `BinarySimilarity`, and `ProbabilitySimilarity`.\n\nSpatial Similarity: Cosine and Euclidean Distances\n\nSpatial similarity functions are available for `f64`, `f32`, `f16`, and `i8` types.\n\nDot-Products: Inner and Complex Inner Products\n\nComplex inner products are available for `f64`, `f32`, and `f16` types.\n\nProbability Distributions: Jensen-Shannon and Kullback-Leibler Divergences\n\nProbability similarity functions are available for `f64`, `f32`, and `f16` types.\n\nBinary Similarity: Hamming and Jaccard Distances\n\nSimilar to spatial distances, one can compute bit-level distance functions between slices of unsigned integers:\n\nBinary similarity functions are available only for `u8` types.\n\nHalf-Precision Floating-Point Numbers\n\nRust has no native support for half-precision floating-point numbers, but SimSIMD provides a `f16` type with built-in conversion methods.\nThe underlying `u16` representation is publicly accessible for direct bit manipulation.\n\nFor interoperability with the `half` crate:\n\nHalf-Precision Brain-Float Numbers\n\nThe \"brain-float-16\" is a popular machine learning format.\nIt's broadly supported in hardware and is very machine-friendly, but software support is still lagging behind.\n[Unlike NumPy](https://github.com/numpy/numpy/issues/19808), you can already use `bf16` datatype in SimSIMD.\nSimSIMD provides a `bf16` type with built-in conversion methods and direct bit access.\n\nDynamic Dispatch in Rust\n\nSimSIMD provides a [dynamic dispatch](#dynamic-dispatch) mechanism to select the most advanced micro-kernel for the current CPU.\nYou can query supported backends and use the `SimSIMD::capabilities` function to select the best one.\n\nUsing SimSIMD in JavaScript\n\nTo install, choose one of the following options depending on your environment:\n\n- `npm install --save simsimd`\n- `yarn add simsimd`\n- `pnpm add simsimd`\n- `bun install simsimd`\n\nThe package is distributed with prebuilt binaries, but if your platform is not supported, you can build the package from the source via `npm run build`.\nThis will automatically happen unless you install the package with the `--ignore-scripts` flag or use Bun.\nAfter you install it, you will be able to call the SimSIMD functions on various `TypedArray` variants:\n\nOther numeric types and precision levels are supported as well.\nFor double-precision floating-point numbers, use `Float64Array`:"}, {"name": "simsimd", "tags": ["dev", "math", "ml", "web"], "summary": "Portable mixed-precision BLAS-like vector math library for x86 and ARM", "text": "When doing machine learning and vector search with high-dimensional vectors you may want to quantize them to 8-bit integers.\nYou may want to project values from the $[-1, 1]$ range to the $[-127, 127]$ range and then cast them to `Int8Array`:\n\nA more extreme quantization case would be to use binary vectors.\nYou can map all positive values to `1` and all negative values and zero to `0`, packing eight values into a single byte.\nAfter that, Hamming and Jaccard distances can be computed.\n\nUsing SimSIMD in Swift\n\nTo install, simply add the following dependency to your `Package.swift`:\n\nThe package provides the most common spatial metrics for `Int8`, `Float16`, `Float32`, and `Float64` vectors.\n\nUsing SimSIMD in C\n\nFor integration within a CMake-based project, add the following segment to your `CMakeLists.txt`:\n\nAfter that, you can use the SimSIMD library in your C code in several ways.\nSimplest of all, you can include the headers, and the compiler will automatically select the most recent CPU extensions that SimSIMD will use.\n\nDynamic Dispatch in C\n\nTo avoid hard-coding the backend, you can rely on `c/lib.c` to prepackage all possible backends in one binary, and select the most recent CPU features at runtime.\nThat feature of the C library is called [dynamic dispatch](#dynamic-dispatch) and is extensively used in the Python, JavaScript, and Rust bindings.\nTo test which CPU features are available on the machine at runtime, use the following APIs:\n\nTo override compilation settings and switch between runtime and compile-time dispatch, define the following macro:\n\nSpatial Distances: Cosine and Euclidean Distances\n\nDot-Products: Inner and Complex Inner Products\n\nBinary Distances: Hamming and Jaccard Distances\n\nProbability Distributions: Jensen-Shannon and Kullback-Leibler Divergences\n\nHalf-Precision Floating-Point Numbers\n\nIf you aim to utilize the `_Float16` functionality with SimSIMD, ensure your development environment is compatible with C 11.\nFor other SimSIMD functionalities, C 99 compatibility will suffice.\nTo explicitly disable half-precision support, define the following macro before imports:\n\nCompilation Settings and Debugging\n\n`SIMSIMD_DYNAMIC_DISPATCH`:\n\n> By default, SimSIMD is a header-only library.\n> But if you are running on different generations of devices, it makes sense to pre-compile the library for all supported generations at once, and dispatch at runtime.\n> This flag does just that and is used to produce the `simsimd.so` shared library, as well as the Python and other bindings.\n\nFor Arm: `SIMSIMD_TARGET_NEON`, `SIMSIMD_TARGET_SVE`, `SIMSIMD_TARGET_SVE2`, `SIMSIMD_TARGET_NEON_F16`, `SIMSIMD_TARGET_SVE_F16`, `SIMSIMD_TARGET_NEON_BF16`, `SIMSIMD_TARGET_SVE_BF16`.\nFor x86: `SIMSIMD_TARGET_HASWELL`, `SIMSIMD_TARGET_SKYLAKE`, `SIMSIMD_TARGET_ICE`, `SIMSIMD_TARGET_GENOA`, `SIMSIMD_TARGET_SAPPHIRE`, `SIMSIMD_TARGET_TURIN`, `SIMSIMD_TARGET_SIERRA`.\n\n> By default, SimSIMD automatically infers the target architecture and pre-compiles as many kernels as possible.\n> In some cases, you may want to explicitly disable some of the kernels.\n> Most often it's due to compiler support issues, like the lack of some recent intrinsics or low-precision numeric types.\n> In other cases, you may want to disable some kernels to speed up the compilation process and trim the binary size.\n\n`SIMSIMD_SQRT`, `SIMSIMD_RSQRT`, `SIMSIMD_LOG`:"}, {"name": "simsimd", "tags": ["dev", "math", "ml", "web"], "summary": "Portable mixed-precision BLAS-like vector math library for x86 and ARM", "text": "> By default, for __non__-SIMD backends, SimSIMD may use `libc` functions like `sqrt` and `log`.\n> Those are generally very accurate, but slow, and introduce a dependency on the C standard library.\n> To avoid that you can override those definitions with your custom implementations, like: `#define SIMSIMD_RSQRT(x) (1 / sqrt(x))`.\n\nAlgorithms & Design Decisions\n\nIn general there are a few principles that SimSIMD follows:\n\n- Avoid loop unrolling.\n- Never allocate memory.\n- Never throw exceptions or set `errno`.\n- Keep all function arguments the size of the pointer.\n- Avoid returning from public interfaces, use out-arguments instead.\n- Don't over-optimize for old CPUs and single- and double-precision floating-point numbers.\n- Prioritize mixed-precision and integer operations, and new ISA extensions.\n- Prefer saturated arithmetic and avoid overflows.\n\nPossibly, in the future:\n\n- Best effort computation silencing `NaN` components in low-precision inputs.\n- Detect overflows and report the distance with a \"signaling\" `NaN`.\n\nLast, but not the least - don't build unless there is a demand for it.\nSo if you have a specific use-case, please open an issue or a pull request, and ideally, bring in more users with similar needs.\n\nCosine Similarity, Reciprocal Square Root, and Newton-Raphson Iteration\n\nThe cosine similarity is the most common and straightforward metric used in machine learning and information retrieval.\nInterestingly, there are multiple ways to shoot yourself in the foot when computing it.\nThe cosine similarity is the inverse of the cosine distance, which is the cosine of the angle between two vectors.\n\nIn NumPy terms, SimSIMD implementation is similar to:\n\nIn SciPy, however, the cosine distance is computed as `1 - ab / np.sqrt(a2 * b2)`.\nIt handles the edge case of a zero and non-zero argument pair differently, resulting in a division by zero error.\nIt's not only less efficient, but also less accurate, given how the reciprocal square roots are computed.\nThe C standard library provides the `sqrt` function, which is generally very accurate, but slow.\nThe `rsqrt` in-hardware implementations are faster, but have different accuracy characteristics.\n\n- SSE `rsqrtps` and AVX `vrsqrtps`: $1.5 \\times 2^{-12}$ maximal relative error.\n- AVX-512 `vrsqrt14pd` instruction: $2^{-14}$ maximal relative error.\n- NEON `frsqrte` instruction has no documented error bounds, but [can be][arm-rsqrt] $2^{-3}$.\n\nTo overcome the limitations of the `rsqrt` instruction, SimSIMD uses the Newton-Raphson iteration to refine the initial estimate for high-precision floating-point numbers.\nIt can be defined as:\n\nOn 1536-dimensional inputs on Intel Sapphire Rapids CPU a single such iteration can result in a 2-3 orders of magnitude relative error reduction:\n\nDatatype\n:---------\n`bfloat16`\n`float16`\n`float32`\n`float64`\n\nCurved Spaces, Mahalanobis Distance, and Bilinear Quadratic Forms\n\nThe Mahalanobis distance is a generalization of the Euclidean distance, which takes into account the covariance of the data.\nIt's very similar in its form to the bilinear form, which is a generalization of the dot product.\n\nBilinear Forms can be seen as one of the most important linear algebraic operations, surprisingly missing in BLAS and LAPACK.\nThey are versatile and appear in various domains:"}, {"name": "simsimd", "tags": ["dev", "math", "ml", "web"], "summary": "Portable mixed-precision BLAS-like vector math library for x86 and ARM", "text": "- In Quantum Mechanics, the expectation value of an observable $A$ in a state $\\psi$ is given by $\\langle \\psi | A | \\psi \\rangle$, which is a bilinear form.\n- In Machine Learning, in Support Vector Machines (SVMs), bilinear forms define kernel functions that measure similarity between data points.\n- In Differential Geometry, the metric tensor, which defines distances and angles on a manifold, is a bilinear form on the tangent space.\n- In Economics, payoff functions in certain Game Theoretic problems can be modeled as bilinear forms of players' strategies.\n- In Physics, interactions between electric and magnetic fields can be expressed using bilinear forms.\n\nBroad applications aside, the lack of a specialized primitive for bilinear forms in BLAS and LAPACK means significant performance overhead.\nA $vector * matrix * vector$ product is a scalar, whereas its constituent parts ($vector * matrix$ and $matrix * vector$) are vectors:\n\n- They need memory to be stored in: $O(n)$ allocation.\n- The data will be written to memory and read back, wasting CPU cycles.\n\nSimSIMD doesn't produce intermediate vector results, like `a @ M @ b`, but computes the bilinear form directly.\n\nSet Intersection, Galloping, and Binary Search\n\nThe set intersection operation is generally defined as the number of elements that are common between two sets, represented as sorted arrays of integers.\nThe most common way to compute it is a linear scan:\n\nAlternatively, one can use the binary search to find the elements in the second array that are present in the first one.\nOn every step the checked region of the second array is halved, which is called the _galloping search_.\nIt's faster, but only when large arrays of very different sizes are intersected.\nThird approach is to use the SIMD instructions to compare multiple elements at once:\n\n- Using string-intersection instructions on x86, like `pcmpestrm`.\n- Using integer-intersection instructions in AVX-512, like `vp2intersectd`.\n- Using vanilla equality checks present in all SIMD instruction sets.\n\nAfter benchmarking, the last approach was chosen, as it's the most flexible and often the fastest.\n\nComplex Dot Products, Conjugate Dot Products, and Complex Numbers\n\nComplex dot products are a generalization of the dot product to complex numbers.\nThey are supported by most BLAS packages, but almost never in mixed precision.\nSimSIMD defines `dot` and `vdot` kernels as:\n\nWhere $\\bar{b_i}$ is the complex conjugate of $b_i$.\nPutting that into Python code for scalar arrays:\n\nLogarithms in Kullback-Leibler & Jensen\u2013Shannon Divergences\n\nThe Kullback-Leibler divergence is a measure of how one probability distribution diverges from a second, expected probability distribution.\nJensen-Shannon divergence is a symmetrized and smoothed version of the Kullback-Leibler divergence, which can be used as a distance metric between probability distributions.\n\nBoth functions are defined for non-negative numbers, and the logarithm is a key part of their computation.\n\nMixed Precision in Fused-Multiply-Add and Weighted Sums\n\nThe Fused-Multiply-Add (FMA) operation is a single operation that combines element-wise multiplication and addition with different scaling factors.\nThe Weighted Sum is its simplified variant without element-wise multiplication."}, {"name": "simsimd", "tags": ["dev", "math", "ml", "web"], "summary": "Portable mixed-precision BLAS-like vector math library for x86 and ARM", "text": "In NumPy terms, the implementation may look like:\n\nThe tricky part is implementing those operations in mixed precision, where the scaling factors are of different precision than the input and output vectors.\nSimSIMD uses double-precision floating-point scaling factors for any input and output precision, including `i8` and `u8` integers and `f16` and `bf16` floats.\nDepending on the generation of the CPU, given native support for `f16` addition and multiplication, the `f16` temporaries are used for `i8` and `u8` multiplication, scaling, and addition.\nFor `bf16`, native support is generally limited to dot-products with subsequent partial accumulation, which is not enough for the FMA and WSum operations, so `f32` is used as a temporary.\n\nAuto-Vectorization & Loop Unrolling\n\nOn the Intel Sapphire Rapids platform, SimSIMD was benchmarked against auto-vectorized code using GCC 12.\nGCC handles single-precision `float` but might not be the best choice for `int8` and `_Float16` arrays, which have been part of the C language since 2011.\n\nKind\n:------------------------\nInner Product\nCosine Distance\nEuclidean Distance \u00b2\nJensen-Shannon Divergence\n\nDynamic Dispatch\n\nMost popular software is precompiled and distributed with fairly conservative CPU optimizations, to ensure compatibility with older hardware.\nDatabase Management platforms, like ClickHouse, and Web Browsers, like Google Chrome,need to run on billions of devices, and they can't afford to be picky about the CPU features.\nFor such users SimSIMD provides a dynamic dispatch mechanism, which selects the most advanced micro-kernel for the current CPU at runtime.\n\nYou can compile SimSIMD on an old CPU, like Intel Haswell, and run it on a new one, like AMD Genoa, and it will automatically use the most advanced instructions available.\nReverse is also true, you can compile on a new CPU and run on an old one, and it will automatically fall back to the most basic instructions.\nMoreover, the very first time you prove for CPU capabilities with `simsimd_capabilities()`, it initializes the dynamic dispatch mechanism, and all subsequent calls will be faster and won't face race conditions in multi-threaded environments.\n\nTarget Specific Backends\n\nSimSIMD exposes all kernels for all backends, and you can select the most advanced one for the current CPU without relying on built-in dispatch mechanisms.\nThat's handy for testing and benchmarking, but also in case you want to dispatch a very specific kernel for a very specific CPU, bypassing SimSIMD assignment logic.\nAll of the function names follow the same pattern: `simsimd_{function}_{type}_{backend}`.\n\n- The backend can be `serial`, `haswell`, `skylake`, `ice`, `genoa`, `sapphire`, `turin`, `neon`, or `sve`.\n- The type can be `f64`, `f32`, `f16`, `bf16`, `f64c`, `f32c`, `f16c`, `bf16c`, `i8`, or `b8`.\n- The function can be `dot`, `vdot`, `cos`, `l2sq`, `hamming`, `jaccard`, `kl`, `js`, or `intersect`.\n\nTo avoid hard-coding the backend, you can use the `simsimd_kernel_punned_t` to pun the function pointer and the `simsimd_capabilities` function to get the available backends at runtime.\nTo match all the function names, consider a RegEx:\n\nOn Linux, you can use the following command to list all unique functions:\n\nLicense"}, {"name": "simsimd", "tags": ["dev", "math", "ml", "web"], "summary": "Portable mixed-precision BLAS-like vector math library for x86 and ARM", "text": "Feel free to use the project under Apache 2.0 or the Three-clause BSD license at your preference."}, {"name": "simsimd", "tags": ["dev", "math", "ml", "web"], "summary": "Portable mixed-precision BLAS-like vector math library for x86 and ARM", "text": "This library is used to accelerate mixed-precision vector math operations in computational pipelines, enabling developers to 10x cost-efficiency in workloads such as AI, Search, and DBMS. It provides a wide range of optimized functions for tasks including spatial distance calculations, dot products, bit-level distances, and set intersections."}, {"name": "sktime", "tags": ["math", "ml", "ui", "web"], "summary": "A unified framework for machine learning with time series", "text": "Welcome to sktime\n\n> A unified interface for machine learning with time series\n\n:rocket: **Version 0.40.1 out now!** [Check out the release notes here](https://www.sktime.net/en/latest/changelog.html).\n\nsktime is a library for time series analysis in Python. It provides a unified interface for multiple time series learning tasks. Currently, this includes forecasting, time series classification, clustering, anomaly/changepoint detection, and other tasks. It comes with [time series algorithms](https://www.sktime.net/en/stable/estimator_overview.html) and [scikit-learn] compatible tools to build, tune, and validate time series models.\n\n**[Documentation](https://www.sktime.net/en/stable/users.html)** \u00b7 **[Tutorials](https://www.sktime.net/en/stable/examples.html)** \u00b7 **[Release Notes](https://www.sktime.net/en/stable/changelog.html)**\n---\n**Open&#160;Source**\n**Tutorials**\n**Community**\n\n**Code**\n**Downloads**\n**Citation**\n\n:books: Documentation\n\nDocumentation\n--------------------------------------\n:star: **[Tutorials]**\n:clipboard: **[Binder Notebooks]**\n:woman_technologist: **[Examples]**\n:scissors: **[Extension Templates]**\n:control_knobs: **[API Reference]**\n:tv: **[Video Tutorial]**\n:hammer_and_wrench: **[Changelog]**\n:deciduous_tree: **[Roadmap]**\n:pencil: **[Related Software]**\n\n:speech_balloon: Where to ask questions\n\nQuestions and feedback are extremely welcome! We strongly believe in the value of sharing help publicly, as it allows a wider audience to benefit from it.\n\nType\n-------------------------------\n:bug: **Bug Reports**\n:sparkles: **Feature Requests & Ideas**\n:woman_technologist: **Usage Questions**\n:speech_balloon: **General Discussion**\n:factory: **Contribution & Development**\n:globe_with_meridians: **Meet-ups and collaboration sessions**\n\n:dizzy: Features\nOur objective is to enhance the interoperability and usability of the time series analysis ecosystem in its entirety. sktime provides a __unified interface for distinct but related time series learning tasks__. It features [__dedicated time series algorithms__](https://www.sktime.net/en/stable/estimator_overview.html) and __tools for composite model building__,  such as pipelining, ensembling, tuning, and reduction, empowering users to apply algorithms designed for one task to another.\n\nsktime also provides **interfaces to related libraries**, for example [scikit-learn], [statsmodels], [tsfresh], [PyOD], and [fbprophet], among others.\n\nModule\n---\n**[Forecasting]**\n**[Time Series Classification]**\n**[Time Series Regression]**\n**[Transformations]**\n**[Detection tasks]**\n**[Parameter fitting]**\n**[Time Series Clustering]**\n**[Time Series Distances/Kernels]**\n**[Time Series Alignment]**\n**[Time Series Splitters]**\n**[Distributions and simulation]**\n\n:hourglass_flowing_sand: Install sktime\nFor troubleshooting and detailed installation instructions, see the [documentation](https://www.sktime.net/en/latest/installation.html).\n\n- **Operating system**: macOS X \u00b7 Linux \u00b7 Windows 8.1 or higher\n- **Python version**: Python 3.10, 3.11, 3.12, and 3.13 (only 64-bit)\n- **Package managers**: [pip] \u00b7 [conda] (via `conda-forge`)\n\npip\nUsing pip, sktime releases are available as source packages and binary wheels.\nAvailable wheels are listed [here](https://pypi.org/simple/sktime/).\n\nor, with maximum dependencies,\n\nFor curated sets of soft dependencies for specific learning tasks:\n\nor similar. Valid sets are:\n\n* `forecasting`\n* `transformations`\n* `classification`\n* `regression`\n* `clustering`\n* `param_est`\n* `networks`\n* `detection`\n* `alignment`\n\nCave: in general, not all soft dependencies for a learning task are installed,\nonly a curated selection.\n\nconda\nYou can also install sktime from `conda` via the `conda-forge` channel.\nThe feedstock including the build recipe and configuration is maintained\nin [this conda-forge repository](https://github.com/conda-forge/sktime-feedstock).\n\nor, with maximum dependencies,\n\n(as `conda` does not support dependency sets,\nflexible choice of soft dependencies is unavailable via `conda`)\n\n:zap: Quickstart\n\nForecasting\n\nTime Series Classification\n\n:wave: How to get involved\n\nThere are many ways to join the sktime community. We follow the [all-contributors](https://github.com/all-contributors/all-contributors) specification: all kinds of contributions are welcome - not just code.\n\nDocumentation\n--------------------------\n:gift_heart: **[Contribute]**\n:school_satchel:  **[Mentoring]**\n:date: **[Meetings]**\n:woman_mechanic:  **[Developer Guides]**\n:construction: **[Enhancement Proposals]**\n:medal_sports: **[Contributors]**\n:raising_hand: **[Roles]**\n:money_with_wings: **[Donate]**\n:classical_building: **[Governance]**\n\n:trophy: Hall of fame\n\nThanks to all our community for all your wonderful contributions, PRs, issues, ideas.\n\n:bulb: Project vision\n\n* **By the community, for the community** -- developed by a friendly and collaborative community.\n* The **right tool for the right task** -- helping users to diagnose their learning problem and suitable scientific model types.\n* **Embedded in state-of-art ecosystems** and **provider of interoperable interfaces** -- interoperable with [scikit-learn], [statsmodels], [tsfresh], and other community favorites.\n* **Rich model composition and reduction functionality** -- build tuning and feature extraction pipelines, solve forecasting tasks with [scikit-learn] regressors.\n* **Clean, descriptive specification syntax** -- based on modern object-oriented design principles for data science.\n* **Fair model assessment and benchmarking** -- build your models, inspect your models, check your models, and avoid pitfalls.\n* **Easily extensible** -- easy extension templates to add your own algorithms compatible with sktime's API."}, {"name": "sktime", "tags": ["math", "ml", "ui", "web"], "summary": "A unified framework for machine learning with time series", "text": "This library is used to provide a unified interface for multiple machine learning tasks with time series data, including forecasting and anomaly detection. This allows developers to build, tune, and validate time series models with ease using sktime's algorithms and scikit-learn compatible tools."}, {"name": "snowflake-connector-python", "tags": ["data", "math", "web"], "summary": "Snowflake Connector for Python", "text": "Release Notes\n- v4.1.1(TBD)\n  - Relaxed pandas dependency requirements for Python below 3.12.\n  - Changed CRL cache cleanup background task to daemon to avoid blocking main thread.\n  - Fixed NO_PROXY issues with PUT operations\n\n- v4.1.0(November 18,2025)\n  - Added the `SNOWFLAKE_AUTH_FORCE_SERVER` environment variable to force the use of the local-listening server when using the `externalbrowser` auth method.\n  - Fix compilation error when building from sources with libc++.\n  - Pin lower versions of dependencies to oldest version without vulnerabilities.\n  - Added no_proxy parameter for proxy configuration without using environmental variables.\n  - Added OAUTH_AUTHORIZATION_CODE and OAUTH_CLIENT_CREDENTIALS to list of authenticators that don't require user to be set\n  - Added `oauth_socket_uri` connection parameter allowing to separate server and redirect URIs for local OAuth server.\n  - Made platform_detection logs silent and improved its timeout handling. Added support for ENV_VAR_DISABLE_PLATFORM_DETECTION environment variable.\n  - Fixed FIPS environments md5 hash issues with multipart upload on Azure.\n\n- v4.0.0(October 09,2025)\n  - Added support for checking certificates revocation using revocation lists (CRLs)\n  - Added `CERT_REVOCATION_CHECK_MODE` to `CLIENT_ENVIRONMENT`\n  - Added the `workload_identity_impersonation_path` parameter to support service account impersonation for Workload Identity Federation on GCP and AWS workloads only\n  - Fixed `get_results_from_sfqid` when using `DictCursor` and executing multiple statements at once\n  - Added the `oauth_credentials_in_body` parameter supporting an option to send the oauth client credentials in the request body\n  - Fix retry behavior for `ECONNRESET` error\n  - Added an option to exclude `botocore` and `boto3` dependencies by setting `SNOWFLAKE_NO_BOTO` environment variable during installation\n  - Revert changing exception type in case of token expired scenario for `Oauth` authenticator back to `DatabaseError`\n  - Enhanced configuration file security checks with stricter permission validation.\n  - Fixed the return type of `SnowflakeConnection.cursor(cursor_class)` to match the type of `cursor_class`\n  - Constrained the types of `fetchone`, `fetchmany`, `fetchall`\n  - Fix \"No AWS region was found\" error if AWS region was set in `AWS_DEFAULT_REGION` variable instead of `AWS_REGION` for `WORKLOAD_IDENTITY` authenticator\n  - Add `ocsp_root_certs_dict_lock_timeout` connection parameter to set the timeout (in seconds) for acquiring the lock on the OCSP root certs dictionary. Default value for this parameter is -1 which indicates no timeout.\n  - Fixed behaviour of trying S3 Transfer Accelerate endpoint by default for internal stages, and always getting HTTP403 due to permissions missing on purpose. Now /accelerate is not attempted.\n\n- v3.18.0(October 03,2025)\n  - Added support for pandas conversion for Day-time and Year-Month Interval types\n\n- v3.17.4(September 22,2025)\n  - Added support for intermediate certificates as roots when they are stored in the trust store\n  - Bumped up vendored `urllib3` to `2.5.0` and `requests` to `v2.32.5`\n  - Dropped support for OpenSSL versions older than 1.1.1\n\n- v3.17.3(September 02,2025)\n  - Enhanced configuration file permission warning messages.\n  - Fixed the bug with staging pandas dataframes on AWS - the regional endpoint is used when required\n\n- v3.17.2(August 23,2025)\n  - Fixed a bug where platform_detection was retrying failed requests with warnings to non-existent endpoints.\n  - Added disabling endpoint-based platform detection by setting `platform_detection_timeout_seconds` to zero."}, {"name": "snowflake-connector-python", "tags": ["data", "math", "web"], "summary": "Snowflake Connector for Python", "text": "- v3.17.1(August 17,2025)\n  - Added `infer_schema` parameter to `write_pandas` to perform schema inference on the passed data.\n  - Namespace `snowlake` reverted back to non-module.\n\n- v3.17.0(August 16,2025)\n  - Added in-band HTTP exception telemetry.\n  - Added an `unsafe_skip_file_permissions_check` flag to skip file permission checks on the cache and configuration.\n  - Added `APPLICATION_PATH` within `CLIENT_ENVIRONMENT` to distinguish between multiple scripts using the Python Connector in the same environment.\n  - Added basic JSON support for Interval types.\n  - Added in-band OCSP exception telemetry.\n  - Added support for new authentication methods with Workload Identity Federation (WIF).\n  - Added support for the `use_vectorized_scanner` parameter in the write_pandas function.\n  - Added support of proxy setup using connection parameters without emitting environment variables.\n  - Added populating of `type_code` in `ResultMetadata` for interval types.\n  - Introduced the `snowflake_version` property to the connection.\n  - Moved `OAUTH_TYPE` to `CLIENT_ENVIROMENT`.\n  - Relaxed the `pyarrow` version constrain; versions >= 19 can now be used.\n  - Disabled token caching for OAuth Client Credentials authentication.\n  - Fixed OAuth authenticator values.\n  - Fixed a bug where a PAT with an external session authenticator was used while `external_session_id` was not provided in `SnowflakeRestful.fetch`.\n  - Fixed the case-sensitivity of `Oauth` and `programmatic_access_token` authenticator values.\n  - Fixed unclear error messages for incorrect `authenticator` values.\n  - Fixed GCS staging by ensuring the endpoint has a scheme.\n  - Fixed a bug where time-zoned timestamps fetched as a `pandas.DataFrame` or `pyarrow.Table` would overflow due to unnecessary precision. A clear error will now be raised if an overflow cannot be prevented.\n\n- v3.16.0(July 04,2025)\n  - Bumped numpy dependency from =16.2.0,=22.0.0,=23.1.0,=23.1.0,=2.6.0,=2.6.0,=1.0.0,=1.0.0,=3.1.0 to >=3.1.0,=16.2.0,=16.2.0,=2.6.0,=2.6.0, ``manager`` renaming more consistent in ``snowflake.connector.config_manager`` module.\n  - Added support for default values for ConfigOptions\n  - Added default_connection_name to config.toml file\n\n- v3.1.1(August 28,2023)\n\n- Fixed a bug in retry logic for okta authentication to refresh token.\n  - Support `RSAPublicKey` when constructing `AuthByKeyPair` in addition to raw bytes.\n  - Fixed a bug when connecting through SOCKS5 proxy, the attribute `proxy_header` is missing on `SOCKSProxyManager`.\n  - Cherry-picked https://github.com/urllib3/urllib3/commit/fd2759aa16b12b33298900c77d29b3813c6582de onto vendored urllib3 (v1.26.15) to enable enforce_content_length by default.\n  - Fixed a bug in tag generation of OOB telemetry event.\n\n- v3.1.0(July 31,2023)\n\n- Added a feature that lets you add connection definitions to the `connections.toml` configuration file. A connection definition refers to a collection of connection parameters, for example, if you wanted to define a connection named `prod``:\n\n- Bumped cryptography dependency from =3.1.0 to >=3.1.0,=1.0.0 to >=1.0.0,=8.0.0,=10.0.1,1 to submit the specified exact number of statements in a multi-statement query\n\n- v2.8.3(November 28,2022)\n\n- Bumped cryptography dependency from =8.0.0,=16.2.0,=2.5 to >=2.5,=16.2.0 to >=16.2.0,=1.0.0 to >=1.0.0,=1.7,<1.8\n\n- v1.3.17 (June 1, 2017)\n\n- v1.3.16 (April 20, 2017)\n\n- v1.3.15 (March 30, 2017)\n\n- v1.3.14 (February 24, 2017)\n\n- v1.3.13 (February 9, 2017)\n\n- v1.3.12 (February 2, 2017)\n\n- v1.3.11 (January 27, 2017)\n\n- v1.3.10 (January 26, 2017)\n\n- v1.3.9 (January 16, 2017)\n\n- v1.3.8 (January 12, 2017)\n\n- v1.3.7 (December 8, 2016)\n\n- v1.3.6 (December 1, 2016)\n\n- v1.3.5 (November 17, 2016)\n\n- v1.3.4 (November 3, 2016)\n\n- v1.3.3 (October 20, 2016)"}, {"name": "snowflake-connector-python", "tags": ["data", "math", "web"], "summary": "Snowflake Connector for Python", "text": "- v1.3.2 (October 12, 2016)\n\n- v1.3.1 (September 30, 2016)\n\n- v1.3.0 (September 26, 2016)\n\n- v1.2.8 (August 16, 2016)\n\n- v1.2.7 (July 31, 2016)\n\n- v1.2.6 (July 13, 2016)\n\n- v1.2.5 (July 8, 2016)\n\n- v1.2.4 (July 6, 2016)\n\n- v1.2.3 (June 29, 2016)\n\n- v1.2.2 (June 21, 2016)\n\n- v1.2.1 (June 13, 2016)\n\n- v1.2.0 (June 10, 2016)\n\n- v1.1.5 (June 2, 2016)\n\n- v1.1.4 (May 21, 2016)\n\n- v1.1.3 (May 5, 2016)\n\n- v1.1.2 (May 4, 2016)\n\n- v1.1.1 (Apr 11, 2016)\n\n- v1.1.0 (Apr 4, 2016)\n\n- v1.0.7 (Mar 21, 2016)\n\n- v1.0.6 (Mar 15, 2016)\n\n- v1.0.5 (Mar 1, 2016)\n\n- v1.0.4 (Feb 15, 2016)\n\n- v1.0.3 (Jan 13, 2016)\n\n- v1.0.2 (Dec 15, 2015)\n\n- v1.0.1 (Dec 8, 2015)\n\n- v1.0.0 (Dec 1, 2015)"}, {"name": "snowflake-connector-python", "tags": ["data", "math", "web"], "summary": "Snowflake Connector for Python", "text": "This library is used to simplify interactions with Snowflake databases in Python applications, allowing for secure and efficient data transfer. It provides a robust connection mechanism and handles various authentication methods, including OAuth and external browser login, making it easier to integrate Snowflake with Python-based systems."}, {"name": "snowflake-legacy", "tags": ["data", "math", "web"], "summary": "You should switch to the snowflake-uuid package", "text": "The snowflake._legacy package\n\nHistorically, the `snowflake` package on PyPI was an unrelated package owned\nby an independent developer not affiliated with\n[Snowflake](https://www.snowflake.com/).  After friendly discussion in\n2023, it was agreed to transfer the PyPI `snowflake` package name to\nSnowflake.\n\nIn order to provide a more manageable transition, Snowflake has agreed to\nmaintain some backward compatibility with the old `snowflake` package, now\nrenamed to [snowflake-uuid](https://pypi.org/project/snowflake-uuid/).  For a\nperiod of one year, existing users of the old package will be able to:\n\n* `import snowflake` and call `snowflake.snowflake()` to return the contents\n  of `/etc/snowflake` as a string, if it exists.  No validation of the value\n  is performed.  If `/etc/snowflake` doesn't exist, a `FileNotFoundError` will\n  be raised.  `snowflake.snowflake()` takes a single string argument which\n  names an alternative path to read from, and this is supported in the legacy\n  API.\n* `snowflake.make_snowflake()` will always raise a `NotImplementedError` and\n  point users to the new `snowflake-uuid` package.\n\nIt is highly recommended that all consumers of the old `snowflake` package\nchange their dependency to `snowflake-uuid` as soon as possible."}, {"name": "snowflake-legacy", "tags": ["data", "math", "web"], "summary": "You should switch to the snowflake-uuid package", "text": "This library is used to provide backward compatibility with an older, unrelated package named `snowflake`, and to allow existing users to continue accessing specific functionality for a limited time. It allows developers to read the contents of `/etc/snowflake` as a string using the `snowflake.snowflake()` function without validation."}, {"name": "snowflake-snowpark-python", "tags": ["data", "math", "ui", "web"], "summary": "Snowflake Snowpark for Python", "text": "Snowflake Snowpark Python and Snowpark pandas APIs\n\n(https://github.com/snowflakedb/snowpark-python/actions/workflows/precommit.yml)\n(https://codecov.io/gh/snowflakedb/snowpark-python)\n(https://pypi.org/project/snowflake-snowpark-python/)\n(http://www.apache.org/licenses/LICENSE-2.0.txt)\n(https://github.com/psf/black)\n\nThe Snowpark library provides intuitive APIs for querying and processing data in a data pipeline.\nUsing this library, you can build applications that process data in Snowflake without having to move data to the system where your application code runs.\n\n[Source code][source code] | [Snowpark Python developer guide][Snowpark Python developer guide] | [Snowpark Python API reference][Snowpark Python api references] | [Snowpark pandas developer guide][Snowpark pandas developer guide] | [Snowpark pandas API reference][Snowpark pandas api references] | [Product documentation][snowpark] | [Samples][samples]\n\nGetting started\n\nHave your Snowflake account ready\nIf you don't have a Snowflake account yet, you can [sign up for a 30-day free trial account][sign up trial].\n\nCreate a Python virtual environment\nYou can use [miniconda][miniconda], [anaconda][anaconda], or [virtualenv][virtualenv]\nto create a Python 3.9, 3.10, 3.11, 3.12 or 3.13 virtual environment.\n\nFor Snowpark pandas, only Python 3.9, 3.10, or 3.11 is supported.\n\nTo have the best experience when using it with UDFs, [creating a local conda environment with the Snowflake channel][use snowflake channel] is recommended.\n\nInstall the library to the Python virtual environment\n\nTo use the [Snowpark pandas API][Snowpark pandas developer guide], you can optionally install the following, which installs [modin][modin] in the same environment. The Snowpark pandas API provides a familiar interface for pandas users to query and process data directly in Snowflake.\n\nCreate a session and use the Snowpark Python API\n\nCreate a session and use the Snowpark pandas API\n\nSamples\nThe [Snowpark Python developer guide][Snowpark Python developer guide], [Snowpark Python API references][Snowpark Python api references], [Snowpark pandas developer guide][Snowpark pandas developer guide], and [Snowpark pandas api references][Snowpark pandas api references] have basic sample code.\n[Snowflake-Labs][snowflake lab sample code] has more curated demos.\n\nLogging\nConfigure logging level for `snowflake.snowpark` for Snowpark Python API logs.\nSnowpark uses the [Snowflake Python Connector][python connector].\nSo you may also want to configure the logging level for `snowflake.connector` when the error is in the Python Connector.\nFor instance,\n\nReading and writing to pandas DataFrame\n\nSnowpark Python API supports reading from and writing to a pandas DataFrame via the [to_pandas][to_pandas] and [write_pandas][write_pandas] commands. \n\nTo use these operations, ensure that pandas is installed in the same environment. You can install pandas alongside Snowpark Python by executing the following command:\n\nOnce pandas is installed, you can convert between a Snowpark DataFrame and pandas DataFrame as follows: \n\nSnowpark pandas API also supports writing to pandas: \n\nNote that the above Snowpark pandas commands will work if Snowpark is installed with the `[modin]` option, the additional `[pandas]` installation is not required.\n\nVerifying Package Signatures\n\nTo ensure the authenticity and integrity of the Python package, follow the steps below to verify the package signature using `cosign`.\n\n**Steps to verify the signature:**\n- Install cosign:\n  - This example is using golang installation: [installing-cosign-with-go](https://edu.chainguard.dev/open-source/sigstore/cosign/how-to-install-cosign/#installing-cosign-with-go)\n- Download the file from the repository like pypi:\n  - https://pypi.org/project/snowflake-snowpark-python/#files\n- Download the signature files from the release tag, replace the version number with the version you are verifying:\n  - https://github.com/snowflakedb/snowpark-python/releases/tag/v1.22.1\n- Verify signature:\n  `\n\nContributing\nPlease refer to [CONTRIBUTING.md][contributing].\n\n[add other sample code repo links]: # (Developer advocacy is open-sourcing a repo that has excellent sample code. The link will be added here.)"}, {"name": "snowflake-snowpark-python", "tags": ["data", "math", "ui", "web"], "summary": "Snowflake Snowpark for Python", "text": "This library is used to build applications that process data in Snowflake without having to move data to the system where your application code runs. It provides intuitive APIs for querying and processing data in a data pipeline, enabling efficient data processing and management."}, {"name": "snowflake-sqlalchemy", "tags": ["data", "math", "web"], "summary": "Snowflake SQLAlchemy Dialect", "text": "Snowflake SQLAlchemy\n\n(https://github.com/snowflakedb/snowflake-sqlalchemy/actions/workflows/build_test.yml)\n(https://codecov.io/gh/snowflakedb/snowflake-sqlalchemy)\n(https://pypi.python.org/pypi/snowflake-sqlalchemy/)\n(http://www.apache.org/licenses/LICENSE-2.0.txt)\n(https://github.com/psf/black)\n\nSnowflake SQLAlchemy runs on the top of the Snowflake Connector for Python as a [dialect](http://docs.sqlalchemy.org/en/latest/dialects/) to bridge a Snowflake database and SQLAlchemy applications.\n\nPrerequisites\n\nSnowflake Connector for Python\n\nThe only requirement for Snowflake SQLAlchemy is the Snowflake Connector for Python; however, the connector does not need to be installed because installing Snowflake SQLAlchemy automatically installs the connector.\n\nData Analytics and Web Application Frameworks (Optional)\n\nSnowflake SQLAlchemy can be used with [Pandas](http://pandas.pydata.org/), [Jupyter](http://jupyter.org/) and [Pyramid](http://www.pylonsproject.org/), which provide higher levels of application frameworks for data analytics and web applications. However, building a working environment from scratch is not a trivial task, particularly for novice users. Installing the frameworks requires C compilers and tools, and choosing the right tools and versions is a hurdle that might deter users from using Python applications.\n\nAn easier way to build an environment is through [Anaconda](https://www.continuum.io/why-anaconda), which provides a complete, precompiled technology stack for all users, including non-Python experts such as data analysts and students. For Anaconda installation instructions, see the [Anaconda install documentation](https://docs.continuum.io/anaconda/install). The Snowflake SQLAlchemy package can then be installed on top of Anaconda using [pip](https://pypi.python.org/pypi/pip).\n\nInstalling Snowflake SQLAlchemy\n\nThe Snowflake SQLAlchemy package can be installed from the public PyPI repository using `pip`:\n\n`pip` automatically installs all required modules, including the Snowflake Connector for Python.\n\nVerifying Your Installation\n\n1. Create a file (e.g. `validate.py`) that contains the following Python sample code,\n   which connects to Snowflake and displays the Snowflake version:\n\n2. Replace ``, ``, and `` with the appropriate values for your Snowflake account and user.\n\n3. Execute the sample code. For example, if you created a file named `validate.py`:\n\nParameters and Behavior\n\nAs much as possible, Snowflake SQLAlchemy provides compatible functionality for SQLAlchemy applications. For information on using SQLAlchemy, see the [SQLAlchemy documentation](http://docs.sqlalchemy.org/en/latest/).\n\nHowever, Snowflake SQLAlchemy also provides Snowflake-specific parameters and behavior, which are described in the following sections.\n\nConnection Parameters\n\nSnowflake SQLAlchemy uses the following syntax for the connection string used to connect to Snowflake and initiate a session:\n\nWhere:\n\n- `` is the login name for your Snowflake user.\n- `` is the password for your Snowflake user.\n- `` is the name of your Snowflake account.\n\nInclude the region in the `` if applicable, more info is available [here](https://docs.snowflake.com/en/user-guide/connecting.html#your-snowflake-account-name).\n\nYou can optionally specify the initial database and schema for the Snowflake session by including them at the end of the connection string, separated by `/`. You can also specify the initial warehouse and role for the session as a parameter string at the end of the connection string:\n\nEscaping Special Characters such as `%, @` signs in Passwords\n\nAs pointed out in [SQLAlchemy](https://docs.sqlalchemy.org/en/14/core/engines.html#escaping-special-characters-such-as-signs-in-passwords), URLs\ncontaining special characters need to be URL encoded to be parsed correctly. This includes the `%, @` signs. Unescaped password containing special\ncharacters could lead to authentication failure.\n\nThe encoding for the password can be generated using `urllib.parse`:\n\n**Note**: `urllib.parse.quote_plus` may also be used if there is no space in the string, as `urllib.parse.quote_plus` will replace space with `+`."}, {"name": "snowflake-sqlalchemy", "tags": ["data", "math", "web"], "summary": "Snowflake SQLAlchemy Dialect", "text": "To create an engine with the proper encodings, either manually constructing the url string by formatting\nor taking advantage of the `snowflake.sqlalchemy.URL` helper method:\n\n**Note**:\nAfter login, the initial database, schema, warehouse and role specified in the connection string can always be changed for the session.\n\nThe following example calls the `create_engine` method with the user name `testuser1`, password `0123456`, account name `abc123`, database `testdb`, schema `public`, warehouse `testwh`, and role `myrole`:\n\nOther parameters, such as `timezone`, can also be specified as a URI parameter or in `connect_args` parameters. For example:\n\nFor convenience, you can use the `snowflake.sqlalchemy.URL` method to construct the connection string and connect to the database. The following example constructs the same connection string from the previous example:\n\nUsing a proxy server\n\nUse the supported environment variables, `HTTPS_PROXY`, `HTTP_PROXY` and `NO_PROXY` to configure a proxy server.\n\nOpening and Closing Connection\n\nOpen a connection by executing `engine.connect()`; avoid using `engine.execute()`. Make certain to close the connection by executing `connection.close()` before\n`engine.dispose()`; otherwise, the Python Garbage collector removes the resources required to communicate with Snowflake, preventing the Python connector from closing the session properly.\n\nAuto-increment Behavior\n\nAuto-incrementing a value requires the `Sequence` object. Include the `Sequence` object in the primary key column to automatically increment the value as each new record is inserted. For example:\n\nObject Name Case Handling\n\nSnowflake stores all case-insensitive object names in uppercase text. In contrast, SQLAlchemy considers all lowercase object names to be case-insensitive. Snowflake SQLAlchemy converts the object name case during schema-level communication, i.e. during table and index reflection. If you use uppercase object names, SQLAlchemy assumes they are case-sensitive and encloses the names with quotes. This behavior will cause mismatches against data dictionary data received from Snowflake, so unless identifier names have been truly created as case sensitive using quotes, e.g., `\"TestDb\"`, all lowercase names should be used on the SQLAlchemy side.\n\nIndex Support\n\nIndexes are supported only for Hybrid Tables in Snowflake SQLAlchemy. For more details on limitations and use cases, refer to the [Create Index documentation](https://docs.snowflake.com/en/sql-reference/constraints-indexes.html). You can create an index using the following methods:\n\nSingle Column Index\n\nYou can create a single column index by setting the `index=True` parameter on the column or by explicitly defining an `Index` object.\n\nMulti-Column Index\n\nFor multi-column indexes, you define the `Index` object specifying the columns that should be indexed.\n\nNumpy Data Type Support\n\nSnowflake SQLAlchemy supports binding and fetching `NumPy` data types. Binding is always supported. To enable fetching `NumPy` data types, add `numpy=True` to the connection parameters.\n\nThe following example shows the round trip of `numpy.datetime64` data:\n\nThe following `NumPy` data types are supported:\n\n- numpy.int64\n- numpy.float64\n- numpy.datatime64\n\nCache Column Metadata\n\nSQLAlchemy provides [the runtime inspection API](http://docs.sqlalchemy.org/en/latest/core/inspection.html) to get the runtime information about the various objects. One of the common use case is get all tables and their column metadata in a schema in order to construct a schema catalog. For example, [alembic](http://alembic.zzzcomputing.com/) on top of SQLAlchemy manages database schema migrations. A pseudo code flow is as follows:"}, {"name": "snowflake-sqlalchemy", "tags": ["data", "math", "web"], "summary": "Snowflake SQLAlchemy Dialect", "text": "In this flow, a potential problem is it may take quite a while as queries run on each table. The results are cached but getting column metadata is expensive.\n\nTo mitigate the problem, Snowflake SQLAlchemy takes a flag `cache_column_metadata=True` such that all of column metadata for all tables are cached when `get_table_names` is called and the rest of `get_columns`, `get_primary_keys` and `get_foreign_keys` can take advantage of the cache.\n\nNote that this flag has been deprecated, as our caching now uses the built-in SQLAlchemy reflection cache, the flag has been removed, but caching has been improved and if possible extra data will be fetched and cached.\n\nVARIANT, ARRAY and OBJECT Support\n\nSnowflake SQLAlchemy supports fetching `VARIANT`, `ARRAY` and `OBJECT` data types. All types are converted into `str` in Python so that you can convert them to native data types using `json.loads`.\n\nThis example shows how to create a table including `VARIANT`, `ARRAY`, and `OBJECT` data type columns.\n\nIn order to retrieve `VARIANT`, `ARRAY`, and `OBJECT` data type columns and convert them to the native Python data types, fetch data and call the `json.loads` method as follows:\n\nStructured Data Types Support\n\nThis module defines custom SQLAlchemy types for Snowflake structured data, specifically for **Iceberg tables**.\nThe types \u2014**MAP**, **OBJECT**, and **ARRAY**\u2014 allow you to store complex data structures in your SQLAlchemy models.\nFor detailed information, refer to the Snowflake [Structured data types](https://docs.snowflake.com/en/sql-reference/data-types-structured) documentation.\n\n---\n\nMAP\n\nThe `MAP` type represents a collection of key-value pairs, where each key and value can have different types.\n\n- **Key Type**: The type of the keys (e.g., `TEXT`, `NUMBER`).\n- **Value Type**: The type of the values (e.g., `TEXT`, `NUMBER`).\n- **Not Null**: Whether `NULL` values are allowed (default is `False`).\n\n*Example Usage*\n\nOBJECT\n\nThe `OBJECT` type represents a semi-structured object with named fields. Each field can have a specific type, and you can also specify whether each field is nullable.\n\n- **Items Types**: A dictionary of field names and their types. The type can optionally include a nullable flag (`True` for not nullable, `False` for nullable, default is `False`).\n\n*Example Usage*\n\nARRAY\n\nThe `ARRAY` type represents an ordered list of values, where each element has the same type. The type of the elements is defined when creating the array.\n\n- **Value Type**: The type of the elements in the array (e.g., `TEXT`, `NUMBER`).\n- **Not Null**: Whether `NULL` values are allowed (default is `False`).\n\n*Example Usage*\n\nCLUSTER BY Support\n\nSnowflake SQLAchemy supports the `CLUSTER BY` parameter for tables. For information about the parameter, see :doc:`/sql-reference/sql/create-table`.\n\nThis example shows how to create a table with two columns, `id` and `name`, as the clustering keys:\n\nAlembic Support\n\n[Alembic](http://alembic.zzzcomputing.com) is a database migration tool on top of `SQLAlchemy`. Snowflake SQLAlchemy works by adding the following code to `alembic/env.py` so that Alembic can recognize Snowflake SQLAlchemy.\n\nSee [Alembic Documentation](http://alembic.zzzcomputing.com/) for general usage.\n\nKey Pair Authentication Support"}, {"name": "snowflake-sqlalchemy", "tags": ["data", "math", "web"], "summary": "Snowflake SQLAlchemy Dialect", "text": "Snowflake SQLAlchemy supports key pair authentication by leveraging its Snowflake Connector for Python underpinnings. See [Using Key Pair Authentication](https://docs.snowflake.net/manuals/user-guide/python-connector-example.html#using-key-pair-authentication) for steps to create the private and public keys.\n\nThe private key parameter is passed through `connect_args` as follows:\n\nWhere `PRIVATE_KEY_PASSPHRASE` is a passphrase to decrypt the private key file, `rsa_key.p8`.\n\nCurrently a private key parameter is not accepted by the `snowflake.sqlalchemy.URL` method.\n\nMerge Command Support\n\nSnowflake SQLAlchemy supports upserting with its `MergeInto` custom expression.\nSee [Merge](https://docs.snowflake.net/manuals/sql-reference/sql/merge.html)  for full documentation.\n\nUse it as follows:\n\nCopyIntoStorage Support\n\nSnowflake SQLAlchemy supports saving tables/query results into different stages, as well as into Azure Containers and\nAWS buckets with its custom `CopyIntoStorage` expression. See [Copy into](https://docs.snowflake.net/manuals/sql-reference/sql/copy-into-location.html)\nfor full documentation.\n\nUse it as follows:\n\nIceberg Table with Snowflake Catalog support\n\nSnowflake SQLAlchemy supports Iceberg Tables with the Snowflake Catalog, along with various related parameters. For detailed information about Iceberg Tables, refer to the Snowflake [CREATE ICEBERG](https://docs.snowflake.com/en/sql-reference/sql/create-iceberg-table-snowflake) documentation.\n\nTo create an Iceberg Table using Snowflake SQLAlchemy, you can define the table using the SQLAlchemy Core syntax as follows:\n\nAlternatively, you can define the table using a declarative approach:\n\nHybrid Table support\n\nSnowflake SQLAlchemy supports Hybrid Tables with indexes. For detailed information, refer to the Snowflake [CREATE HYBRID TABLE](https://docs.snowflake.com/en/sql-reference/sql/create-hybrid-table) documentation.\n\nTo create a Hybrid Table and add an index, you can use the SQLAlchemy Core syntax as follows:\n\nAlternatively, you can define the table using the declarative approach:\n\nDynamic Tables support\n\nSnowflake SQLAlchemy supports Dynamic Tables. For detailed information, refer to the Snowflake [CREATE DYNAMIC TABLE](https://docs.snowflake.com/en/sql-reference/sql/create-dynamic-table) documentation.\n\nTo create a Dynamic Table, you can use the SQLAlchemy Core syntax as follows:\n\nAlternatively, you can define a table without columns using the SQLAlchemy `select()` construct:\n\nNotes\n\n- Defining a primary key in a Dynamic Table is not supported, meaning declarative tables don\u2019t support Dynamic Tables.\n- When using the `as_query` parameter with a string, you must explicitly define the columns. However, if you use the SQLAlchemy `select()` construct, you don\u2019t need to explicitly define the columns.\n- Direct data insertion into Dynamic Tables is not supported.\n\nVerifying Package Signatures\n\nTo ensure the authenticity and integrity of the Python package, follow the steps below to verify the package signature using `cosign`.\n\n**Steps to verify the signature:**\n- Install cosign:\n  - This example is using golang installation: [installing-cosign-with-go](https://edu.chainguard.dev/open-source/sigstore/cosign/how-to-install-cosign/#installing-cosign-with-go)\n- Download the file from the repository like pypi:\n  - https://pypi.org/project/snowflake-sqlalchemy/#files\n- Download the signature files from the release tag, replace the version number with the version you are verifying:\n  - https://github.com/snowflakedb/snowflake-sqlalchemy/releases/tag/v1.7.3\n- Verify signature:\n  `\n\nSupport\n\nFeel free to file an issue or submit a PR here for general cases. For official support, contact Snowflake support at:"}, {"name": "snowflake-sqlalchemy", "tags": ["data", "math", "web"], "summary": "Snowflake SQLAlchemy Dialect", "text": "This library is used to enable seamless interaction between a Snowflake database and SQLAlchemy applications, allowing developers to leverage the power of both technologies in their projects. With snowflake-sqlalchemy, developers can easily integrate Snowflake databases into their data analytics and web applications using SQLAlchemy's ORM capabilities."}, {"name": "snowflake", "tags": ["data", "math", "web"], "summary": "Snowflake Python API", "text": "The Snowflake Python API is the unified Python API across all Snowflake workloads, providing APIs for all Snowflake resources across data engineering, Snowpark, Snowpark ML, and client application workloads.\n\nThe `snowflake` package is the PEP 420 namespace parent package for the Snowflake Python API, including Snowpark.  \nInstalling snowflake automatically installs all subpackages as dependencies.\n\n[Developer guide](https://docs.snowflake.com/developer-guide/snowflake-python-api/snowflake-python-overview)\n\n[API reference](https://docs.snowflake.com/developer-guide/snowflake-python-api/reference/latest/index)"}, {"name": "snowflake", "tags": ["data", "math", "web"], "summary": "Snowflake Python API", "text": "This library is used to unify access to Snowflake resources across various workloads, including data engineering, Snowpark, and client applications. With this library, developers can build Python applications that seamlessly integrate with Snowflake databases using a consistent API interface."}, {"name": "soxr", "tags": ["math"], "summary": "High quality, one-dimensional sample-rate conversion library", "text": "Python-SoXR\n\n(https://github.com/dofuuz/python-soxr) (https://pypi.org/project/soxr/) (https://anaconda.org/conda-forge/soxr-python) (https://repology.org/project/python:soxr/versions) (https://python-soxr.readthedocs.io)\n\nHigh quality, one-dimensional sample-rate conversion library for Python.\n\nKeywords: Resampler, Audio resampling, Samplerate conversion, DSP(Digital Signal Processing)\n\nPython-SoXR is a Python wrapper of [libsoxr](https://sourceforge.net/projects/soxr/).\n\nInstallation\n\nIf installation fails, upgrade pip with `python -m pip install --upgrade pip` and try again.\n\nin Conda environment\n\nNote: Conda packge name is `soxr-python`, not python-soxr.\n\nBasic usage\n\nIf input is not `numpy.ndarray`, it will be converted to `numpy.ndarray(dtype='float32')`.  \ndtype should be one of float32, float64, int16, int32.\n\nOutput is `numpy.ndarray` with same dimension and data type of input.\n\nStreaming usage\n\nUse `ResampleStream` for real-time processing or very long signal.\n\nOutput frame count may not be consistent. This is normal operation.  \n(ex. [0, 0, 0, 186, 186, 166, 186, 186, 168, ...])\n\n [More code examples](https://dofuuz.github.io/dsp/2024/05/26/sample-rate-conversion-in-python.html)\n\nBenchmark\n\nSweep, impulse, speed compairsion with other resamplers for Python.\n\nSpeed comparison summary\n\nDownsampling 10 sec of 48000 Hz to 44100 Hz.  \nRan on Google Colab.\n\nLibrary                  | Time on CPU (ms)\n------------------------ | ----------------\nsoxr (HQ)                | 10.8\ntorchaudio               | 13.8\nsoxr (VHQ)               | 14.5\nscipy.signal.resample    | 21.3\nlilfilter                | 24.7\njulius                   | 31\nresampy (kaiser_fast)    | 108\nsamplerate (sinc_medium) | 223\nresampy (kaiser_best)    | 310\nsamplerate (sinc_best)   | 794\n\nTechnical detail\n\nFor technical details behind resampler, see libsoxr docs.\n\nPython-SoXR package comes with [modified version](https://github.com/dofuuz/soxr) of libsoxr. [See changes here](https://github.com/dofuuz/soxr/compare/0.1.3...master).  \nThese changes do not apply to dynamic-linked builds (e.g. conda-forge build).  \nTo check the version of libsoxr, use `soxr.__libsoxr_version__`.\n\nCredit and License\n\nPython-SoXR is LGPL v2.1+ licensed, following libsoxr's license.\n\nOSS libraries used\n\nlibsoxr (LGPLv2.1+)\nThe SoX Resampler library  \n\nPython-SoXR is a Python wrapper of libsoxr.\n\nPFFFT (BSD-like)\nPFFFT: a pretty fast FFT.  \n\nlibsoxr dependency."}, {"name": "soxr", "tags": ["math"], "summary": "High quality, one-dimensional sample-rate conversion library", "text": "This library is used to perform high-quality, one-dimensional sample-rate conversion and resampling of audio signals. It provides a Python wrapper for libsoxr, allowing developers to easily implement accurate and efficient rate conversion in their projects."}, {"name": "spacy-legacy", "tags": ["math", "ml"], "summary": "Legacy registered functions for spaCy backwards compatibility", "text": "spacy-legacy: Legacy functions and architectures for backwards compatibility\n\nThis package includes outdated registered functions for [spaCy](https://spacy.io) v3.x, for example model architectures, pipeline components and utilities. It's **installed automatically** as a dependency of spaCy, and allows us to provide backwards compatibility, while keeping the core library tidy and up to date. All of this happens under the hood, so you typically shouldn't have to care about this package.\n\n(https://dev.azure.com/explosion-ai/public/_build?definitionId=21)\n(https://pypi.org/project/spacy-legacy/)\n\nHow it works\n\nWhenever a new backwards-incompatible version of a registered function is available, e.g. `spacy.Tok2Vec.v1` &rarr; `spacy.Tok2Vec.v2`, the legacy version is moved to `spacy-legacy`, and exposed via [entry points](setup.cfg). This means that it will still be available if your config files use it, even though the core library only includes the latest version.\n\nLegacy functions are exposed with the prefix `spacy-legacy`, e.g. `spacy-legacy.Tok2Vec.v1`. When spaCy resolves your config and a function is not available in the core library, e.g. `spacy.Tok2Vec.v1`, it will check if there's a legacy function available and fall back to that. You can also explicitly refer to legacy functions in your config, to indicate that a newer version is available."}, {"name": "spacy-legacy", "tags": ["math", "ml"], "summary": "Legacy registered functions for spaCy backwards compatibility", "text": "This library is used to provide backwards compatibility with outdated functions and architectures in spaCy, allowing developers to maintain support for older versions of their models and pipelines. It automatically installs as a dependency of spaCy, ensuring that legacy functionality remains accessible while keeping the core library up to date."}, {"name": "spark-nlp", "tags": ["dev", "math", "ml", "web"], "summary": "John Snow Labs Spark NLP is a natural language processing library built on top of Apache Spark ML. It provides simple, performant & accurate NLP annotations for machine learning pipelines, that scale easily in a distributed environment.", "text": "Spark NLP: State-of-the-Art Natural Language Processing & LLMs Library\n\nSpark NLP is a state-of-the-art Natural Language Processing library built on top of Apache Spark. It provides **simple**, **performant** & **accurate** NLP annotations for machine learning pipelines that **scale** easily in a distributed environment.\n\nSpark NLP comes with **100000+** pretrained **pipelines** and **models** in more than **200+** languages.\nIt also offers tasks such as **Tokenization**, **Word Segmentation**, **Part-of-Speech Tagging**, Word and Sentence **Embeddings**, **Named Entity Recognition**, **Dependency Parsing**, **Spell Checking**, **Text Classification**, **Sentiment Analysis**, **Token Classification**, **Machine Translation** (+180 languages), **Summarization**, **Question Answering**, **Table Question Answering**, **Text Generation**, **Image Classification**, **Image to Text (captioning)**, **Automatic Speech Recognition**, **Zero-Shot Learning**, and many more [NLP tasks](#features).\n\n**Spark NLP** is the only open-source NLP library in **production** that offers state-of-the-art transformers such as **BERT**, **CamemBERT**, **ALBERT**, **ELECTRA**, **XLNet**, **DistilBERT**, **RoBERTa**, **DeBERTa**, **XLM-RoBERTa**, **Longformer**, **ELMO**, **Universal Sentence Encoder**, **Llama-2**, **M2M100**, **BART**, **Instructor**, **E5**, **Google T5**, **MarianMT**, **OpenAI GPT2**, **Vision Transformers (ViT)**, **OpenAI Whisper**, **Llama**, **Mistral**, **Phi**, **Qwen2**, and many more not only to **Python** and **R**, but also to **JVM** ecosystem (**Java**, **Scala**, and **Kotlin**) at **scale** by extending **Apache Spark** natively.\n\nModel Importing Support\n\nSpark NLP provides easy support for importing models from various popular frameworks:\n\n- **TensorFlow**\n- **ONNX**\n- **OpenVINO**\n- **Llama.cpp (GGUF)**\n\nThis wide range of support allows you to seamlessly integrate models from different sources into your Spark NLP workflows, enhancing flexibility and compatibility with existing machine learning ecosystems.\n\nProject's website\n\nTake a look at our official Spark NLP page: [https://sparknlp.org/](https://sparknlp.org/) for user\ndocumentation and examples\n\nFeatures\n\nQuick Start\n\nThis is a quick example of how to use a Spark NLP pre-trained pipeline in Python and PySpark:\n\nIn Python console or Jupyter `Python3` kernel:\n\nFor more examples, you can visit our dedicated [examples](https://github.com/JohnSnowLabs/spark-nlp/tree/master/examples) to showcase all Spark NLP use cases!\n\nPackages Cheatsheet\n\nThis is a cheatsheet for corresponding Spark NLP Maven package to Apache Spark / PySpark major version:\n\nApache Spark\n-------------------------\n3.0/3.1/3.2/3.3/3.4/3.5\nStart Function\n\nNOTE: `M1/M2` and `AArch64` are under `experimental` support. Access and support to these architectures are limited by the\ncommunity and we had to build most of the dependencies by ourselves to make them compatible. We support these two\narchitectures, however, they may not work in some environments.\n\nPipelines and Models\n\nFor a quick example of using pipelines and models take a look at our official [documentation](https://sparknlp.org/docs/en/install#pipelines-and-models)\n\nPlease check out our Models Hub for the full list of [pre-trained models](https://sparknlp.org/models) with examples, demo, benchmark, and more\n\nPlatform and Ecosystem Support\n\nApache Spark Support\n\nSpark NLP *6.3.0* has been built on top of Apache Spark 3.4 while fully supports Apache Spark 3.0.x, 3.1.x, 3.2.x, 3.3.x, 3.4.x, and 3.5.x\n\nSpark NLP\n-----------\n6.x.x and up\n5.5.x\n5.4.x\n5.3.x\n5.2.x\n5.1.x\n5.0.x\n\nFind out more about `Spark NLP` versions from our [release notes](https://github.com/JohnSnowLabs/spark-nlp/releases).\n\nScala and Python Support\n\nSpark NLP\n-----------\n6.0.x\n5.5.x\n5.4.x\n5.3.x\n5.2.x\n5.1.x\n5.0.x\n\nFind out more about 4.x `SparkNLP` versions in our official [documentation](https://sparknlp.org/docs/en/install#apache-spark-support)\n\nDatabricks Support"}, {"name": "spark-nlp", "tags": ["dev", "math", "ml", "web"], "summary": "John Snow Labs Spark NLP is a natural language processing library built on top of Apache Spark ML. It provides simple, performant & accurate NLP annotations for machine learning pipelines, that scale easily in a distributed environment.", "text": "Spark NLP 6.3.0 has been tested and is compatible with the following runtimes:\n\n**CPU**\n--------------------\n14.1 / 14.1 ML\n14.2 / 14.2 ML\n14.3 / 14.3 ML\n15.0 / 15.0 ML\n15.1 / 15.1 ML\n15.2 / 15.2 ML\n15.3 / 15.3 ML\n15.4 / 15.4 ML\n16.4 / 16.4 ML\n\nWe are compatible with older runtimes. For a full list check databricks support in our official [documentation](https://sparknlp.org/docs/en/install#databricks-support)\n\nEMR Support\n\nSpark NLP 6.3.0 has been tested and is compatible with the following EMR releases:\n\n**EMR Release**\n--------------------\nemr-6.13.0\nemr-6.14.0\nemr-6.15.0\nemr-7.0.0\nemr-7.1.0\nemr-7.2.0\nemr-7.3.0\nemr-7.4.0\nemr-7.5.0\nemr-7.6.0\nemr-7.7.0\nemr-7.8.0\n\nWe are compatible with older EMR releases. For a full list check EMR support in our official [documentation](https://sparknlp.org/docs/en/install#emr-support)\n\nFull list of [Amazon EMR 6.x releases](https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-release-6x.html)\nFull list of [Amazon EMR 7.x releases](https://docs.aws.amazon.com/emr/latest/ReleaseGuide/emr-release-7x.html)\n\nNOTE: The EMR 6.1.0 and 6.1.1 are not supported.\n\nInstallation\n\nCommand line (requires internet connection)\n\nTo install spark-nlp packages through command line follow [these instructions](https://sparknlp.org/docs/en/install#command-line) from our official documentation\n\nScala\n\nSpark NLP supports Scala 2.12.15 if you are using Apache Spark 3.0.x, 3.1.x, 3.2.x, 3.3.x, and 3.4.x versions. Our packages are\ndeployed to Maven central. To add any of our packages as a dependency in your application you can follow [these instructions](https://sparknlp.org/docs/en/install#scala-and-java)\nfrom our official documentation.\n\nIf you are interested, there is a simple SBT project for Spark NLP to guide you on how to use it in your\nprojects [Spark NLP Starter](https://github.com/maziyarpanahi/spark-nlp-starter)\n\nPython\n\nSpark NLP supports Python 3.7.x and above depending on your major PySpark version.\nCheck all available installations for Python in our official [documentation](https://sparknlp.org/docs/en/install#python)\n\nCompiled JARs\n\nTo compile the jars from source follow [these instructions](https://sparknlp.org/docs/en/compiled#jars) from our official documentation\n\nPlatform-Specific Instructions\n\nFor detailed instructions on how to use Spark NLP on supported platforms, please refer to our official documentation:\n\nPlatform\n-------------------------\n[Apache Zeppelin](https://sparknlp.org/docs/en/install#apache-zeppelin)\n[Jupyter Notebook](https://sparknlp.org/docs/en/install#jupter-notebook)\n[Google Colab Notebook](https://sparknlp.org/docs/en/install#google-colab-notebook)\n[Kaggle Kernel](https://sparknlp.org/docs/en/install#kaggle-kernel)\n[Databricks Cluster](https://sparknlp.org/docs/en/install#databricks-cluster)\n[EMR Cluster](https://sparknlp.org/docs/en/install#emr-cluster)\n[GCP Dataproc Cluster](https://sparknlp.org/docs/en/install#gcp-dataproc)\n\nOffline\n\nSpark NLP library and all the pre-trained models/pipelines can be used entirely offline with no access to the Internet.\nPlease check [these instructions](https://sparknlp.org/docs/en/install#s3-integration) from our official documentation\nto use Spark NLP offline.\n\nAdvanced Settings\n\nYou can change Spark NLP configurations via Spark properties configuration.\nPlease check [these instructions](https://sparknlp.org/docs/en/install#sparknlp-properties) from our official documentation.\n\nS3 Integration\n\nIn Spark NLP we can define S3 locations to:\n\n- Export log files of training models\n- Store tensorflow graphs used in `NerDLApproach`\n\nPlease check [these instructions](https://sparknlp.org/docs/en/install#s3-integration) from our official documentation.\n\nDocumentation\n\nExamples\n\nNeed more **examples**? Check out our dedicated [Spark NLP Examples](https://github.com/JohnSnowLabs/spark-nlp/tree/master/examples)\nrepository to showcase all Spark NLP use cases!\n\nAlso, don't forget to check [Spark NLP in Action](https://sparknlp.org/demos) built by Streamlit.\n\nAll examples: [spark-nlp/examples](https://github.com/JohnSnowLabs/spark-nlp/tree/master/examples)\n\nFAQ\n\n[Check our Articles and Videos page here](https://sparknlp.org/learn)\n\nCitation\n\nWe have published a [paper](https://www.sciencedirect.com/science/article/pii/S2665963821000063) that you can cite for\nthe Spark NLP library:\n\nCommunity support\n\nand show off how you use Spark NLP!\n- [Medium](https://medium.com/spark-nlp) Spark NLP articles\n- [YouTube](https://www.youtube.com/channel/UCmFOjlpYEhxf_wJUDuz6xxQ/videos) Spark NLP video tutorials\n\nContributing\n\nWe appreciate any sort of contributions:"}, {"name": "spark-nlp", "tags": ["dev", "math", "ml", "web"], "summary": "John Snow Labs Spark NLP is a natural language processing library built on top of Apache Spark ML. It provides simple, performant & accurate NLP annotations for machine learning pipelines, that scale easily in a distributed environment.", "text": "- ideas\n- feedback\n- documentation\n- bug reports\n- NLP training and testing corpora\n- Development and testing\n\nClone the repo and submit your pull-requests! Or directly create issues in this repo.\n\nJohn Snow Labs\n\n[http://johnsnowlabs.com](http://johnsnowlabs.com)"}, {"name": "spark-nlp", "tags": ["dev", "math", "ml", "web"], "summary": "John Snow Labs Spark NLP is a natural language processing library built on top of Apache Spark ML. It provides simple, performant & accurate NLP annotations for machine learning pipelines, that scale easily in a distributed environment.", "text": "This library is used to perform efficient and accurate natural language processing tasks, such as text classification, sentiment analysis, named entity recognition, and machine translation, at scale in a distributed environment. It provides simple access to state-of-the-art models and pipelines for a wide range of NLP tasks across over 200 languages."}, {"name": "spython", "tags": ["math", "web"], "summary": "Command line python tool for working with singularity.", "text": "Singularity Python\n\n(https://travis-ci.org/singularityhub/singularity-cli)\n(https://github.com/singularityhub/singularity-cli/actions?query=branch%3Amaster+workflow%3Aspython-ci)\n\nSingularity Python (spython) is the Python API for working with Singularity containers. See\nthe [documentation](https://singularityhub.github.io/singularity-cli) for installation and usage, and\nthe [install instructions](https://singularityhub.github.io/singularity-cli/install) for a quick start.\n\n**This library does not support Singularity 2.x! It won't work and we no longer support it.**\n\nWe provide a [Singularity](Singularity) recipe for you to use if more convenient, along with the [full modules docstring](https://singularityhub.github.io/singularity-cli/api/source/spython.main.base.html#module-spython.main.base).\n\nAs of version 0.1.0, we only support Singularity > 3.5.2. This is done to encourage using\nnewer versions of Singularity with security fixes. If you want to use an older version of Singularity,\nyou will need to use version 0.0.85 or earlier.\n\n\ufe0f Contributors \ufe0f\n\nWe use the [all-contributors](https://github.com/all-contributors/all-contributors)\ntool to generate a contributors graphic below.\n\nLicense\n\nThis code is licensed under the MPL 2.0 [LICENSE](LICENSE).\n\nHelp and Contribution\n\nPlease contribute to the package, or post feedback and questions as issues. For points that require discussion of the larger group, please use the Singularity List"}, {"name": "spython", "tags": ["math", "web"], "summary": "Command line python tool for working with singularity.", "text": "This library is used to interact with Singularity containers from Python scripts through a command-line interface. With this library, developers can work with Singularity containers programmatically and automate tasks that would otherwise require manual interaction."}, {"name": "sqlalchemy", "tags": ["data", "web"], "summary": "Database Abstraction Library", "text": "SQLAlchemy\n==========\n\nPyPI\n\n.. |PyPI| image:: https://img.shields.io/pypi/v/sqlalchemy\n\n.. |Python| image:: https://img.shields.io/pypi/pyversions/sqlalchemy\n\nThe Python SQL Toolkit and Object Relational Mapper\n\nIntroduction\n-------------\n\nSQLAlchemy is the Python SQL toolkit and Object Relational Mapper\nthat gives application developers the full power and\nflexibility of SQL. SQLAlchemy provides a full suite\nof well known enterprise-level persistence patterns,\ndesigned for efficient and high-performing database\naccess, adapted into a simple and Pythonic domain\nlanguage.\n\nMajor SQLAlchemy features include:\n\n* An industrial strength ORM, built\n  from the core on the identity map, unit of work,\n  and data mapper patterns.   These patterns\n  allow transparent persistence of objects\n  using a declarative configuration system.\n  Domain models\n  can be constructed and manipulated naturally,\n  and changes are synchronized with the\n  current transaction automatically.\n* A relationally-oriented query system, exposing\n  the full range of SQL's capabilities\n  explicitly, including joins, subqueries,\n  correlation, and most everything else,\n  in terms of the object model.\n  Writing queries with the ORM uses the same\n  techniques of relational composition you use\n  when writing SQL.  While you can drop into\n  literal SQL at any time, it's virtually never\n  needed.\n* A comprehensive and flexible system\n  of eager loading for related collections and objects.\n  Collections are cached within a session,\n  and can be loaded on individual access, all\n  at once using joins, or by query per collection\n  across the full result set.\n* A Core SQL construction system and DBAPI\n  interaction layer.  The SQLAlchemy Core is\n  separate from the ORM and is a full database\n  abstraction layer in its own right, and includes\n  an extensible Python-based SQL expression\n  language, schema metadata, connection pooling,\n  type coercion, and custom types.\n* All primary and foreign key constraints are\n  assumed to be composite and natural.  Surrogate\n  integer primary keys are of course still the\n  norm, but SQLAlchemy never assumes or hardcodes\n  to this model.\n* Database introspection and generation.  Database\n  schemas can be \"reflected\" in one step into\n  Python structures representing database metadata;\n  those same structures can then generate\n  CREATE statements right back out - all within\n  the Core, independent of the ORM.\n\nSQLAlchemy's philosophy:\n\n* SQL databases behave less and less like object\n  collections the more size and performance start to\n  matter; object collections behave less and less like\n  tables and rows the more abstraction starts to matter.\n  SQLAlchemy aims to accommodate both of these\n  principles.\n* An ORM doesn't need to hide the \"R\".   A relational\n  database provides rich, set-based functionality\n  that should be fully exposed.   SQLAlchemy's\n  ORM provides an open-ended set of patterns\n  that allow a developer to construct a custom\n  mediation layer between a domain model and\n  a relational schema, turning the so-called\n  \"object relational impedance\" issue into\n  a distant memory.\n* The developer, in all cases, makes all decisions\n  regarding the design, structure, and naming conventions\n  of both the object model as well as the relational\n  schema.   SQLAlchemy only provides the means\n  to automate the execution of these decisions.\n* With SQLAlchemy, there's no such thing as\n  \"the ORM generated a bad query\" - you\n  retain full control over the structure of\n  queries, including how joins are organized,\n  how subqueries and correlation is used, what\n  columns are requested.  Everything SQLAlchemy\n  does is ultimately the result of a developer-initiated \n  decision.\n* Don't use an ORM if the problem doesn't need one.\n  SQLAlchemy consists of a Core and separate ORM\n  component.   The Core offers a full SQL expression\n  language that allows Pythonic construction\n  of SQL constructs that render directly to SQL\n  strings for a target database, returning\n  result sets that are essentially enhanced DBAPI\n  cursors.\n* Transactions should be the norm.  With SQLAlchemy's\n  ORM, nothing goes to permanent storage until\n  commit() is called.  SQLAlchemy encourages applications\n  to create a consistent means of delineating\n  the start and end of a series of operations.\n* Never render a literal value in a SQL statement.\n  Bound parameters are used to the greatest degree\n  possible, allowing query optimizers to cache\n  query plans effectively and making SQL injection\n  attacks a non-issue.\n\nDocumentation\n-------------\n\nLatest documentation is at:\n\nInstallation / Requirements\n---------------------------\n\nFull documentation for installation is at\n`Installation `_.\n\nGetting Help / Development / Bug reporting\n------------------------------------------\n\nPlease refer to the `SQLAlchemy Community Guide `_.\n\nCode of Conduct\n---------------\n\nAbove all, SQLAlchemy places great emphasis on polite, thoughtful, and\nconstructive communication between users and developers.\nPlease see our current Code of Conduct at\n`Code of Conduct `_.\n\nLicense\n-------\n\nSQLAlchemy is distributed under the `MIT license\n`_."}, {"name": "sqlalchemy", "tags": ["data", "web"], "summary": "Database Abstraction Library", "text": "This library is used to simplify database interactions in Python applications by providing an Object Relational Mapper (ORM) that allows developers to persist objects using SQL queries. With SQLAlchemy, developers can efficiently interact with databases and implement enterprise-level persistence patterns."}, {"name": "srsly", "tags": ["data", "math", "web"], "summary": "Modern high-performance serialization utilities for Python", "text": "srsly: Modern high-performance serialization utilities for Python\n\nThis package bundles some of the best Python serialization libraries into one\nstandalone package, with a high-level API that makes it easy to write code\nthat's correct across platforms and Pythons. This allows us to provide all the\nserialization utilities we need in a single binary wheel. Currently supports\n**JSON**, **JSONL**, **MessagePack**, **Pickle** and **YAML**.\n\n(https://github.com/explosion/srsly/actions/workflows/tests.yml)\n(https://pypi.python.org/pypi/srsly)\n(https://anaconda.org/conda-forge/srsly)\n(https://github.com/explosion/srsly)\n(https://github.com/explosion/wheelwright/releases)\n\nMotivation\n\nSerialization is hard, especially across Python versions and multiple platforms.\nAfter dealing with many subtle bugs over the years (encodings, locales, large\nfiles) our libraries like [spaCy](https://github.com/explosion/spaCy) and\n[Prodigy](https://prodi.gy) had steadily grown a number of utility functions to\nwrap the multiple serialization formats we need to support (especially `json`,\n`msgpack` and `pickle`). These wrapping functions ended up duplicated across our\ncodebases, so we wanted to put them in one place.\n\nAt the same time, we noticed that having a lot of small dependencies was making\nmaintenance harder, and making installation slower. To solve this, we've made\n`srsly` standalone, by including the component packages directly within it. This\nway we can provide all the serialization utilities we need in a single binary\nwheel.\n\n`srsly` currently includes forks of the following packages:\n\n  implementations!)\n\nInstallation\n\n> \u26a0\ufe0f Note that `v2.x` is only compatible with **Python 3.6+**. For 2.7+\n> compatibility, use `v1.x`.\n\n`srsly` can be installed from pip. Before installing, make sure that your `pip`,\n`setuptools` and `wheel` are up to date.\n\nOr from conda via conda-forge:\n\nAlternatively, you can also compile the library from source. You'll need to make\nsure that you have a development environment with a Python distribution\nincluding header files, a compiler (XCode command-line tools on macOS / OS X or\nVisual C++ build tools on Windows), pip and git installed.\n\nInstall from source:\n\nFor developers, install requirements separately and then install in editable\nmode without build isolation:\n\nAPI\n\nJSON\n\n>  The underlying module is exposed via `srsly.ujson`. However, we normally\n> interact with it via the utility functions only.\n\nfunction `srsly.json_dumps`\n\nSerialize an object to a JSON string. Falls back to `json` if `sort_keys=True`\nis used (until it's fixed in `ujson`).\n\nArgument\n-----------\n`data`\n`indent`\n`sort_keys`\n**RETURNS**\n\nfunction `srsly.json_loads`\n\nDeserialize unicode or bytes to a Python object.\n\nArgument\n-----------\n`data`\n**RETURNS**\n\nfunction `srsly.write_json`\n\nCreate a JSON file and dump contents or write to standard output.\n\nArgument\n--------\n`path`\n`data`\n`indent`\n\nfunction `srsly.read_json`\n\nLoad JSON from a file or standard input.\n\nArgument\n-----------\n`path`\n**RETURNS**\n\nfunction `srsly.write_gzip_json`\n\nCreate a gzipped JSON file and dump contents.\n\nArgument\n--------\n`path`\n`data`\n`indent`\n\nfunction `srsly.write_gzip_jsonl`\n\nCreate a gzipped JSONL file and dump contents.\n\nArgument\n-----------------\n`path`\n`lines`\n`append`\n`append_new_line`\n\nfunction `srsly.read_gzip_json`\n\nLoad gzipped JSON from a file.\n\nArgument\n-----------\n`path`\n**RETURNS**\n\nfunction `srsly.read_gzip_jsonl`\n\nLoad gzipped JSONL from a file.\n\nArgument\n-----------\n`path`\n**RETURNS**\n\nfunction `srsly.write_jsonl`\n\nCreate a JSONL file (newline-delimited JSON) and dump contents line by line, or\nwrite to standard output.\n\nArgument\n-----------------\n`path`\n`lines`\n`append`\n`append_new_line`\n\nfunction `srsly.read_jsonl`\n\nRead a JSONL file (newline-delimited JSON) or from JSONL data from standard\ninput and yield contents line by line. Blank lines will always be skipped.\n\nArgument\n----------\n`path`\n`skip`\n**YIELDS**\n\nfunction `srsly.is_json_serializable`\n\nCheck if a Python object is JSON-serializable.\n\nArgument\n-----------\n`obj`\n**RETURNS**\n\nmsgpack\n\n>  The underlying module is exposed via `srsly.msgpack`. However, we normally\n> interact with it via the utility functions only.\n\nfunction `srsly.msgpack_dumps`\n\nSerialize an object to a msgpack byte string.\n\nArgument\n-----------\n`data`\n**RETURNS**\n\nfunction `srsly.msgpack_loads`\n\nDeserialize msgpack bytes to a Python object.\n\nArgument\n-----------\n`data`\n`use_list`\n**RETURNS**\n\nfunction `srsly.write_msgpack`\n\nCreate a msgpack file and dump contents.\n\nArgument\n--------\n`path`\n`data`\n\nfunction `srsly.read_msgpack`\n\nLoad a msgpack file.\n\nArgument\n-----------\n`path`\n`use_list`\n**RETURNS**\n\npickle\n\n>  The underlying module is exposed via `srsly.cloudpickle`. However, we\n> normally interact with it via the utility functions only.\n\nfunction `srsly.pickle_dumps`\n\nSerialize a Python object with pickle.\n\nArgument\n-----------\n`data`\n`protocol`\n**RETURNS**\n\nfunction `srsly.pickle_loads`\n\nDeserialize bytes with pickle.\n\nArgument\n-----------\n`data`\n**RETURNS**\n\nYAML\n\n>  The underlying module is exposed via `srsly.ruamel_yaml`. However, we\n> normally interact with it via the utility functions only.\n\nfunction `srsly.yaml_dumps`\n\nSerialize an object to a YAML string. See the\n[`ruamel.yaml` docs](https://yaml.readthedocs.io/en/latest/detail.html?highlight=indentation#indentation-of-block-sequences)\nfor details on the indentation format.\n\nArgument\n-----------------\n`data`\n`indent_mapping`\n`indent_sequence`\n`indent_offset`\n`sort_keys`\n**RETURNS**\n\nfunction `srsly.yaml_loads`\n\nDeserialize unicode or a file object to a Python object.\n\nArgument\n-----------\n`data`\n**RETURNS**\n\nfunction `srsly.write_yaml`\n\nCreate a YAML file and dump contents or write to standard output.\n\nArgument\n-----------------\n`path`\n`data`\n`indent_mapping`\n`indent_sequence`\n`indent_offset`\n`sort_keys`\n\nfunction `srsly.read_yaml`\n\nLoad YAML from a file or standard input.\n\nArgument\n-----------\n`path`\n**RETURNS**\n\nfunction `srsly.is_yaml_serializable`\n\nCheck if a Python object is YAML-serializable.\n\nArgument\n-----------\n`obj`\n**RETURNS**"}, {"name": "srsly", "tags": ["data", "math", "web"], "summary": "Modern high-performance serialization utilities for Python", "text": "This library is used to efficiently serialize data in various formats, such as JSON, MessagePack, and YAML, ensuring compatibility across different Python versions and platforms. Developers can use it to easily write platform-agnostic code that handles serialization tasks correctly and with high performance."}, {"name": "stanza", "tags": ["math", "ml", "ui", "web"], "summary": "A Python NLP Library for Many Human Languages, by the Stanford NLP Group", "text": "References\n\nIf you use this library in your research, please kindly cite our [ACL2020 Stanza system demo paper](https://arxiv.org/abs/2003.07082):\n\nIf you use our biomedical and clinical models, please also cite our [Stanza Biomedical Models description paper](https://arxiv.org/abs/2007.14640):\n\nThe PyTorch implementation of the neural pipeline in this repository is due to [Peng Qi](http://qipeng.me) (@qipeng), [Yuhao Zhang](http://yuhao.im) (@yuhaozhang), and [Yuhui Zhang](https://cs.stanford.edu/~yuhuiz/) (@yuhui-zh15), with help from [Jason Bolton](mailto:jebolton@stanford.edu) (@j38), [Tim Dozat](https://web.stanford.edu/~tdozat/) (@tdozat) and [John Bauer](https://www.linkedin.com/in/john-bauer-b3883b60/) (@AngledLuffa). Maintenance of this repo is currently led by [John Bauer](https://www.linkedin.com/in/john-bauer-b3883b60/).\n\nIf you use the CoreNLP software through Stanza, please cite the CoreNLP software package and the respective modules as described [here](https://stanfordnlp.github.io/CoreNLP/#citing-stanford-corenlp-in-papers) (\"Citing Stanford CoreNLP in papers\"). The CoreNLP client is mostly written by [Arun Chaganty](http://arun.chagantys.org/), and [Jason Bolton](mailto:jebolton@stanford.edu) spearheaded merging the two projects together.\n\nIf you use the Semgrex or Ssurgeon part of CoreNLP, please cite [our GURT paper on Semgrex and Ssurgeon](https://aclanthology.org/2023.tlt-1.7/):\n\nIssues and Usage Q&A\n\nTo ask questions, report issues or request features , please use the [GitHub Issue Tracker](https://github.com/stanfordnlp/stanza/issues). Before creating a new issue, please make sure to search for existing issues that may solve your problem, or visit the [Frequently Asked Questions (FAQ) page](https://stanfordnlp.github.io/stanza/faq.html) on our website.\n\nContributing to Stanza\n\nWe welcome community contributions to Stanza in the form of bugfixes \ufe0f and enhancements ! If you want to contribute, please first read [our contribution guideline](CONTRIBUTING.md).\n\nInstallation\n\npip\n\nStanza supports Python 3.6 or later. We recommend that you install Stanza via [pip](https://pip.pypa.io/en/stable/installing/), the Python package manager. To install, simply run:\n\nThis should also help resolve all of the dependencies of Stanza, for instance [PyTorch](https://pytorch.org/) 1.3.0 or above.\n\nIf you currently have a previous version of `stanza` installed, use:\n\nAnaconda\n\nTo install Stanza via Anaconda, use the following conda command:\n\nNote that for now installing Stanza via Anaconda does not work for Python 3.10. For Python 3.10 please use pip installation.\n\nFrom Source\n\nAlternatively, you can also install from source of this git repository, which will give you more flexibility in developing on top of Stanza. For this option, run\n\nRunning Stanza\n\nGetting Started with the neural pipeline\n\nTo run your first Stanza pipeline, simply follow these steps in your Python interactive interpreter:\n\nIf you encounter `requests.exceptions.ConnectionError`, please try to use a proxy:\n\nThe last command will print out the words in the first sentence in the input string (or [`Document`](https://stanfordnlp.github.io/stanza/data_objects.html#document), as it is represented in Stanza), as well as the indices for the word that governs it in the Universal Dependencies parse of that sentence (its \"head\"), along with the dependency relation between the words. The output should look like:\n\nSee [our getting started guide](https://stanfordnlp.github.io/stanza/installation_usage.html#getting-started) for more details.\n\nAccessing Java Stanford CoreNLP software\n\nAside from the neural pipeline, this package also includes an official wrapper for accessing the Java Stanford CoreNLP software with Python code.\n\nThere are a few initial setup steps."}, {"name": "stanza", "tags": ["math", "ml", "ui", "web"], "summary": "A Python NLP Library for Many Human Languages, by the Stanford NLP Group", "text": "* Download [Stanford CoreNLP](https://stanfordnlp.github.io/CoreNLP/) and models for the language you wish to use\n* Put the model jars in the distribution folder\n* Tell the Python code where Stanford CoreNLP is located by setting the `CORENLP_HOME` environment variable (e.g., in *nix): `export CORENLP_HOME=/path/to/stanford-corenlp-4.5.3`\n\nWe provide [comprehensive examples](https://stanfordnlp.github.io/stanza/corenlp_client.html) in our documentation that show how one can use CoreNLP through Stanza and extract various annotations from it.\n\nOnline Colab Notebooks\n\nTo get your started, we also provide interactive Jupyter notebooks in the `demo` folder. You can also open these notebooks and run them interactively on [Google Colab](https://colab.research.google.com). To view all available notebooks, follow these steps:\n\n* Go to the [Google Colab website](https://colab.research.google.com)\n* Navigate to `File` -> `Open notebook`, and choose `GitHub` in the pop-up menu\n* Note that you do **not** need to give Colab access permission to your GitHub account\n* Type `stanfordnlp/stanza` in the search bar, and click enter\n\nTrained Models for the Neural Pipeline\n\nWe currently provide models for all of the [Universal Dependencies](https://universaldependencies.org/) treebanks v2.8, as well as NER models for a few widely-spoken languages. You can find instructions for downloading and using these models [here](https://stanfordnlp.github.io/stanza/models.html).\n\nBatching To Maximize Pipeline Speed\n\nTo maximize speed performance, it is essential to run the pipeline on batches of documents. Running a for loop on one sentence at a time will be very slow. The best approach at this time is to concatenate documents together, with each document separated by a blank line (i.e., two line breaks `\\n\\n`).  The tokenizer will recognize blank lines as sentence breaks. We are actively working on improving multi-document processing.\n\nTraining your own neural pipelines\n\nAll neural modules in this library can be trained with your own data. The tokenizer, the multi-word token (MWT) expander, the POS/morphological features tagger, the lemmatizer and the dependency parser require [CoNLL-U](https://universaldependencies.org/format.html) formatted data, while the NER model requires the BIOES format. Currently, we do not support model training via the `Pipeline` interface. Therefore, to train your own models, you need to clone this git repository and run training from the source.\n\nFor detailed step-by-step guidance on how to train and evaluate your own models, please visit our [training documentation](https://stanfordnlp.github.io/stanza/training.html).\n\nLICENSE\n\nStanza is released under the Apache License, Version 2.0. See the [LICENSE](https://github.com/stanfordnlp/stanza/blob/master/LICENSE) file for more details."}, {"name": "stanza", "tags": ["math", "ml", "ui", "web"], "summary": "A Python NLP Library for Many Human Languages, by the Stanford NLP Group", "text": "This library is used to perform NLP tasks in many human languages, enabling developers to analyze and process text data with high accuracy. By leveraging the Stanza library, developers can build applications that understand and generate text in multiple languages, facilitating global communication and content creation."}, {"name": "statsmodels", "tags": ["data", "dev", "math", "web"], "summary": "Statistical computations and models for Python", "text": "About statsmodels: statsmodels is a Python package that provides a complement to scipy for\nstatistical computations including descriptive statistics and estimation\nand inference for statistical models. Main Features Linear regression models:\n\n  - Ordinary least squares\n  - Generalized least squares\n  - Weighted least squares\n  - Least squares with autoregressive errors\n  - Quantile regression\n  - Recursive least squares\n\n* Mixed Linear Model with mixed effects and variance components\n* GLM: Generalized linear models with support for all of the one-parameter\n  exponential family distributions\n* Bayesian Mixed GLM for Binomial and Poisson\n* GEE: Generalized Estimating Equations for one-way clustered or longitudinal data\n* Discrete models:\n\n  - Logit and Probit\n  - Multinomial logit (MNLogit)\n  - Poisson and Generalized Poisson regression\n  - Negative Binomial regression\n  - Zero-Inflated Count models\n\n* RLM: Robust linear models with support for several M-estimators.\n* Time Series Analysis: models for time series analysis\n\n  - Complete StateSpace modeling framework\n\n  - Markov switching models (MSAR), also known as Hidden Markov Models (HMM)\n  - Univariate time series analysis: AR, ARIMA\n  - Vector autoregressive models, VAR and structural VAR\n  - Vector error correction model, VECM\n  - exponential smoothing, Holt-Winters\n  - Hypothesis tests for time series: unit root, cointegration and others\n  - Descriptive statistics and process models for time series analysis\n\n* Survival analysis:\n\n  - Proportional hazards regression (Cox models)\n  - Survivor function estimation (Kaplan-Meier)\n  - Cumulative incidence function estimation\n\n* Multivariate:\n\n  - Principal Component Analysis with missing data\n  - Factor Analysis with rotation\n  - MANOVA\n  - Canonical Correlation\n\n* Nonparametric statistics: Univariate and multivariate kernel density estimators\n* Datasets: Datasets used for examples and in testing\n* Statistics: a wide range of statistical tests\n\n  - diagnostics and specification tests\n  - goodness-of-fit and normality tests\n  - functions for multiple testing\n  - various additional statistical tests\n\n* Imputation with MICE, regression on order statistic and Gaussian imputation\n* Mediation analysis\n* Graphics includes plot functions for visual analysis of data and model results\n\n* I/O\n\n  - Tools for reading Stata .dta files, but pandas has a more recent version\n  - Table output to ascii, latex, and html\n\n* Miscellaneous models\n* Sandbox: statsmodels contains a sandbox folder with code in various stages of\n  development and testing which is not considered \"production ready\".  This covers\n  among others\n\n  - Generalized method of moments (GMM) estimators\n  - Kernel regression\n  - Various extensions to scipy.stats.distributions\n  - Panel data models\n  - Information theoretic measures\n\nHow to get it\n=============\nThe main branch on GitHub is the most up to date code\n\nSource download of release tags are available on GitHub\n\nBinaries and source distributions are available from PyPi\n\nBinaries can be installed in Anaconda\n\nGetting the latest code\n=======================\n\nInstalling the most recent nightly wheel\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\nThe most recent nightly wheel can be installed using pip.\n\n.. code:: bash\n\n   python -m pip install -i https://pypi.anaconda.org/scientific-python-nightly-wheels/simple statsmodels --upgrade --use-deprecated=legacy-resolver\n\nInstalling from sources\n~~~~~~~~~~~~~~~~~~~~~~~\n\nSee INSTALL.txt for requirements or see the documentation\n\nContributing\n============\nContributions in any form are welcome, including:\n\n* Documentation improvements\n* Additional tests\n* New features to existing models\n* New models\n\nfor instructions on installing statsmodels in *editable* mode.\n\nLicense\n=======\n\nModified BSD (3-clause)\n\nDiscussion and Development\n==========================\n\nDiscussions take place on the mailing list\n\nand in the issue tracker. We are very interested in feedback\nabout usability and suggestions for improvements.\n\nBug Reports\n===========\n\nBug reports can be submitted to the issue tracker at\n\n.. |Azure CI Build Status| image:: https://dev.azure.com/statsmodels/statsmodels-testing/_apis/build/status/statsmodels.statsmodels?branchName=main\n   :target: https://dev.azure.com/statsmodels/statsmodels-testing/_build/latest?definitionId=1&branchName=main\n\n   :target: https://codecov.io/gh/statsmodels/statsmodels\n\n   :target: https://coveralls.io/github/statsmodels/statsmodels?branch=main\n.. |PyPI downloads| image:: https://img.shields.io/pypi/dm/statsmodels?label=PyPI%20Downloads\n   :alt: PyPI - Downloads\n   :target: https://pypi.org/project/statsmodels/\n.. |Conda downloads| image:: https://img.shields.io/conda/dn/conda-forge/statsmodels.svg?label=Conda%20downloads\n   :target: https://anaconda.org/conda-forge/statsmodels/\n.. |PyPI Version| image:: https://img.shields.io/pypi/v/statsmodels.svg\n   :target: https://pypi.org/project/statsmodels/\n\n   :target: https://anaconda.org/conda-forge/statsmodels/\n.. |License| image:: https://img.shields.io/pypi/l/statsmodels.svg\n   :target: https://github.com/statsmodels/statsmodels/blob/main/LICENSE.txt"}, {"name": "statsmodels", "tags": ["data", "dev", "math", "web"], "summary": "Statistical computations and models for Python", "text": "This library is used to perform various statistical computations, estimation, and inference tasks using a range of statistical models, including linear regression, generalized linear models, and mixed effects models. With statsmodels, developers can build and analyze complex statistical models with ease, making it an essential tool for data analysis and machine learning applications."}, {"name": "strands-agents", "tags": ["math", "ml", "web"], "summary": "A model-driven approach to building AI agents in just a few lines of code", "text": "Feature Overview\n\n- **Lightweight & Flexible**: Simple agent loop that just works and is fully customizable\n- **Model Agnostic**: Support for Amazon Bedrock, Anthropic, Gemini, LiteLLM, Llama, Ollama, OpenAI, Writer, and custom providers\n- **Advanced Capabilities**: Multi-agent systems, autonomous agents, and streaming support\n- **Built-in MCP**: Native support for Model Context Protocol (MCP) servers, enabling access to thousands of pre-built tools\n\nQuick Start\n\n> **Note**: For the default Amazon Bedrock model provider, you'll need AWS credentials configured and model access enabled for Claude 4 Sonnet in the us-west-2 region. See the [Quickstart Guide](https://strandsagents.com/) for details on configuring other model providers.\n\nInstallation\n\nEnsure you have Python 3.10+ installed, then:\n\nFeatures at a Glance\n\nPython-Based Tools\n\nEasily build tools using Python decorators:\n\n**Hot Reloading from Directory:**\nEnable automatic tool loading and reloading from the `./tools/` directory:\n\nMCP Support\n\nSeamlessly integrate Model Context Protocol (MCP) servers:\n\nMultiple Model Providers\n\nSupport for various model providers:\n\nBuilt-in providers:\n\nCustom providers can be implemented using [Custom Providers](https://strandsagents.com/latest/user-guide/concepts/model-providers/custom_model_provider/)\n\nExample tools\n\nStrands offers an optional strands-agents-tools package with pre-built tools for quick experimentation:\n\nIt's also available on GitHub via [strands-agents/tools](https://github.com/strands-agents/tools).\n\nBidirectional Streaming\n\n> **\u26a0\ufe0f Experimental Feature**: Bidirectional streaming is currently in experimental status. APIs may change in future releases as we refine the feature based on user feedback and evolving model capabilities.\n\nBuild real-time voice and audio conversations with persistent streaming connections. Unlike traditional request-response patterns, bidirectional streaming maintains long-running conversations where users can interrupt, provide continuous input, and receive real-time audio responses. Get started with your first BidiAgent by following the [Quickstart](https://strandsagents.com/latest/documentation/docs/user-guide/concepts/experimental/bidirectional-streaming/quickstart) guide. \n\n**Supported Model Providers:**\n- Amazon Nova Sonic (`amazon.nova-sonic-v1:0`)\n- Google Gemini Live (`gemini-2.5-flash-native-audio-preview-09-2025`)\n- OpenAI Realtime API (`gpt-realtime`)\n\n**Quick Example:**\n\n**Configuration Options:**\n\nDocumentation\n\nFor detailed guidance & examples, explore our documentation:\n\nContributing \ufe0f\n\nWe welcome contributions! See our [Contributing Guide](CONTRIBUTING.md) for details on:\n- Reporting bugs & features\n- Development setup\n- Contributing via Pull Requests\n- Code of Conduct\n- Reporting of security issues\n\nLicense\n\nThis project is licensed under the Apache License 2.0 - see the [LICENSE](LICENSE) file for details.\n\nSecurity\n\nSee [CONTRIBUTING](CONTRIBUTING.md#security-issue-notifications) for more information."}, {"name": "strands-agents", "tags": ["math", "ml", "web"], "summary": "A model-driven approach to building AI agents in just a few lines of code", "text": "This library is used to build AI agents with a model-driven approach, enabling developers to create and manage intelligent systems quickly and efficiently in just a few lines of code. It supports various model providers, including Amazon Bedrock, OpenAI, and custom ones, allowing for seamless integration into existing workflows."}, {"name": "streamlit", "tags": ["data", "math", "visualization", "web"], "summary": "A faster way to build and share data apps", "text": "Welcome to Streamlit \n\n**A faster way to build and share data apps.**\n\nWhat is Streamlit?\n\nStreamlit lets you transform Python scripts into interactive web apps in minutes, instead of weeks. Build dashboards, generate reports, or create chat apps. Once you\u2019ve created an app, you can use our [Community Cloud platform](https://streamlit.io/cloud) to deploy, manage, and share your app.\n\nWhy choose Streamlit?\n\n- **Simple and Pythonic:** Write beautiful, easy-to-read code.\n- **Fast, interactive prototyping:** Let others interact with your data and provide feedback quickly.\n- **Live editing:** See your app update instantly as you edit your script.\n- **Open-source and free:** Join a vibrant community and contribute to Streamlit's future.\n\nInstallation\n\nOpen a terminal and run:\n\nIf this opens our sweet _Streamlit Hello_ app in your browser, you're all set! If not, head over to [our docs](https://docs.streamlit.io/get-started) for specific installs.\n\nThe app features a bunch of examples of what you can do with Streamlit. Jump to the [quickstart](#quickstart) section to understand how that all works.\n\nQuickstart\n\nA little example\n\nCreate a new file named `streamlit_app.py` in your project directory with the following code:\n\nNow run it to open the app!\n\nGive me more!\n\nStreamlit comes in with [a ton of additional powerful elements](https://docs.streamlit.io/develop/api-reference) to spice up your data apps and delight your viewers. Some examples:\n\n  \n  \n  \n  \n\nOur vibrant creators community also extends Streamlit capabilities using \u00a0 [Streamlit Components](https://streamlit.io/components).\n\nGet inspired\n\nThere's so much you can build with Streamlit:\n- and more!\n\n**Check out [our gallery!](https://streamlit.io/gallery)**\n\nCommunity Cloud\n\nDeploy, manage and share your apps for free using our [Community Cloud](https://streamlit.io/cloud)! Sign-up [here](https://share.streamlit.io/signup).\n\nResources\n\n(https://share.streamlit.io/streamlit/roadmap)\n\nContribute\n\n Thanks for your interest in helping improve Streamlit! \n\nBefore contributing, please read our guidelines here: https://github.com/streamlit/streamlit/wiki/Contributing\n\nLicense\n\nStreamlit is completely free and open-source and licensed under the [Apache 2.0](https://www.apache.org/licenses/LICENSE-2.0) license."}, {"name": "streamlit", "tags": ["data", "math", "visualization", "web"], "summary": "A faster way to build and share data apps", "text": "This library is used to transform Python scripts into interactive web apps in minutes, enabling developers to build dashboards, generate reports, or create chat apps. With Streamlit, you can rapidly prototype and share data-driven applications with others through its Community Cloud platform."}, {"name": "stringzilla", "tags": ["data", "dev", "math", "ui", "web"], "summary": "Search, hash, sort, and process strings faster via SWAR and SIMD", "text": "StringZilla\n\nStrings are the first fundamental data type every programming language implements in software rather than hardware, so dedicated CPU instructions are rare - and the few that exist are hardly ideal.\nThat's why most languages lean on the C standard library (libc) for their string operations, which, despite its name, ships its hottest code in hand-tuned assembly.\nIt does exploit SIMD, but it isn't perfect.\n1\ufe0f\u20e3 Even on ubiquitous hardware - over a billion 64-bit ARM CPUs - routines such as `strstr` and `memmem` top out at roughly one-third of available throughput.\n2\ufe0f\u20e3 SIMD coverage is uneven: fast forward scans don't guarantee speedy reverse searches, hashing and case-mapping is not even part of the standard.\n3\ufe0f\u20e3 Many higher-level languages can't rely on libc at all because their strings aren't NUL-terminated - or may even contain embedded zeroes.\nThat's why StringZilla exists: predictable, high performance on every modern platform, OS, and programming language.\n\n(https://github.com/ashvardanian/stringzilla)\n(https://crates.io/crates/stringzilla)\n\n(https://github.com/ashvardanian/StringZilla/actions/workflows/release.yml)\n(https://github.com/ashvardanian/StringZilla/actions/workflows/release.yml)\n(https://github.com/ashvardanian/StringZilla/actions/workflows/release.yml)\n-->\n\nStringZilla is the GodZilla of string libraries, using [SIMD][faq-simd] and [SWAR][faq-swar] to accelerate binary and UTF-8 string operations on modern CPUs and GPUs.\nIt delivers up to __10x higher CPU throughput in C, C++, Rust, Python__, and other languages, and can be __100x faster than existing GPU kernels__, covering a broad range of functionality.\nIt __accelerates exact and fuzzy string matching, hashing, edit distance computations, sorting, provides allocation-free lazily-evaluated smart-iterators, and even random-string generators__.\n\n-  __[C](#basic-usage-with-c-99-and-newer):__ Upgrade LibC's `` to ``  in C 99\n-  __[C++](#basic-usage-with-c-11-and-newer):__ Upgrade STL's `` to `` in C++ 11\n-  __[CUDA](#cuda):__ Process in-bulk with `` in CUDA C++ 17\n-  __[Python](#quick-start-python):__ Upgrade your `str` to faster `Str`\n-  __[Rust](#quick-start-rust):__ Use the `StringZilla` traits crate\n-  __[Go](#quick-start-golang):__ Use the `StringZilla` cGo module\n-  __[Swift](#quick-start-swift):__ Use the `String+StringZilla` extension\n-  __[JavaScript](#quick-start-javascript):__ Use the `StringZilla` library\n-  __[Shell][faq-shell]__: Accelerate common CLI tools with `sz-` prefix\n-  Researcher? Jump to [Algorithms & Design Decisions](#algorithms--design-decisions)\n-  Thinking to contribute? Look for [\"good first issues\"][first-issues]\n-  And check the [guide](https://github.com/ashvardanian/StringZilla/blob/main/CONTRIBUTING.md) to set up the environment\n- Want more bindings or features? Let [me](https://github.com/ashvardanian) know!\n\n__Who is this for?__\n\n- For data-engineers parsing large datasets, like the [CommonCrawl](https://commoncrawl.org/), [RedPajama](https://github.com/togethercomputer/RedPajama-Data), or [LAION](https://laion.ai/blog/laion-5b/).\n- For software engineers optimizing strings in their apps and services.\n- For bioinformaticians and search engineers looking for edit-distances for [USearch](https://github.com/unum-cloud/usearch).\n- For [DBMS][faq-dbms] devs, optimizing `LIKE`, `ORDER BY`, and `GROUP BY` operations.\n- For hardware designers, needing a SWAR baseline for string-processing functionality.\n- For students studying SIMD/SWAR applications to non-data-parallel operations.\n\nPerformance\n\nMost StringZilla modules ship ready-to-run benchmarks for C, C++, Python, and more.\nGrab them from `./scripts`, and see [`CONTRIBUTING.md`](CONTRIBUTING.md) for instructions.\nOn CPUs that permit misaligned loads, even the 64-bit SWAR baseline outruns both libc and the STL.\nFor wider head-to-heads against Rust and Python favorites, browse the __[StringWars][stringwars]__ repository.\nTo inspect collision resistance and distribution shapes for our hashers, see __[HashEvals][hashevals]__."}, {"name": "stringzilla", "tags": ["data", "dev", "math", "ui", "web"], "summary": "Search, hash, sort, and process strings faster via SWAR and SIMD", "text": "> Most benchmarks were conducted on a 1 GB English text corpus, with an average word length of 6 characters.\n> The code was compiled with GCC 12, using `glibc` v2.35.\n> The benchmarks were performed on Arm-based Graviton3 AWS `c7g` instances and `r7iz` Intel Sapphire Rapids.\n> Most modern Arm-based 64-bit CPUs will have similar relative speedups.\n> Variance within x86 CPUs will be larger.\n> For CUDA benchmarks, the Nvidia H100 GPUs were used.\n> 1 Unlike other libraries, LibC requires strings to be NULL-terminated.\n> 2 Six whitespaces in the ASCII set are: ` \\t\\n\\v\\f\\r`. Python's and other standard libraries have specialized functions for those.\n> 3 All modulo operations were conducted with `uint8_t` to allow compilers more optimization opportunities.\n> The C++ STL and StringZilla benchmarks used a 64-bit [Mersenne Twister][faq-mersenne-twister] as the generator.\n> For C, C++, and StringZilla, an in-place update of the string was used.\n> In Python every string had to be allocated as a new object, which makes it less fair.\n> 4 Contrary to the popular opinion, Python's default `sorted` function works faster than the C and C++ standard libraries.\n> That holds for large lists or tuples of strings, but fails as soon as you need more complex logic, like sorting dictionaries by a string key, or producing the \"sorted order\" permutation.\n> The latter is very common in database engines and is most similar to `numpy.argsort`.\n> The current StringZilla solution can be at least 4x faster without loss of generality.\n> 5 Most Python libraries for strings are also implemented in C.\n> 6 Unlike the rest of BioPython, the alignment score computation is [implemented in C](https://github.com/biopython/biopython/blob/master/Bio/Align/_pairwisealigner.c).\n\nFunctionality\n\nStringZilla is compatible with most modern CPUs, and provides a broad range of functionality.\nIt's split into 2 layers:\n\n1. StringZilla: single-header C library and C++ wrapper for high-performance string operations.\n2. StringZillas: parallel CPU/GPU backends used for large-batch operations and accelerators.\n\nHaving a second C++/CUDA layer greatly simplifies the implementation of similarity scoring and fingerprinting functions, which would otherwise require too much error-prone boilerplate code in pure C.\nBoth layers are designed to be extremely portable:\n\n- [x] across both little-endian and big-endian architectures.\n- [x] across 32-bit and 64-bit hardware architectures.\n- [x] across operating systems and compilers.\n- [x] across ASCII and UTF-8 encoded inputs.\n\nNot all features are available across all bindings.\nConsider contributing if you need a feature that's not yet implemented.\n\nMaturity\n:-----------------------------\nSubstring Search\nCharacter Set Search\nSorting & Sequence Operations\nLazy Ranges, Compressed Arrays\nOne-Shot & Streaming Hashes\nCryptographic Hashes\nSmall String Class\nRandom String Generation\nUnicode Case Folding\nCase-Insensitive UTF-8 Search\nTR29 Word Boundary Detection\nParallel Similarity Scoring\nParallel Rolling Fingerprints\n\n>  parts are used in production.\n>  parts are in beta.\n>  parts are under active development, and are likely to break in subsequent releases.\n>  are implemented.\n> \u26aa are considered.\n>  are not intended.\n\nQuick Start: Python"}, {"name": "stringzilla", "tags": ["data", "dev", "math", "ui", "web"], "summary": "Search, hash, sort, and process strings faster via SWAR and SIMD", "text": "Python bindings are available on PyPI for Python 3.8+, and can be installed with `pip`.\n\nYou can immediately check the installed version and the used hardware capabilities with following commands:\n\nBasic Usage\n\nIf you've ever used the Python `str`, `bytes`, `bytearray`, or `memoryview` classes, you'll know what to expect.\nStringZilla's `Str` class is a hybrid of the above, providing a `str`-like interface to byte arrays.\n\nThe `File` class memory-maps a file from persistent storage without loading its copy into RAM.\nThe contents of that file would remain immutable, and the mapping can be shared by multiple Python processes simultaneously.\nA standard dataset pre-processing use case would be to map a sizable textual dataset like Common Crawl into memory, spawn child processes, and split the job between them.\n\nBasic Operations\n\n- Length: `len(text) -> int`\n- Indexing: `text[42] -> str`\n- Slicing: `text[42:46] -> Str`\n- Substring check: `'substring' in text -> bool`\n- Hashing: `hash(text) -> int`\n- String conversion: `str(text) -> str`\n\nAdvanced Operations\n\nIt's important to note that the last function's behavior is slightly different from Python's `str.splitlines`.\nThe [native version][faq-splitlines] matches `\\n`, `\\r`, `\\v` or `\\x0b`, `\\f` or `\\x0c`, `\\x1c`, `\\x1d`, `\\x1e`, `\\x85`, `\\r\\n`, `\\u2028`, `\\u2029`, including 3x two-byte-long runes.\nThe StringZilla version matches only `\\n`, `\\v`, `\\f`, `\\r`, `\\x1c`, `\\x1d`, `\\x1e`, `\\x85`, avoiding two-byte-long runes.\n\nCharacter Set Operations\n\nPython strings don't natively support character set operations.\nThis forces people to use regular expressions, which are slow and hard to read.\nTo avoid the need for `re.finditer`, StringZilla provides the following interfaces:\n\nStringZilla also provides string trimming functions and random string generation:\n\nYou can also transform the string using Look-Up Tables (LUTs), mapping it to a different character set.\nThis would result in a copy - `str` for `str` inputs and `bytes` for other types.\n\nFor efficiency reasons, pass the LUT as a string or bytes object, not as a dictionary.\nThis can be useful in high-throughput applications dealing with binary data, including bioinformatics and image processing.\nHere is an example:\n\nHash\n\nSingle-shot and incremental hashing are both supported:\n\nSHA-256 Checksums\n\nSHA-256 cryptographic checksums are also available for single-shot and incremental hashing:\n\nStringZilla integrates seamlessly with memory-mapped files for efficient large file processing.\nThe traditional approach with `hashlib`:\n\nCan be simplified with StringZilla:\n\nBoth output the same digest: `7278165ce01a4ac1e8806c97f32feae908036ca3d910f5177d2cf375e20aeae1`.\nOpenSSL (powering `hashlib`) has faster Assembly kernels, but StringZilla avoids file I/O overhead with memory mapping and skips Python's abstraction layers:\n\n- OpenSSL-backed `hashlib.sha256`: 12.6s\n- StringZilla end-to-end: 4.0s \u2014 __3\u00d7 faster!__\n\nUnicode Case-Folding and Case-Insensitive Search\n\nStringZilla implements both Unicode Case Folding and Case-Insensitive UTF-8 Search.\nUnlike most libraries only capable of lower-casing ASCII-represented English alphabet, StringZilla covers over 1M+ codepoints.\nThe case-folding API expects the output buffer to be at least 3\u00d7 larger than the input, to accommodate for the worst-case character expansions scenarios.\n\nThe case-insensitive search returns the byte offset of the match, handling expansions correctly.\n\nCollection-Level Operations"}, {"name": "stringzilla", "tags": ["data", "dev", "math", "ui", "web"], "summary": "Search, hash, sort, and process strings faster via SWAR and SIMD", "text": "Once split into a `Strs` object, you can sort, shuffle, and reorganize the slices with minimal memory footprint.\nIf all the chunks are located in consecutive memory regions, the memory overhead can be as low as 4 bytes per chunk.\n\nWorking on [RedPajama][redpajama], addressing 20 billion annotated English documents, one will need only 160 GB of RAM instead of terabytes.\nOnce loaded, the data will be memory-mapped, and can be reused between multiple Python processes without copies.\nAnd of course, you can use slices to navigate the dataset and shard it between multiple workers.\n\nIterators and Memory Efficiency\n\nPython's operations like `split()` and `readlines()` immediately materialize a `list` of copied parts.\nThis can be very memory-inefficient for large datasets.\nStringZilla saves a lot of memory by viewing existing memory regions as substrings, but even more memory can be saved by using lazily evaluated iterators.\n\nStringZilla can easily be 10x more memory efficient than native Python classes for tokenization.\nWith lazy operations, it practically becomes free.\n\nLow-Level Python API\n\nAside from calling the methods on the `Str` and `Strs` classes, you can also call the global functions directly on `str` and `bytes` instances.\nAssuming StringZilla CPython bindings are implemented [without any intermediate tools like SWIG or PyBind](https://ashvardanian.com/posts/pybind11-cpython-tutorial/), the call latency should be similar to native classes.\n\nSimilarity Scores\n\nStringZilla exposes high-performance, batch-oriented similarity via the `stringzillas` module. \nUse `DeviceScope` to pick hardware and optionally limit capabilities per engine.\n\nNote, that this computes byte-level distances.\nFor UTF-8 codepoints, use a different engine class:\n\nFor alignment scoring provide a 256\u00d7256 substitution matrix using NumPy:\n\nSeveral Python libraries provide edit distance computation.\nMost are implemented in C but may be slower than StringZilla on large inputs.\nFor proteins ~10k chars, 100 pairs:\n\n- [JellyFish](https://github.com/jamesturk/jellyfish): 62.3s\n- [EditDistance](https://github.com/roy-ht/editdistance): 32.9s\n- StringZilla: __0.8s__\n\nUsing the same proteins for Needleman-Wunsch alignment scores:\n\n- [BioPython](https://github.com/biopython/biopython): 25.8s\n- StringZilla: __7.8s__\n\n\u00a7 Example converting from BioPython to StringZilla.\n\nRolling Fingerprints\n\nMinHashing is a common technique for Information Retrieval, producing compact representations of large documents.\nFor $D$ hash-functions and a text of length $L$, in the worst case it involves computing $O(D \\cdot L)$ hashes.\n\nSerialization\n\nFilesystem\n\nSimilar to how `File` can be used to read a large file, other interfaces can be used to dump strings to disk faster.\nThe `Str` class has `write_to` to write the string to a file, and `offset_within` to obtain integer offsets of substring view in larger string for navigation.\n\nPyArrow\n\nA `Str` is easy to cast to [PyArrow](https://arrow.apache.org/docs/python/arrays.html#string-and-binary-types) buffers.\n\nAnd only slightly harder to convert in reverse direction:\n\nThat means you can convert `Str` to `pyarrow.Buffer` and `Strs` to `pyarrow.Array` without extra copies.\nFor more details on the tape-like layouts, refer to the [StringTape](https://github.com/ashvardanian/StringTape) repository.\n\nQuick Start: C/C++\n\nThe C library is header-only, so you can just copy the `stringzilla.h` header into your project.\nSame applies to C++, where you would copy the `stringzilla.hpp` header.\nAlternatively, add it as a submodule, and include it in your build system.\n\nOr using a pure CMake approach:"}, {"name": "stringzilla", "tags": ["data", "dev", "math", "ui", "web"], "summary": "Search, hash, sort, and process strings faster via SWAR and SIMD", "text": "Last, but not the least, you can also install it as a library, and link against it.\nThis approach is worse for inlining, but brings [dynamic runtime dispatch](#dynamic-dispatch) for the most advanced CPU features.\n\nBasic Usage with C 99 and Newer\n\nThere is a stable C 99 interface, where all function names are prefixed with `sz_`.\nMost interfaces are well documented, and come with self-explanatory names and examples.\nIn some cases, hardware specific overloads are available, like `sz_find_skylake` or `sz_find_neon`.\nBoth are companions of the `sz_find`, first for x86 CPUs with AVX-512 support, and second for Arm NEON-capable CPUs.\n\n\u00a7 Mapping from LibC to StringZilla.\n\nBy design, StringZilla has a couple of notable differences from LibC:\n\n1. all strings are expected to have a length, and are not necessarily null-terminated.\n2. every operations has a reverse order counterpart.\n\nThat way `sz_find` and `sz_rfind` are similar to `strstr` and `strrstr` in LibC.\nSimilarly, `sz_find_byte` and `sz_rfind_byte` replace `memchr` and `memrchr`.\nThe `sz_find_byteset` maps to `strspn` and `strcspn`, while `sz_rfind_byteset` has no sibling in LibC.\n\nBasic Usage with C++ 11 and Newer\n\nThere is a stable C++ 11 interface available in the `ashvardanian::stringzilla` namespace.\nIt comes with two STL-like classes: `string_view` and `string`.\nThe first is a non-owning view of a string, and the second is a mutable string with a [Small String Optimization][faq-sso].\n\nStringZilla also provides string literals for automatic type resolution, [similar to STL][stl-literal]:\n\nUnicode Case-Folding and Case-Insensitive Search\n\nStringZilla implements both Unicode Case Folding and Case-Insensitive UTF-8 Search.\nUnlike most libraries only capable of lower-casing ASCII-represented English alphabet, StringZilla covers over 1M+ codepoints.\nThe case-folding API expects the output buffer to be at least 3\u00d7 larger than the input, to accommodate for the worst-case character expansions scenarios.\n\nThe case-insensitive search API returns a pointer to the start of the first relevant glyph in the haystack, or `NULL` if not found.\nIt outputs the length of the matched haystack substring in bytes, and accepts a metadata structure to speed up repeated searches for the same needle.\n\nSame functionality is available in C++:\n\nSimilarity Scores\n\nStringZilla exposes high-performance, batch-oriented similarity via the `stringzillas/stringzillas.h` header. \nUse `szs_device_scope_t` to pick hardware and optionally limit capabilities per engine.\n\nTo target a different device, use the appropriate `szs_device_scope_init_{cpu_cores,gpu_device}` function.\nWhen dealing with GPU backends, make sure to use the \"unified memory\" allocators exposed as `szs_unified_{alloc,free}`.\nSimilar stable C ABIs are exposed for other workloads as well.\n\n- UTF-8: `szs_levenshtein_distances_utf8_{sequence,u32tape,u64tape}`\n- Needleman-Wunsch: `szs_needleman_wunsch_scores_{sequence,u32tape,u64tape}`\n- Smith-Waterman: `szs_smith_waterman_scores_{sequence,u32tape,u64tape}`\n\nMoreover, in C++ codebases one can tap into the raw templates implementing that functionality, customizing them with custom executors, SIMD plugins, etc.\nFor that include `stringzillas/similarities.hpp` for C++ and `stringzillas/similarities.cuh` for CUDA.\n\nAll of the potentially failing StringZillas' interfaces return error codes, and none raise C++ exceptions.\nParallelism is enabled at both collection-level and within individual pairs of large inputs.\n\nRolling Fingerprints\n\nStringZilla exposes parallel fingerprinting (Min-Hashes or Count-Min-Sketches) via the `stringzillas/stringzillas.h` header. \nUse `szs_device_scope_t` to pick hardware and optionally limit capabilities per engine."}, {"name": "stringzilla", "tags": ["data", "dev", "math", "ui", "web"], "summary": "Search, hash, sort, and process strings faster via SWAR and SIMD", "text": "Moreover, in C++ codebases one can tap into the raw templates implementing that functionality, customizing them with custom executors, SIMD plugins, etc.\nFor that include `stringzillas/fingerprints.hpp` for C++ and `stringzillas/fingerprints.cuh` for CUDA.\n\nCUDA\n\nStringZilla provides CUDA C++ templates for composable string batch-processing operations.\nDifferent GPUs have varying warp sizes, shared memory capacities, and register counts, affecting algorithm selection, so it's important to query the `gpu_specs_t` via `gpu_specs_fetch`.\nFor memory management, ensure that you use GPU-visible' unified memory` exposed in an STL-compatible manner as a `unified_alloc` template class.\nFor error handling, `cuda_status_t` extends the traditional `status_t` with GPU-specific information.\nIt's implicitly convertible to `status_t`, so you can use it in places expecting a `status_t`.\n\nMost algorithms can load-balance both a large number of small strings and a small number of large strings.\nStill, with large H100-scale GPUs, it's best to submit thousands of inputs at once.\n\nMemory Ownership and Small String Optimization\n\nMost operations in StringZilla don't assume any memory ownership.\nBut in addition to the read-only search-like operations StringZilla provides a minimalistic C and C++ implementations for a memory owning string \"class\".\nLike other efficient string implementations, it uses the [Small String Optimization][faq-sso] (SSO) to avoid heap allocations for short strings.\n\nAs one can see, a short string can be kept on the stack, if it fits within `internal.chars` array.\nBefore 2015 GCC string implementation was just 8 bytes, and could only fit 7 characters.\nDifferent STL implementations today have different thresholds for the Small String Optimization.\nSimilar to GCC, StringZilla is 32 bytes in size, and similar to Clang it can fit 22 characters on stack.\nOur layout might be preferential, if you want to avoid branches.\nIf you use a different compiler, you may want to check its SSO buffer size with a [simple Gist](https://gist.github.com/ashvardanian/c197f15732d9855c4e070797adf17b21).\n\n`libstdc++` in  GCC 13\n:--------------\nString `sizeof`\nInner Capacity\n\nThis design has been since ported to many high-level programming languages.\nSwift, for example, [can store 15 bytes](https://developer.apple.com/documentation/swift/substring/withutf8(_:)#discussion) in the `String` instance itself.\nStringZilla implements SSO at the C level, providing the `sz_string_t` union and a simple API for primary operations.\n\nUnlike the conventional C strings, the `sz_string_t` is allowed to contain null characters.\nTo safely print those, pass the `string_length` to `printf` as well.\n\nWhat's Wrong with the C Standard Library?\n\nStringZilla is not a drop-in replacement for the C Standard Library.\nIt's designed to be a safer and more modern alternative.\nConceptually:\n\n1. LibC strings are expected to be null-terminated, so to use the efficient LibC implementations on slices of larger strings, you'd have to copy them, which is more expensive than the original string operation.\n2. LibC functionality is asymmetric - you can find the first and the last occurrence of a character within a string, but you can't find the last occurrence of a substring.\n3. LibC function names are typically very short and cryptic.\n4. LibC lacks crucial functionality like hashing and doesn't provide primitives for less critical but relevant operations like fuzzy matching."}, {"name": "stringzilla", "tags": ["data", "dev", "math", "ui", "web"], "summary": "Search, hash, sort, and process strings faster via SWAR and SIMD", "text": "Something has to be said about its support for UTF-8.\nAside from a single-byte `char` type, LibC provides `wchar_t`:\n\n- The size of `wchar_t` is not consistent across platforms. On Windows, it's typically 16 bits (suitable for UTF-16), while on Unix-like systems, it's usually 32 bits (suitable for UTF-32). This inconsistency can lead to portability issues when writing cross-platform code.\n- `wchar_t` is designed to represent wide characters in a fixed-width format (UTF-16 or UTF-32). In contrast, UTF-8 is a variable-length encoding, where each character can take from 1 to 4 bytes. This fundamental difference means that `wchar_t` and UTF-8 are incompatible.\n\nStringZilla [partially addresses those issues](#unicode-utf-8-and-wide-characters).\n\nWhat's Wrong with the C++ Standard Library?\n\nC++ Code\n:-----------------------------------\n`\"Loose\"s.replace(2, 2, \"vath\"s, 1)`\n`\"Loose\"s.replace(2, 2, \"vath\", 1)`\n\nStringZilla is designed to be a drop-in replacement for the C++ Standard Templates Library.\nThat said, some of the design decisions of STL strings are highly controversial, error-prone, and expensive.\nMost notably:\n\n1. Argument order for `replace`, `insert`, `erase` and similar functions is impossible to guess.\n2. Bounds-checking exceptions for `substr`-like functions are only thrown for one side of the range.\n3. Returning string copies in `substr`-like functions results in absurd volume of allocations.\n4. Incremental construction via `push_back`-like functions goes through too many branches.\n5. Inconsistency between `string` and `string_view` methods, like the lack of `remove_prefix` and `remove_suffix`.\n\nCheck the following set of asserts validating the `std::string` specification.\nIt's not realistic to expect the average developer to remember the [14 overloads of `std::string::replace`][stl-replace].\n\nTo avoid those issues, StringZilla provides an alternative consistent interface.\nIt supports signed arguments, and doesn't have more than 3 arguments per function or\nThe standard API and our alternative can be conditionally disabled with `SZ_SAFETY_OVER_COMPATIBILITY=1`.\nWhen it's enabled, the _~~subjectively~~_ risky overloads from the Standard will be disabled.\n\nAssuming StringZilla is a header-only library you can use the full API in some translation units and gradually transition to safer restricted API in others.\nBonus - all the bound checking is branchless, so it has a constant cost and won't hurt your branch predictor.\n\nBeyond the C++ Standard Library - Learning from Python\n\nPython is arguably the most popular programming language for data science.\nIn part, that's due to the simplicity of its standard interfaces.\nStringZilla brings some of that functionality to C++.\n\n- Content checks: `isalnum`, `isalpha`, `isascii`, `isdigit`, `islower`, `isspace`, `isupper`.\n- Trimming character sets: `lstrip`, `rstrip`, `strip`.\n- Trimming string matches: `remove_prefix`, `remove_suffix`.\n- Ranges of search results: `splitlines`, `split`, `rsplit`.\n- Number of non-overlapping substring matches: `count`.\n- Partitioning: `partition`, `rpartition`.\n\nFor example, when parsing documents, it is often useful to split it into substrings.\nMost often, after that, you would compute the length of the skipped part, the offset and the length of the remaining part.\nThis results in a lot of pointer arithmetic and is error-prone.\nStringZilla provides a convenient `partition` function, which returns a tuple of three string views, making the code cleaner."}, {"name": "stringzilla", "tags": ["data", "dev", "math", "ui", "web"], "summary": "Search, hash, sort, and process strings faster via SWAR and SIMD", "text": "Combining those with the `split` function, one can easily parse a CSV file or HTTP headers.\n\nSome other extensions are not present in the Python standard library either.\nLet's go through the C++ functionality category by category.\n\n- [Splits and Ranges](#splits-and-ranges).\n- [Concatenating Strings without Allocations](#concatenating-strings-without-allocations).\n- [Random Generation](#random-generation).\n- [Edit Distances and Fuzzy Search](#levenshtein-edit-distance-and-alignment-scores).\n\nSome of the StringZilla interfaces are not available even Python's native `str` class.\nHere is a sneak peek of the most useful ones.\n\nSplits and Ranges\n\nOne of the most common use cases is to split a string into a collection of substrings.\nWhich would often result in [StackOverflow lookups][so-split] and snippets like the one below.\n\nThose allocate memory for each string and the temporary vectors.\nEach allocation can be orders of magnitude more expensive, than even serial `for`-loop over characters.\nTo avoid those, StringZilla provides lazily-evaluated ranges, compatible with the [Range-v3][range-v3] library.\n\nConcatenating Strings without Allocations\n\nAnother common string operation is concatenation.\nThe STL provides `std::string::operator+` and `std::string::append`, but those are not very efficient, if multiple invocations are performed.\n\nThe efficient approach would be to pre-allocate the memory and copy the strings into it.\n\nThat's mouthful and error-prone.\nStringZilla provides a more convenient `concatenate` function, which takes a variadic number of arguments.\nIt also overrides the `operator|` to concatenate strings lazily, without any allocations.\n\nRandom Generation\n\nSoftware developers often need to generate random strings for testing purposes.\nThe STL provides `std::generate` and `std::random_device`, that can be used with StringZilla.\n\nMouthful and slow.\nStringZilla provides a C native method - `sz_fill_random` and a convenient C++ wrapper - `sz::generate`.\nSimilar to Python it also defines the commonly used character sets.\n\nBulk Replacements\n\nIn text processing, it's often necessary to replace all occurrences of a specific substring or set of characters within a string.\nStandard library functions may not offer the most efficient or convenient methods for performing bulk replacements, especially when dealing with large strings or performance-critical applications.\n\n- `haystack.replace_all(needle_string, replacement_string)`\n- `haystack.replace_all(sz::byteset(\"\"), replacement_string)`\n- `haystack.try_replace_all(needle_string, replacement_string)`\n- `haystack.try_replace_all(sz::byteset(\"\"), replacement_string)`\n- `haystack.lookup(sz::look_up_table::identity())`\n- `haystack.lookup(sz::look_up_table::identity(), haystack.data())`\n\nSorting in C and C++\n\nLibC provides `qsort` and STL provides `std::sort`.\nBoth have their quirks.\nThe LibC standard has no way to pass a context to the comparison function, that's only possible with platform-specific extensions.\nThose have [different arguments order](https://stackoverflow.com/a/39561369) on every OS.\n\nC++ generic algorithm is not perfect either.\nThere is no guarantee in the standard that `std::sort` won't allocate any memory.\nIf you are running on embedded, in real-time or on 100+ CPU cores per node, you may want to avoid that.\nStringZilla doesn't solve the general case, but hopes to improve the performance for strings.\nUse `sz_sequence_argsort`, or the high-level `sz::argsort`, which can be used sort any collection of elements convertible to `sz::string_view`.\n\nStandard C++ Containers with String Keys\n\nThe C++ Standard Templates Library provides several associative containers, often used with string keys."}, {"name": "stringzilla", "tags": ["data", "dev", "math", "ui", "web"], "summary": "Search, hash, sort, and process strings faster via SWAR and SIMD", "text": "The performance of those containers is often limited by the performance of the string keys, especially on reads.\nStringZilla can be used to accelerate containers with `std::string` keys, by overriding the default comparator and hash functions.\n\nAlternatively, a better approach would be to use the `sz::string` class as a key.\nThe right hash function and comparator would be automatically selected and the performance gains would be more noticeable if the keys are short.\n\nCompilation Settings and Debugging\n\n__`SZ_DEBUG`__:\n\n> For maximal performance, the C library does not perform any bounds checking in Release builds.\n> In C++, bounds checking happens only in places where the STL `std::string` would do it.\n> If you want to enable more aggressive bounds-checking, define `SZ_DEBUG` before including the header.\n> If not explicitly set, it will be inferred from the build type.\n\n__`SZ_USE_GOLDMONT`, `SZ_USE_WESTMERE`, `SZ_USE_HASWELL`, `SZ_USE_SKYLAKE`, `SZ_USE_ICE`, `SZ_USE_NEON`, `SZ_USE_NEON_AES`, `SZ_USE_NEON_SHA`, `SZ_USE_SVE`, `SZ_USE_SVE2`, `SZ_USE_SVE2_AES`__:\n\n> One can explicitly disable certain families of SIMD instructions for compatibility purposes.\n> Default values are inferred at compile time depending on compiler support (for dynamic dispatch) and the target architecture (for static dispatch).\n\n__`SZ_USE_CUDA`, `SZ_USE_KEPLER`, `SZ_USE_HOPPER`__:\n\n> One can explicitly disable certain families of PTX instructions for compatibility purposes.\n> Default values are inferred at compile time depending on compiler support (for dynamic dispatch) and the target architecture (for static dispatch).\n\n__`SZ_ENFORCE_SVE_OVER_NEON`__:\n\n> SVE and SVE2 are expected to supersede NEON on ARM architectures.\n> Still, oftentimes the equivalent SVE kernels are slower due to equally small register files and higher complexity of the instructions.\n> By default, when both SVE and NEON are available, SVE is used selectively only for the algorithms that benefit from it.\n> If you want to enforce SVE usage everywhere, define this flag.\n\n__`SZ_DYNAMIC_DISPATCH`__:\n\n> By default, StringZilla is a header-only library.\n> But if you are running on different generations of devices, it makes sense to pre-compile the library for all supported generations at once, and dispatch at runtime.\n> This flag does just that and is used to produce the `stringzilla.so` shared library, as well as the Python bindings.\n\n__`SZ_USE_MISALIGNED_LOADS`__:\n\n> Default is platform-dependent: enabled on x86 (where unaligned accesses are fast), disabled on others by default.\n> When enabled, many byte-level operations use word-sized loads, which can significantly accelerate the serial (SWAR) backend.\n> Consider enabling it explicitly if you are targeting platforms that support fast unaligned loads.\n\n__`SZ_AVOID_LIBC`__ and __`SZ_OVERRIDE_LIBC`__:\n\n> When using the C header-only library one can disable the use of LibC.\n> This may affect the type resolution system on obscure hardware platforms. \n> Moreover, one may let `stringzilla` override the common symbols like the `memcpy` and `memset` with its own implementations.\n> In that case you can use the [`LD_PRELOAD` trick][ld-preload-trick] to prioritize its symbols over the ones from the LibC and accelerate existing string-heavy applications without recompiling them.\n> It also adds a layer of security, as the `stringzilla` isn't [undefined for NULL inputs][redhat-memcpy-ub] like `memcpy(NULL, NULL, 0)`.\n\n__`SZ_AVOID_STL`__ and __`SZ_SAFETY_OVER_COMPATIBILITY`__:"}, {"name": "stringzilla", "tags": ["data", "dev", "math", "ui", "web"], "summary": "Search, hash, sort, and process strings faster via SWAR and SIMD", "text": "> When using the C++ interface one can disable implicit conversions from `std::string` to `sz::string` and back.\n> If not needed, the `` and `` headers will be excluded, reducing compilation time.\n> Moreover, if STL compatibility is a low priority, one can make the API safer by disabling the overloads, which are subjectively error prone.\n\n__`STRINGZILLA_BUILD_SHARED`, `STRINGZILLA_BUILD_TEST`, `STRINGZILLA_BUILD_BENCHMARK`, `STRINGZILLA_TARGET_ARCH`__ for CMake users:\n\n> When compiling the tests and benchmarks, you can explicitly set the target hardware architecture.\n> It's synonymous to GCC's `-march` flag and is used to enable/disable the appropriate instruction sets.\n> You can also disable the shared library build, if you don't need it.\n\nQuick Start: Rust\n\nStringZilla is available as a Rust crate, with documentation available on [docs.rs/stringzilla](https://docs.rs/stringzilla).\nYou can immediately check the installed version and the used hardware capabilities with following commands:\n\nTo use the latest crate release in your project, add the following to your `Cargo.toml`:\n\nOr if you want to use the latest pre-release version from the repository:\n\nOnce installed, all of the functionality is available through the `stringzilla` namespace.\nMany interfaces will look familiar to the users of the `memchr` Rust crate.\n\nIt also provides no constraints on the size of the character set, while `memchr` allows only 1, 2, or 3 characters.\nIn addition to global functions, `stringzilla` provides a `StringZilla` extension trait:\n\nHash\n\nSingle-shot and incremental hashing are both supported:\n\nTo use StringZilla with `std::collections`:\n\nSHA-256 Checksums\n\nSHA-256 cryptographic checksums are available:\n\nUnicode Case-Folding and Case-Insensitive Search\n\nStringZilla implements both Unicode Case Folding and Case-Insensitive UTF-8 Search.\nUnlike most libraries only capable of lower-casing ASCII-represented English alphabet, StringZilla covers over 1M+ codepoints.\nThe case-folding API expects the output buffer to be at least 3\u00d7 larger than the input, to accommodate for the worst-case character expansions scenarios.\n\nThe case-insensitive search returns `Some((offset, matched_length))` or `None`.\nThe `matched_length` may differ from needle length due to expansions.\n\nSimilarity Scores\n\nStringZilla exposes high-performance, batch-oriented similarity via the `szs` module.\nUse `DeviceScope` to pick hardware and optionally limit capabilities per engine.\n\nNote, that this computes byte-level distances.\nFor UTF-8 codepoints, use a different engine class:\n\nSimilarly, for variable substitution costs, also pass in a a weights matrix:\n\nOr for local alignment scores:\n\nFor high-performance applications, use the [StringTape](https://github.com/ashvardanian/StringTape) crate to pass strings to `compute_into` methods without extra memory allocations:\n\nRolling Fingerprints\n\nMinHashing is a common technique for Information Retrieval, producing compact representations of large documents.\nFor $D$ hash-functions and a text of length $L$, in the worst case it involves computing $O(D \\cdot L)$ hashes.\n\nFor zero-copy processing with [StringTape](https://github.com/ashvardanian/StringTape) format and unified memory:\n\nQuick Start: JavaScript\n\nInstall the Node.js package and use zero-copy `Buffer` APIs.\n\nUnicode Case-Folding and Case-Insensitive Search\n\nStringZilla provides full Unicode case folding (including expansions like `\u00df \u2192 ss`, ligatures like `\ufb01 \u2192 fi`, and special folds like `\u00b5 \u2192 \u03bc`, `\u212a \u2192 k`)\nand a case-insensitive substring search that accounts for those expansions.\n\nHash\n\nSingle-shot and incremental hashing are both supported:\n\nSHA-256 Checksums\n\nSHA-256 cryptographic checksums are available:"}, {"name": "stringzilla", "tags": ["data", "dev", "math", "ui", "web"], "summary": "Search, hash, sort, and process strings faster via SWAR and SIMD", "text": "Quick Start: Swift\n\nStringZilla can be added as a dependency in the Swift Package Manager.\nIn your `Package.swift` file, add the following:\n\nThe package currently covers only the most basic functionality, but is planned to be extended to cover the full C++ API.\n\nUnicode Case-Folding and Case-Insensitive Search\n\nHash\n\nStringZilla provides high-performance hashing for Swift strings:\n\nSHA-256 Checksums\n\nSHA-256 cryptographic checksums are available:\n\nQuick Start: GoLang\n\nAdd the Go binding as a module dependency:\n\nBuild the shared C library once, then ensure your runtime can locate it (Linux shown):\n\nUse finders (substring, bytes, and sets):\n\nUnicode Case-Folding and Case-Insensitive Search\n\nHash\n\nSingle-shot and incremental hashing are both supported.\nThe `Hasher` type implements Go's standard `hash.Hash64` and `io.Writer` interfaces:\n\nSHA-256 Checksums\n\nSHA-256 cryptographic checksums are available.\nThe `Sha256` type implements Go's standard `hash.Hash` and `io.Writer` interfaces:\n\nAlgorithms & Design Decisions\n\nStringZilla aims to optimize some of the slowest string operations.\nSome popular operations, however, like equality comparisons and relative order checking, almost always complete on some of the very first bytes in either string.\nIn such operations vectorization is almost useless, unless huge and very similar strings are considered.\nStringZilla implements those operations as well, but won't result in substantial speedups.\nWhere vectorization stops being effective, parallelism takes over with the new layered cake architecture:\n\n- StringZilla C library w/out dependencies\n- StringZillas parallel extensions:\n  - Parallel C++ algorithms built with [Fork Union](https://github.com/ashvardanian/fork_union)\n  - Parallel CUDA algorithms for Nvidia GPUs\n  - Parallel ROCm algorithms for AMD GPUs\n\nExact Substring Search\n\nSubstring search algorithms are generally divided into: comparison-based, automaton-based, and bit-parallel.\nDifferent families are effective for different alphabet sizes and needle lengths.\nThe more operations are needed per-character - the more effective SIMD would be.\nThe longer the needle - the more effective the skip-tables are.\nStringZilla uses different exact substring search algorithms for different needle lengths and backends:\n\n- When no SIMD is available - SWAR (SIMD Within A Register) algorithms are used on 64-bit words.\n- Boyer-Moore-Horspool (BMH) algorithm with Raita heuristic variation for longer needles.\n- SIMD backends compare characters at multiple strategically chosen offsets within the needle to reduce degeneracy.\n\nOn very short needles, especially 1-4 characters long, brute force with SIMD is the fastest solution.\nOn mid-length needles, bit-parallel algorithms are effective, as the character masks fit into 32-bit or 64-bit words.\nEither way, if the needle is under 64-bytes long, on haystack traversal we will still fetch every CPU cache line.\nSo the only way to improve performance is to reduce the number of comparisons.\n\n> For 2-byte needles, see `sz_find_2byte_serial_` in `include/stringzilla/find.h`:\n\nGoing beyond that, to long needles, Boyer-Moore (BM) and its variants are often the best choice.\nIt has two tables: the good-suffix shift and the bad-character shift.\nCommon choice is to use the simplified BMH algorithm, which only uses the bad-character shift table, reducing the pre-processing time.\nWe do the same for mid-length needles up to 256 bytes long.\nThat way the stack-allocated shift table remains small."}, {"name": "stringzilla", "tags": ["data", "dev", "math", "ui", "web"], "summary": "Search, hash, sort, and process strings faster via SWAR and SIMD", "text": "> For mid-length needles (\u2264256 bytes), see `sz_find_horspool_upto_256bytes_serial_` in `include/stringzilla/find.h`:\n\nIn the C++ Standards Library, the `std::string::find` function uses the BMH algorithm with Raita's heuristic.\nBefore comparing the entire string, it matches the first, last, and the middle character.\nVery practical, but can be slow for repetitive characters.\nBoth SWAR and SIMD backends of StringZilla have a cheap pre-processing step, where we locate unique characters.\nThis makes the library a lot more practical when dealing with non-English corpora.\n\n> The offset selection heuristic is implemented in `sz_locate_needle_anomalies_` in `include/stringzilla/find.h`:\n\nAll those, still, have $O(hn)$ worst case complexity.\nTo guarantee $O(h)$ worst case time complexity, the Apostolico-Giancarlo (AG) algorithm adds an additional skip-table.\nPreprocessing phase is $O(n+sigma)$ in time and space.\nOn traversal, performs from $(h/n)$ to $(3h/2)$ comparisons.\nIt however, isn't practical on modern CPUs.\nA simpler idea, the Galil-rule might be a more relevant optimizations, if many matches must be found.\n\nOther algorithms previously considered and deprecated:\n\n- Apostolico-Giancarlo algorithm for longer needles. _Control-flow is too complex for efficient vectorization._\n- Shift-Or-based Bitap algorithm for short needles. _Slower than SWAR._\n- Horspool-style bad-character check in SIMD backends. _Effective only for very long needles, and very uneven character distributions between the needle and the haystack. Faster \"character-in-set\" check needed to generalize._\n\n> \u00a7 Reading materials.\n> [Exact String Matching Algorithms in Java](https://www-igm.univ-mlv.fr/~lecroq/string).\n> [SIMD-friendly algorithms for substring searching](http://0x80.pl/articles/simd-strfind.html).\n\nExact Multiple Substring Search\n\nFew algorithms for multiple substring search are known.\nMost are based on the Aho-Corasick automaton, which is a generalization of the KMP algorithm.\nThe naive implementation, however:\n\n- Allocates disjoint memory for each Trie node and Automaton state.\n- Requires a lot of pointer chasing, limiting speculative execution.\n- Has a lot of branches and conditional moves, which are hard to predict.\n- Matches text a character at a time, which is slow on modern CPUs.\n\nThere are several ways to improve the original algorithm.\nOne is to use sparse DFA representation, which is more cache-friendly, but would require extra processing to navigate state transitions.\n\nLevenshtein Edit Distance\n\nLevenshtein distance is the best known edit-distance for strings, that checks, how many insertions, deletions, and substitutions are needed to transform one string to another.\nIt's extensively used in approximate string-matching, spell-checking, and bioinformatics.\n\nThe computational cost of the Levenshtein distance is $O(n * m)$, where $n$ and $m$ are the lengths of the string arguments.\nTo compute that, the naive approach requires $O(n * m)$ space to store the \"Levenshtein matrix\", the bottom-right corner of which will contain the Levenshtein distance.\nThe algorithm producing the matrix has been simultaneously studied/discovered by the Soviet mathematicians Vladimir Levenshtein in 1965, Taras Vintsyuk in 1968, and American computer scientists - Robert Wagner, David Sankoff, Michael J. Fischer in the following years.\nSeveral optimizations are known:"}, {"name": "stringzilla", "tags": ["data", "dev", "math", "ui", "web"], "summary": "Search, hash, sort, and process strings faster via SWAR and SIMD", "text": "1. __Space Optimization__: The matrix can be computed in $O(min(n,m))$ space, by only storing the last two rows of the matrix.\n2. __Divide and Conquer__: Hirschberg's algorithm can be applied to decompose the computation into subtasks.\n3. __Automata__: Levenshtein automata can be effective, if one of the strings doesn't change, and is a subject to many comparisons.\n4. __Shift-Or__: Bit-parallel algorithms transpose the matrix into a bit-matrix, and perform bitwise operations on it.\n\nThe last approach is quite powerful and performant, and is used by the great [RapidFuzz][rapidfuzz] library.\nIt's less known, than the others, derived from the Baeza-Yates-Gonnet algorithm, extended to bounded edit-distance search by Manber and Wu in 1990s, and further extended by Gene Myers in 1999 and Heikki Hyyro between 2002 and 2004.\n\nStringZilla focuses on a different approach, extensively used in Unum's internal combinatorial optimization libraries.\nIt doesn't change the number of trivial operations, but performs them in a different order, removing the data dependency, that occurs when computing the insertion costs.\nStringZilla __evaluates diagonals instead of rows__, exploiting the fact that all cells within a diagonal are independent, and can be computed in parallel.\nWe'll store 3 diagonals instead of the 2 rows, and each consecutive diagonal will be computed from the previous two.\nSubstitution costs will come from the sooner diagonal, while insertion and deletion costs will come from the later diagonal.\n\nRow-by-Row Algorithm\nComputing row 4:\n\n\u2205  0  1  2  3  4  5\n P  1  \u2591  \u2591  \u2591  \u2591  \u2591\n Q  2  \u25a0  \u25a0  \u25a0  \u25a0  \u25a0\n R  3  \u25a0  \u25a0  \u25a1  \u2192  .\n S  4  .  .  .  .  .\n T  5  .  .  .  .  .\n\nAnti-Diagonal Algorithm\nComputing diagonal 5:\n\n\u2205  0  1  2  3  4  5\n P  1  \u2591  \u2591  \u25a0  \u25a0  \u25a1\n Q  2  \u2591  \u25a0  \u25a0  \u25a1  \u2198\n R  3  \u25a0  \u25a0  \u25a1  \u2198  .\n S  4  \u25a0  \u25a1  \u2198  .  .\n T  5  \u25a1  \u2198  .  .  .\n\nLegend:\n0,1,2,3... = initialization constants &nbsp;&nbsp;\n\u2591 = cells processed and forgotten &nbsp;&nbsp;\n\u25a0 = stored cells &nbsp;&nbsp;\n\u25a1 = computing in parallel &nbsp;&nbsp;\n\u2192 \u2198 = movement direction &nbsp;&nbsp;\n. = cells to compute later\n\nThis results in much better vectorization for intra-core parallelism and potentially multi-core evaluation of a single request.\nMoreover, it's easy to generalize to weighted edit-distances, where the cost of a substitution between two characters may not be the same for all pairs, often used in bioinformatics.\n\n> \u00a7 Reading materials.\n> [Faster Levenshtein Distances with a SIMD-friendly Traversal Order](https://ashvardanian.com/posts/levenshtein-diagonal).\n\nNeedleman-Wunsch and Smith-Waterman Scores for Bioinformatics\n\nThe field of bioinformatics studies various representations of biological structures.\nThe \"primary\" representations are generally strings over sparse alphabets:"}, {"name": "stringzilla", "tags": ["data", "dev", "math", "ui", "web"], "summary": "Search, hash, sort, and process strings faster via SWAR and SIMD", "text": "- [DNA][faq-dna] sequences, where the alphabet is {A, C, G, T}, ranging from ~100 characters for short reads to 3 billion for the human genome.\n- [RNA][faq-rna] sequences, where the alphabet is {A, C, G, U}, ranging from ~50 characters for tRNA to thousands for mRNA.\n- [Proteins][faq-protein], where the alphabet is made of 22 amino acids, ranging from 2 characters for [dipeptide][faq-dipeptide] to 35,000 for [Titin][faq-titin], the longest protein.\n\nThe shorter the representation, the more often researchers may want to use custom substitution matrices.\nMeaning that the cost of a substitution between two characters may not be the same for all pairs.\nIn the general case the serial algorithm is supposed to work for arbitrary substitution costs for each of 256\u00d7256 possible character pairs.\nThat lookup table, however, is too large to fit into CPU registers, so instead, the upcoming design focuses on 32\u00d732 substitution matrices, which fit into 1 KB with single-byte \"error costs\".\nThat said, most [BLOSUM][faq-blosum] and [PAM][faq-pam] substitution matrices only contain 4-bit values, so they can be packed even further.\n\nNext design goals:\n\n- [ ] Needleman-Wunsch Automata\n\nMemory Copying, Fills, and Moves\n\nA lot has been written about the time computers spend copying memory and how that operation is implemented in LibC.\nInterestingly, the operation can still be improved, as most Assembly implementations use outdated instructions.\nEven performance-oriented STL replacements, like Meta's [Folly v2024.09.23 focus on AVX2](https://github.com/facebook/folly/blob/main/folly/memset.S), and don't take advantage of the new masked instructions in AVX-512 or SVE.\n\nIn AVX-512, StringZilla uses non-temporal stores to avoid cache pollution, when dealing with very large strings.\nMoreover, it handles the unaligned head and the tails of the `target` buffer separately, ensuring that writes in big copies are always aligned to cache-line boundaries.\nThat's true for both AVX2 and AVX-512 backends.\n\nStringZilla also contains \"drafts\" of smarter, but less efficient algorithms, that minimize the number of unaligned loads, performing shuffles and permutations.\nThat's a topic for future research, as the performance gains are not yet satisfactory.\n\n> \u00a7 Reading materials.\n> [`memset` benchmarks](https://github.com/nadavrot/memset_benchmark?tab=readme-ov-file) by Nadav Rotem.\n> [Cache Associativity](https://en.algorithmica.org/hpc/cpu-cache/associativity/) by Sergey Slotin.\n\nHashing\n\nStringZilla implements a high-performance 64-bit hash function inspired by the \"AquaHash\", \"aHash\", and \"GxHash\" design and optimized for modern CPU architectures.\nThe algorithm utilizes AES encryption rounds combined with shuffle-and-add operations to achieve exceptional mixing properties while maintaining consistent output across platforms.\nIt passes the rigorous SMHasher test suite, including the `--extra` flag with no collisions.\n\nThe core algorithm operates on a dual-state design:\n\n- __AES State__: Initialized with seed `XOR`-ed against \u03c0 constants.\n- __Sum State__: Accumulates shuffled input data with a permutation.\n\nFor strings \u226464 bytes, a minimal state processes data in 16-byte blocks.\nLonger strings employ a 4\u00d7 wider state (512 bits) that processes 64-byte chunks, maximizing throughput on modern superscalar CPUs.\nThe algorithm can be expressed in pseudocode as:"}, {"name": "stringzilla", "tags": ["data", "dev", "math", "ui", "web"], "summary": "Search, hash, sort, and process strings faster via SWAR and SIMD", "text": "This allows us to balance several design trade-offs.\nFirst, it allows us to achieve a high port-level parallelism.\nLooking at AVX-512 capable CPUs and their ZMM instructions, on each cycle, we'll have at least 2 ports busy when dealing with long strings:\n\n- `VAESENC`: 5 cycles on port 0 on Intel Ice Lake, 4 cycles on ports 0/1 on AMD Zen4.\n- `VPSHUFB_Z`: 3 cycles on port 5 on Intel Ice Lake, 2 cycles on ports 1/2 on AMD Zen4.\n- `VPADDQ`: 1 cycle on ports 0/5 on Intel Ice Lake, 1 cycle on ports 0/1/2/3 on AMD Zen4.\n\nWhen dealing with smaller strings, we design our approach to avoid large registers and maintain the CPU at the same energy state, thereby avoiding downclocking and expensive power-state transitions.\n\nUnlike some AES-accelerated alternatives, the length of the input is not mixed into the AES block at the start to allow incremental construction, when the final length is not known in advance.\nAlso, unlike some alternatives, with \"masked\" AVX-512 and \"predicated\" SVE loads, we avoid expensive block-shuffling procedures on non-divisible-by-16 lengths.\n\n> \u00a7 Reading materials.\n> [Stress-testing hash functions for avalance behaviour, collision bias, and distribution](https://github.com/ashvardanian/HashEvals).\n\nSHA-256 Checksums\n\nIn addition to the fast AES-based hash, StringZilla implements hardware-accelerated SHA-256 cryptographic checksums.\nThe implementation follows the [FIPS 180-4 specification](https://nvlpubs.nist.gov/nistpubs/FIPS/NIST.FIPS.180-4.pdf) and provides multiple backends.\n\nRandom Generation\n\nStringZilla implements a fast [Pseudorandom Number Generator][faq-prng] inspired by the \"AES-CTR-128\" algorithm, reusing the same AES primitives as the hash function.\nUnlike \"NIST SP 800-90A\" which uses multiple AES rounds, StringZilla uses only one round of AES mixing for performance while maintaining reproducible output across platforms.\nThe generator operates in counter mode with `AESENC(nonce + lane_index, nonce \u2295 pi_constants)`, rotating through the first 512 bits of \u03c0 for each 16-byte block.\nThe only state required to reproduce an output is a 64-bit `nonce`, which is much cheaper than a Mersenne Twister.\n\nSorting\n\nFor lexicographic sorting of string collections, StringZilla exports pointer-sized n\u2011grams (\"pgrams\") into a contiguous buffer to improve locality, then recursively QuickSorts those pgrams with a 3\u2011way partition and dives into equal pgrams to compare deeper characters.\nVery small inputs fall back to insertion sort.\n\n- Average time complexity: O(n log n)\n- Worst-case time complexity: quadratic (due to QuickSort), mitigated in practice by 3\u2011way partitioning and the n\u2011gram staging\n\nUnicode 17, UTF-8, and Wide Characters\n\nMost StringZilla operations are byte-level, so they work well with ASCII and UTF-8 content out of the box.\nIn some cases, like edit-distance computation, the result of byte-level evaluation and character-level evaluation may differ.\n\n- `szs_levenshtein_distances_utf8(\"\u03b1\u03b2\u03b3\u03b4\", \"\u03b1\u03b3\u03b4\") == 1` \u2014 one unicode symbol.\n- `szs_levenshtein_distances(\"\u03b1\u03b2\u03b3\u03b4\", \"\u03b1\u03b3\u03b4\") == 2` \u2014 one unicode symbol is two bytes long."}, {"name": "stringzilla", "tags": ["data", "dev", "math", "ui", "web"], "summary": "Search, hash, sort, and process strings faster via SWAR and SIMD", "text": "Java, JavaScript, Python 2, C#, and Objective-C, however, use wide characters (`wchar`) - two byte long codes, instead of the more reasonable fixed-length UTF-32 or variable-length UTF-8.\nThis leads [to all kinds of offset-counting issues][wide-char-offsets] when facing four-byte long Unicode characters.\nStringZilla uses proper 32-bit \"runes\" to represent unpacked Unicode codepoints, ensuring correct results in all operations.\nMoreover, it implements the Unicode 17.0 standard, being practically the only library besides ICU and PCRE2 to do so, but with order(s) of magnitude better performance.\n\nCase-Folding and Case-Insensitive Search\n\nStringZilla provides Unicode-aware case-insensitive substring search that handles the full complexity of Unicode case folding.\nThis includes multi-character expansions:\n\nCharacter\n---------\n`\u00df`\n`\ufb03`\n`\u0130`\n\nThe search returns byte offsets and lengths in the original haystack, correctly handling length differences.\nFor example, searching for `\"STRASSE\"` (7 bytes) in `\"Stra\u00dfe\"` (7 bytes: 53 74 72 61 C3 9F 65) succeeds because both case-fold to `\"strasse\"`.\n\nNote that Turkish `\u0130` and ASCII `I` are distinct: `\u0130stanbul` case-folds to `i\u0307stanbul` (with combining dot), while `ISTANBUL` case-folds to `istanbul` (without).\nThey will not match each other \u2014 this is correct Unicode behavior for Turkish locale handling.\n\nFor wide-character environments (Java, JavaScript, Python 2, C#), consider transcoding with [simdutf](https://github.com/simdutf/simdutf).\n\nDynamic Dispatch\n\nDue to the high-level of fragmentation of SIMD support in different CPUs, StringZilla uses the names of select Intel and ARM CPU generations for its backends.\nYou can query supported backends and use them manually.\nUse it to guarantee constant performance, or to explore how different algorithms scale on your hardware.\n\nStringZilla automatically picks the most advanced backend for the given CPU.\nSimilarly, in Python, you can log the auto-detected capabilities:\n\nYou can also explicitly set the backend to use, or scope the backend to a specific function.\n\nContributing\n\nPlease check out the [contributing guide](https://github.com/ashvardanian/StringZilla/blob/main/CONTRIBUTING.md) for more details on how to set up the development environment and contribute to this project.\nIf you like this project, you may also enjoy [USearch][usearch], [UCall][ucall], [UForm][uform], and [SimSIMD][simsimd].\n\nIf you like strings and value efficiency, you may also enjoy the following projects:\n\nIf you are looking for more reading materials on this topic, consider the following:\n\nLicense\n\nFeel free to use the project under Apache 2.0 or the Three-clause BSD license at your preference."}, {"name": "stringzilla", "tags": ["data", "dev", "math", "ui", "web"], "summary": "Search, hash, sort, and process strings faster via SWAR and SIMD", "text": "This library is used to accelerate string operations such as search, hash, sort, and processing through optimized SWAR (Simultaneous Word Access Register) and SIMD (Single Instruction, Multiple Data) instructions. With StringZilla, developers can achieve significantly faster performance for various string-related tasks in their applications."}, {"name": "sympy", "tags": ["math", "web"], "summary": "Computer algebra system (CAS) in Python", "text": "SymPy\n\n(https://pypi.python.org/pypi/sympy)\n\n(https://pepy.tech/project/sympy)\n(https://github.com/sympy/sympy/issues)\n(https://git-scm.com/book/en/v2/GitHub-Contributing-to-a-Project)\n(https://numfocus.org)\n(https://github.com/sympy/sympy/releases)\n\n(https://sympy.org/)\n\nSee the [AUTHORS](AUTHORS) file for the list of authors.\n\nAnd many more people helped on the SymPy mailing list, reported bugs,\nhelped organize SymPy's participation in the Google Summer of Code, the\nGoogle Highly Open Participation Contest, Google Code-In, wrote and\nblogged about SymPy...\n\nLicense: New BSD License (see the [LICENSE](LICENSE) file for details) covers all\nfiles in the sympy repository unless stated otherwise.\n\nOur mailing list is at\n.\n\nWe have a community chat at [Gitter](https://gitter.im/sympy/sympy). Feel\nfree to ask us anything there. We have a very welcoming and helpful\ncommunity.\n\nDownload\n\nThe recommended installation method is through Anaconda,\n\nYou can also get the latest version of SymPy from\n\nTo get the git version do\n\nFor other options (tarballs, debs, etc.), see\n.\n\nDocumentation and Usage\n\nFor in-depth instructions on installation and building the\ndocumentation, see the [SymPy Documentation Style Guide](https://docs.sympy.org/dev/documentation-style-guide.html).\n\nEverything is at:\n\nYou can generate everything at the above site in your local copy of\nSymPy by:\n\nThen the docs will be in \\_build/html. If\nyou don't want to read that, here is a short usage:\n\nFrom this directory, start Python and:\n\nSymPy also comes with a console that is a simple wrapper around the\nclassic python console (or IPython when available) that loads the SymPy\nnamespace and executes some common commands for you.\n\nTo start it, issue:\n\nfrom this directory, if SymPy is not installed or simply:\n\nif SymPy is installed.\n\nInstallation\n\nTo install SymPy using PyPI, run the following command:\n\nTo install SymPy using Anaconda, run the following command:\n\nTo install SymPy from GitHub source, first clone SymPy using `git`:\n\nThen, in the `sympy` repository that you cloned, simply run:\n\nSee  for more information.\n\nContributing\n\nWe welcome contributions from anyone, even if you are new to open\nsource. Please read our [Introduction to Contributing](https://docs.sympy.org/dev/contributing/introduction-to-contributing.html)\npage and the [SymPy Documentation Style Guide](https://docs.sympy.org/dev/documentation-style-guide.html). If you\nare new and looking for some way to contribute, a good place to start is\nto look at the issues tagged [Easy to Fix](https://github.com/sympy/sympy/issues?q=is%3Aopen+is%3Aissue+label%3A%22Easy+to+Fix%22).\n\nPlease note that all participants in this project are expected to follow\nour Code of Conduct. By participating in this project you agree to abide\nby its terms. See [CODE\\_OF\\_CONDUCT.md](CODE_OF_CONDUCT.md).\n\nTests\n\nTo execute all tests, run:\n\nin the current directory.\n\nFor the more fine-grained running of tests or doctests, use `bin/test`\nor respectively `bin/doctest`. The master branch is automatically tested\nby GitHub Actions.\n\nTo test pull requests, use\n[sympy-bot](https://github.com/sympy/sympy-bot).\n\nRegenerate Experimental LaTeX Parser/Lexer\n\nThe parser and lexer were generated with the [ANTLR4](http://antlr4.org)\ntoolchain in `sympy/parsing/latex/_antlr` and checked into the repo.\nPresently, most users should not need to regenerate these files, but\nif you plan to work on this feature, you will need the `antlr4`\ncommand-line tool (and you must ensure that it is in your `PATH`).\nOne way to get it is:"}, {"name": "sympy", "tags": ["math", "web"], "summary": "Computer algebra system (CAS) in Python", "text": "Alternatively, follow the instructions on the ANTLR website and download\nthe `antlr-4.11.1-complete.jar`. Then export the `CLASSPATH` as instructed\nand instead of creating `antlr4` as an alias, make it an executable file\nwith the following contents:\n\nAfter making changes to `sympy/parsing/latex/LaTeX.g4`, run:\n\nClean\n\nTo clean everything (thus getting the same tree as in the repository):\n\nwhich will clear everything ignored by `.gitignore`, and:\n\nto clear all untracked files. You can revert the most recent changes in\ngit with:\n\nWARNING: The above commands will all clear changes you may have made,\nand you will lose them forever. Be sure to check things with `git\nstatus`, `git diff`, `git clean -Xn`, and `git clean -n` before doing any\nof those.\n\nBugs\n\nOur issue tracker is at . Please\nreport any bugs that you find. Or, even better, fork the repository on\nGitHub and create a pull request. We welcome all changes, big or small,\nand we will help you make the pull request if you are new to git (just\nask on our mailing list or Gitter Channel). If you further have any queries, you can find answers\non Stack Overflow using the [sympy](https://stackoverflow.com/questions/tagged/sympy) tag.\n\nBrief History\n\nSymPy was started by Ond\u0159ej \u010cert\u00edk in 2005, he wrote some code during\nthe summer, then he wrote some more code during summer 2006. In February\n2007, Fabian Pedregosa joined the project and helped fix many things,\ncontributed documentation, and made it alive again. 5 students (Mateusz\nPaprocki, Brian Jorgensen, Jason Gedge, Robert Schwarz, and Chris Wu)\nimproved SymPy incredibly during summer 2007 as part of the Google\nSummer of Code. Pearu Peterson joined the development during the summer\n2007 and he has made SymPy much more competitive by rewriting the core\nfrom scratch, which has made it from 10x to 100x faster. Jurjen N.E. Bos\nhas contributed pretty-printing and other patches. Fredrik Johansson has\nwritten mpmath and contributed a lot of patches.\n\nSymPy has participated in every Google Summer of Code since 2007. You\ncan see  for\nfull details. Each year has improved SymPy by bounds. Most of SymPy's\ndevelopment has come from Google Summer of Code students.\n\nIn 2011, Ond\u0159ej \u010cert\u00edk stepped down as lead developer, with Aaron\nMeurer, who also started as a Google Summer of Code student, taking his\nplace. Ond\u0159ej \u010cert\u00edk is still active in the community but is too busy\nwith work and family to play a lead development role.\n\nSince then, a lot more people have joined the development and some\npeople have also left. You can see the full list in doc/src/aboutus.rst,\nor online at:\n\nThe git history goes back to 2007 when development moved from svn to hg.\nTo see the history before that point, look at\n.\n\nYou can use git to see the biggest developers. The command:\n\nwill show each developer, sorted by commits to the project. The command:\n\nwill show the top developers from the last year.\n\nCitation\n\nTo cite SymPy in publications use"}, {"name": "sympy", "tags": ["math", "web"], "summary": "Computer algebra system (CAS) in Python", "text": "> Meurer A, Smith CP, Paprocki M, \u010cert\u00edk O, Kirpichev SB, Rocklin M,\n> Kumar A, Ivanov S, Moore JK, Singh S, Rathnayake T, Vig S, Granger BE,\n> Muller RP, Bonazzi F, Gupta H, Vats S, Johansson F, Pedregosa F, Curry\n> MJ, Terrel AR, Rou\u010dka \u0160, Saboo A, Fernando I, Kulal S, Cimrman R,\n> Scopatz A. (2017) SymPy: symbolic computing in Python. *PeerJ Computer\n> Science* 3:e103\n\nA BibTeX entry for LaTeX users is\n\nSymPy is BSD licensed, so you are free to use it whatever you like, be\nit academic, commercial, creating forks or derivatives, as long as you\ncopy the BSD statement if you redistribute it (see the LICENSE file for\ndetails). That said, although not required by the SymPy license, if it\nis convenient for you, please cite SymPy when using it in your work and\nalso consider contributing all your changes back, so that we can\nincorporate it and all of us will benefit in the end."}, {"name": "sympy", "tags": ["math", "web"], "summary": "Computer algebra system (CAS) in Python", "text": "This library is used to provide a comprehensive computer algebra system (CAS) for Python, enabling developers to perform complex mathematical calculations and operations. With SymPy, developers can solve equations, manipulate mathematical expressions, and handle various types of mathematical objects with ease."}, {"name": "tangled-up-in-unicode", "tags": ["data", "math"], "summary": "Access to the Unicode Character Database (UCD)", "text": "Tangled up in Unicode\n\nThis module provides access to character properties for all Unicode characters, from the Unicode Character Database (UCD) .\nThis module provides an alternative to Python's standard library [`unicodedata`](https://docs.python.org/3/library/unicodedata.html).\n`Tangled up in Unicode` provides four main benefits compared to the standard library:\n- The [latest version](http://www.unicode.org/versions/latest/) of the Unicode database is used.\n- Adds human-readable class names (Property value aliases).\n- Extends the properties to use more potential of the database.\n- UCD version independent of Python version (Python 3.6 has UCD 9.0, 3.7 has UCD 11.0.0, 3.8 has 12.0.1, 3.9 has 13.0.0)\n\nNote that Python 3 added unicode support, but that this is different from the UCD.\nUnicode support handles storing and manipulating unicode characters, while this package aims to provide properties of specific characters.\n\nExample\n\nThe default lookup in `unicodedata` for `$`:\n\nProperty\n---------------------------\nName\nCategory (Short)\nBidirectional (Short)\nCombining\nMirrored\nEast Asian Width (Short)\nDecomposition\n\nExtra information provided by this package\n\nProperty\n-------------------------------\nCategory Alias (Long)\nBidirectional Alias (Long)\nEast Asian Width Alias (Long)\nScript (Long)\nScript (Short)\nBlock (Long)\nBlock (Short)\nPropList\nUppercase Character\nLowercase Character\nTitlecase\tCharacter\n\nProperties comparison\n\nProperty\n---------------------------\nName\nDecimal\nDigit\nNumeric\nCombining\nMirrored\nDecomposition\nCategory\nBidirectional\nEast Asian Width\nScript\nBlock\nAge\nBinary Property Values\nVersion\n\n_Table 1: presence of properties is denoted by &#9745; (Unicode Character 'BALLOT BOX WITH CHECK' (U+2611))._\n\nUsage\n\nThe package can be installed via pip:\n\nPerformance\n\nThe module is written in Python. \nIt can be compiled with Cython to gain [competitive performance](# \"Meaning the null hypothesis of the two libraries having the same average runtime could not be rejected.\") with the native library.\n\nUnsupported features\n\nSome of the features in `unicodedata` are not supported. \n\nFeature\n-----------------------\nlookup\nnormalize\nucd_3_2_0\n\nAcknowledgements\nWhere possible, code and documentation of the original module are used.\nThis repository is part of the Dylan Profiling project."}, {"name": "tangled-up-in-unicode", "tags": ["data", "math"], "summary": "Access to the Unicode Character Database (UCD)", "text": "This library is used to access and utilize the Unicode Character Database (UCD) for all Unicode characters, offering a more up-to-date and feature-rich alternative to Python's standard `unicodedata` library. With it, developers can take advantage of the latest UCD version, human-readable property names, and extended properties not available in the standard library."}, {"name": "tb-nightly", "tags": ["math", "web"], "summary": "TensorBoard lets you watch Tensors Flow", "text": "TensorBoard is a suite of web applications for inspecting and understanding\nyour TensorFlow runs and graphs.\n\nReleases prior to 1.6.0 were published under the ``tensorflow-tensorboard`` name\nand may be found at https://pypi.python.org/pypi/tensorflow-tensorboard."}, {"name": "tb-nightly", "tags": ["math", "web"], "summary": "TensorBoard lets you watch Tensors Flow", "text": "This library is used to inspect and understand your TensorFlow runs and graphs through a suite of web applications. This allows developers to effectively monitor and analyze their machine learning models' performance in real-time."}, {"name": "tensorboard-plugin-profile", "tags": ["math", "visualization", "web"], "summary": "XProf Profiler Plugin", "text": "XProf (+ Tensorboard Profiler Plugin)\n\nXProf offers a number of tools to analyse and visualize the\nperformance of your model across multiple devices. Some of the tools include:\n\n*   **Overview**: A high-level overview of the performance of your model. This\n*   **Trace Viewer**: Displays a timeline of the execution of your model that shows:\n*   **Memory Profile Viewer**: Monitors the memory usage of your model.\n*   **Graph Viewer**: A visualization of the graph structure of HLOs of your model.\n\nTo learn more about the various XProf tools, check out the [XProf documentation](https://openxla.org/xprof)\n\nDemo\nFirst time user? Come and check out this [Colab Demo](https://docs.jaxstack.ai/en/latest/JAX_for_LLM_pretraining.html).\n\nQuick Start\n\nPrerequisites\n\n* xprof >= 2.20.0\n* (optional) TensorBoard >= 2.20.0\n\nNote: XProf requires access to the Internet to load the [Google Chart library](https://developers.google.com/chart/interactive/docs/basic_load_libs#basic-library-loading).\nSome charts and tables may be missing if you run XProf entirely offline on\nyour local machine, behind a corporate firewall, or in a datacenter.\n\nIf you use Google Cloud to run your workloads, we recommend the\n[xprofiler tool](https://github.com/AI-Hypercomputer/cloud-diagnostics-xprof).\nIt provides a streamlined profile collection and viewing experience using VMs\nrunning XProf.\n\nInstallation\n\nTo get the most recent release version of XProf, install it via pip:\n\nRunning XProf\n\nXProf can be launched as a standalone server or used as a plugin within\nTensorBoard. For large-scale use, it can be deployed in a distributed mode with\nseparate aggregator and worker instances ([more details on it later in the\ndoc](#distributed-profiling)).\n\nCommand-Line Arguments\n\nWhen launching XProf from the command line, you can use the following arguments:\n\n*   **`logdir`** (optional): The directory containing XProf profile data (files\n*   **`-p `**, **`--port `**: The port for the XProf web server.\n*   **`-gp `**, **`--grpc_port `**: The port for the gRPC\n*   **`-wsa `**, **`--worker_service_address `**: A\n*   **`-hcpb`**, **`--hide_capture_profile_button`**: If set, hides the 'Capture\n\nStandalone\n\nIf you have profile data in a directory (e.g., `profiler/demo`), you can view it\nby running:\n\nOr with the optional flag:\n\nWith TensorBoard\n\nIf you have TensorBoard installed, you can run:\n\nIf you are behind a corporate firewall, you may need to include the `--bind_all`\ntensorboard flag.\n\nGo to `localhost:6006/#profile` of your browser, you should now see the demo\noverview page show up.\nCongratulations! You're now ready to capture a profile.\n\nLog Directory Structure\n\nWhen using XProf, profile data must be placed in a specific directory structure.\nXProf expects `.xplane.pb` files to be in the following path:\n\n*   ``: This is the root directory that you supply to `tensorboard\n*   `plugins/profile/`: This is a required subdirectory.\n*   `/`: Each subdirectory inside `plugins/profile/` represents a\n\n**Example:**\n\nIf your log directory is structured like this:\n\nYou would launch TensorBoard with:\n\nThe runs `my_experiment_run_1` and `benchmark_20251107` will be available in the\n\"Sessions\" tab of the UI.\n\nYou can also dynamically load sessions from a GCS bucket or local filesystem by\npassing URL parameters when loading XProf in your browser. This method works\nwhether or not you provided a `logdir` at startup and is useful for viewing\nprofiles from various locations without restarting XProf."}, {"name": "tensorboard-plugin-profile", "tags": ["math", "visualization", "web"], "summary": "XProf Profiler Plugin", "text": "For example, if you start XProf with no log directory:\n\nYou can load sessions using the following URL parameters.\n\nAssume you have profile data stored on GCS or locally, structured like this:\n\nThere are two URL parameters you can use:\n\n*   **`session_path`**: Use this to load a *single* session directly. The path\n\n*   **`run_path`**: Use this to point to a directory that contains *multiple*\n\n**Loading Precedence**\n\nIf multiple sources are provided, XProf uses the following order of precedence\nto determine which profiles to load:\n\n1.  **`session_path`** URL parameter\n2.  **`run_path`** URL parameter\n3.  **`logdir`** command-line argument\n\nDistributed Profiling\n\nXProf supports distributed profile processing by using an aggregator that\ndistributes work to multiple XProf workers. This is useful for processing large\nprofiles or handling multiple users.\n\n**Note**: Currently, distributed processing only benefits the following tools:\n`overview_page`, `framework_op_stats`, `input_pipeline`, and `pod_viewer`.\n\n**Note**: The ports used in these examples (`6006` for the aggregator HTTP\nserver, `9999` for the worker HTTP server, and `50051` for the worker gRPC\nserver) are suggestions and can be customized.\n\n**Worker Node**\n\nEach worker node should run XProf with a gRPC port exposed so it can receive\nprocessing requests. You should also hide the capture button as workers are not\nmeant to be interacted with directly.\n\n**Aggregator Node**\n\nThe aggregator node runs XProf with the `--worker_service_address` flag pointing\nto all available workers. Users will interact with aggregator node's UI.\n\nReplace `, ` with the addresses of your worker machines.\nRequests sent to the aggregator on port 6006 will be distributed among the\nworkers for processing.\n\nFor deploying a distributed XProf setup in a Kubernetes environment, see\n[Kubernetes Deployment Guide](docs/kubernetes_deployment.md).\n\nNightlies\n\nEvery night, a nightly version of the package is released under the name of\n`xprof-nightly`. This package contains the latest changes made by the XProf\ndevelopers.\n\nTo install the nightly version of profiler:\n\nNext Steps\n\n* [JAX Profiling Guide](https://jax.readthedocs.io/en/latest/profiling.html#xprof-tensorboard-profiling)\n* [PyTorch/XLA Profiling Guide](https://cloud.google.com/tpu/docs/pytorch-xla-performance-profiling-tpu-vm)\n* [TensorFlow Profiling Guide](https://tensorflow.org/guide/profiler)\n* [Cloud TPU Profiling Guide](https://cloud.google.com/tpu/docs/cloud-tpu-tools)\n* [Colab Tutorial](https://www.tensorflow.org/tensorboard/tensorboard_profiling_keras)\n* [Tensorflow Colab](https://www.tensorflow.org/tensorboard/tensorboard_profiling_keras)"}, {"name": "tensorboard-plugin-profile", "tags": ["math", "visualization", "web"], "summary": "XProf Profiler Plugin", "text": "This library is used to analyze and visualize the performance of a model across multiple devices, providing tools such as an overview, trace viewer, memory profile viewer, and graph viewer. Developers can use this library with XProf and Tensorboard to gain insights into their model's execution timeline, memory usage, and graph structure."}, {"name": "tensorboard", "tags": ["math", "web"], "summary": "TensorBoard lets you watch Tensors Flow", "text": "TensorBoard is a suite of web applications for inspecting and understanding\nyour TensorFlow runs and graphs.\n\nReleases prior to 1.6.0 were published under the ``tensorflow-tensorboard`` name\nand may be found at https://pypi.python.org/pypi/tensorflow-tensorboard."}, {"name": "tensorboard", "tags": ["math", "web"], "summary": "TensorBoard lets you watch Tensors Flow", "text": "This library is used to visually inspect and understand your TensorFlow models' runs and graphs, providing a suite of web applications for monitoring and troubleshooting. With this library, developers can gain valuable insights into their machine learning workflows and optimize performance accordingly."}, {"name": "tensorflow-addons", "tags": ["math", "web"], "summary": "TensorFlow Addons.", "text": "TensorFlow Addons is a repository of contributions that conform to well-\nestablished API patterns, but implement new functionality not available\nin core TensorFlow. TensorFlow natively supports a large number of\noperators, layers, metrics, losses, and optimizers. However, in a fast\nmoving field like ML, there are many interesting new developments that\ncannot be integrated into core TensorFlow (because their broad\napplicability is not yet clear, or it is mostly used by a smaller subset\nof the community)."}, {"name": "tensorflow-addons", "tags": ["math", "web"], "summary": "TensorFlow Addons.", "text": "This library is used to extend and complement TensorFlow's existing functionality with newly developed techniques in machine learning. It provides additional operators, layers, metrics, losses, and optimizers that are not available natively in core TensorFlow."}, {"name": "tensorflow-cpu", "tags": ["math", "ml"], "summary": "TensorFlow is an open source machine learning framework for everyone.", "text": "TensorFlow is an open source software library for high performance numerical\ncomputation. Its flexible architecture allows easy deployment of computation\nacross a variety of platforms (CPUs, GPUs, TPUs), and from desktops to clusters\nof servers to mobile and edge devices.\n\nOriginally developed by researchers and engineers from the Google Brain team\nwithin Google's AI organization, it comes with strong support for machine\nlearning and deep learning and the flexible numerical computation core is used\nacross many other scientific domains. TensorFlow is licensed under [Apache\n2.0](https://github.com/tensorflow/tensorflow/blob/master/LICENSE)."}, {"name": "tensorflow-cpu", "tags": ["math", "ml"], "summary": "TensorFlow is an open source machine learning framework for everyone.", "text": "This library is used to enable high-performance numerical computations across various platforms, including CPUs, GPUs, and TPUs, for machine learning, deep learning, and other scientific domains. Developers can leverage this framework to deploy scalable computation on desktops, servers, mobile devices, or edge devices."}, {"name": "tensorflow-decision-forests", "tags": ["math", "ml", "web"], "summary": "Collection of training and inference decision forest algorithms.", "text": "Usage example\n\nA minimal end-to-end run looks as follows:\n\nGoogle I/O Presentation\n\nDocumentation & Resources\n\nThe following resources are available:\n\n-   [Known issues](documentation/known_issues.md)\n-   [Changelog](CHANGELOG.md)\n-   [More examples](documentation/more_examples.md)\n\nInstallation\n\nTo install TensorFlow Decision Forests, run:\n\nSee the [installation](documentation/installation.md) page for more details,\ntroubleshooting and alternative installation solutions.\n\nContributing\n\nContributions to TensorFlow Decision Forests and Yggdrasil Decision Forests are\nwelcome. If you want to contribute, make sure to review the\n[developer manual](documentation/developer_manual.md) and\n[contribution guidelines](CONTRIBUTING.md).\n\nCitation\n\nIf you us Tensorflow Decision Forests in a scientific publication, please cite\nthe following paper:\n[Yggdrasil Decision Forests: A Fast and Extensible Decision Forests Library](https://doi.org/10.1145/3580305.3599933).\n\n**Bibtex**\n\n**Raw**\n\nYggdrasil Decision Forests: A Fast and Extensible Decision Forests Library,\nGuillame-Bert et al., KDD 2023: 4068-4077. doi:10.1145/3580305.3599933\n\nContact\n\nYou can contact the core development team at\n[decision-forests-contact@google.com](mailto:decision-forests-contact@google.com).\n\nCredits\n\nTensorFlow Decision Forests was developed by:\n\n-   Mathieu Guillame-Bert (gbm AT google DOT com)\n-   Jan Pfeifer (janpf AT google DOT com)\n-   Richard Stotz (richardstotz AT google DOT com)\n-   Sebastian Bruch (sebastian AT bruch DOT io)\n-   Arvind Srinivasan (arvnd AT google DOT com)\n\nLicense\n\n[Apache License 2.0](LICENSE)"}, {"name": "tensorflow-decision-forests", "tags": ["math", "ml", "web"], "summary": "Collection of training and inference decision forest algorithms.", "text": "This library is used to train and deploy decision forest algorithms for machine learning tasks, with support for both training and inference workflows. By using this library, developers can efficiently implement and integrate decision forests into their projects with minimal code."}, {"name": "tensorflow-io-gcs-filesystem", "tags": ["data", "math", "ml", "web"], "summary": "TensorFlow IO", "text": "TensorFlow I/O\n\n(https://github.com/tensorflow/io/actions?query=branch%3Amaster)\n(https://pypi.org/project/tensorflow-io/)\n(https://github.com/tensorflow/io/blob/master/LICENSE)\n(https://www.tensorflow.org/io)\n\nTensorFlow I/O is a collection of file systems and file formats that are not\navailable in TensorFlow's built-in support. A full list of supported file systems\nand file formats by TensorFlow I/O can be found [here](https://www.tensorflow.org/io/api_docs/python/tfio).\n\nThe use of tensorflow-io is straightforward with keras. Below is an example\nto [Get Started with TensorFlow](https://www.tensorflow.org/tutorials/quickstart/beginner) with\nthe data processing aspect replaced by tensorflow-io:\n\nIn the above [MNIST](http://yann.lecun.com/exdb/mnist/) example, the URL's\nto access the dataset files are passed directly to the `tfio.IODataset.from_mnist` API call.\nThis is due to the inherent support that `tensorflow-io` provides for `HTTP`/`HTTPS` file system,\nthus eliminating the need for downloading and saving datasets on a local directory.\n\nNOTE: Since `tensorflow-io` is able to detect and uncompress the MNIST dataset automatically if needed,\nwe can pass the URL's for the compressed files (gzip) to the API call as is.\n\nPlease check the official [documentation](https://www.tensorflow.org/io) for more\ndetailed and interesting usages of the package.\n\nInstallation\n\nPython Package\n\nThe `tensorflow-io` Python package can be installed with pip directly using:\n\nPeople who are a little more adventurous can also try our nightly binaries:\n\nTo ensure you have a version of TensorFlow that is compatible with TensorFlow-IO,\nyou can specify the `tensorflow` extra requirement during install:\n\nSimilar extras exist for the `tensorflow-gpu`, `tensorflow-cpu` and `tensorflow-rocm`\npackages.\n\nDocker Images\n\nIn addition to the pip packages, the docker images can be used to quickly get started.\n\nFor stable builds:\n\nFor nightly builds:\n\nR Package\n\nOnce the `tensorflow-io` Python package has been successfully installed, you\ncan install the development version of the R package from GitHub via the following:\n\nTensorFlow Version Compatibility\n\nTo ensure compatibility with TensorFlow, it is recommended to install a matching\nversion of TensorFlow I/O according to the table below. You can find the list\nof releases [here](https://github.com/tensorflow/io/releases).\n\nTensorFlow I/O Version\n---\n0.37.1\n0.37.0\n0.36.0\n0.35.0\n0.34.0\n0.33.0\n0.32.0\n0.31.0\n0.30.0\n0.29.0\n0.28.0\n0.27.0\n0.26.0\n0.25.0\n0.24.0\n0.23.1\n0.23.0\n0.22.0\n0.21.0\n0.20.0\n0.19.1\n0.19.0\n0.18.0\n0.17.1\n0.17.0\n0.16.0\n0.15.0\n0.14.0\n0.13.0\n0.12.0\n0.11.0\n0.10.0\n0.9.1\n0.9.0\n0.8.1\n0.8.0\n0.7.2\n0.7.1\n0.7.0\n0.6.0\n0.5.0\n0.4.0\n0.3.0\n0.2.0\n0.1.0\n\nPerformance Benchmarking\n\nWe use [github-pages](https://tensorflow.github.io/io/dev/bench/) to document the results of API performance benchmarks. The benchmark job is triggered on every commit to `master` branch and\nfacilitates tracking performance w.r.t commits.\n\nContributing\n\nTensorflow I/O is a community led open source project. As such, the project\ndepends on public contributions, bug-fixes, and documentation. Please see:\n\n- [contribution guidelines](CONTRIBUTING.md) for a guide on how to contribute.\n- [development doc](docs/development.md) for instructions on the development environment setup.\n- [tutorials](docs/tutorials) for a list of tutorial notebooks and instructions on how to write one.\n\nBuild Status and CI\n\nBuild\n---\n\nBecause of manylinux2010 requirement, TensorFlow I/O is built with\nUbuntu:16.04 + Developer Toolset 7 (GCC 7.3) on Linux. Configuration\nwith Ubuntu 16.04 with Developer Toolset 7 is not exactly straightforward.\nIf the system have docker installed, then the following command\nwill automatically build manylinux2010 compatible whl package:"}, {"name": "tensorflow-io-gcs-filesystem", "tags": ["data", "math", "ml", "web"], "summary": "TensorFlow IO", "text": "It takes some time to build, but once complete, there will be python\n`3.5`, `3.6`, `3.7` compatible whl packages available in `wheelhouse`\ndirectory.\n\nOn macOS, the same command could be used. However, the script expects `python` in shell\nand will only generate a whl package that matches the version of `python` in shell. If\nyou want to build a whl package for a specific python then you have to alias this version\nof python to `python` in shell. See [.github/workflows/build.yml](.github/workflows/build.yml)\nAuditwheel step for instructions how to do that.\n\nNote the above command is also the command we use when releasing packages for Linux and macOS.\n\nTensorFlow I/O uses both GitHub Workflows and Google CI (Kokoro) for continuous integration.\nGitHub Workflows is used for macOS build and test. Kokoro is used for Linux build and test.\nAgain, because of the manylinux2010 requirement, on Linux whl packages are always\nbuilt with Ubuntu 16.04 + Developer Toolset 7. Tests are done on a variatiy of systems\nwith different python3 versions to ensure a good coverage:\n\nPython\n-------\n2.7\n3.7\n3.8\n\nTensorFlow I/O has integrations with many systems and cloud vendors such as\nPrometheus, Apache Kafka, Apache Ignite, Google Cloud PubSub, AWS Kinesis,\nMicrosoft Azure Storage, Alibaba Cloud OSS etc.\n\nWe tried our best to test against those systems in our continuous integration\nwhenever possible. Some tests such as Prometheus, Kafka, and Ignite\nare done with live systems, meaning we install Prometheus/Kafka/Ignite on CI machine before\nthe test is run. Some tests such as Kinesis, PubSub, and Azure Storage are done\nthrough official or non-official emulators. Offline tests are also performed whenever\npossible, though systems covered through offine tests may not have the same\nlevel of coverage as live systems or emulators.\n\nLive System\n-------\nApache Kafka\nApache Ignite\nPrometheus\nGoogle PubSub\nAzure Storage\nAWS Kinesis\nAlibaba Cloud OSS\nGoogle BigTable/BigQuery\nElasticsearch (experimental)\nMongoDB (experimental)\n\nReferences for emulators:\n\nCommunity\n\n* SIG IO [Google Group](https://groups.google.com/a/tensorflow.org/forum/#!forum/io) and mailing list: [io@tensorflow.org](io@tensorflow.org)\n* SIG IO [Monthly Meeting Notes](https://docs.google.com/document/d/1CB51yJxns5WA4Ylv89D-a5qReiGTC0GYum6DU-9nKGo/edit)\n* Gitter room: [tensorflow/sig-io](https://gitter.im/tensorflow/sig-io)\n\nAdditional Information\n\n* [Streaming Machine Learning with Tiered Storage and Without a Data Lake](https://www.confluent.io/blog/streaming-machine-learning-with-tiered-storage/) - [Kai Waehner](https://github.com/kaiwaehner)\n* [TensorFlow with Apache Arrow Datasets](https://medium.com/tensorflow/tensorflow-with-apache-arrow-datasets-cdbcfe80a59f) - [Bryan Cutler](https://github.com/BryanCutler)\n* [How to build a custom Dataset for Tensorflow](https://towardsdatascience.com/how-to-build-a-custom-dataset-for-tensorflow-1fe3967544d8) - [Ivelin Ivanov](https://github.com/ivelin)\n* [TensorFlow on Apache Ignite](https://medium.com/tensorflow/tensorflow-on-apache-ignite-99f1fc60efeb) - [Anton Dmitriev](https://github.com/dmitrievanthony)\n\nLicense\n\n[Apache License 2.0](LICENSE)"}, {"name": "tensorflow-io-gcs-filesystem", "tags": ["data", "math", "ml", "web"], "summary": "TensorFlow IO", "text": "This library is used to provide additional file systems and formats that can be accessed from TensorFlow, extending its built-in capabilities. This allows developers to seamlessly read and write various file types within their TensorFlow applications."}, {"name": "tensorflow-io", "tags": ["data", "math", "ml", "web"], "summary": "TensorFlow IO", "text": "TensorFlow I/O\n\n(https://github.com/tensorflow/io/actions?query=branch%3Amaster)\n(https://pypi.org/project/tensorflow-io/)\n(https://github.com/tensorflow/io/blob/master/LICENSE)\n(https://www.tensorflow.org/io)\n\nTensorFlow I/O is a collection of file systems and file formats that are not\navailable in TensorFlow's built-in support. A full list of supported file systems\nand file formats by TensorFlow I/O can be found [here](https://www.tensorflow.org/io/api_docs/python/tfio).\n\nThe use of tensorflow-io is straightforward with keras. Below is an example\nto [Get Started with TensorFlow](https://www.tensorflow.org/tutorials/quickstart/beginner) with\nthe data processing aspect replaced by tensorflow-io:\n\nIn the above [MNIST](http://yann.lecun.com/exdb/mnist/) example, the URL's\nto access the dataset files are passed directly to the `tfio.IODataset.from_mnist` API call.\nThis is due to the inherent support that `tensorflow-io` provides for `HTTP`/`HTTPS` file system,\nthus eliminating the need for downloading and saving datasets on a local directory.\n\nNOTE: Since `tensorflow-io` is able to detect and uncompress the MNIST dataset automatically if needed,\nwe can pass the URL's for the compressed files (gzip) to the API call as is.\n\nPlease check the official [documentation](https://www.tensorflow.org/io) for more\ndetailed and interesting usages of the package.\n\nInstallation\n\nPython Package\n\nThe `tensorflow-io` Python package can be installed with pip directly using:\n\nPeople who are a little more adventurous can also try our nightly binaries:\n\nTo ensure you have a version of TensorFlow that is compatible with TensorFlow-IO,\nyou can specify the `tensorflow` extra requirement during install:\n\nSimilar extras exist for the `tensorflow-gpu`, `tensorflow-cpu` and `tensorflow-rocm`\npackages.\n\nDocker Images\n\nIn addition to the pip packages, the docker images can be used to quickly get started.\n\nFor stable builds:\n\nFor nightly builds:\n\nR Package\n\nOnce the `tensorflow-io` Python package has been successfully installed, you\ncan install the development version of the R package from GitHub via the following:\n\nTensorFlow Version Compatibility\n\nTo ensure compatibility with TensorFlow, it is recommended to install a matching\nversion of TensorFlow I/O according to the table below. You can find the list\nof releases [here](https://github.com/tensorflow/io/releases).\n\nTensorFlow I/O Version\n---\n0.37.1\n0.37.0\n0.36.0\n0.35.0\n0.34.0\n0.33.0\n0.32.0\n0.31.0\n0.30.0\n0.29.0\n0.28.0\n0.27.0\n0.26.0\n0.25.0\n0.24.0\n0.23.1\n0.23.0\n0.22.0\n0.21.0\n0.20.0\n0.19.1\n0.19.0\n0.18.0\n0.17.1\n0.17.0\n0.16.0\n0.15.0\n0.14.0\n0.13.0\n0.12.0\n0.11.0\n0.10.0\n0.9.1\n0.9.0\n0.8.1\n0.8.0\n0.7.2\n0.7.1\n0.7.0\n0.6.0\n0.5.0\n0.4.0\n0.3.0\n0.2.0\n0.1.0\n\nPerformance Benchmarking\n\nWe use [github-pages](https://tensorflow.github.io/io/dev/bench/) to document the results of API performance benchmarks. The benchmark job is triggered on every commit to `master` branch and\nfacilitates tracking performance w.r.t commits.\n\nContributing\n\nTensorflow I/O is a community led open source project. As such, the project\ndepends on public contributions, bug-fixes, and documentation. Please see:\n\n- [contribution guidelines](CONTRIBUTING.md) for a guide on how to contribute.\n- [development doc](docs/development.md) for instructions on the development environment setup.\n- [tutorials](docs/tutorials) for a list of tutorial notebooks and instructions on how to write one.\n\nBuild Status and CI\n\nBuild\n---\n\nBecause of manylinux2010 requirement, TensorFlow I/O is built with\nUbuntu:16.04 + Developer Toolset 7 (GCC 7.3) on Linux. Configuration\nwith Ubuntu 16.04 with Developer Toolset 7 is not exactly straightforward.\nIf the system have docker installed, then the following command\nwill automatically build manylinux2010 compatible whl package:"}, {"name": "tensorflow-io", "tags": ["data", "math", "ml", "web"], "summary": "TensorFlow IO", "text": "It takes some time to build, but once complete, there will be python\n`3.5`, `3.6`, `3.7` compatible whl packages available in `wheelhouse`\ndirectory.\n\nOn macOS, the same command could be used. However, the script expects `python` in shell\nand will only generate a whl package that matches the version of `python` in shell. If\nyou want to build a whl package for a specific python then you have to alias this version\nof python to `python` in shell. See [.github/workflows/build.yml](.github/workflows/build.yml)\nAuditwheel step for instructions how to do that.\n\nNote the above command is also the command we use when releasing packages for Linux and macOS.\n\nTensorFlow I/O uses both GitHub Workflows and Google CI (Kokoro) for continuous integration.\nGitHub Workflows is used for macOS build and test. Kokoro is used for Linux build and test.\nAgain, because of the manylinux2010 requirement, on Linux whl packages are always\nbuilt with Ubuntu 16.04 + Developer Toolset 7. Tests are done on a variatiy of systems\nwith different python3 versions to ensure a good coverage:\n\nPython\n-------\n2.7\n3.7\n3.8\n\nTensorFlow I/O has integrations with many systems and cloud vendors such as\nPrometheus, Apache Kafka, Apache Ignite, Google Cloud PubSub, AWS Kinesis,\nMicrosoft Azure Storage, Alibaba Cloud OSS etc.\n\nWe tried our best to test against those systems in our continuous integration\nwhenever possible. Some tests such as Prometheus, Kafka, and Ignite\nare done with live systems, meaning we install Prometheus/Kafka/Ignite on CI machine before\nthe test is run. Some tests such as Kinesis, PubSub, and Azure Storage are done\nthrough official or non-official emulators. Offline tests are also performed whenever\npossible, though systems covered through offine tests may not have the same\nlevel of coverage as live systems or emulators.\n\nLive System\n-------\nApache Kafka\nApache Ignite\nPrometheus\nGoogle PubSub\nAzure Storage\nAWS Kinesis\nAlibaba Cloud OSS\nGoogle BigTable/BigQuery\nElasticsearch (experimental)\nMongoDB (experimental)\n\nReferences for emulators:\n\nCommunity\n\n* SIG IO [Google Group](https://groups.google.com/a/tensorflow.org/forum/#!forum/io) and mailing list: [io@tensorflow.org](io@tensorflow.org)\n* SIG IO [Monthly Meeting Notes](https://docs.google.com/document/d/1CB51yJxns5WA4Ylv89D-a5qReiGTC0GYum6DU-9nKGo/edit)\n* Gitter room: [tensorflow/sig-io](https://gitter.im/tensorflow/sig-io)\n\nAdditional Information\n\n* [Streaming Machine Learning with Tiered Storage and Without a Data Lake](https://www.confluent.io/blog/streaming-machine-learning-with-tiered-storage/) - [Kai Waehner](https://github.com/kaiwaehner)\n* [TensorFlow with Apache Arrow Datasets](https://medium.com/tensorflow/tensorflow-with-apache-arrow-datasets-cdbcfe80a59f) - [Bryan Cutler](https://github.com/BryanCutler)\n* [How to build a custom Dataset for Tensorflow](https://towardsdatascience.com/how-to-build-a-custom-dataset-for-tensorflow-1fe3967544d8) - [Ivelin Ivanov](https://github.com/ivelin)\n* [TensorFlow on Apache Ignite](https://medium.com/tensorflow/tensorflow-on-apache-ignite-99f1fc60efeb) - [Anton Dmitriev](https://github.com/dmitrievanthony)\n\nLicense\n\n[Apache License 2.0](LICENSE)"}, {"name": "tensorflow-io", "tags": ["data", "math", "ml", "web"], "summary": "TensorFlow IO", "text": "This library is used to provide additional file systems and formats for TensorFlow, allowing developers to work with various data storage options. This includes enabling support for external file systems and formats through a unified API, simplifying data processing and integration within TensorFlow applications."}, {"name": "tensorflow-metadata", "tags": ["data", "math", "ml"], "summary": "Library and standards for schema and statistics.", "text": "TensorFlow Metadata\n\n(https://github.com/tensorflow/metadata)\n\nTensorFlow Metadata provides standard representations for metadata that are\nuseful when training machine learning models with TensorFlow.\n\nThe metadata serialization formats include:\n\n* A schema describing tabular data (e.g., tf.Examples).\n* A collection of summary statistics over such datasets.\n* A problem statement quantifying the objectives of a model.\n\nThe metadata may be produced by hand or automatically during input data\nanalysis, and may be consumed for data validation, exploration, and\ntransformation."}, {"name": "tensorflow-metadata", "tags": ["data", "math", "ml"], "summary": "Library and standards for schema and statistics.", "text": "This library is used to provide standard representations for metadata that are useful when training machine learning models with TensorFlow. It enables developers to define schemas, collect summary statistics, and quantify objectives of a model, facilitating data validation, exploration, and transformation."}, {"name": "tensorflow-probability", "tags": ["data", "math", "ml", "ui", "web"], "summary": "Probabilistic modeling and statistical inference in TensorFlow", "text": "TensorFlow Probability\n\nTensorFlow Probability is a library for probabilistic reasoning and statistical\nanalysis in TensorFlow. As part of the TensorFlow ecosystem, TensorFlow\nProbability provides integration of probabilistic methods with deep networks,\ngradient-based inference via automatic differentiation, and scalability to\nlarge datasets and models via hardware acceleration (e.g., GPUs) and distributed\ncomputation.\n\n__TFP also works as \"Tensor-friendly Probability\" in pure JAX!__:\n`from tensorflow_probability.substrates import jax as tfp` --\nLearn more [here](https://www.tensorflow.org/probability/examples/TensorFlow_Probability_on_JAX).\n\nOur probabilistic machine learning tools are structured as follows.\n\n__Layer 0: TensorFlow.__ Numerical operations. In particular, the LinearOperator\nclass enables matrix-free implementations that can exploit special structure\n(diagonal, low-rank, etc.) for efficient computation. It is built and maintained\nby the TensorFlow Probability team and is now part of\n[`tf.linalg`](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/python/ops/linalg)\nin core TF.\n\n__Layer 1: Statistical Building Blocks__\n\n* Distributions ([`tfp.distributions`](https://github.com/tensorflow/probability/tree/main/tensorflow_probability/python/distributions)):\n  A large collection of probability\n  distributions and related statistics with batch and\n  [broadcasting](https://docs.scipy.org/doc/numpy/user/basics.broadcasting.html)\n  semantics. See the\n  [Distributions Tutorial](https://github.com/tensorflow/probability/blob/main/tensorflow_probability/examples/jupyter_notebooks/TensorFlow_Distributions_Tutorial.ipynb).\n* Bijectors ([`tfp.bijectors`](https://github.com/tensorflow/probability/tree/main/tensorflow_probability/python/bijectors)):\n  Reversible and composable transformations of random variables. Bijectors\n  provide a rich class of transformed distributions, from classical examples\n  like the\n  [log-normal distribution](https://en.wikipedia.org/wiki/Log-normal_distribution)\n  to sophisticated deep learning models such as\n  [masked autoregressive flows](https://arxiv.org/abs/1705.07057).\n\n__Layer 2: Model Building__\n\n* Joint Distributions (e.g., [`tfp.distributions.JointDistributionSequential`](https://github.com/tensorflow/probability/tree/main/tensorflow_probability/python/distributions/joint_distribution_sequential.py)):\n* Probabilistic Layers ([`tfp.layers`](https://github.com/tensorflow/probability/tree/main/tensorflow_probability/python/layers)):\n  Neural network layers with uncertainty over the functions they represent,\n  extending TensorFlow Layers.\n\n__Layer 3: Probabilistic Inference__\n\n* Markov chain Monte Carlo ([`tfp.mcmc`](https://github.com/tensorflow/probability/tree/main/tensorflow_probability/python/mcmc)):\n  Algorithms for approximating integrals via sampling. Includes\n  [Hamiltonian Monte Carlo](https://en.wikipedia.org/wiki/Hamiltonian_Monte_Carlo),\n  random-walk Metropolis-Hastings, and the ability to build custom transition\n  kernels.\n* Variational Inference ([`tfp.vi`](https://github.com/tensorflow/probability/tree/main/tensorflow_probability/python/vi)):\n  Algorithms for approximating integrals via optimization.\n* Optimizers ([`tfp.optimizer`](https://github.com/tensorflow/probability/tree/main/tensorflow_probability/python/optimizer)):\n  Stochastic optimization methods, extending TensorFlow Optimizers. Includes\n  [Stochastic Gradient Langevin Dynamics](http://www.icml-2011.org/papers/398_icmlpaper.pdf).\n* Monte Carlo ([`tfp.monte_carlo`](https://github.com/tensorflow/probability/blob/main/tensorflow_probability/python/monte_carlo)):\n  Tools for computing Monte Carlo expectations.\n\nTensorFlow Probability is under active development. Interfaces may change at any\ntime.\n\nExamples\n\nSee [`tensorflow_probability/examples/`](https://github.com/tensorflow/probability/tree/main/tensorflow_probability/examples/)\nfor end-to-end examples. It includes tutorial notebooks such as:\n\n* [Linear Mixed Effects Models](https://github.com/tensorflow/probability/blob/main/tensorflow_probability/examples/jupyter_notebooks/Linear_Mixed_Effects_Models.ipynb).\n  A hierarchical linear model for sharing statistical strength across examples.\n* [Eight Schools](https://github.com/tensorflow/probability/blob/main/tensorflow_probability/examples/jupyter_notebooks/Eight_Schools.ipynb).\n  A hierarchical normal model for exchangeable treatment effects.\n* [Hierarchical Linear Models](https://github.com/tensorflow/probability/blob/main/tensorflow_probability/examples/jupyter_notebooks/HLM_TFP_R_Stan.ipynb).\n  Hierarchical linear models compared among TensorFlow Probability, R, and Stan.\n* [Bayesian Gaussian Mixture Models](https://github.com/tensorflow/probability/blob/main/tensorflow_probability/examples/jupyter_notebooks/Bayesian_Gaussian_Mixture_Model.ipynb).\n  Clustering with a probabilistic generative model.\n* [Probabilistic Principal Components Analysis](https://github.com/tensorflow/probability/blob/main/tensorflow_probability/examples/jupyter_notebooks/Probabilistic_PCA.ipynb).\n  Dimensionality reduction with latent variables.\n* [Gaussian Copulas](https://github.com/tensorflow/probability/blob/main/tensorflow_probability/examples/jupyter_notebooks/Gaussian_Copula.ipynb).\n  Probability distributions for capturing dependence across random variables.\n* [TensorFlow Distributions: A Gentle Introduction](https://github.com/tensorflow/probability/blob/main/tensorflow_probability/examples/jupyter_notebooks/TensorFlow_Distributions_Tutorial.ipynb).\n  Introduction to TensorFlow Distributions.\n* [Understanding TensorFlow Distributions Shapes](https://github.com/tensorflow/probability/blob/main/tensorflow_probability/examples/jupyter_notebooks/Understanding_TensorFlow_Distributions_Shapes.ipynb).\n  How to distinguish between samples, batches, and events for arbitrarily shaped\n  probabilistic computations.\n* [TensorFlow Probability Case Study: Covariance Estimation](https://github.com/tensorflow/probability/blob/main/tensorflow_probability/examples/jupyter_notebooks/TensorFlow_Probability_Case_Study_Covariance_Estimation.ipynb).\n  A user's case study in applying TensorFlow Probability to estimate covariances.\n\nIt also includes example scripts such as:\n\nRepresentation learning with a latent code and variational inference.\n* [Vector-Quantized Autoencoder](https://github.com/tensorflow/probability/tree/main/tensorflow_probability/examples/vq_vae.py).\n  Discrete representation learning with vector quantization.\n* [Disentangled Sequential Variational Autoencoder](https://github.com/tensorflow/probability/tree/main/tensorflow_probability/examples/disentangled_vae.py)\n  Disentangled representation learning over sequences with variational inference.\n* [Bayesian Neural Networks](https://github.com/tensorflow/probability/tree/main/tensorflow_probability/examples/bayesian_neural_network.py).\n  Neural networks with uncertainty over their weights.\n* [Bayesian Logistic Regression](https://github.com/tensorflow/probability/tree/main/tensorflow_probability/examples/logistic_regression.py).\n  Bayesian inference for binary classification.\n\nInstallation\n\nFor additional details on installing TensorFlow, guidance installing\nprerequisites, and (optionally) setting up virtual environments, see the\n[TensorFlow installation guide](https://www.tensorflow.org/install).\n\nStable Builds"}, {"name": "tensorflow-probability", "tags": ["data", "math", "ml", "ui", "web"], "summary": "Probabilistic modeling and statistical inference in TensorFlow", "text": "To install the latest stable version, run the following:\n\nFor CPU-only usage (and a smaller install), install with `tensorflow-cpu`.\n\nTo use a pre-2.0 version of TensorFlow, run:\n\nNote: Since [TensorFlow](https://www.tensorflow.org/install) is *not* included\nas a dependency of the TensorFlow Probability package (in `setup.py`), you must\nexplicitly install the TensorFlow package (`tensorflow` or `tensorflow-cpu`).\nThis allows us to maintain one package instead of separate packages for CPU and\nGPU-enabled TensorFlow. See the\n[TFP release notes](https://github.com/tensorflow/probability/releases) for more\ndetails about dependencies between TensorFlow and TensorFlow Probability.\n\nNightly Builds\n\nThere are also nightly builds of TensorFlow Probability under the pip package\n`tfp-nightly`, which depends on one of `tf-nightly` or `tf-nightly-cpu`.\nNightly builds include newer features, but may be less stable than the\nversioned releases. Both stable and nightly docs are available\n[here](https://www.tensorflow.org/probability/api_docs/python/tfp?version=nightly).\n\nInstalling from Source\n\nYou can also install from source. This requires the [Bazel](\nthe nightly build of TensorFlow (`tf-nightly`) before trying to build\nTensorFlow Probability from source. The most recent version of Bazel that TFP\ncurrently supports is 6.4.0; support for 7.0.0+ is WIP.\n\nCommunity\n\nAs part of TensorFlow, we're committed to fostering an open and welcoming\nenvironment.\n\n* [Stack Overflow](https://stackoverflow.com/questions/tagged/tensorflow): Ask\n  or answer technical questions.\n* [GitHub](https://github.com/tensorflow/probability/issues): Report bugs or\n  make feature requests.\n* [TensorFlow Blog](https://blog.tensorflow.org/): Stay up to date on content\n  from the TensorFlow team and best articles from the community.\n* [Youtube Channel](http://youtube.com/tensorflow/): Follow TensorFlow shows.\n* [tfprobability@tensorflow.org](https://groups.google.com/a/tensorflow.org/forum/#!forum/tfprobability):\n  Open mailing list for discussion and questions.\n\nSee the [TensorFlow Community](https://www.tensorflow.org/community/) page for\nmore details. Check out our latest publicity here:\n\n+ [Coffee with a Googler: Probabilistic Machine Learning in TensorFlow](\n+ [Introducing TensorFlow Probability](\n\nContributing\n\nWe're eager to collaborate with you! See [`CONTRIBUTING.md`](CONTRIBUTING.md)\nfor a guide on how to contribute. This project adheres to TensorFlow's\n[code of conduct](CODE_OF_CONDUCT.md). By participating, you are expected to\nuphold this code.\n\nReferences\n\nIf you use TensorFlow Probability in a paper, please cite:\n\n+ _TensorFlow Distributions._ Joshua V. Dillon, Ian Langmore, Dustin Tran,\nEugene Brevdo, Srinivas Vasudevan, Dave Moore, Brian Patton, Alex Alemi, Matt\nHoffman, Rif A. Saurous.\n[arXiv preprint arXiv:1711.10604, 2017](https://arxiv.org/abs/1711.10604).\n\n(We're aware there's a lot more to TensorFlow Probability than Distributions, but the Distributions paper lays out our vision and is a fine thing to cite for now.)"}, {"name": "tensorflow-probability", "tags": ["data", "math", "ml", "ui", "web"], "summary": "Probabilistic modeling and statistical inference in TensorFlow", "text": "This library is used to enable probabilistic modeling and statistical inference in TensorFlow, providing a suite of tools for scalable and efficient probabilistic machine learning tasks. With TensorFlow Probability, developers can integrate probabilistic methods with deep networks and leverage gradient-based inference, large-scale hardware acceleration, and distributed computation."}, {"name": "tensorflow-serving-api", "tags": ["math", "ml"], "summary": "TensorFlow Serving Python API.", "text": "TensorFlow Serving is a flexible, high-performance serving system for machine\nlearning models, designed for production environments.TensorFlow Serving makes\nit easy to deploy new algorithms and experiments, while keeping the same server\narchitecture and APIs. TensorFlow Serving provides out-of-the-box integration\nwith TensorFlow models, but can be easily extended to serve other types of\nmodels and data.\n\nThis package contains the TensorFlow Serving Python APIs."}, {"name": "tensorflow-serving-api", "tags": ["math", "ml"], "summary": "TensorFlow Serving Python API.", "text": "This library is used to deploy and manage machine learning models in production environments with ease and high performance. It allows developers to integrate their TensorFlow models with a flexible and scalable serving system."}, {"name": "tensorflow-text", "tags": ["math", "ml"], "summary": "TF.Text is a TensorFlow library of text related ops, modules, and subgraphs.", "text": "TF.Text is a TensorFlow library of text related ops, modules, and subgraphs. The\nlibrary can perform the preprocessing regularly required by text-based models,\nand includes other features useful for sequence modeling not provided by core\nTensorFlow.\n\nSee the README on GitHub for further documentation."}, {"name": "tensorflow-text", "tags": ["math", "ml"], "summary": "TF.Text is a TensorFlow library of text related ops, modules, and subgraphs.", "text": "This library is used to simplify text preprocessing tasks required by text-based models in TensorFlow, and provide additional features for sequence modeling. It enables developers to efficiently manipulate and process text data within their TensorFlow applications."}, {"name": "tensorflow", "tags": ["math", "ml"], "summary": "TensorFlow is an open source machine learning framework for everyone.", "text": "TensorFlow is an open source software library for high performance numerical\ncomputation. Its flexible architecture allows easy deployment of computation\nacross a variety of platforms (CPUs, GPUs, TPUs), and from desktops to clusters\nof servers to mobile and edge devices.\n\nOriginally developed by researchers and engineers from the Google Brain team\nwithin Google's AI organization, it comes with strong support for machine\nlearning and deep learning and the flexible numerical computation core is used\nacross many other scientific domains. TensorFlow is licensed under [Apache\n2.0](https://github.com/tensorflow/tensorflow/blob/master/LICENSE)."}, {"name": "tensorflow", "tags": ["math", "ml"], "summary": "TensorFlow is an open source machine learning framework for everyone.", "text": "This library is used to enable high-performance numerical computations across various platforms, facilitating easy deployment of machine learning and deep learning models on desktops, servers, mobile devices, or edge devices. With TensorFlow, developers can leverage its flexible architecture for efficient computation in a wide range of scientific domains, including AI, machine learning, and deep learning applications."}, {"name": "tensorflowjs", "tags": ["math", "ml"], "summary": null, "text": "tensorflowjs: The Python Package for TensorFlow.js\n\nThe **tensorflowjs** pip package contains libraries and tools for\n[TensorFlow.js](https://js.tensorflow.org).\n\nUse following command to install the library with support of interactive CLI:\n\nThen, run the following to see a list of CLI options\n\nor, use the wizard\n\nAlternatively, run the converter via its Bazel target. This must be run from withing the tfjs repo:\n\nDevelopment\n\nThe python tests are run with Bazel.\n\nAlternatively, run `yarn run-python-tests` to run the above command.\n\nTo debug a specific test case, use the `--test_filter` option. For example,\n\nInteractive debugging with breakpoints is supported by `debugpy` in VSCode.\nTo enable debugging, put this code at the top of the test file you want to\ndebug.\n\nYou may also need to add the following dependency to the test target in the\nBazel `BUILD` file if it's not already present.\n\nThen, run the test with `bazel run --config=debugpy` and connect\nthe VSCode debugger by selecting the `Python: Attach (Converter)` option."}, {"name": "tensorflowjs", "tags": ["math", "ml"], "summary": null, "text": "This library is used to convert TensorFlow models for use in browser-based applications, providing a seamless integration between Python and JavaScript environments. Developers can leverage tensorflowjs to deploy machine learning models directly within web pages or hybrid mobile apps with minimal modification."}, {"name": "textdistance", "tags": ["dev", "math", "ui", "web"], "summary": "Compute distance between the two texts.", "text": "TextDistance\n\n(https://travis-ci.org/life4/textdistance) (https://pypi.python.org/pypi/textdistance) (https://pypi.python.org/pypi/textdistance) (LICENSE)\n\n**TextDistance** -- python library for comparing distance between two or more sequences by many algorithms.\n\nFeatures:\n\n- 30+ algorithms\n- Pure python implementation\n- Simple usage\n- More than two sequences comparing\n- Some algorithms have more than one implementation in one class.\n- Optional numpy usage for maximum speed.\n\nAlgorithms\n\nEdit based\n\nAlgorithm\n-------------------------------------------------------------------------------------------\n[Hamming](https://en.wikipedia.org/wiki/Hamming_distance)\n[MLIPNS](http://www.sial.iias.spb.su/files/386-386-1-PB.pdf)\n[Levenshtein](https://en.wikipedia.org/wiki/Levenshtein_distance)\n[Damerau-Levenshtein](https://en.wikipedia.org/wiki/Damerau%E2%80%93Levenshtein_distance)\n[Jaro-Winkler](https://en.wikipedia.org/wiki/Jaro%E2%80%93Winkler_distance)\n[Strcmp95](http://cpansearch.perl.org/src/SCW/Text-JaroWinkler-0.1/strcmp95.c)\n[Needleman-Wunsch](https://en.wikipedia.org/wiki/Needleman%E2%80%93Wunsch_algorithm)\n[Gotoh](http://bioinfo.ict.ac.cn/~dbu/AlgorithmCourses/Lectures/LOA/Lec6-Sequence-Alignment-Affine-Gaps-Gotoh1982.pdf)\n[Smith-Waterman](https://en.wikipedia.org/wiki/Smith%E2%80%93Waterman_algorithm)\n\nToken based\n\nAlgorithm\n-------------------------------------------------------------------------------------------\n[Jaccard index](https://en.wikipedia.org/wiki/Jaccard_index)\n[S\u00f8rensen\u2013Dice coefficient](https://en.wikipedia.org/wiki/S%C3%B8rensen%E2%80%93Dice_coefficient)\n[Tversky index](https://en.wikipedia.org/wiki/Tversky_index)\n[Overlap coefficient](https://en.wikipedia.org/wiki/Overlap_coefficient)\n[Tanimoto distance](https://en.wikipedia.org/wiki/Jaccard_index#Tanimoto_similarity_and_distance)\n[Cosine similarity](https://en.wikipedia.org/wiki/Cosine_similarity)\n[Monge-Elkan](https://www.academia.edu/200314/Generalized_Monge-Elkan_Method_for_Approximate_Text_String_Comparison)\n[Bag distance](https://github.com/Yomguithereal/talisman/blob/master/src/metrics/bag.js)\n\nSequence based\n\nAlgorithm\n-----------\n[longest common subsequence similarity](https://en.wikipedia.org/wiki/Longest_common_subsequence_problem)\n[longest common substring similarity](https://docs.python.org/2/library/difflib.html#difflib.SequenceMatcher)\n[Ratcliff-Obershelp similarity](https://en.wikipedia.org/wiki/Gestalt_Pattern_Matching)\n\nCompression based\n\n[Normalized compression distance](https://en.wikipedia.org/wiki/Normalized_compression_distance#Normalized_compression_distance) with different compression algorithms.\n\nClassic compression algorithms:\n\nAlgorithm\n----------------------------------------------------------------------------\n[Arithmetic coding](https://en.wikipedia.org/wiki/Arithmetic_coding)\n[RLE](https://en.wikipedia.org/wiki/Run-length_encoding)\n[BWT RLE](https://en.wikipedia.org/wiki/Burrows%E2%80%93Wheeler_transform)\n\nNormal compression algorithms:\n\nAlgorithm\n----------------------------------------------------------------------------\nSquare Root\n[Entropy](https://en.wikipedia.org/wiki/Entropy_(information_theory))\n\nWork in progress algorithms that compare two strings as array of bits:\n\nAlgorithm\n--------------------------------------------\n[BZ2](https://en.wikipedia.org/wiki/Bzip2)\n[LZMA](https://en.wikipedia.org/wiki/LZMA)\n[ZLib](https://en.wikipedia.org/wiki/Zlib)\n\nSee [blog post](https://articles.life4web.ru/other/ncd/) for more details about NCD.\n\nPhonetic\n\nAlgorithm\n------------------------------------------------------------------------------\n[MRA](https://en.wikipedia.org/wiki/Match_rating_approach)\n[Editex](https://anhaidgroup.github.io/py_stringmatching/v0.3.x/Editex.html)\n\nSimple\n\nAlgorithm\n---------------------\nPrefix similarity\nPostfix similarity\nLength distance\nIdentity similarity\nMatrix similarity\n\nInstallation\n\nStable\n\nOnly pure python implementation:\n\nWith extra libraries for maximum speed:\n\nWith all libraries (required for [benchmarking](#benchmarks) and [testing](#running-tests)):\n\nWith algorithm specific extras:\n\nAlgorithms with available extras: `DamerauLevenshtein`, `Hamming`, `Jaro`, `JaroWinkler`, `Levenshtein`.\n\nDev\n\nVia pip:\n\nOr clone repo and install with some extras:\n\nUsage\n\nAll algorithms have 2 interfaces:\n\n1. Class with algorithm-specific params for customizing.\n1. Class instance with default params for quick and simple usage.\n\nAll algorithms have some common methods:\n\n1. `.distance(*sequences)` -- calculate distance between sequences.\n1. `.similarity(*sequences)` -- calculate similarity for sequences.\n1. `.maximum(*sequences)` -- maximum possible value for distance and similarity. For any sequence: `distance + similarity == maximum`.\n1. `.normalized_distance(*sequences)` -- normalized distance between sequences. The return value is a float between 0 and 1, where 0 means equal, and 1 totally different.\n1. `.normalized_similarity(*sequences)` -- normalized similarity for sequences. The return value is a float between 0 and 1, where 0 means totally different, and 1 equal.\n\nMost common init arguments:\n\n1. `qval` -- q-value for split sequences into q-grams. Possible values:\n1. `as_set` -- for token-based algorithms:\n\nExamples\n\nFor example, [Hamming distance](https://en.wikipedia.org/wiki/Hamming_distance):\n\nAny other algorithms have same interface.\n\nArticles\n\nA few articles with examples how to use textdistance in the real world:\n\nExtra libraries\n\nFor main algorithms textdistance try to call known external libraries (fastest first) if available (installed in your system) and possible (this implementation can compare this type of sequences). [Install](#installation) textdistance with extras for this feature.\n\nYou can disable this by passing `external=False` argument on init:\n\nSupported libraries:\n\n1. [Distance](https://github.com/doukremt/distance)\n1. [jellyfish](https://github.com/jamesturk/jellyfish)\n1. [py_stringmatching](https://github.com/anhaidgroup/py_stringmatching)\n1. [pylev](https://github.com/toastdriven/pylev)\n1. [Levenshtein](https://github.com/maxbachmann/Levenshtein)\n1. [pyxDamerauLevenshtein](https://github.com/gfairchild/pyxDamerauLevenshtein)\n\nAlgorithms:\n\n1. DamerauLevenshtein\n1. Hamming\n1. Jaro\n1. JaroWinkler\n1. Levenshtein\n\nBenchmarks\n\nWithout extras installation:\n\nalgorithm\n--------------------\nDamerauLevenshtein\nDamerauLevenshtein\nDamerauLevenshtein\nDamerauLevenshtein\nHamming\nHamming\nHamming\nHamming\nHamming\nJaro\nJaro\nJaro\nJaroWinkler\nJaroWinkler\nJaroWinkler\nLevenshtein\nLevenshtein\nLevenshtein\nLevenshtein\nLevenshtein\nLevenshtein\n\nTotal: 24 libs.\n\nYeah, so slow. Use TextDistance on production only with extras.\n\nTextdistance use benchmark's results for algorithm's optimization and try to call fastest external lib first (if possible).\n\nYou can run benchmark manually on your system:\n\nTextDistance show benchmarks results table for your system and save libraries priorities into `libraries.json` file in TextDistance's folder. This file will be used by textdistance for calling fastest algorithm implementation. Default [libraries.json](textdistance/libraries.json) already included in package.\n\nRunning tests\n\nAll you need is [task](https://taskfile.dev/). See [Taskfile.yml](./Taskfile.yml) for the list of available commands. For example, to run tests including third-party libraries usage, execute `task pytest-external:run`.\n\nContributing\n\nPRs are welcome!\n\n- Found a bug? Fix it!\n- Want to add more algorithms? Sure! Just make it with the same interface as other algorithms in the lib and add some tests.\n- Can make something faster? Great! Just avoid external dependencies and remember that everything should work not only with strings.\n- Something else that do you think is good? Do it! Just make sure that CI passes and everything from the README is still applicable (interface, features, and so on).\n- Have no time to code? Tell your friends and subscribers about `textdistance`. More users, more contributions, more amazing features.\n\nThank you :heart:"}, {"name": "textdistance", "tags": ["dev", "math", "ui", "web"], "summary": "Compute distance between the two texts.", "text": "This library is used to compute distance between two or more sequences by implementing over 30 different algorithms for text comparison. It allows developers to easily compare and measure the similarity or dissimilarity of multiple strings using various metrics, such as Hamming, Levenshtein, and Damerau distances."}, {"name": "tf-keras-nightly", "tags": ["math", "ml"], "summary": "Deep learning for humans.", "text": "TF-Keras is a deep learning API written in Python,\nrunning on top of the machine learning platform TensorFlow.\n\nIt was developed with a focus on enabling fast experimentation and\nproviding a delightful developer experience.\nThe purpose of TF-Keras is to give an *unfair advantage* to any developer\nlooking to ship ML-powered apps."}, {"name": "tf-keras-nightly", "tags": ["math", "ml"], "summary": "Deep learning for humans.", "text": "This library is used to quickly experiment and develop deep learning models with a streamlined Python API, leveraging the power of TensorFlow. With TF-Keras, developers can gain a significant edge in building and shipping machine learning-powered applications."}, {"name": "tf-keras", "tags": ["math", "ml"], "summary": "Deep learning for humans.", "text": "TF-Keras is a deep learning API written in Python,\nrunning on top of the machine learning platform TensorFlow.\n\nIt was developed with a focus on enabling fast experimentation and\nproviding a delightful developer experience.\nThe purpose of TF-Keras is to give an *unfair advantage* to any developer\nlooking to ship ML-powered apps."}, {"name": "tf-keras", "tags": ["math", "ml"], "summary": "Deep learning for humans.", "text": "This library is used to enable rapid development and deployment of deep learning models using TensorFlow, allowing developers to focus on experimentation and innovation. With tf-keras, developers can leverage its simplicity and flexibility to build and deploy high-quality machine learning applications quickly and efficiently."}, {"name": "thinc", "tags": ["dev", "math", "ui", "web"], "summary": "A refreshing functional take on deep learning, compatible with your favorite libraries", "text": "Thinc: A refreshing functional take on deep learning, compatible with your favorite libraries\n\nFrom the makers of [spaCy](https://spacy.io) and [Prodigy](https://prodi.gy)\n\n[Thinc](https://thinc.ai) is a **lightweight deep learning library** that offers\nan elegant, type-checked, functional-programming API for **composing models**,\nwith support for layers defined in other frameworks such as **PyTorch,\nTensorFlow and MXNet**. You can use Thinc as an interface layer, a standalone\ntoolkit or a flexible way to develop new models. Previous versions of Thinc have\nbeen running quietly in production in thousands of companies, via both\n[spaCy](https://spacy.io) and [Prodigy](https://prodi.gy). We wrote the new\nversion to let users **compose, configure and deploy custom models** built with\ntheir favorite framework.\n\n(https://github.com/explosion/thinc/actions/workflows/tests.yml)\n(https://github.com/explosion/thinc/releases)\n(https://pypi.python.org/pypi/thinc)\n(https://anaconda.org/conda-forge/thinc)\n(https://github.com/explosion/wheelwright/releases)\n(https://github.com/ambv/black)\n[![Open demo in Colab][colab]][intro_to_thinc_colab]\n\nFeatures\n\n- **Type-check** your model definitions with custom types and\n  [`mypy`](https://mypy.readthedocs.io/en/latest/) plugin.\n- Wrap **PyTorch**, **TensorFlow** and **MXNet** models for use in your network.\n- Concise **functional-programming** approach to model definition, using\n  composition rather than inheritance.\n- Optional custom infix notation via **operator overloading**.\n- Integrated **config system** to describe trees of objects and hyperparameters.\n- Choice of **extensible backends**.\n- **[Read more &rarr;](https://thinc.ai/docs)**\n\nQuickstart\n\nThinc is compatible with **Python 3.6+** and runs on **Linux**, **macOS** and\n**Windows**. The latest releases with binary wheels are available from\n[pip](https://pypi.python.org/pypi/thinc). Before you install Thinc and its\ndependencies, make sure that your `pip`, `setuptools` and `wheel` are up to\ndate. For the most recent releases, pip 19.3 or newer is recommended.\n\nSee the [extended installation docs](https://thinc.ai/docs/install#extended) for\ndetails on optional dependencies for different backends and GPU. You might also\nwant to\n[set up static type checking](https://thinc.ai/docs/install#type-checking) to\ntake advantage of Thinc's type system.\n\n> \u26a0\ufe0f If you have installed PyTorch and you are using Python 3.7+, uninstall the\n> package `dataclasses` with `pip uninstall dataclasses`, since it may have been\n> installed by PyTorch and is incompatible with Python 3.7+.\n\nSelected examples and notebooks\n\nAlso see the [`/examples`](examples) directory and\n[usage documentation](https://thinc.ai/docs) for more examples. Most examples\nare Jupyter notebooks \u2013 to launch them on\n[Google Colab](https://colab.research.google.com) (with GPU support!) click on\nthe button next to the notebook name.\n\nNotebook\n---------------------------------------------------------------------------------------------------------------------\n[`intro_to_thinc`][intro_to_thinc][![Open in Colab][colab]][intro_to_thinc_colab]\n[`transformers_tagger_bert`][transformers_tagger_bert][![Open in Colab][colab]][transformers_tagger_bert_colab]\n[`pos_tagger_basic_cnn`][pos_tagger_basic_cnn][![Open in Colab][colab]][pos_tagger_basic_cnn_colab]\n[`parallel_training_ray`][parallel_training_ray][.\n\n**[View more &rarr;](examples)**\n\n[colab]:\n\n[intro_to_thinc]: examples/00_intro_to_thinc.ipynb\n[intro_to_thinc_colab]:\n[transformers_tagger_bert]: examples/02_transformers_tagger_bert.ipynb\n[transformers_tagger_bert_colab]:\n[pos_tagger_basic_cnn]: examples/03_pos_tagger_basic_cnn.ipynb\n[pos_tagger_basic_cnn_colab]:\n[parallel_training_ray]: examples/04_parallel_training_ray.ipynb\n[parallel_training_ray_colab]:\n\nDocumentation & usage guides\n\nDocumentation\n---------------------------------------------------------------------------------\n[Introduction](https://thinc.ai/docs)\n[Concept & Design](https://thinc.ai/docs/concept)\n[Defining and using models](https://thinc.ai/docs/usage-models)\n[Configuration system](https://thinc.ai/docs/usage-config)\n[Integrating PyTorch, TensorFlow & MXNet](https://thinc.ai/docs/usage-frameworks)\n[Layers API](https://thinc.ai/docs/api-layers)\n[Type Checking](https://thinc.ai/docs/usage-type-checking)\n\nWhat's where\n\nModule\n-----------------------------------------\n[`thinc.api`](thinc/api.py)\n[`thinc.types`](thinc/types.py)\n[`thinc.model`](thinc/model.py)\n[`thinc.layers`](thinc/layers)\n[`thinc.shims`](thinc/shims)\n[`thinc.loss`](thinc/loss.py)\n[`thinc.optimizers`](thinc/optimizers.py)\n[`thinc.schedules`](thinc/schedules.py)\n[`thinc.backends`](thinc/backends)\n[`thinc.config`](thinc/config.py)\n[`thinc.util`](thinc/util.py)\n\nDevelopment notes\n\nThinc uses [`black`](https://github.com/psf/black) for auto-formatting,\n[`flake8`](http://flake8.pycqa.org/en/latest/) for linting and\n[`mypy`](https://mypy.readthedocs.io/en/latest/) for type checking. All code is\nwritten compatible with **Python 3.6+**, with type hints wherever possible. See\nthe [type reference](https://thinc.ai/docs/api-types) for more details on\nThinc's custom types.\n\n\u200d\u2640\ufe0f Building Thinc from source\n\nBuilding Thinc from source requires the full dependencies listed in\n[`requirements.txt`](requirements.txt) to be installed. You'll also need a\ncompiler to build the C extensions.\n\nAlternatively, install in editable mode:\n\nOr by setting `PYTHONPATH`:\n\nRunning tests\n\nThinc comes with an [extensive test suite](thinc/tests). The following should\nall pass and not report any warnings or errors:\n\nTo view test coverage, you can run `python -m pytest thinc --cov=thinc`. We aim\nfor a 100% test coverage. This doesn't mean that we meticulously write tests for\nevery single line \u2013 we ignore blocks that are not relevant or difficult to test\nand make sure that the tests execute all code paths."}, {"name": "thinc", "tags": ["dev", "math", "ui", "web"], "summary": "A refreshing functional take on deep learning, compatible with your favorite libraries", "text": "This library is used to compose, configure, and deploy custom deep learning models using a lightweight, type-checked, and functional-programming API. It offers compatibility with popular frameworks such as PyTorch, TensorFlow, and MXNet for seamless integration and model development."}, {"name": "timm", "tags": ["data", "dev", "math", "ml", "ui", "web"], "summary": "PyTorch Image Models", "text": "PyTorch Image Models\n- [What's New](#whats-new)\n- [Introduction](#introduction)\n- [Models](#models)\n- [Features](#features)\n- [Results](#results)\n- [Getting Started (Documentation)](#getting-started-documentation)\n- [Train, Validation, Inference Scripts](#train-validation-inference-scripts)\n- [Awesome PyTorch Resources](#awesome-pytorch-resources)\n- [Licenses](#licenses)\n- [Citing](#citing)\n\nWhat's New\n\nNov 4, 2025\n* Fix LayerScale / LayerScale2d init bug (init values ignored), introduced in 1.0.21. Thanks https://github.com/Ilya-Fradlin\n* Release 1.0.22\n\nOct 31, 2025 \n* Update imagenet & OOD variant result csv files to include a few new models and verify correctness over several torch & timm versions\n* EfficientNet-X and EfficientNet-H B5 model weights added as part of a hparam search for AdamW vs Muon (still iterating on Muon runs)\n\nOct 16-20, 2025\n* Add an impl of the Muon optimizer (based on https://github.com/KellerJordan/Muon) with customizations\n  * extra flexibility and improved handling for conv weights and fallbacks for weight shapes not suited for orthogonalization\n  * small speedup for NS iterations by reducing allocs and using fused (b)add(b)mm ops\n  * by default uses AdamW (or NAdamW if `nesterov=True`) updates if muon not suitable for parameter shape (or excluded via param group flag)\n  * like torch impl, select from several LR scale adjustment fns via `adjust_lr_fn`\n  * select from several NS coefficient presets or specify your own via `ns_coefficients`\n* First 2 steps of 'meta' device model initialization supported\n  * Fix several ops that were breaking creation under 'meta' device context\n  * Add device & dtype factory kwarg support to all models and modules (anything inherting from nn.Module) in `timm`\n* License fields added to pretrained cfgs in code\n* Release 1.0.21\n\nSept 21, 2025\n* Remap DINOv3 ViT weight tags from `lvd_1689m` -> `lvd1689m` to match (same for `sat_493m` -> `sat493m`)\n* Release 1.0.20\n\nSept 17, 2025\n* DINOv3 (https://arxiv.org/abs/2508.10104) ConvNeXt and ViT models added. ConvNeXt models were mapped to existing `timm` model. ViT support done via the EVA base model w/ a new `RotaryEmbeddingDinoV3` to match the DINOv3 specific RoPE impl\n  * HuggingFace Hub: https://huggingface.co/collections/timm/timm-dinov3-68cb08bb0bee365973d52a4d\n* MobileCLIP-2 (https://arxiv.org/abs/2508.20691) vision encoders. New MCI3/MCI4 FastViT variants added and weights mapped to existing FastViT and B, L/14 ViTs.\n* MetaCLIP-2 Worldwide (https://arxiv.org/abs/2507.22062) ViT encoder weights added.\n* SigLIP-2 (https://arxiv.org/abs/2502.14786) NaFlex ViT encoder weights added via timm NaFlexViT model.\n* Misc fixes and contributions\n\nJuly 23, 2025\n* Add `set_input_size()` method to EVA models, used by OpenCLIP 3.0.0 to allow resizing for timm based encoder models.\n* Release 1.0.18, needed for PE-Core S & T models in OpenCLIP 3.0.0\n* Fix small typing issue that broke Python 3.9 compat. 1.0.19 patch release."}, {"name": "timm", "tags": ["data", "dev", "math", "ml", "ui", "web"], "summary": "PyTorch Image Models", "text": "July 21, 2025\n* ROPE support added to NaFlexViT. All models covered by the EVA base (`eva.py`) including EVA, EVA02, Meta PE ViT, `timm` SBB ViT w/ ROPE, and Naver ROPE-ViT can be now loaded in NaFlexViT when `use_naflex=True` passed at model creation time\n* More Meta PE ViT encoders added, including small/tiny variants, lang variants w/ tiling, and more spatial variants.\n* PatchDropout fixed with NaFlexViT and also w/ EVA models (regression after adding Naver ROPE-ViT)\n* Fix XY order with grid_indexing='xy', impacted non-square image use in 'xy' mode (only ROPE-ViT and PE impacted).\n\nJuly 7, 2025\n* MobileNet-v5 backbone tweaks for improved Google Gemma 3n behaviour (to pair with updated official weights)\n  * Add stem bias (zero'd in updated weights, compat break with old weights)\n  * GELU -> GELU (tanh approx). A minor change to be closer to JAX\n* Add two arguments to layer-decay support, a min scale clamp and 'no optimization' scale threshold\n* Add 'Fp32' LayerNorm, RMSNorm, SimpleNorm variants that can be enabled to force computation of norm in float32\n* Some typing, argument cleanup for norm, norm+act layers done with above\n* Support Naver ROPE-ViT (https://github.com/naver-ai/rope-vit) in `eva.py`, add RotaryEmbeddingMixed module for mixed mode, weights on HuggingFace Hub\n\nmodel\n--------------------------------------------------\nvit_large_patch16_rope_mixed_ape_224.naver_in1k\nvit_large_patch16_rope_mixed_224.naver_in1k\nvit_large_patch16_rope_ape_224.naver_in1k\nvit_large_patch16_rope_224.naver_in1k\nvit_base_patch16_rope_mixed_ape_224.naver_in1k\nvit_base_patch16_rope_mixed_224.naver_in1k\nvit_base_patch16_rope_ape_224.naver_in1k\nvit_base_patch16_rope_224.naver_in1k\nvit_small_patch16_rope_224.naver_in1k\nvit_small_patch16_rope_mixed_224.naver_in1k\nvit_small_patch16_rope_ape_224.naver_in1k\nvit_small_patch16_rope_mixed_ape_224.naver_in1k\n* Some cleanup of ROPE modules, helpers, and FX tracing leaf registration\n* Preparing version 1.0.17 release\n\nJune 26, 2025\n* MobileNetV5 backbone (w/ encoder only variant) for [Gemma 3n](https://ai.google.dev/gemma/docs/gemma-3n#parameters) image encoder\n* Version 1.0.16 released\n\nJune 23, 2025\n* Add F.grid_sample based 2D and factorized pos embed resize to NaFlexViT. Faster when lots of different sizes (based on example by https://github.com/stas-sl).\n* Further speed up patch embed resample by replacing vmap with matmul (based on snippet by https://github.com/stas-sl).\n* Add 3 initial native aspect NaFlexViT checkpoints created while testing, ImageNet-1k and 3 different pos embed configs w/ same hparams.\n\nModel\n:---\n[naflexvit_base_patch16_par_gap.e300_s576_in1k](https://hf.co/timm/naflexvit_base_patch16_par_gap.e300_s576_in1k)\n[naflexvit_base_patch16_parfac_gap.e300_s576_in1k](https://hf.co/timm/naflexvit_base_patch16_parfac_gap.e300_s576_in1k)\n[naflexvit_base_patch16_gap.e300_s576_in1k](https://hf.co/timm/naflexvit_base_patch16_gap.e300_s576_in1k)\n* Support gradient checkpointing for `forward_intermediates` and fix some checkpointing bugs. Thanks https://github.com/brianhou0208\n* Add 'corrected weight decay' (https://arxiv.org/abs/2506.02285) as option to AdamW (legacy), Adopt, Kron, Adafactor (BV), Lamb, LaProp, Lion, NadamW, RmsPropTF, SGDW optimizers\n* Switch PE (perception encoder) ViT models to use native timm weights instead of remapping on the fly\n* Fix cuda stream bug in prefetch loader"}, {"name": "timm", "tags": ["data", "dev", "math", "ml", "ui", "web"], "summary": "PyTorch Image Models", "text": "June 5, 2025\n* Initial NaFlexVit model code. NaFlexVit is a Vision Transformer with:\n  1. Encapsulated embedding and position encoding in a single module\n  2. Support for nn.Linear patch embedding on pre-patchified (dictionary) inputs\n  3. Support for NaFlex variable aspect, variable resolution (SigLip-2: https://arxiv.org/abs/2502.14786)\n  4. Support for FlexiViT variable patch size (https://arxiv.org/abs/2212.08013)\n  5. Support for NaViT fractional/factorized position embedding (https://arxiv.org/abs/2307.06304)\n* Existing vit models in `vision_transformer.py` can be loaded into the NaFlexVit model by adding the `use_naflex=True` flag to `create_model`\n  * Some native weights coming soon\n* A full NaFlex data pipeline is available that allows training / fine-tuning / evaluating with variable aspect / size images\n  * To enable in `train.py` and `validate.py` add the `--naflex-loader` arg, must be used with a NaFlexVit\n* To evaluate an existing (classic) ViT loaded in NaFlexVit model w/ NaFlex data pipe:\n  * `python validate.py /imagenet --amp -j 8 --model vit_base_patch16_224 --model-kwargs use_naflex=True --naflex-loader --naflex-max-seq-len 256` \n* The training has some extra args features worth noting\n  * The `--naflex-train-seq-lens'` argument specifies which sequence lengths to randomly pick from per batch during training\n  * The `--naflex-max-seq-len` argument sets the target sequence length for validation\n  * Adding `--model-kwargs enable_patch_interpolator=True --naflex-patch-sizes 12 16 24` will enable random patch size selection per-batch w/ interpolation\n  * The `--naflex-loss-scale` arg changes loss scaling mode per batch relative to the batch size, `timm` NaFlex loading changes the batch size for each seq len\n\nMay 28, 2025\n* Add a number of small/fast models thanks to https://github.com/brianhou0208\n  * SwiftFormer - [(ICCV2023) SwiftFormer: Efficient Additive Attention for Transformer-based Real-time Mobile Vision Applications](https://github.com/Amshaker/SwiftFormer) \n  * FasterNet - [(CVPR2023) Run, Don\u2019t Walk: Chasing Higher FLOPS for Faster Neural Networks](https://github.com/JierunChen/FasterNet)\n  * SHViT - [(CVPR2024) SHViT: Single-Head Vision Transformer with Memory Efficient](https://github.com/ysj9909/SHViT)\n  * StarNet - [(CVPR2024) Rewrite the Stars](https://github.com/ma-xu/Rewrite-the-Stars)\n  * GhostNet-V3 [GhostNetV3: Exploring the Training Strategies for Compact Models](https://github.com/huawei-noah/Efficient-AI-Backbones/tree/master/ghostnetv3_pytorch)\n* Update EVA ViT (closest match) to support Perception Encoder models (https://arxiv.org/abs/2504.13181) from Meta, loading Hub weights but I still need to push dedicated `timm` weights\n  * Add some flexibility to ROPE impl\n* Big increase in number of models supporting `forward_intermediates()` and some additional fixes thanks to https://github.com/brianhou0208\n  * DaViT, EdgeNeXt, EfficientFormerV2, EfficientViT(MIT), EfficientViT(MSRA), FocalNet, GCViT, HGNet /V2, InceptionNeXt, Inception-V4, MambaOut, MetaFormer, NesT, Next-ViT, PiT, PVT V2, RepGhostNet, RepViT, ResNetV2, ReXNet, TinyViT, TResNet, VoV\n* TNT model updated w/ new weights `forward_intermediates()` thanks to https://github.com/brianhou0208\n* Add `local-dir:` pretrained schema, can use `local-dir:/path/to/model/folder` for model name to source model / pretrained cfg & weights Hugging Face Hub models (config.json + weights file) from a local folder.\n* Fixes, improvements for onnx export\n\nFeb 21, 2025\n* SigLIP 2 ViT image encoders added (https://huggingface.co/collections/timm/siglip-2-67b8e72ba08b09dd97aecaf9)\n  * Variable resolution / aspect NaFlex versions are a WIP\n* Add 'SO150M2' ViT weights trained with SBB recipes, great results, better for ImageNet than previous attempt w/ less training.\n  * `vit_so150m2_patch16_reg1_gap_448.sbb_e200_in12k_ft_in1k` - 88.1% top-1\n  * `vit_so150m2_patch16_reg1_gap_384.sbb_e200_in12k_ft_in1k` - 87.9% top-1\n  * `vit_so150m2_patch16_reg1_gap_256.sbb_e200_in12k_ft_in1k` - 87.3% top-1\n  * `vit_so150m2_patch16_reg4_gap_256.sbb_e200_in12k`\n* Updated InternViT-300M '2.5' weights\n* Release 1.0.15"}, {"name": "timm", "tags": ["data", "dev", "math", "ml", "ui", "web"], "summary": "PyTorch Image Models", "text": "Feb 1, 2025\n* FYI PyTorch 2.6 & Python 3.13 are tested and working w/ current main and released version of `timm`\n\nJan 27, 2025\n* Add Kron Optimizer (PSGD w/ Kronecker-factored preconditioner) \n  * Code from https://github.com/evanatyourservice/kron_torch\n  * See also https://sites.google.com/site/lixilinx/home/psgd\n\nJan 19, 2025\n* Fix loading of LeViT safetensor weights, remove conversion code which should have been deactivated\n* Add 'SO150M' ViT weights trained with SBB recipes, decent results, but not optimal shape for ImageNet-12k/1k pretrain/ft\n  * `vit_so150m_patch16_reg4_gap_256.sbb_e250_in12k_ft_in1k` - 86.7% top-1\n  * `vit_so150m_patch16_reg4_gap_384.sbb_e250_in12k_ft_in1k` - 87.4% top-1\n  * `vit_so150m_patch16_reg4_gap_256.sbb_e250_in12k`\n* Misc typing, typo, etc. cleanup\n* 1.0.14 release to get above LeViT fix out\n\nJan 9, 2025\n* Add support to train and validate in pure `bfloat16` or `float16`\n* `wandb` project name arg added by https://github.com/caojiaolong, use arg.experiment for name\n* Fix old issue w/ checkpoint saving not working on filesystem w/o hard-link support (e.g. FUSE fs mounts)\n* 1.0.13 release\n\nJan 6, 2025\n* Add `torch.utils.checkpoint.checkpoint()` wrapper in `timm.models` that defaults `use_reentrant=False`, unless `TIMM_REENTRANT_CKPT=1` is set in env.\n\nDec 31, 2024\n* `convnext_nano` 384x384 ImageNet-12k pretrain & fine-tune. https://huggingface.co/models?search=convnext_nano%20r384\n* Add AIM-v2 encoders from https://github.com/apple/ml-aim, see on Hub: https://huggingface.co/models?search=timm%20aimv2\n* Add PaliGemma2 encoders from https://github.com/google-research/big_vision to existing PaliGemma, see on Hub: https://huggingface.co/models?search=timm%20pali2\n* Add missing L/14 DFN2B 39B CLIP ViT, `vit_large_patch14_clip_224.dfn2b_s39b`\n* Fix existing `RmsNorm` layer & fn to match standard formulation, use PT 2.5 impl when possible. Move old impl to `SimpleNorm` layer, it's LN w/o centering or bias. There were only two `timm` models using it, and they have been updated.\n* Allow override of `cache_dir` arg for model creation\n* Pass through `trust_remote_code` for HF datasets wrapper\n* `inception_next_atto` model added by creator\n* Adan optimizer caution, and Lamb decoupled weight decay options\n* Some feature_info metadata fixed by https://github.com/brianhou0208\n* All OpenCLIP and JAX (CLIP, SigLIP, Pali, etc) model weights that used load time remapping were given their own HF Hub instances so that they work with `hf-hub:` based loading, and thus will work with new Transformers `TimmWrapperModel`\n\nNov 28, 2024\n* More optimizers\n  * Add MARS optimizer (https://arxiv.org/abs/2411.10438, https://github.com/AGI-Arena/MARS)\n  * Add LaProp optimizer (https://arxiv.org/abs/2002.04839, https://github.com/Z-T-WANG/LaProp-Optimizer)\n  * Add masking from 'Cautious Optimizers' (https://arxiv.org/abs/2411.16085, https://github.com/kyleliang919/C-Optim) to Adafactor, Adafactor Big Vision, AdamW (legacy), Adopt, Lamb, LaProp, Lion, NadamW, RMSPropTF, SGDW\n  * Cleanup some docstrings and type annotations re optimizers and factory\n* Add MobileNet-V4 Conv Medium models pretrained on in12k and fine-tuned in1k @ 384x384\n  * https://huggingface.co/timm/mobilenetv4_conv_medium.e250_r384_in12k_ft_in1k\n  * https://huggingface.co/timm/mobilenetv4_conv_medium.e250_r384_in12k\n  * https://huggingface.co/timm/mobilenetv4_conv_medium.e180_ad_r384_in12k\n  * https://huggingface.co/timm/mobilenetv4_conv_medium.e180_r384_in12k\n* Add small cs3darknet, quite good for the speed\n  * https://huggingface.co/timm/cs3darknet_focus_s.ra4_e3600_r256_in1k"}, {"name": "timm", "tags": ["data", "dev", "math", "ml", "ui", "web"], "summary": "PyTorch Image Models", "text": "Nov 12, 2024\n* Optimizer factory refactor\n  * New factory works by registering optimizers using an OptimInfo dataclass w/ some key traits\n  * Add `list_optimizers`, `get_optimizer_class`, `get_optimizer_info` to reworked `create_optimizer_v2` fn to explore optimizers, get info or class\n  * deprecate `optim.optim_factory`, move fns to `optim/_optim_factory.py` and `optim/_param_groups.py` and encourage import via `timm.optim`\n* Add Adopt (https://github.com/iShohei220/adopt) optimizer\n* Add 'Big Vision' variant of Adafactor (https://github.com/google-research/big_vision/blob/main/big_vision/optax.py) optimizer\n* Fix original Adafactor to pick better factorization dims for convolutions\n* Tweak LAMB optimizer with some improvements in torch.where functionality since original, refactor clipping a bit\n* dynamic img size support in vit, deit, eva improved to support resize from non-square patch grids, thanks https://github.com/wojtke\n*\n\nOct 31, 2024\nAdd a set of new very well trained ResNet & ResNet-V2 18/34 (basic block) weights. See https://huggingface.co/blog/rwightman/resnet-trick-or-treat\n\nOct 19, 2024\n* Cleanup torch amp usage to avoid cuda specific calls, merge support for Ascend (NPU) devices from [MengqingCao](https://github.com/MengqingCao) that should work now in PyTorch 2.5 w/ new device extension autoloading feature. Tested Intel Arc (XPU) in Pytorch 2.5 too and it (mostly) worked.\n\nOct 16, 2024\n* Fix error on importing from deprecated path `timm.models.registry`, increased priority of existing deprecation warnings to be visible\n* Port weights of InternViT-300M (https://huggingface.co/OpenGVLab/InternViT-300M-448px) to `timm` as `vit_intern300m_patch14_448`\n\nOct 14, 2024\n* Pre-activation (ResNetV2) version of 18/18d/34/34d ResNet model defs added by request (weights pending)\n* Release 1.0.10\n\nOct 11, 2024\n* MambaOut (https://github.com/yuweihao/MambaOut) model & weights added. A cheeky take on SSM vision models w/o the SSM (essentially ConvNeXt w/ gating). A mix of original weights + custom variations & weights.\n\nmodel\n---------------------------------------------------------------------------------------------------------------------\n[mambaout_base_plus_rw.sw_e150_r384_in12k_ft_in1k](http://huggingface.co/timm/mambaout_base_plus_rw.sw_e150_r384_in12k_ft_in1k)\n[mambaout_base_plus_rw.sw_e150_in12k_ft_in1k](http://huggingface.co/timm/mambaout_base_plus_rw.sw_e150_in12k_ft_in1k)\n[mambaout_base_plus_rw.sw_e150_in12k_ft_in1k](http://huggingface.co/timm/mambaout_base_plus_rw.sw_e150_in12k_ft_in1k)\n[mambaout_base_tall_rw.sw_e500_in1k](http://huggingface.co/timm/mambaout_base_tall_rw.sw_e500_in1k)\n[mambaout_base_wide_rw.sw_e500_in1k](http://huggingface.co/timm/mambaout_base_wide_rw.sw_e500_in1k)\n[mambaout_base_short_rw.sw_e500_in1k](http://huggingface.co/timm/mambaout_base_short_rw.sw_e500_in1k)\n[mambaout_base.in1k](http://huggingface.co/timm/mambaout_base.in1k)\n[mambaout_small_rw.sw_e450_in1k](http://huggingface.co/timm/mambaout_small_rw.sw_e450_in1k)\n[mambaout_small.in1k](http://huggingface.co/timm/mambaout_small.in1k)\n[mambaout_base_wide_rw.sw_e500_in1k](http://huggingface.co/timm/mambaout_base_wide_rw.sw_e500_in1k)\n[mambaout_base_tall_rw.sw_e500_in1k](http://huggingface.co/timm/mambaout_base_tall_rw.sw_e500_in1k)\n[mambaout_base_short_rw.sw_e500_in1k](http://huggingface.co/timm/mambaout_base_short_rw.sw_e500_in1k)\n[mambaout_base.in1k](http://huggingface.co/timm/mambaout_base.in1k)\n[mambaout_small.in1k](http://huggingface.co/timm/mambaout_small.in1k)\n[mambaout_small_rw.sw_e450_in1k](http://huggingface.co/timm/mambaout_small_rw.sw_e450_in1k)\n[mambaout_tiny.in1k](http://huggingface.co/timm/mambaout_tiny.in1k)\n[mambaout_tiny.in1k](http://huggingface.co/timm/mambaout_tiny.in1k)\n[mambaout_kobe.in1k](http://huggingface.co/timm/mambaout_kobe.in1k)\n[mambaout_kobe.in1k](http://huggingface.co/timm/mambaout_kobe.in1k)\n[mambaout_femto.in1k](http://huggingface.co/timm/mambaout_femto.in1k)\n[mambaout_femto.in1k](http://huggingface.co/timm/mambaout_femto.in1k)\n\n* SigLIP SO400M ViT fine-tunes on ImageNet-1k @ 378x378, added 378x378 option for existing SigLIP 384x384 models\n  *  [vit_so400m_patch14_siglip_378.webli_ft_in1k](https://huggingface.co/timm/vit_so400m_patch14_siglip_378.webli_ft_in1k) - 89.42 top-1\n  *  [vit_so400m_patch14_siglip_gap_378.webli_ft_in1k](https://huggingface.co/timm/vit_so400m_patch14_siglip_gap_378.webli_ft_in1k) - 89.03\n* SigLIP SO400M ViT encoder from recent multi-lingual (i18n) variant, patch16 @ 256x256 (https://huggingface.co/timm/ViT-SO400M-16-SigLIP-i18n-256). OpenCLIP update pending.\n* Add two ConvNeXt 'Zepto' models & weights (one w/ overlapped stem and one w/ patch stem). Uses RMSNorm, smaller than previous 'Atto', 2.2M params.\n  * [convnext_zepto_rms_ols.ra4_e3600_r224_in1k](https://huggingface.co/timm/convnext_zepto_rms_ols.ra4_e3600_r224_in1k) - 73.20 top-1 @ 224\n  * [convnext_zepto_rms.ra4_e3600_r224_in1k](https://huggingface.co/timm/convnext_zepto_rms.ra4_e3600_r224_in1k) - 72.81 @ 224\n\nSept 2024\n* Add a suite of tiny test models for improved unit tests and niche low-resource applications (https://huggingface.co/blog/rwightman/timm-tiny-test)\n* Add MobileNetV4-Conv-Small (0.5x) model (https://huggingface.co/posts/rwightman/793053396198664)\n  * [mobilenetv4_conv_small_050.e3000_r224_in1k](http://hf.co/timm/mobilenetv4_conv_small_050.e3000_r224_in1k) - 65.81 top-1 @ 256, 64.76 @ 224\n* Add MobileNetV3-Large variants trained with MNV4 Small recipe\n  * [mobilenetv3_large_150d.ra4_e3600_r256_in1k](http://hf.co/timm/mobilenetv3_large_150d.ra4_e3600_r256_in1k) - 81.81 @ 320, 80.94 @ 256\n  * [mobilenetv3_large_100.ra4_e3600_r224_in1k](http://hf.co/timm/mobilenetv3_large_100.ra4_e3600_r224_in1k) - 77.16 @ 256, 76.31 @ 224\n\nAug 21, 2024\n* Updated SBB ViT models trained on ImageNet-12k and fine-tuned on ImageNet-1k, challenging quite a number of much larger, slower models\n\nmodel\n--------------------------------------------------\n[vit_mediumd_patch16_reg4_gap_384.sbb2_e200_in12k_ft_in1k](https://huggingface.co/timm/vit_mediumd_patch16_reg4_gap_384.sbb2_e200_in12k_ft_in1k)\n[vit_mediumd_patch16_reg4_gap_256.sbb2_e200_in12k_ft_in1k](https://huggingface.co/timm/vit_mediumd_patch16_reg4_gap_256.sbb2_e200_in12k_ft_in1k)\n[vit_betwixt_patch16_reg4_gap_384.sbb2_e200_in12k_ft_in1k](https://huggingface.co/timm/vit_betwixt_patch16_reg4_gap_384.sbb2_e200_in12k_ft_in1k)\n[vit_betwixt_patch16_reg4_gap_256.sbb2_e200_in12k_ft_in1k](https://huggingface.co/timm/vit_betwixt_patch16_reg4_gap_256.sbb2_e200_in12k_ft_in1k)\n* MobileNet-V1 1.25, EfficientNet-B1, & ResNet50-D weights w/ MNV4 baseline challenge recipe\n\nmodel\n--------------------------------------------------------------------------------------------------------------------------\n[resnet50d.ra4_e3600_r224_in1k](http://hf.co/timm/resnet50d.ra4_e3600_r224_in1k)\n[efficientnet_b1.ra4_e3600_r240_in1k](http://hf.co/timm/efficientnet_b1.ra4_e3600_r240_in1k)\n[resnet50d.ra4_e3600_r224_in1k](http://hf.co/timm/resnet50d.ra4_e3600_r224_in1k)\n[efficientnet_b1.ra4_e3600_r240_in1k](http://hf.co/timm/efficientnet_b1.ra4_e3600_r240_in1k)\n[mobilenetv1_125.ra4_e3600_r224_in1k](http://hf.co/timm/mobilenetv1_125.ra4_e3600_r224_in1k)\n[mobilenetv1_125.ra4_e3600_r224_in1k](http://hf.co/timm/mobilenetv1_125.ra4_e3600_r224_in1k)"}, {"name": "timm", "tags": ["data", "dev", "math", "ml", "ui", "web"], "summary": "PyTorch Image Models", "text": "* Add SAM2 (HieraDet) backbone arch & weight loading support\n* Add Hiera Small weights trained w/ abswin pos embed on in12k & fine-tuned on 1k\n\nmodel\n---------------------------------\nhiera_small_abswin_256.sbb2_e200_in12k_ft_in1k\nhiera_small_abswin_256.sbb2_pd_e200_in12k_ft_in1k\n\nAug 8, 2024\n* Add RDNet ('DenseNets Reloaded', https://arxiv.org/abs/2403.19588), thanks [Donghyun Kim](https://github.com/dhkim0225)\n\nJuly 28, 2024\n* Add `mobilenet_edgetpu_v2_m` weights w/ `ra4` mnv4-small based recipe. 80.1% top-1 @ 224 and 80.7 @ 256.\n* Release 1.0.8\n\nJuly 26, 2024\n* More MobileNet-v4 weights, ImageNet-12k pretrain w/ fine-tunes, and anti-aliased ConvLarge models\n\nmodel\n--------------------------------------------------------------------------------------------------\n[mobilenetv4_conv_aa_large.e230_r448_in12k_ft_in1k](http://hf.co/timm/mobilenetv4_conv_aa_large.e230_r448_in12k_ft_in1k)\n[mobilenetv4_conv_aa_large.e230_r384_in12k_ft_in1k](http://hf.co/timm/mobilenetv4_conv_aa_large.e230_r384_in12k_ft_in1k)\n[mobilenetv4_conv_aa_large.e230_r448_in12k_ft_in1k](http://hf.co/timm/mobilenetv4_conv_aa_large.e230_r448_in12k_ft_in1k)\n[mobilenetv4_conv_aa_large.e230_r384_in12k_ft_in1k](http://hf.co/timm/mobilenetv4_conv_aa_large.e230_r384_in12k_ft_in1k)\n[mobilenetv4_conv_aa_large.e600_r384_in1k](http://hf.co/timm/mobilenetv4_conv_aa_large.e600_r384_in1k)\n[mobilenetv4_conv_aa_large.e600_r384_in1k](http://hf.co/timm/mobilenetv4_conv_aa_large.e600_r384_in1k)\n[mobilenetv4_hybrid_medium.e200_r256_in12k_ft_in1k](http://hf.co/timm/mobilenetv4_hybrid_medium.e200_r256_in12k_ft_in1k)\n[mobilenetv4_hybrid_medium.e200_r256_in12k_ft_in1k](http://hf.co/timm/mobilenetv4_hybrid_medium.e200_r256_in12k_ft_in1k)\n\n* Impressive MobileNet-V1 and EfficientNet-B0 baseline challenges (https://huggingface.co/blog/rwightman/mobilenet-baselines)\n  \nmodel\n--------------------------------------------------------------------------------------------------\n[efficientnet_b0.ra4_e3600_r224_in1k](http://hf.co/timm/efficientnet_b0.ra4_e3600_r224_in1k)\n[efficientnet_b0.ra4_e3600_r224_in1k](http://hf.co/timm/efficientnet_b0.ra4_e3600_r224_in1k)\n[mobilenetv1_100h.ra4_e3600_r224_in1k](http://hf.co/timm/mobilenetv1_100h.ra4_e3600_r224_in1k)\n[mobilenetv1_100.ra4_e3600_r224_in1k](http://hf.co/timm/mobilenetv1_100.ra4_e3600_r224_in1k)\n[mobilenetv1_100h.ra4_e3600_r224_in1k](http://hf.co/timm/mobilenetv1_100h.ra4_e3600_r224_in1k)\n[mobilenetv1_100.ra4_e3600_r224_in1k](http://hf.co/timm/mobilenetv1_100.ra4_e3600_r224_in1k)\n\n* Prototype of `set_input_size()` added to vit and swin v1/v2 models to allow changing image size, patch size, window size after model creation.\n* Improved support in swin for different size handling, in addition to `set_input_size`, `always_partition` and `strict_img_size` args have been added to `__init__` to allow more flexible input size constraints\n* Fix out of order indices info for intermediate 'Getter' feature wrapper, check out or range indices for same.\n* Add several `tiny` ` optimizers by name with [NVIDIA Apex](https://github.com/NVIDIA/apex/tree/master/apex/optimizers) installed\n* `bnb` optimizers by name with [BitsAndBytes](https://github.com/TimDettmers/bitsandbytes) installed\n* `cadamw`, `clion`, and more 'Cautious' optimizers from https://github.com/kyleliang919/C-Optim - https://arxiv.org/abs/2411.16085\n* `adam`, `adamw`, `rmsprop`, `adadelta`, `adagrad`, and `sgd` pass through to `torch.optim` implementations\n* `c` suffix (eg `adamc`, `nadamc` to implement 'corrected weight decay' in https://arxiv.org/abs/2506.02285)\n\nAugmentations\n* Random Erasing from [Zhun Zhong](https://github.com/zhunzhong07/Random-Erasing/blob/master/transforms.py) - https://arxiv.org/abs/1708.04896)\n* Mixup - https://arxiv.org/abs/1710.09412\n* CutMix - https://arxiv.org/abs/1905.04899\n* AutoAugment (https://arxiv.org/abs/1805.09501) and RandAugment (https://arxiv.org/abs/1909.13719) ImageNet configurations modeled after impl for EfficientNet training (https://github.com/tensorflow/tpu/blob/master/models/official/efficientnet/autoaugment.py)\n* AugMix w/ JSD loss, JSD w/ clean + augmented mixing support works with AutoAugment and RandAugment as well - https://arxiv.org/abs/1912.02781\n* SplitBachNorm - allows splitting batch norm layers between clean and augmented (auxiliary batch norm) data\n\nRegularization\n* DropPath aka \"Stochastic Depth\" - https://arxiv.org/abs/1603.09382\n* DropBlock - https://arxiv.org/abs/1810.12890\n* Blur Pooling - https://arxiv.org/abs/1904.11486\n\nOther\n\nSeveral (less common) features that I often utilize in my projects are included. Many of their additions are the reason why I maintain my own set of models, instead of using others' via PIP:"}, {"name": "timm", "tags": ["data", "dev", "math", "ml", "ui", "web"], "summary": "PyTorch Image Models", "text": "* All models have a common default configuration interface and API for\n* All models support multi-scale feature map extraction (feature pyramids) via create_model (see [documentation](https://huggingface.co/docs/timm/feature_extraction))\n* All models have a consistent pretrained weight loader that adapts last linear if necessary, and from 3 to 1 channel input if desired\n* High performance [reference training, validation, and inference scripts](https://huggingface.co/docs/timm/training_script) that work in several process/GPU modes:\n* A dynamic global pool implementation that allows selecting from average pooling, max pooling, average + max, or concat([average, max]) at model creation. All global pooling is adaptive average by default and compatible with pretrained weights.\n* A 'Test Time Pool' wrapper that can wrap any of the included models and usually provides improved performance doing inference with input images larger than the training size. Idea adapted from original DPN implementation when I ported (https://github.com/cypw/DPNs)\n* Learning rate schedulers\n  * Ideas adopted from\n  * Schedulers include `step`, `cosine` w/ restarts, `tanh` w/ restarts, `plateau`\n* Space-to-Depth by [mrT23](https://github.com/mrT23/TResNet/blob/master/src/models/tresnet/layers/space_to_depth.py) (https://arxiv.org/abs/1801.04590) -- original paper?\n* Adaptive Gradient Clipping (https://arxiv.org/abs/2102.06171, https://github.com/deepmind/deepmind-research/tree/master/nfnets)\n* An extensive selection of channel and/or spatial attention modules:\n\nResults\n\nModel validation results can be found in the [results tables](results/README.md)\n\nGetting Started (Documentation)\n\nThe official documentation can be found at https://huggingface.co/docs/hub/timm. Documentation contributions are welcome.\n\n[Getting Started with PyTorch Image Models (timm): A Practitioner\u2019s Guide](https://towardsdatascience.com/getting-started-with-pytorch-image-models-timm-a-practitioners-guide-4e77b4bf9055-2/) by [Chris Hughes](https://github.com/Chris-hughes10) is an extensive blog post covering many aspects of `timm` in detail.\n\n[timmdocs](http://timm.fast.ai/) is an alternate set of documentation for `timm`. A big thanks to [Aman Arora](https://github.com/amaarora) for his efforts creating timmdocs.\n\n[paperswithcode](https://paperswithcode.com/lib/timm) is a good resource for browsing the models within `timm`.\n\nTrain, Validation, Inference Scripts\n\nThe root folder of the repository contains reference train, validation, and inference scripts that work with the included models and other features of this repository. They are adaptable for other datasets and use cases with a little hacking. See [documentation](https://huggingface.co/docs/timm/training_script).\n\nAwesome PyTorch Resources\n\nOne of the greatest assets of PyTorch is the community and their contributions. A few of my favourite resources that pair well with the models and components here are listed below.\n\nObject Detection, Instance and Semantic Segmentation\n* Detectron2 - https://github.com/facebookresearch/detectron2\n* Segmentation Models (Semantic) - https://github.com/qubvel/segmentation_models.pytorch\n* EfficientDet (Obj Det, Semantic soon) - https://github.com/rwightman/efficientdet-pytorch\n\nComputer Vision / Image Augmentation\n* Albumentations - https://github.com/albumentations-team/albumentations\n* Kornia - https://github.com/kornia/kornia\n\nKnowledge Distillation\n* RepDistiller - https://github.com/HobbitLong/RepDistiller\n* torchdistill - https://github.com/yoshitomo-matsubara/torchdistill\n\nMetric Learning\n* PyTorch Metric Learning - https://github.com/KevinMusgrave/pytorch-metric-learning\n\nTraining / Frameworks\n* fastai - https://github.com/fastai/fastai\n* lightly_train - https://github.com/lightly-ai/lightly-train\n\nLicenses\n\nCode\nThe code here is licensed Apache 2.0. I've taken care to make sure any third party code included or adapted has compatible (permissive) licenses such as MIT, BSD, etc. I've made an effort to avoid any GPL / LGPL conflicts. That said, it is your responsibility to ensure you comply with licenses here and conditions of any dependent licenses. Where applicable, I've linked the sources/references for various components in docstrings. If you think I've missed anything please create an issue."}, {"name": "timm", "tags": ["data", "dev", "math", "ml", "ui", "web"], "summary": "PyTorch Image Models", "text": "Pretrained Weights\nSo far all of the pretrained weights available here are pretrained on ImageNet with a select few that have some additional pretraining (see extra note below). ImageNet was released for non-commercial research purposes only (https://image-net.org/download). It's not clear what the implications of that are for the use of pretrained weights from that dataset. Any models I have trained with ImageNet are done for research purposes and one should assume that the original dataset license applies to the weights. It's best to seek legal advice if you intend to use the pretrained weights in a commercial product.\n\nPretrained on more than ImageNet\nSeveral weights included or references here were pretrained with proprietary datasets that I do not have access to. These include the Facebook WSL, SSL, SWSL ResNe(Xt) and the Google Noisy Student EfficientNet models. The Facebook models have an explicit non-commercial license (CC-BY-NC 4.0, https://github.com/facebookresearch/semi-supervised-ImageNet1K-models, https://github.com/facebookresearch/WSL-Images). The Google models do not appear to have any restriction beyond the Apache 2.0 license (and ImageNet concerns). In either case, you should contact Facebook or Google with any questions.\n\nCiting\n\nBibTeX\n\nLatest DOI"}, {"name": "timm", "tags": ["data", "dev", "math", "ml", "ui", "web"], "summary": "PyTorch Image Models", "text": "This library is used to provide pre-trained PyTorch models and utilities for computer vision tasks, making it easier to implement state-of-the-art image classification and detection models. With timm, developers can quickly integrate efficient and accurate image processing capabilities into their applications."}, {"name": "tinysegmenter", "tags": ["math", "ml", "web"], "summary": "Very compact Japanese tokenizer", "text": "TinySegmenter\n=============\n\n\u201cTinySegmenter in Python\u201d is a Python port_ by Masato Hagiwara of TinySegmenter_, which is an extremely compact Japanese tokenizer originally written in JavaScript by Mr. Taku Kudo.\n\nThe library has been finally packaged by Jehan. It resulted into this fork because Masako Hagiwara did not answer emails, and packaging patches\ncould therefore not be committed upstream. But this is a friendly fork, and Masako Hagiwara is welcome to take back maintainance over his\nproject.\nFor the time being, I (Jehan) took up the maintenance, so please refer to this new website_ as being official, and\ndirect any new patch_ there. I will follow up on patchs and bug reports, but probably won't maintain an active development. Anyone wishing to\nimprove the library is welcome to participate and will be gladly given committer rights.\n\nIt works on Python 2.6 or above (works on Python 3 too).\n\n.. _port: http://lilyx.net/tinysegmenter-in-python/\n.. _TinySegmenter: http://chasen.org/~taku/software/TinySegmenter/\n.. _website: http://tinysegmenter.tuxfamily.org/\n\nAuthors\n-------\n\nSee all authors and contributors in ``AUTHORS`` file.\n\nDownload and Installation\n-------------------------\n\nThis library can be installed the common ways: with a setup.py, as a pip package...\nSee the ``INSTALL`` file in the package for more details.\n\nIf you simply want to download the source package, refer to the pypi repository: http://pypi.python.org/pypi/tinysegmenter\n\nDevelopment version can be downloaded anonymously at the Git repository::\n\nor browsed online at: http://git.tuxfamily.org/tinysegmente/tinysegmenter/\n\nUsage\n-----\n\nExample code for direct usage::\n\nTinySegmenter\u2018s interface is compatible with ``NLTK``\u2019s ``TokenizerI`` class, although the distribution does not directly depend on NLTK.\nHere is one way to use it as a tokenizer in NLTK (order of the multiple base classes matters)::\n\nFor more about NLTK (*Natural Language Toolkit* module), see: http://nltk.org/api/nltk.tokenize.html#nltk.tokenize.api.TokenizerI\n\n.. _patch:\n\nContact, Bugs and Contributing\n------------------------------\n\nAll bug, patch, question, etc. can be sent to `tinysegmenter` at `zemarmot` dot `net`.\n\nLicense\n-------\n\nThis package is distributed under a New BSD License (see ``COPYING`` file)."}, {"name": "tinysegmenter", "tags": ["math", "ml", "web"], "summary": "Very compact Japanese tokenizer", "text": "This library is used to enable compact Japanese text tokenization in Python applications. It provides a lightweight and efficient way to break down Japanese texts into individual segments or tokens."}, {"name": "torch-audiomentations", "tags": ["data", "dev", "math", "ml", "web"], "summary": "A Pytorch library for audio data augmentation. Inspired by audiomentations. Useful for deep learning.", "text": "Setup\n\n(https://pypi.org/project/torch-audiomentations/)\n(https://pypi.org/project/torch-audiomentations/)\n\n`pip install torch-audiomentations`\n\nUsage example\n\nKnown issues\n\n* Target data processing is still in an experimental state ([#3](https://github.com/asteroid-team/torch-audiomentations/issues/3)). Workaround: Use `freeze_parameters` and `unfreeze_parameters` for now if the target data is audio with the same shape as the input.\n* Using torch-audiomentations in a multiprocessing context can lead to memory leaks ([#132](https://github.com/asteroid-team/torch-audiomentations/issues/132)). Workaround: If using torch-audiomentations in a multiprocessing context, it'll probably work better to run the transforms on CPU.\n* Multi-GPU / DDP is not officially supported ([#136](https://github.com/asteroid-team/torch-audiomentations/issues/136)). The author does not have a multi-GPU setup to test & fix this. Get in touch if you want to donate some hardware for this. Workaround: Run the transforms on single GPU instead.\n* `PitchShift` does not support small pitch shifts, especially for low sample rates ([#151](https://github.com/asteroid-team/torch-audiomentations/issues/151)). Workaround: If you need small pitch shifts applied to low sample rates, use [PitchShift in audiomentations](https://iver56.github.io/audiomentations/waveform_transforms/pitch_shift/) or [torch-pitch-shift](https://github.com/KentoNishi/torch-pitch-shift/) directly without the function for calculating efficient pitch-shift targets.\n\nContribute\n\nContributors welcome! \n[Join the Asteroid's slack](https://join.slack.com/t/asteroid-dev/shared_invite/zt-cn9y85t3-QNHXKD1Et7qoyzu1Ji5bcA)\nto start discussing about `torch-audiomentations` with us.\n\nMotivation: Speed\n\nWe don't want data augmentation to be a bottleneck in model training speed. Here is a\ncomparison of the time it takes to run 1D convolution:\n\nNote: Not all transforms have a speedup this impressive compared to CPU. In general, running audio data augmentation on GPU is not always the best option. For more info, see this article: [https://iver56.github.io/audiomentations/guides/cpu_vs_gpu/](https://iver56.github.io/audiomentations/guides/cpu_vs_gpu/)\n\nCurrent state\n\ntorch-audiomentations is in an early development stage, so the APIs are subject to change.\n\nWaveform transforms\n\nEvery transform has `mode`, `p`, and `p_mode` -- the parameters that decide how the augmentation is performed.\n- `mode` decides how the randomization of the augmentation is grouped and applied.\n- `p` decides the on/off probability of applying the augmentation.   \n- `p_mode` decides how the on/off of the augmentation is applied.\n\nThis visualization shows how different combinations of `mode` and `p_mode` would perform an augmentation.\n\nAddBackgroundNoise\n\n_Added in v0.5.0_\n\nAdd background noise to the input audio.\n\nAddColoredNoise\n\n_Added in v0.7.0_\n\nAdd colored noise to the input audio.\n\nApplyImpulseResponse\n\n_Added in v0.5.0_\n\nConvolve the given audio with impulse responses.\n\nBandPassFilter\n\n_Added in v0.9.0_\n\nApply band-pass filtering to the input audio.\n\nBandStopFilter\n\n_Added in v0.10.0_\n\nApply band-stop filtering to the input audio. Also known as notch filter.\n\nGain\n\n_Added in v0.1.0_\n\nMultiply the audio by a random amplitude factor to reduce or increase the volume. This\ntechnique can help a model become somewhat invariant to the overall gain of the input audio.\n\nWarning: This transform can return samples outside the [-1, 1] range, which may lead to\nclipping or wrap distortion, depending on what you do with the audio in a later stage.\nSee also https://en.wikipedia.org/wiki/Clipping_(audio)#Digital_clipping\n\nHighPassFilter\n\n_Added in v0.8.0_\n\nApply high-pass filtering to the input audio.\n\nIdentity\n\n_Added in v0.11.0_\n\nThis transform returns the input unchanged. It can be used for simplifying the code\nin cases where data augmentation should be disabled.\n\nLowPassFilter\n\n_Added in v0.8.0_\n\nApply low-pass filtering to the input audio.\n\nPeakNormalization\n\n_Added in v0.2.0_"}, {"name": "torch-audiomentations", "tags": ["data", "dev", "math", "ml", "web"], "summary": "A Pytorch library for audio data augmentation. Inspired by audiomentations. Useful for deep learning.", "text": "Apply a constant amount of gain, so that highest signal level present in each audio snippet\nin the batch becomes 0 dBFS, i.e. the loudest level allowed if all samples must be between\n-1 and 1.\n\nThis transform has an alternative mode (apply_to=\"only_too_loud_sounds\") where it only\napplies to audio snippets that have extreme values outside the [-1, 1] range. This is useful\nfor avoiding digital clipping in audio that is too loud, while leaving other audio\nuntouched.\n\nPitchShift\n\n_Added in v0.9.0_\n\nPitch-shift sounds up or down without changing the tempo.\n\nPolarityInversion\n\n_Added in v0.1.0_\n\nFlip the audio samples upside-down, reversing their polarity. In other words, multiply the\nwaveform by -1, so negative values become positive, and vice versa. The result will sound\nthe same compared to the original when played back in isolation. However, when mixed with\nother audio sources, the result may be different. This waveform inversion technique\nis sometimes used for audio cancellation or obtaining the difference between two waveforms.\nHowever, in the context of audio data augmentation, this transform can be useful when\ntraining phase-aware machine learning models.\n\nShift\n\n_Added in v0.5.0_\n\nShift the audio forwards or backwards, with or without rollover\n\nShuffleChannels\n\n_Added in v0.6.0_\n\nGiven multichannel audio input (e.g. stereo), shuffle the channels, e.g. so left can become right and vice versa.\nThis transform can help combat positional bias in machine learning models that input multichannel waveforms.\n\nIf the input audio is mono, this transform does nothing except emit a warning.\n\nTimeInversion\n\n_Added in v0.10.0_\n\nReverse (invert) the audio along the time axis similar to random flip of\nan image in the visual domain. This can be relevant in the context of audio\nclassification. It was successfully applied in the paper\n[AudioCLIP: Extending CLIP to Image, Text and Audio](https://arxiv.org/pdf/2106.13043.pdf)\n\nChangelog\n\nUnreleased\n\nAdded\n\n* Add new transforms: `Mix`, `Padding`, `RandomCrop` and `SpliceOut`\n\n[v0.12.0] - 2025-01-15\n\nRemoved\n\n* Remove `librosa` dependency in favor of `torchaudio`\n\n[v0.11.2] - 2025-01-09\n\nFixed\n\n* Fix a device-related bug in `transform_parameters` when training on multiple GPUs\n* Fix a shape-related edge case bug in `AddColoredNoise`\n* Fix a bug where an incompatible Path data type was passed to torchaudio.info\n\n[v0.11.1] - 2024-02-07\n\nChanged\n\n* Add support for constant cutoff frequency in `LowPassFilter` and `HighPassFilter`\n* Add support for min_f_decay==max_f_decay in `AddColoredNoise`\n* Bump torchaudio dependency from >=0.7.0 to >=0.9.0\n\nFixed\n\n* Fix inaccurate type hints in `Shift`\n* Remove `set_backend` to avoid `UserWarning` from torchaudio\n\n[v0.11.0] - 2022-06-29\n\nAdded\n\n* Add new transform: `Identity`\n* Add API for processing targets alongside inputs. Some transforms experimentally\n  support this feature already.\n\nChanged"}, {"name": "torch-audiomentations", "tags": ["data", "dev", "math", "ml", "web"], "summary": "A Pytorch library for audio data augmentation. Inspired by audiomentations. Useful for deep learning.", "text": "* Add `ObjectDict` output type as alternative to `torch.Tensor`. This alternative is opt-in for\n  now (for backwards-compatibility), but note that the old output type (`torch.Tensor`) is\n  deprecated and support for it will be removed in a future version.\n* Allow specifying a file path, a folder path, a list of files or a list of folders to\n  `AddBackgroundNoise` and `ApplyImpulseResponse`\n* Require newer version of `torch-pitch-shift` to ensure support for torchaudio 0.11 in `PitchShift`\n\nFixed\n\n* Fix a bug where `BandPassFilter` didn't work on GPU\n\n[v0.10.1] - 2022-03-24\n\nAdded\n\n* Add support for min SNR == max SNR in `AddBackgroundNoise`\n* Add support for librosa 0.9.0\n\nFixed\n\n* Fix a bug where loaded audio snippets were sometimes resampled to an incompatible\n length in `AddBackgroundNoise`\n\n[v0.10.0] - 2022-02-11\n\nAdded\n\n* Implement `OneOf` and `SomeOf` for applying one or more of a given set of transforms\n* Implement new transforms: `BandStopFilter` and `TimeInversion`\n\nChanged\n\n* Put `ir_paths` in transform_parameters in `ApplyImpulseResponse` so it is possible\n to inspect what impulse responses were used. This also gives `freeze_parameters()`\n the expected behavior.\n\nFixed\n\n* Fix a bug where the actual bandwidth was twice as large as expected in\n `BandPassFilter`. The default values have been updated accordingly.\n If you were previously specifying `min_bandwidth_fraction` and/or `max_bandwidth_fraction`,\n you now need to double those numbers to get the same behavior as before.\n\n[v0.9.1] - 2021-12-20\n\nAdded\n\n* Officially mark python>=3.9 as supported\n\n[v0.9.0] - 2021-10-11\n\nAdded\n\n* Add parameter `compensate_for_propagation_delay` in `ApplyImpulseResponse`\n* Implement `BandPassFilter`\n* Implement `PitchShift`\n\nRemoved\n\n* Support for torchaudio<=0.6 has been removed\n\n[v0.8.0] - 2021-06-15\n\nAdded\n\n* Implement `HighPassFilter` and `LowPassFilter`\n\nDeprecated\n\n* Support for torchaudio<=0.6 is deprecated and will be removed in the future\n\nRemoved\n\n* Support for pytorch<=1.6 has been removed\n\n[v0.7.0] - 2021-04-16\n\nAdded\n\n* Implement `AddColoredNoise`\n\nDeprecated\n\n* Support for pytorch<=1.6 is deprecated and will be removed in the future\n\n[v0.6.0] - 2021-02-22\n\nAdded\n\n* Implement `ShuffleChannels`\n\n[v0.5.1] - 2020-12-18\n\nFixed\n\n* Fix a bug where `AddBackgroundNoise` did not work on CUDA\n* Fix a bug where symlinked audio files/folders were not found when looking for audio files\n* Use torch.fft.rfft instead of the torch.rfft (deprecated in pytorch 1.7) when possible. As a\nbonus, the change also improves performance in `ApplyImpulseResponse`.\n\n[v0.5.0] - 2020-12-08\n\nAdded\n\n* Release `AddBackgroundNoise` and `ApplyImpulseResponse`\n* Implement `Shift`\n\nChanged\n\n* Make `sample_rate` optional. Allow specifying `sample_rate` in `__init__` instead of `forward`. This means torchaudio transforms can be used in `Compose` now.\n\nRemoved\n\n* Remove support for 1-dimensional and 2-dimensional audio tensors. Only 3-dimensional audio\n tensors are supported now.\n\nFixed\n\n* Fix a bug where one could not use the `parameters` method of the `nn.Module` subclass\n* Fix a bug where files with uppercase filename extension were not found\n\n[v0.4.0] - 2020-11-10\n\nAdded\n\n* Implement `Compose` for applying multiple transforms\n* Implement utility functions `from_dict` and `from_yaml` for loading data augmentation\nconfigurations from dict, json or yaml\n* Officially support differentiability in most transforms\n\n[v0.3.0] - 2020-10-27\n\nAdded\n\n* Add support for alternative modes `per_batch` and `per_channel`"}, {"name": "torch-audiomentations", "tags": ["data", "dev", "math", "ml", "web"], "summary": "A Pytorch library for audio data augmentation. Inspired by audiomentations. Useful for deep learning.", "text": "Changed\n\n* Transforms now return the input unchanged when they are in eval mode\n\n[v0.2.0] - 2020-10-19\n\nAdded\n\n* Implement `PeakNormalization`\n* Expose `convolve` in the API\n\nChanged\n\n* Simplify API for using CUDA tensors. The device is now inferred from the input tensor.\n\n[v0.1.0] - 2020-10-12\n\nAdded\n\n* Initial release with `Gain` and `PolarityInversion`\n\nDevelopment\n\nSetup\n\nA GPU-enabled development environment for torch-audiomentations can be created with conda:\n\n* `conda env create`\n\nRun tests\n\n`pytest`\n\nConventions\n\n* Format python code with [black](https://github.com/psf/black)\n* Use [Google-style docstrings](https://google.github.io/styleguide/pyguide.html#381-docstrings)\n* Use explicit relative imports, not absolute imports\n\nAcknowledgements\n\nThe development of torch-audiomentations is kindly backed by [Nomono](https://nomono.co/).\n\nThanks to [all contributors](https://github.com/asteroid-team/torch-audiomentations/graphs/contributors) who help improving torch-audiomentations."}, {"name": "torch-audiomentations", "tags": ["data", "dev", "math", "ml", "web"], "summary": "A Pytorch library for audio data augmentation. Inspired by audiomentations. Useful for deep learning.", "text": "This library is used to apply various audio data augmentations in Pytorch, enabling developers to enhance and diversify their audio datasets for deep learning applications. With this library, developers can easily add noise, time-stretching, pitch-shifting, and other effects to their audio data, improving model robustness and performance."}, {"name": "torch", "tags": ["math", "ml", "ui", "web"], "summary": "Tensors and Dynamic neural networks in Python with strong GPU acceleration", "text": "More About PyTorch\n\n[Learn the basics of PyTorch](https://pytorch.org/tutorials/beginner/basics/intro.html)\n\nAt a granular level, PyTorch is a library that consists of the following components:\n\nComponent\n----\n[**torch**](https://pytorch.org/docs/stable/torch.html)\n[**torch.autograd**](https://pytorch.org/docs/stable/autograd.html)\n[**torch.jit**](https://pytorch.org/docs/stable/jit.html)\n[**torch.nn**](https://pytorch.org/docs/stable/nn.html)\n[**torch.multiprocessing**](https://pytorch.org/docs/stable/multiprocessing.html)\n[**torch.utils**](https://pytorch.org/docs/stable/data.html)\n\nUsually, PyTorch is used either as:\n\n- A replacement for NumPy to use the power of GPUs.\n- A deep learning research platform that provides maximum flexibility and speed.\n\nElaborating Further:\n\nA GPU-Ready Tensor Library\n\nIf you use NumPy, then you have used Tensors (a.k.a. ndarray).\n\nPyTorch provides Tensors that can live either on the CPU or the GPU and accelerates the\ncomputation by a huge amount.\n\nWe provide a wide variety of tensor routines to accelerate and fit your scientific computation needs\nsuch as slicing, indexing, mathematical operations, linear algebra, reductions.\nAnd they are fast!\n\nDynamic Neural Networks: Tape-Based Autograd\n\nPyTorch has a unique way of building neural networks: using and replaying a tape recorder.\n\nMost frameworks such as TensorFlow, Theano, Caffe, and CNTK have a static view of the world.\nOne has to build a neural network and reuse the same structure again and again.\nChanging the way the network behaves means that one has to start from scratch.\n\nWith PyTorch, we use a technique called reverse-mode auto-differentiation, which allows you to\nchange the way your network behaves arbitrarily with zero lag or overhead. Our inspiration comes\nfrom several research papers on this topic, as well as current and past work such as\n[torch-autograd](https://github.com/twitter/torch-autograd),\n[autograd](https://github.com/HIPS/autograd),\n[Chainer](https://chainer.org), etc.\n\nWhile this technique is not unique to PyTorch, it's one of the fastest implementations of it to date.\nYou get the best of speed and flexibility for your crazy research.\n\nPython First\n\nPyTorch is not a Python binding into a monolithic C++ framework.\nIt is built to be deeply integrated into Python.\nYou can use it naturally like you would use [NumPy](https://www.numpy.org/) / [SciPy](https://www.scipy.org/) / [scikit-learn](https://scikit-learn.org) etc.\nYou can write your new neural network layers in Python itself, using your favorite libraries\nand use packages such as [Cython](https://cython.org/) and [Numba](http://numba.pydata.org/).\nOur goal is to not reinvent the wheel where appropriate.\n\nImperative Experiences\n\nPyTorch is designed to be intuitive, linear in thought, and easy to use.\nWhen you execute a line of code, it gets executed. There isn't an asynchronous view of the world.\nWhen you drop into a debugger or receive error messages and stack traces, understanding them is straightforward.\nThe stack trace points to exactly where your code was defined.\nWe hope you never spend hours debugging your code because of bad stack traces or asynchronous and opaque execution engines.\n\nFast and Lean\n\nPyTorch has minimal framework overhead. We integrate acceleration libraries\nsuch as [Intel MKL](https://software.intel.com/mkl) and NVIDIA ([cuDNN](https://developer.nvidia.com/cudnn), [NCCL](https://developer.nvidia.com/nccl)) to maximize speed.\nAt the core, its CPU and GPU Tensor and neural network backends\nare mature and have been tested for years.\n\nHence, PyTorch is quite fast \u2014 whether you run small or large neural networks."}, {"name": "torch", "tags": ["math", "ml", "ui", "web"], "summary": "Tensors and Dynamic neural networks in Python with strong GPU acceleration", "text": "The memory usage in PyTorch is extremely efficient compared to Torch or some of the alternatives.\nWe've written custom memory allocators for the GPU to make sure that\nyour deep learning models are maximally memory efficient.\nThis enables you to train bigger deep learning models than before.\n\nExtensions Without Pain\n\nWriting new neural network modules, or interfacing with PyTorch's Tensor API was designed to be straightforward\nand with minimal abstractions.\n\nYou can write new neural network layers in Python using the torch API\n[or your favorite NumPy-based libraries such as SciPy](https://pytorch.org/tutorials/advanced/numpy_extensions_tutorial.html).\n\nIf you want to write your layers in C/C++, we provide a convenient extension API that is efficient and with minimal boilerplate.\nNo wrapper code needs to be written. You can see [a tutorial here](https://pytorch.org/tutorials/advanced/cpp_extension.html) and [an example here](https://github.com/pytorch/extension-cpp).\n\nInstallation\n\nBinaries\nCommands to install binaries via Conda or pip wheels are on our website: [https://pytorch.org/get-started/locally/](https://pytorch.org/get-started/locally/)\n\nNVIDIA Jetson Platforms\n\nPython wheels for NVIDIA's Jetson Nano, Jetson TX1/TX2, Jetson Xavier NX/AGX, and Jetson AGX Orin are provided [here](https://forums.developer.nvidia.com/t/pytorch-for-jetson-version-1-10-now-available/72048) and the L4T container is published [here](https://catalog.ngc.nvidia.com/orgs/nvidia/containers/l4t-pytorch)\n\nThey require JetPack 4.2 and above, and [@dusty-nv](https://github.com/dusty-nv) and [@ptrblck](https://github.com/ptrblck) are maintaining them.\n\nFrom Source\n\nPrerequisites\nIf you are installing from source, you will need:\n- Python 3.9 or later\n- A compiler that fully supports C++17, such as clang or gcc (gcc 9.4.0 or newer is required, on Linux)\n- Visual Studio or Visual Studio Build Tool (Windows only)\n\n\\* PyTorch CI uses Visual C++ BuildTools, which come with Visual Studio Enterprise,\nProfessional, or Community Editions. You can also install the build tools from\ncome with Visual Studio Code by default.\n\nAn example of environment setup is shown below:\n\n* Linux:\n\n* Windows:\n\nA conda environment is not required.  You can also do a PyTorch build in a\nstandard virtual environment, e.g., created with tools like `uv`, provided\nyour system has installed all the necessary dependencies unavailable as pip\npackages (e.g., CUDA, MKL.)\n\nNVIDIA CUDA Support\nIf you want to compile with CUDA support, [select a supported version of CUDA from our support matrix](https://pytorch.org/get-started/locally/), then install the following:\n\nNote: You could refer to the [cuDNN Support Matrix](https://docs.nvidia.com/deeplearning/cudnn/backend/latest/reference/support-matrix.html) for cuDNN versions with the various supported CUDA, CUDA driver, and NVIDIA hardware.\n\nIf you want to disable CUDA support, export the environment variable `USE_CUDA=0`.\nOther potentially useful environment variables may be found in `setup.py`.  If\nCUDA is installed in a non-standard location, set PATH so that the nvcc you\nwant to use can be found (e.g., `export PATH=/usr/local/cuda-12.8/bin:$PATH`).\n\nIf you are building for NVIDIA's Jetson platforms (Jetson Nano, TX1, TX2, AGX Xavier), Instructions to install PyTorch for Jetson Nano are [available here](https://devtalk.nvidia.com/default/topic/1049071/jetson-nano/pytorch-for-jetson-nano/)\n\nAMD ROCm Support\nIf you want to compile with ROCm support, install\n- [AMD ROCm](https://rocm.docs.amd.com/en/latest/deploy/linux/quick_start.html) 4.0 and above installation\n- ROCm is currently supported only for Linux systems."}, {"name": "torch", "tags": ["math", "ml", "ui", "web"], "summary": "Tensors and Dynamic neural networks in Python with strong GPU acceleration", "text": "By default the build system expects ROCm to be installed in `/opt/rocm`. If ROCm is installed in a different directory, the `ROCM_PATH` environment variable must be set to the ROCm installation directory. The build system automatically detects the AMD GPU architecture. Optionally, the AMD GPU architecture can be explicitly set with the `PYTORCH_ROCM_ARCH` environment variable [AMD GPU architecture](https://rocm.docs.amd.com/projects/install-on-linux/en/latest/reference/system-requirements.html#supported-gpus)\n\nIf you want to disable ROCm support, export the environment variable `USE_ROCM=0`.\nOther potentially useful environment variables may be found in `setup.py`.\n\nIntel GPU Support\nIf you want to compile with Intel GPU support, follow these\n- [PyTorch Prerequisites for Intel GPUs](https://www.intel.com/content/www/us/en/developer/articles/tool/pytorch-prerequisites-for-intel-gpus.html) instructions.\n- Intel GPU is supported for Linux and Windows.\n\nIf you want to disable Intel GPU support, export the environment variable `USE_XPU=0`.\nOther potentially useful environment variables may be found in `setup.py`.\n\nGet the PyTorch Source\n\nInstall Dependencies\n\n**Common**\n\n**On Linux**\n\n**On MacOS**\n\n**On Windows**\n\nInstall PyTorch\n\n**On Linux**\n\nIf you're compiling for AMD ROCm then first run this command:\n\nInstall PyTorch\n\n**On macOS**\n\n**On Windows**\n\nIf you want to build legacy python code, please refer to [Building on legacy code and CUDA](https://github.com/pytorch/pytorch/blob/main/CONTRIBUTING.md#building-on-legacy-code-and-cuda)\n\n**CPU-only builds**\n\nIn this mode PyTorch computations will run on your CPU, not your GPU.\n\nNote on OpenMP: The desired OpenMP implementation is Intel OpenMP (iomp). In order to link against iomp, you'll need to manually download the library and set up the building environment by tweaking `CMAKE_INCLUDE_PATH` and `LIB`. The instruction [here](https://github.com/pytorch/pytorch/blob/main/docs/source/notes/windows.rst#building-from-source) is an example for setting up both MKL and Intel OpenMP. Without these configurations for CMake, Microsoft Visual C OpenMP runtime (vcomp) will be used.\n\n**CUDA based build**\n\nIn this mode PyTorch computations will leverage your GPU via CUDA for faster number crunching\n\n[NVTX](https://docs.nvidia.com/gameworks/content/gameworkslibrary/nvtx/nvidia_tools_extension_library_nvtx.htm) is needed to build Pytorch with CUDA.\nNVTX is a part of CUDA distributive, where it is called \"Nsight Compute\". To install it onto an already installed CUDA run CUDA installation once again and check the corresponding checkbox.\nMake sure that CUDA with Nsight Compute is installed after Visual Studio.\n\nCurrently, VS 2017 / 2019, and Ninja are supported as the generator of CMake. If `ninja.exe` is detected in `PATH`, then Ninja will be used as the default generator, otherwise, it will use VS 2017 / 2019.\n If Ninja is selected as the generator, the latest MSVC will get selected as the underlying toolchain.\n\nAdditional libraries such as\n[Magma](https://developer.nvidia.com/magma), [oneDNN, a.k.a. MKLDNN or DNNL](https://github.com/oneapi-src/oneDNN), and [Sccache](https://github.com/mozilla/sccache) are often needed. Please refer to the [installation-helper](https://github.com/pytorch/pytorch/tree/main/.ci/pytorch/win-test-helpers/installation-helpers) to install them.\n\nYou can refer to the [build_pytorch.bat](https://github.com/pytorch/pytorch/blob/main/.ci/pytorch/win-test-helpers/build_pytorch.bat) script for some other environment variables configurations\n\n**Intel GPU builds**\n\nIn this mode PyTorch with Intel GPU support will be built.\n\nPlease make sure [the common prerequisites](#prerequisites) as well as [the prerequisites for Intel GPU](#intel-gpu-support) are properly installed and the environment variables are configured prior to starting the build. For build tool support, `Visual Studio 2022` is required.\n\nThen PyTorch can be built with the command:\n\nAdjust Build Options (Optional)"}, {"name": "torch", "tags": ["math", "ml", "ui", "web"], "summary": "Tensors and Dynamic neural networks in Python with strong GPU acceleration", "text": "You can adjust the configuration of cmake variables optionally (without building first), by doing\nthe following. For example, adjusting the pre-detected directories for CuDNN or BLAS can be done\nwith such a step.\n\nOn Linux\n\nOn macOS\n\nDocker Image\n\nUsing pre-built images\n\nYou can also pull a pre-built docker image from Docker Hub and run with docker v19.03+\n\nPlease note that PyTorch uses shared memory to share data between processes, so if torch multiprocessing is used (e.g.\nfor multithreaded data loaders) the default shared memory segment size that container runs with is not enough, and you\nshould increase shared memory size either with `--ipc=host` or `--shm-size` command line options to `nvidia-docker run`.\n\nBuilding the image yourself\n\n**NOTE:** Must be built with a docker version > 18.06\n\nThe `Dockerfile` is supplied to build images with CUDA 11.1 support and cuDNN v8.\nYou can pass `PYTHON_VERSION=x.y` make variable to specify which Python version is to be used by Miniconda, or leave it\nunset to use the default.\n\nYou can also pass the `CMAKE_VARS=\"...\"` environment variable to specify additional CMake variables to be passed to CMake during the build.\nSee [setup.py](./setup.py) for the list of available variables.\n\nBuilding the Documentation\n\nTo build documentation in various formats, you will need [Sphinx](http://www.sphinx-doc.org)\nand the pytorch_sphinx_theme2.\n\nBefore you build the documentation locally, ensure `torch` is\ninstalled in your environment. For small fixes, you can install the\nnightly version as described in [Getting Started](https://pytorch.org/get-started/locally/).\n\nFor more complex fixes, such as adding a new module and docstrings for\nthe new module, you might need to install torch [from source](#from-source).\nSee [Docstring Guidelines](https://github.com/pytorch/pytorch/wiki/Docstring-Guidelines)\nfor docstring conventions.\n\nRun `make` to get a list of all available output formats.\n\nIf you get a katex error run `npm install katex`.  If it persists, try\n`npm install -g katex`\n\n> [!NOTE]\n> If you installed `nodejs` with a different package manager (e.g.,\n> `conda`) then `npm` will probably install a version of `katex` that is not\n> compatible with your version of `nodejs` and doc builds will fail.\n> A combination of versions that is known to work is `node@6.13.1` and\n> `katex@0.13.18`. To install the latter with `npm` you can run\n>\n\n> [!NOTE]\n> If you see a numpy incompatibility error, run:\n>\n\nWhen you make changes to the dependencies run by CI, edit the\n`.ci/docker/requirements-docs.txt` file.\n\nBuilding a PDF\n\nTo compile a PDF of all PyTorch documentation, ensure you have\n`texlive` and LaTeX installed. On macOS, you can install them using:\n\nTo create the PDF:\n\n1. Run:\n\nThis will generate the necessary files in the `build/latex` directory.\n\n2. Navigate to this directory and execute:\n\nThis will produce a `pytorch.pdf` with the desired content. Run this\n   command one more time so that it generates the correct table\n   of contents and index.\n\n> [!NOTE]\n> To view the Table of Contents, switch to the **Table of Contents**\n> view in your PDF viewer.\n\nPrevious Versions\n\nInstallation instructions and binaries for previous PyTorch versions may be found\non [our website](https://pytorch.org/get-started/previous-versions).\n\nGetting Started"}, {"name": "torch", "tags": ["math", "ml", "ui", "web"], "summary": "Tensors and Dynamic neural networks in Python with strong GPU acceleration", "text": "Three pointers to get you started:\n\nResources\n\n* [PyTorch.org](https://pytorch.org/)\n* [PyTorch Tutorials](https://pytorch.org/tutorials/)\n* [PyTorch Examples](https://github.com/pytorch/examples)\n* [PyTorch Models](https://pytorch.org/hub/)\n* [Intro to Deep Learning with PyTorch from Udacity](https://www.udacity.com/course/deep-learning-pytorch--ud188)\n* [Intro to Machine Learning with PyTorch from Udacity](https://www.udacity.com/course/intro-to-machine-learning-nanodegree--nd229)\n* [Deep Neural Networks with PyTorch from Coursera](https://www.coursera.org/learn/deep-neural-networks-with-pytorch)\n* [PyTorch Twitter](https://twitter.com/PyTorch)\n* [PyTorch Blog](https://pytorch.org/blog/)\n* [PyTorch YouTube](https://www.youtube.com/channel/UCWXI5YeOsh03QvJ59PMaXFw)\n\nCommunication\n* Forums: Discuss implementations, research, etc. https://discuss.pytorch.org\n* GitHub Issues: Bug reports, feature requests, install issues, RFCs, thoughts, etc.\n* Slack: The [PyTorch Slack](https://pytorch.slack.com/) hosts a primary audience of moderate to experienced PyTorch users and developers for general chat, online discussions, collaboration, etc. If you are a beginner looking for help, the primary medium is [PyTorch Forums](https://discuss.pytorch.org). If you need a slack invite, please fill this form: https://goo.gl/forms/PP1AGvNHpSaJP8to1\n* Newsletter: No-noise, a one-way email newsletter with important announcements about PyTorch. You can sign-up here: https://eepurl.com/cbG0rv\n* Facebook Page: Important announcements about PyTorch. https://www.facebook.com/pytorch\n* For brand guidelines, please visit our website at [pytorch.org](https://pytorch.org/)\n\nReleases and Contributing\n\nTypically, PyTorch has three minor releases a year. Please let us know if you encounter a bug by [filing an issue](https://github.com/pytorch/pytorch/issues).\n\nWe appreciate all contributions. If you are planning to contribute back bug-fixes, please do so without any further discussion.\n\nIf you plan to contribute new features, utility functions, or extensions to the core, please first open an issue and discuss the feature with us.\nSending a PR without discussion might end up resulting in a rejected PR because we might be taking the core in a different direction than you might be aware of.\n\nTo learn more about making a contribution to Pytorch, please see our [Contribution page](CONTRIBUTING.md). For more information about PyTorch releases, see [Release page](RELEASE.md).\n\nThe Team\n\nPyTorch is a community-driven project with several skillful engineers and researchers contributing to it.\n\nPyTorch is currently maintained by [Soumith Chintala](http://soumith.ch), [Gregory Chanan](https://github.com/gchanan), [Dmytro Dzhulgakov](https://github.com/dzhulgakov), [Edward Yang](https://github.com/ezyang), [Alban Desmaison](https://github.com/albanD), [Piotr Bialecki](https://github.com/ptrblck) and [Nikita Shulga](https://github.com/malfet) with major contributions coming from hundreds of talented individuals in various forms and means.\nA non-exhaustive but growing list needs to mention: [Trevor Killeen](https://github.com/killeent), [Sasank Chilamkurthy](https://github.com/chsasank), [Sergey Zagoruyko](https://github.com/szagoruyko), [Adam Lerer](https://github.com/adamlerer), [Francisco Massa](https://github.com/fmassa), [Alykhan Tejani](https://github.com/alykhantejani), [Luca Antiga](https://github.com/lantiga), [Alban Desmaison](https://github.com/albanD), [Andreas Koepf](https://github.com/andreaskoepf), [James Bradbury](https://github.com/jekbradbury), [Zeming Lin](https://github.com/ebetica), [Yuandong Tian](https://github.com/yuandong-tian), [Guillaume Lample](https://github.com/glample), [Marat Dukhan](https://github.com/Maratyszcza), [Natalia Gimelshein](https://github.com/ngimel), [Christian Sarofeen](https://github.com/csarofeen), [Martin Raison](https://github.com/martinraison), [Edward Yang](https://github.com/ezyang), [Zachary Devito](https://github.com/zdevito).\n\nNote: This project is unrelated to [hughperkins/pytorch](https://github.com/hughperkins/pytorch) with the same name. Hugh is a valuable contributor to the Torch community and has helped with many things Torch and PyTorch.\n\nLicense\n\nPyTorch has a BSD-style license, as found in the [LICENSE](LICENSE) file."}, {"name": "torch", "tags": ["math", "ml", "ui", "web"], "summary": "Tensors and Dynamic neural networks in Python with strong GPU acceleration", "text": "This library is used to build dynamic neural networks in Python with strong GPU acceleration, allowing developers to leverage their computer's graphics processing unit for faster computations. With this library, developers can easily train, test, and deploy deep learning models on a large scale."}, {"name": "torchaudio", "tags": ["data", "math", "ml", "ui", "web"], "summary": "An audio package for PyTorch", "text": "torchaudio: an audio library for PyTorch\n========================================\n\n(https://pytorch.org/audio/main/)\n(https://anaconda.org/pytorch/torchaudio)\n(https://anaconda.org/pytorch/torchaudio)\n\n> [!NOTE]\n> **We have transitioned TorchAudio into a\n>  maintenance phase. This process removed some user-facing\n>  features. These features were deprecated from TorchAudio 2.8 and removed in 2.9.\n>  Our main goals were to reduce redundancies with the rest of the\n>  PyTorch ecosystem, make it easier to maintain, and create a version of\n>  TorchAudio that is more tightly scoped to its strengths: processing audio\n>  data for ML. Please see\n>  [our community message](https://github.com/pytorch/audio/issues/3902)\n>  for more details.**\n\nThe aim of torchaudio is to apply [PyTorch](https://github.com/pytorch/pytorch) to\nthe audio domain. By supporting PyTorch, torchaudio follows the same philosophy\nof providing strong GPU acceleration, having a focus on trainable features through\nthe autograd system, and having consistent style (tensor names and dimension names).\nTherefore, it is primarily a machine learning library and not a general signal\nprocessing library. The benefits of PyTorch can be seen in torchaudio through\nhaving all the computations be through PyTorch operations which makes it easy\nto use and feel like a natural extension.\n\n- [Dataloaders for common audio datasets](http://pytorch.org/audio/main/datasets.html)\n- Audio and speech processing functions\n  - [forced_align](https://pytorch.org/audio/main/generated/torchaudio.functional.forced_align.html)\n- Common audio transforms\n  - [Spectrogram, AmplitudeToDB, MelScale, MelSpectrogram, MFCC, MuLawEncoding, MuLawDecoding, Resample](http://pytorch.org/audio/main/transforms.html)\n- Compliance interfaces: Run code using PyTorch that align with other libraries\n  - [Kaldi: spectrogram, fbank, mfcc](https://pytorch.org/audio/main/compliance.kaldi.html)\n\nInstallation\n------------\n\nPlease refer to https://pytorch.org/audio/main/installation.html for installation and build process of TorchAudio.\n\nAPI Reference\n-------------\n\nAPI Reference is located here: http://pytorch.org/audio/main/\n\nContributing Guidelines\n-----------------------\n\nPlease refer to [CONTRIBUTING.md](./CONTRIBUTING.md)\n\nCitation\n--------\n\nIf you find this package useful, please cite as:\n\nDisclaimer on Datasets\n----------------------\n\nThis is a utility library that downloads and prepares public datasets. We do not host or distribute these datasets, vouch for their quality or fairness, or claim that you have license to use the dataset. It is your responsibility to determine whether you have permission to use the dataset under the dataset's license.\n\nIf you're a dataset owner and wish to update any part of it (description, citation, etc.), or do not want your dataset to be included in this library, please get in touch through a GitHub issue. Thanks for your contribution to the ML community!\n\nPre-trained Model License\n-------------------------\n\nThe pre-trained models provided in this library may have their own licenses or terms and conditions derived from the dataset used for training. It is your responsibility to determine whether you have permission to use the models for your use case.\n\nFor instance, SquimSubjective model is released under the Creative Commons Attribution Non Commercial 4.0 International (CC-BY-NC 4.0) license. See [the link](https://zenodo.org/record/4660670#.ZBtWPOxuerN) for additional details.\n\nOther pre-trained models that have different license are noted in documentation. Please checkout the [documentation page](https://pytorch.org/audio/main/)."}, {"name": "torchaudio", "tags": ["data", "math", "ml", "ui", "web"], "summary": "An audio package for PyTorch", "text": "This library is used to process audio data for machine learning tasks, providing a tightly scoped package that leverages PyTorch's strengths in audio processing. Developers can achieve efficient and effective handling of audio data with this library."}, {"name": "torchdata", "tags": ["data", "math", "ml", "web"], "summary": "Composable data loading modules for PyTorch", "text": "TorchData (see note below on current status)\n\n[**What is TorchData?**](#what-is-torchdata) | [**Stateful DataLoader**](#stateful-dataloader) |\n[**Install guide**](#installation) | [**Contributing**](#contributing) | [**License**](#license)\n\n**:warning: June 2024 Status Update: Removing DataPipes and DataLoader V2**\n\n**We are re-focusing the torchdata repo to be an iterative enhancement of torch.utils.data.DataLoader. We do not plan on\ncontinuing development or maintaining the [`DataPipes`] and [`DataLoaderV2`] solutions, and they will be removed from\nthe torchdata repo. We'll also be revisiting the `DataPipes` references in pytorch/pytorch. In release\n`torchdata==0.8.0` (July 2024) they will be marked as deprecated, and sometime after 0.9.0 (Oct 2024) they will be\ndeleted. Existing users are advised to pin to `torchdata==0.9.0` or an older version until they are able to migrate\naway. Subsequent releases will not include DataPipes or DataLoaderV2. The old version of this README is\n[available here](https://github.com/pytorch/data/blob/v0.7.1/README.md). Please reach out if you suggestions or comments\n(please use [#1196](https://github.com/pytorch/data/issues/1196) for feedback).**\n\n## What is TorchData?\n\nThe TorchData project is an iterative enhancement to the PyTorch torch.utils.data.DataLoader and\ntorch.utils.data.Dataset/IterableDataset to make them scalable, performant dataloading solutions. We will be iterating\non the enhancements under [the torchdata repo](torchdata).\n\nOur first change begins with adding checkpointing to torch.utils.data.DataLoader, which can be found in\n[stateful_dataloader, a drop-in replacement for torch.utils.data.DataLoader](torchdata/stateful_dataloader), by defining\n`load_state_dict` and `state_dict` methods that enable mid-epoch checkpointing, and an API for users to track custom\niteration progress, and other custom states from the dataloader workers such as token buffers and/or RNG states.\n\nStateful DataLoader\n\n`torchdata.stateful_dataloader.StatefulDataLoader` is a drop-in replacement for torch.utils.data.DataLoader which\nprovides state_dict and load_state_dict functionality. See\n[the Stateful DataLoader main page](torchdata/stateful_dataloader) for more information and examples. Also check out the\nexamples\n[in this Colab notebook](https://colab.research.google.com/drive/1tonoovEd7Tsi8EW8ZHXf0v3yHJGwZP8M?usp=sharing).\n\ntorchdata.nodes\n\ntorchdata.nodes is a library of composable iterators (not iterables!) that let you chain together common dataloading and\npre-proc operations. It follows a streaming programming model, although \"sampler + Map-style\" can still be configured if\nyou desire. See [torchdata.nodes main page](torchdata/nodes) for more details. Stay tuned for tutorial on\ntorchdata.nodes coming soon!\n\nInstallation\n\nVersion Compatibility\n\nThe following is the corresponding `torchdata` versions and supported Python versions.\n\n`torch`\n--------------------\n`master` / `nightly`\n\nLocal pip or conda\n\nFirst, set up an environment. We will be installing a PyTorch binary as well as torchdata. If you're using conda, create\na conda environment:\n\nIf you wish to use `venv` instead:\n\nInstall torchdata:\n\nUsing pip:\n\nUsing conda:\n\nFrom source\n\nIn case building TorchData from source fails, install the nightly version of PyTorch following the linked guide on the\n[contributing page](CONTRIBUTING.md#install-pytorch-nightly).\n\nFrom nightly\n\nThe nightly version of TorchData is also provided and updated daily from main branch.\n\nUsing pip:\n\nUsing conda:\n\nContributing\n\nWe welcome PRs! See the [CONTRIBUTING](CONTRIBUTING.md) file.\n\nBeta Usage and Feedback\n\nWe'd love to hear from and work with early adopters to shape our designs. Please reach out by raising an issue if you're\ninterested in using this tooling for your project.\n\nLicense\n\nTorchData is BSD licensed, as found in the [LICENSE](LICENSE) file."}, {"name": "torchdata", "tags": ["data", "math", "ml", "web"], "summary": "Composable data loading modules for PyTorch", "text": "This library is used to enhance the functionality of PyTorch's built-in DataLoader by providing composable data loading modules for more efficient and flexible data processing. By leveraging torchdata, developers can create complex data pipelines with ease, streamlining their machine learning workflows and improving overall performance."}, {"name": "tqdm", "tags": ["cli", "ui", "web"], "summary": "Fast, Extensible Progress Meter", "text": "Logo\n\ntqdm\n====\n\nPy-Versions\n\nBuild-Status\n\nLICENCE\n\n``tqdm`` derives from the Arabic word *taqaddum* (\u062a\u0642\u062f\u0651\u0645) which can mean \"progress,\"\nand is an abbreviation for \"I love you so much\" in Spanish (*te quiero demasiado*).\n\nInstantly make your loops show a smart progress meter - just wrap any\niterable with ``tqdm(iterable)``, and you're done!\n\n.. code:: python\n\n``76%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 | 7568/10000 [00:33 backup.tgz\nan 800ns/iter overhead.\n\nIn addition to its low overhead, ``tqdm`` uses smart algorithms to predict\nthe remaining time and to skip unnecessary iteration displays, which allows\nfor a negligible overhead in most cases.\n\n``tqdm`` works on any platform\n(Linux, Windows, Mac, FreeBSD, NetBSD, Solaris/SunOS),\nin any console or in a GUI, and is also friendly with IPython/Jupyter notebooks.\n\n``tqdm`` does not require any dependencies (not even ``curses``!), just\nPython and an environment supporting ``carriage return \\r`` and\n``line feed \\n`` control characters.\n\n------------------------------------------\n\n.. contents:: Table of contents\n   :backlinks: top\n   :local:\n\nInstallation\n------------\n\nLatest PyPI stable release\n~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nVersions\n\n.. code:: sh\n\nLatest development release on GitHub\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nGitHub-Status\n\nPull and install pre-release ``devel`` branch:\n\n.. code:: sh\n\nLatest Conda release\n~~~~~~~~~~~~~~~~~~~~\n\nConda-Forge-Status\n\n.. code:: sh\n\nLatest Snapcraft release\n~~~~~~~~~~~~~~~~~~~~~~~~\n\nSnapcraft\n\nThere are 3 channels to choose from:\n\n.. code:: sh\n\nNote that ``snap`` binaries are purely for CLI use (not ``import``-able), and\nautomatically set up ``bash`` tab-completion.\n\nLatest Docker release\n~~~~~~~~~~~~~~~~~~~~~\n\nDocker\n\n.. code:: sh\n\nOther\n~~~~~\n\nThere are other (unofficial) places where ``tqdm`` may be downloaded, particularly for CLI use:\n\nRepology\n\n:target: https://repology.org/project/python:tqdm/versions\n\nChangelog\n---------\n\nThe list of all changes is available either on GitHub's Releases:\nGitHub-Status\n`wiki `__, or on the\n`website `__.\n\nUsage\n-----\n\n``tqdm`` is very versatile and can be used in a number of ways.\nThe three main ones are given below.\n\nIterable-based\n~~~~~~~~~~~~~~\n\nWrap ``tqdm()`` around any iterable:\n\n.. code:: python\n\n``trange(i)`` is a special optimised instance of ``tqdm(range(i))``:\n\n.. code:: python\n\nInstantiation outside of the loop allows for manual control over ``tqdm()``:\n\n.. code:: python\n\nManual\n~~~~~~\n\nManual control of ``tqdm()`` updates using a ``with`` statement:\n\n.. code:: python\n\nIf the optional variable ``total`` (or an iterable with ``len()``) is\nprovided, predictive stats are displayed.\n\n``with`` is also optional (you can just assign ``tqdm()`` to a variable,\nbut in this case don't forget to ``del`` or ``close()`` at the end:\n\n.. code:: python\n\nModule\n~~~~~~\n\nPerhaps the most wonderful use of ``tqdm`` is in a script or on the command\nline. Simply inserting ``tqdm`` (or ``python -m tqdm``) between pipes will pass\nthrough all ``stdin`` to ``stdout`` while printing progress to ``stderr``.\n\nThe example below demonstrate counting the number of lines in all Python files\nin the current directory, with timing information included.\n\n.. code:: sh\n\nNote that the usual arguments for ``tqdm`` can also be specified.\n\n.. code:: sh\n\n- Nested progress bars:\n\n* Consoles in general: require support for moving cursors up to the\n  * Windows: additionally may require the Python module ``colorama``\n\n- Unicode:"}, {"name": "tqdm", "tags": ["cli", "ui", "web"], "summary": "Fast, Extensible Progress Meter", "text": "* Environments which report that they support unicode will have solid smooth\n  * Windows consoles often only partially support unicode and thus\n\n- Wrapping generators:\n\n* Generator wrapper functions tend to hide the length of iterables.\n  * Replace ``tqdm(enumerate(...))`` with ``enumerate(tqdm(...))`` or\n  * Replace ``tqdm(zip(a, b))`` with ``zip(tqdm(a), b)`` or even\n  * The same applies to ``itertools``.\n  * Some useful convenience functions can be found under ``tqdm.contrib``.\n\n- `No intermediate output in docker-compose `__:\n  use ``docker-compose run`` instead of ``docker-compose up`` and ``tty: true``.\n\n- Overriding defaults via environment variables:\n  e.g. in CI/cloud jobs, ``export TQDM_MININTERVAL=5`` to avoid log spam.\n  This override logic is handled by the ``tqdm.utils.envwrap`` decorator\n  (useful independent of ``tqdm``).\n\nIf you come across any other difficulties, browse and file |GitHub-Issues|.\n\nDocumentation\n-------------\n\nPy-Versions\n\n.. code:: python\n\nParameters\n~~~~~~~~~~\n\n* iterable  : iterable, optional  \n* desc  : str, optional  \n* total  : int or float, optional  \n* leave  : bool, optional  \n* file  : ``io.TextIOWrapper`` or ``io.StringIO``, optional  \n* ncols  : int, optional  \n* mininterval  : float, optional  \n* maxinterval  : float, optional  \n* miniters  : int or float, optional  \n* ascii  : bool or str, optional  \n* disable  : bool, optional  \n* unit  : str, optional  \n* unit_scale  : bool or int or float, optional  \n* dynamic_ncols  : bool, optional  \n* smoothing  : float, optional  \n* bar_format  : str, optional\n\nConvenience Functions\n~~~~~~~~~~~~~~~~~~~~~\n\n.. code:: python\n\nSubmodules\n~~~~~~~~~~\n\n.. code:: python\n\n``contrib``\n+++++++++++\n\nThe ``tqdm.contrib`` package also contains experimental modules:\n\n- ``tqdm.contrib.itertools``: Thin wrappers around ``itertools``\n- ``tqdm.contrib.concurrent``: Thin wrappers around ``concurrent.futures``\n- ``tqdm.contrib.slack``: Posts to `Slack `__ bots\n- ``tqdm.contrib.discord``: Posts to `Discord `__ bots\n- ``tqdm.contrib.telegram``: Posts to `Telegram `__ bots\n- ``tqdm.contrib.bells``: Automagically enables all optional features\n\n* ``auto``, ``pandas``, ``slack``, ``discord``, ``telegram``\n\nExamples and Advanced Usage\n---------------------------\n\n- See the `examples `__\n  folder;\n- import the module and run ``help()``;\n- consult the `wiki `__;\n\n* this has an\n\n- check out the `slides from PyData London `__, or\n- run the |binder-demo|.\n\nDescription and additional stats\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nCustom information can be displayed and updated dynamically on ``tqdm`` bars\nwith the ``desc`` and ``postfix`` arguments:\n\n.. code:: python\n\nPoints to remember when using ``{postfix[...]}`` in the ``bar_format`` string:\n\n- ``postfix`` also needs to be passed as an initial argument in a compatible\n  format, and\n- ``postfix`` will be auto-converted to a string if it is a ``dict``-like\n  object. To prevent this behaviour, insert an extra item into the dictionary\n  where the key is not a string.\n\nAdditional ``bar_format`` parameters may also be defined by overriding\n``format_dict``, and the bar itself may be modified using ``ascii``:\n\n.. code:: python\n\n.. code::\n\n* ``int `__.\nFunctional alternative in\n`examples/tqdm_wget.py `__.\n\nIt is recommend to use ``miniters=1`` whenever there is potentially\nlarge differences in iteration speed (e.g. downloading a file over\na patchy connection).\n\n**Wrapping read/write methods**\n\nTo measure throughput through a file-like object's ``read`` or ``write``\nmethods, use ``CallbackIOWrapper``:\n\n.. code:: python"}, {"name": "tqdm", "tags": ["cli", "ui", "web"], "summary": "Fast, Extensible Progress Meter", "text": "Alternatively, use the even simpler ``wrapattr`` convenience function,\nwhich would condense both the ``urllib`` and ``CallbackIOWrapper`` examples\ndown to:\n\n.. code:: python\n\nThe ``requests`` equivalent is nearly identical:\n\n.. code:: python\n\n**Custom callback**\n\n``tqdm`` is known for intelligently skipping unnecessary displays. To make a\ncustom callback take advantage of this, simply use the return value of\n``update()``. This is set to ``True`` if a ``display()`` was triggered.\n\n.. code:: python\n\n``asyncio``\n~~~~~~~~~~~\n\nNote that ``break`` isn't currently caught by asynchronous iterators.\nThis means that ``tqdm`` cannot clean up after itself in this case:\n\n.. code:: python\n\nInstead, either call ``pbar.close()`` manually or use the context manager syntax:\n\n.. code:: python\n\nPandas Integration\n~~~~~~~~~~~~~~~~~~\n\nDue to popular demand we've added support for ``pandas`` -- here's an example\nfor ``DataFrame.progress_apply`` and ``DataFrameGroupBy.progress_apply``:\n\n.. code:: python\n\nIn case you're interested in how this works (and how to modify it for your\nown callbacks), see the\n`examples `__\nfolder or import the module and run ``help()``.\n\nKeras Integration\n~~~~~~~~~~~~~~~~~\n\nA ``keras`` callback is also available:\n\n.. code:: python\n\nDask Integration\n~~~~~~~~~~~~~~~~\n\nA ``dask`` callback is also available:\n\n.. code:: python\n\nIPython/Jupyter Integration\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nIPython/Jupyter is supported via the ``tqdm.notebook`` submodule:\n\n.. code:: python\n\nIn addition to ``tqdm`` features, the submodule provides a native Jupyter\nwidget (compatible with IPython v1-v4 and Jupyter), fully working nested bars\nand colour hints (blue: normal, green: completed, red: error/interrupt,\nlight blue: no ETA); as demonstrated below.\n\nScreenshot-Jupyter1\nScreenshot-Jupyter2\nScreenshot-Jupyter3\n\nThe ``notebook`` version supports percentage or pixels for overall width\n(e.g.: ``ncols='100%'`` or ``ncols='480px'``).\n\nIt is also possible to let ``tqdm`` automatically choose between\nconsole or notebook versions by using the ``autonotebook`` submodule:\n\n.. code:: python\n\nNote that this will issue a ``TqdmExperimentalWarning`` if run in a notebook\nsince it is not meant to be possible to distinguish between ``jupyter notebook``\nand ``jupyter console``. Use ``auto`` instead of ``autonotebook`` to suppress\nthis warning.\n\nNote that notebooks will display the bar in the cell where it was created.\nThis may be a different cell from the one where it is used.\nIf this is not desired, either\n\n- delay the creation of the bar to the cell where it must be displayed, or\n- create the bar with ``display=False``, and in a later cell call\n  ``display(bar.container)``:\n\n.. code:: python\n\n.. code:: python\n\nThe ``keras`` callback has a ``display()`` method which can be used likewise:\n\n.. code:: python\n\n.. code:: python\n\nAnother possibility is to have a single bar (near the top of the notebook)\nwhich is constantly re-used (using ``reset()`` rather than ``close()``).\nFor this reason, the notebook version (unlike the CLI version) does not\nautomatically call ``close()`` upon ``Exception``.\n\n.. code:: python\n\n.. code:: python\n\nCustom Integration\n~~~~~~~~~~~~~~~~~~\n\nTo change the default arguments (such as making ``dynamic_ncols=True``),\nsimply use built-in Python magic:\n\n.. code:: python\n\nFor further customisation,\n``tqdm`` may be inherited from to create custom callbacks (as with the\n``TqdmUpTo`` example `above `__) or for custom frontends\n(e.g. GUIs such as notebook or plotting packages). In the latter case:"}, {"name": "tqdm", "tags": ["cli", "ui", "web"], "summary": "Fast, Extensible Progress Meter", "text": "1. ``def __init__()`` to call ``super().__init__(..., gui=True)`` to disable\n   terminal ``status_printer`` creation.\n2. Redefine: ``close()``, ``clear()``, ``display()``.\n\nConsider overloading ``display()`` to use e.g.\n``self.frontend(**self.format_dict)`` instead of ``self.sp(repr(self))``.\n\nSome submodule examples of inheritance:\n\n- `tqdm/notebook.py `__\n- `tqdm/gui.py `__\n- `tqdm/tk.py `__\n- `tqdm/contrib/slack.py `__\n- `tqdm/contrib/discord.py `__\n- `tqdm/contrib/telegram.py `__\n\nDynamic Monitor/Meter\n~~~~~~~~~~~~~~~~~~~~~\n\nYou can use a ``tqdm`` as a meter which is not monotonically increasing.\nThis could be because ``n`` decreases (e.g. a CPU usage monitor) or ``total``\nchanges.\n\nOne example would be recursively searching for files. The ``total`` is the\nnumber of objects found so far, while ``n`` is the number of those objects which\nare files (rather than folders):\n\n.. code:: python\n\nUsing ``update(0)`` is a handy way to let ``tqdm`` decide when to trigger a\ndisplay refresh to avoid console spamming.\n\nWriting messages\n~~~~~~~~~~~~~~~~\n\nThis is a work in progress (see\n`#737 `__).\n\nSince ``tqdm`` uses a simple printing mechanism to display progress bars,\nyou should not write any message in the terminal using ``print()`` while\na progressbar is open.\n\nTo write messages in the terminal without any collision with ``tqdm`` bar\ndisplay, a ``.write()`` method is provided:\n\n.. code:: python\n\nBy default, this will print to standard output ``sys.stdout``. but you can\nspecify any file-like object using the ``file`` argument. For example, this\ncan be used to redirect the messages writing to a log file or class.\n\nRedirecting writing\n~~~~~~~~~~~~~~~~~~~\n\nIf using a library that can print messages to the console, editing the library\nby  replacing ``print()`` with ``tqdm.write()`` may not be desirable.\nIn that case, redirecting ``sys.stdout`` to ``tqdm.write()`` is an option.\n\nTo redirect ``sys.stdout``, create a file-like class that will write\nany input string to ``tqdm.write()``, and supply the arguments\n``file=sys.stdout, dynamic_ncols=True``.\n\nA reusable canonical example is given below:\n\n.. code:: python\n\nRedirecting ``logging``\n~~~~~~~~~~~~~~~~~~~~~~~\n\nSimilar to ``sys.stdout``/``sys.stderr`` as detailed above, console ``logging``\nmay also be redirected to ``tqdm.write()``.\n\nWarning: if also redirecting ``sys.stdout``/``sys.stderr``, make sure to\nredirect ``logging`` first if needed.\n\nHelper methods are available in ``tqdm.contrib.logging``. For example:\n\n.. code:: python\n\nMonitoring thread, intervals and miniters\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n``tqdm`` implements a few tricks to increase efficiency and reduce overhead.\n\n- Avoid unnecessary frequent bar refreshing: ``mininterval`` defines how long\n  to wait between each refresh. ``tqdm`` always gets updated in the background,\n  but it will display only every ``mininterval``.\n- Reduce number of calls to check system clock/time.\n- ``mininterval`` is more intuitive to configure than ``miniters``.\n  A clever adjustment system ``dynamic_miniters`` will automatically adjust\n  ``miniters`` to the amount of iterations that fit into time ``mininterval``.\n  Essentially, ``tqdm`` will check if it's time to print without actually\n  checking time. This behaviour can be still be bypassed by manually setting\n  ``miniters``.\n\nHowever, consider a case with a combination of fast and slow iterations.\nAfter a few fast iterations, ``dynamic_miniters`` will set ``miniters`` to a\nlarge number. When iteration rate subsequently slows, ``miniters`` will\nremain large and thus reduce display update frequency. To address this:"}, {"name": "tqdm", "tags": ["cli", "ui", "web"], "summary": "Fast, Extensible Progress Meter", "text": "- ``maxinterval`` defines the maximum time between display refreshes.\n  A concurrent monitoring thread checks for overdue updates and forces one\n  where necessary.\n\nThe monitoring thread should not have a noticeable overhead, and guarantees\nupdates at least every 10 seconds by default.\nThis value can be directly changed by setting the ``monitor_interval`` of\nany ``tqdm`` instance (i.e. ``t = tqdm.tqdm(...); t.monitor_interval = 2``).\nThe monitor thread may be disabled application-wide by setting\n``tqdm.tqdm.monitor_interval = 0`` before instantiation of any ``tqdm`` bar.\n\nMerch\n-----\n\nYou can buy `tqdm branded merch `__ now!\n\nContributions\n-------------\n\nGitHub-Commits\n\nAll source code is hosted on `GitHub `__.\nContributions are welcome.\n\nSee the\n`CONTRIBUTING `__\nfile for more information.\n\nDevelopers who have made significant contributions, ranked by *SLoC*\n(surviving lines of code,\n`git fame `__ ``-wMC --excl '\\.(png|gif|jpg)$'``),\nare:\n\n==================== ======================================================== ==== ================================\nName                 ID                                                       SLoC Notes\n==================== ======================================================== ==== ================================\nCasper da Costa-Luis `casperdcl `__             ~80% primary maintainer |Gift-Casper|\nStephen Larroque     `lrq3000 `__                 ~9%  team member\nMartin Zugnoni       `martinzugnoni `__     ~3%\nDaniel Ecer          `de-code `__                 ~2%\nRichard Sheridan     `richardsheridan `__ ~1%\nGuangshuo Chen       `chengs `__                   ~1%\nHelio Machado        `0x2b3bfa0 `__             ~1%\nKyle Altendorf       `altendky `__               `__               `__       `__                     `__                     `__.\n\nLICENCE\n-------\n\nOpen Source (OSI approved): |LICENCE|\n\nCitation information: |DOI|\n\nREADME-Hits\n\n.. |Logo| image:: https://tqdm.github.io/img/logo.gif\n.. |Screenshot| image:: https://tqdm.github.io/img/tqdm.gif\n.. |Video| image:: https://tqdm.github.io/img/video.jpg\n   :target: https://tqdm.github.io/video\n.. |Slides| image:: https://tqdm.github.io/img/slides.jpg\n   :target: https://tqdm.github.io/PyData2019/slides.html\n.. |Merch| image:: https://tqdm.github.io/img/merch.jpg\n   :target: https://tqdm.github.io/merch\n.. |Build-Status| image:: https://img.shields.io/github/actions/workflow/status/tqdm/tqdm/test.yml?branch=master&label=tqdm&logo=GitHub\n   :target: https://github.com/tqdm/tqdm/actions/workflows/test.yml\n.. |Coverage-Status| image:: https://img.shields.io/coveralls/github/tqdm/tqdm/master?logo=coveralls\n   :target: https://coveralls.io/github/tqdm/tqdm\n\n:target: https://codecov.io/gh/tqdm/tqdm\n\n:target: https://www.codacy.com/gh/tqdm/tqdm/dashboard\n\n:target: https://bestpractices.coreinfrastructure.org/projects/3264\n.. |GitHub-Status| image:: https://img.shields.io/github/tag/tqdm/tqdm.svg?maxAge=86400&logo=github&logoColor=white\n   :target: https://github.com/tqdm/tqdm/releases\n.. |GitHub-Forks| image:: https://img.shields.io/github/forks/tqdm/tqdm.svg?logo=github&logoColor=white\n   :target: https://github.com/tqdm/tqdm/network\n.. |GitHub-Stars| image:: https://img.shields.io/github/stars/tqdm/tqdm.svg?logo=github&logoColor=white\n   :target: https://github.com/tqdm/tqdm/stargazers\n.. |GitHub-Commits| image:: https://img.shields.io/github/commit-activity/y/tqdm/tqdm.svg?logo=git&logoColor=white\n   :target: https://github.com/tqdm/tqdm/graphs/commit-activity\n.. |GitHub-Issues| image:: https://img.shields.io/github/issues-closed/tqdm/tqdm.svg?logo=github&logoColor=white\n   :target: https://github.com/tqdm/tqdm/issues?q=\n.. |GitHub-PRs| image:: https://img.shields.io/github/issues-pr-closed/tqdm/tqdm.svg?logo=github&logoColor=white\n   :target: https://github.com/tqdm/tqdm/pulls\n.. |GitHub-Contributions| image:: https://img.shields.io/github/contributors/tqdm/tqdm.svg?logo=github&logoColor=white\n   :target: https://github.com/tqdm/tqdm/graphs/contributors\n.. |GitHub-Updated| image:: https://img.shields.io/github/last-commit/tqdm/tqdm/master.svg?logo=github&logoColor=white&label=pushed\n   :target: https://github.com/tqdm/tqdm/pulse\n\n:target: https://cdcl.ml/sponsor\n.. |Versions| image:: https://img.shields.io/pypi/v/tqdm.svg\n   :target: https://tqdm.github.io/releases\n.. |PyPI-Downloads| image:: https://img.shields.io/pypi/dm/tqdm.svg?label=pypi%20downloads&logo=PyPI&logoColor=white\n   :target: https://pepy.tech/project/tqdm\n.. |Py-Versions| image:: https://img.shields.io/pypi/pyversions/tqdm.svg?logo=python&logoColor=white\n   :target: https://pypi.org/project/tqdm\n.. |Conda-Forge-Status| image:: https://img.shields.io/conda/v/conda-forge/tqdm.svg?label=conda-forge&logo=conda-forge\n   :target: https://anaconda.org/conda-forge/tqdm\n\n:target: https://snapcraft.io/tqdm\n\n:target: https://hub.docker.com/r/tqdm/tqdm\n.. |Libraries-Rank| image:: https://img.shields.io/librariesio/sourcerank/pypi/tqdm.svg?logo=koding&logoColor=white\n   :target: https://libraries.io/pypi/tqdm\n.. |Libraries-Dependents| image:: https://img.shields.io/librariesio/dependent-repos/pypi/tqdm.svg?logo=koding&logoColor=white\n\n:target: https://github.com/vinta/awesome-python\n.. |LICENCE| image:: https://img.shields.io/pypi/l/tqdm.svg\n   :target: https://raw.githubusercontent.com/tqdm/tqdm/master/LICENCE\n\n:target: https://doi.org/10.5281/zenodo.595120\n\n:target: https://mybinder.org/v2/gh/tqdm/tqdm/master?filepath=DEMO.ipynb\n.. |Screenshot-Jupyter1| image:: https://tqdm.github.io/img/jupyter-1.gif\n.. |Screenshot-Jupyter2| image:: https://tqdm.github.io/img/jupyter-2.gif\n.. |Screenshot-Jupyter3| image:: https://tqdm.github.io/img/jupyter-3.gif\n.. |README-Hits| image:: https://cgi.cdcl.ml/hits?q=tqdm&style=social&r=https://github.com/tqdm/tqdm&l=https://tqdm.github.io/img/favicon.png&f=https://tqdm.github.io/img/logo.gif\n   :target: https://cgi.cdcl.ml/hits?q=tqdm&a=plot&r=https://github.com/tqdm/tqdm&l=https://tqdm.github.io/img/favicon.png&f=https://tqdm.github.io/img/logo.gif&style=social"}, {"name": "tqdm", "tags": ["cli", "ui", "web"], "summary": "Fast, Extensible Progress Meter", "text": "This library is used to display a smart progress meter for loops, allowing developers to track the progress of their code with minimal overhead. By wrapping an iterable with tqdm, developers can instantly visualize the completion percentage and estimated remaining time for any loop."}, {"name": "trafilatura", "tags": ["data", "math", "web"], "summary": "Python & Command-line tool to gather text and metadata on the Web: Crawling, scraping, extraction, output as CSV, JSON, HTML, MD, TXT, XML.", "text": "Trafilatura: Discover and Extract Text Data on the Web\n\n(https://pypi.python.org/pypi/trafilatura)\n(https://pypi.python.org/pypi/trafilatura)\n\n(https://codecov.io/gh/adbar/trafilatura)\n(https://pepy.tech/project/trafilatura)\n(https://aclanthology.org/2021.acl-demo.15/)\n\nIntroduction\n\nTrafilatura is a cutting-edge **Python package and command-line tool**\ndesigned to **gather text on the Web and simplify the process of turning\nraw HTML into structured, meaningful data**. It includes all necessary\ndiscovery and text processing components to perform **web crawling,\ndownloads, scraping, and extraction** of main texts, metadata and\ncomments. It aims at staying **handy and modular**: no database is\nrequired, the output can be converted to commonly used formats.\n\nGoing from HTML bulk to essential parts can alleviate many problems\nrelated to text quality, by **focusing on the actual content**,\n**avoiding the noise** caused by recurring elements like headers and footers\nand by **making sense of the data and metadata** with selected information.\nThe extractor strikes a balance between limiting noise (precision) and\nincluding all valid parts (recall). It is **robust and reasonably fast**.\n\nTrafilatura is [widely used](https://trafilatura.readthedocs.io/en/latest/used-by.html)\nand integrated into [thousands of projects](https://github.com/adbar/trafilatura/network/dependents>)\nby companies like HuggingFace, IBM, and Microsoft Research as well as institutions like\nthe Allen Institute, Stanford, the Tokyo Institute of Technology, and\nthe University of Munich.\n\nFeatures\n\n- Advanced web crawling and text discovery:\n   - Support for sitemaps (TXT, XML) and feeds (ATOM, JSON, RSS)\n   - Smart crawling and URL management (filtering and deduplication)\n\n- Parallel processing of online and offline input:\n   - Live URLs, efficient and polite processing of download queues\n   - Previously downloaded HTML files and parsed HTML trees\n\n- Robust and configurable extraction of key elements:\n   - Main text (common patterns and generic algorithms like jusText and readability)\n   - Metadata (title, author, date, site name, categories and tags)\n   - Formatting and structure: paragraphs, titles, lists, quotes, code, line breaks, in-line text formatting\n   - Optional elements: comments, links, images, tables\n\n- Multiple output formats:\n   - TXT and Markdown\n   - CSV\n   - JSON\n   - HTML, XML and [XML-TEI](https://tei-c.org/)\n\n- Optional add-ons:\n   - Language detection on extracted content\n   - Speed optimizations\n\n- Actively maintained with support from the open-source community:\n   - Regular updates, feature additions, and optimizations\n   - Comprehensive documentation\n\nEvaluation and alternatives\n\nTrafilatura consistently outperforms other open-source libraries in text\nextraction benchmarks, showcasing its efficiency and accuracy in\nextracting web content. The extractor tries to strike a balance between\nlimiting noise and including all valid parts.\n\nFor more information see the [benchmark section](https://trafilatura.readthedocs.io/en/latest/evaluation.html)\nand the [evaluation readme](https://github.com/adbar/trafilatura/blob/master/tests/README.rst)\nto run the evaluation with the latest data and packages.\n\nOther evaluations:\n\n- Most efficient open-source library in *ScrapingHub*'s [article extraction benchmark](https://github.com/scrapinghub/article-extraction-benchmark)\n- Best overall tool according to [Bien choisir son outil d'extraction de contenu \u00e0 partir du Web](https://hal.archives-ouvertes.fr/hal-02768510v3/document)\n  (Lejeune & Barbaresi 2020)\n- Best single tool by ROUGE-LSum Mean F1 Page Scores in [An Empirical Comparison of Web Content Extraction Algorithms](https://webis.de/downloads/publications/papers/bevendorff_2023b.pdf)\n  (Bevendorff et al. 2023)\n\nUsage and documentation\n\n[Getting started with Trafilatura](https://trafilatura.readthedocs.io/en/latest/quickstart.html)\nis straightforward. For more information and detailed guides, visit\n[Trafilatura's documentation](https://trafilatura.readthedocs.io/):\n\n- [Installation](https://trafilatura.readthedocs.io/en/latest/installation.html)\n- Usage:\n  [On the command-line](https://trafilatura.readthedocs.io/en/latest/usage-cli.html),\n  [With Python](https://trafilatura.readthedocs.io/en/latest/usage-python.html),\n  [With R](https://trafilatura.readthedocs.io/en/latest/usage-r.html)\n- [Core Python functions](https://trafilatura.readthedocs.io/en/latest/corefunctions.html)\n- Interactive Python Notebook: [Trafilatura Overview](docs/Trafilatura_Overview.ipynb)\n- [Tutorials and use cases](https://trafilatura.readthedocs.io/en/latest/tutorials.html)"}, {"name": "trafilatura", "tags": ["data", "math", "web"], "summary": "Python & Command-line tool to gather text and metadata on the Web: Crawling, scraping, extraction, output as CSV, JSON, HTML, MD, TXT, XML.", "text": "Youtube playlist with video tutorials in several languages:\n\n- [Web scraping tutorials and how-tos](https://www.youtube.com/watch?v=8GkiOM17t0Q&list=PL-pKWbySIRGMgxXQOtGIz1-nbfYLvqrci)\n\nLicense\n\nThis package is distributed under the [Apache 2.0 license](https://www.apache.org/licenses/LICENSE-2.0.html).\n\nVersions prior to v1.8.0 are under GPLv3+ license.\n\nContributing\n\nContributions of all kinds are welcome. Visit the [Contributing\npage](https://github.com/adbar/trafilatura/blob/master/CONTRIBUTING.md)\nfor more information. Bug reports can be filed on the [dedicated issue\npage](https://github.com/adbar/trafilatura/issues).\n\nMany thanks to the\n[contributors](https://github.com/adbar/trafilatura/graphs/contributors)\nwho extended the docs or submitted bug reports, features and bugfixes!\n\nContext\n\nThis work started as a PhD project at the crossroads of linguistics and\nNLP, this expertise has been instrumental in shaping Trafilatura over\nthe years. Initially launched to create text databases for research purposes\nat the Berlin-Brandenburg Academy of Sciences (DWDS and ZDL units),\nthis package continues to be maintained but its future development\ndepends on community support.\n\n**If you value this software or depend on it for your product, consider\nsponsoring it and contributing to its codebase**. Your support will\nhelp maintain and enhance this popular package, ensuring its growth,\nrobustness, and accessibility for developers and users around the world.\n\n*Trafilatura* is an Italian word for [wire\ndrawing](https://en.wikipedia.org/wiki/Wire_drawing) symbolizing the\nrefinement and conversion process. It is also the way shapes of pasta\nare formed.\n\nAuthor\n\nReach out via ia the software repository or the [contact\npage](https://adrien.barbaresi.eu/) for inquiries, collaborations, or\nfeedback. See also social networks for the latest updates.\n\n-   Barbaresi, A. [Trafilatura: A Web Scraping Library and Command-Line\n-   Barbaresi, A. \"[Generic Web Content Extraction with Open-Source\n-   Barbaresi, A. \"[Efficient construction of metadata-enhanced web\n\nCiting Trafilatura\n\nTrafilatura is widely used in the academic domain, chiefly for data\nacquisition. Here is how to cite it:\n\n(https://aclanthology.org/2021.acl-demo.15/)\n(https://doi.org/10.5281/zenodo.3460969)\n\nSoftware ecosystem\n\nJointly developed plugins and additional packages also contribute to the\nfield of web data extraction and analysis:\n\nCorresponding posts can be found on [Bits of\nLanguage](https://adrien.barbaresi.eu/blog/tag/trafilatura.html).\n\nImpressive, you have reached the end of the page: Thank you for your\ninterest!"}, {"name": "trafilatura", "tags": ["data", "math", "web"], "summary": "Python & Command-line tool to gather text and metadata on the Web: Crawling, scraping, extraction, output as CSV, JSON, HTML, MD, TXT, XML.", "text": "This library is used to gather text and metadata on the Web through web crawling, scraping, and extraction, making it easy to turn raw HTML into structured data. With Trafilatura, developers can simplify the process of discovering and extracting text data from websites for use in various applications."}, {"name": "transformers", "tags": ["math", "ml", "web"], "summary": "State-of-the-art Machine Learning for JAX, PyTorch and TensorFlow", "text": "Installation\n\nTransformers works with Python 3.9+ [PyTorch](https://pytorch.org/get-started/locally/) 2.1+, [TensorFlow](https://www.tensorflow.org/install/pip) 2.6+, and [Flax](https://flax.readthedocs.io/en/latest/) 0.4.1+.\n\nCreate and activate a virtual environment with [venv](https://docs.python.org/3/library/venv.html) or [uv](https://docs.astral.sh/uv/), a fast Rust-based Python package and project manager.\n\nInstall Transformers in your virtual environment.\n\nInstall Transformers from source if you want the latest changes in the library or are interested in contributing. However, the *latest* version may not be stable. Feel free to open an [issue](https://github.com/huggingface/transformers/issues) if you encounter an error.\n\nQuickstart\n\nGet started with Transformers right away with the [Pipeline](https://huggingface.co/docs/transformers/pipeline_tutorial) API. The `Pipeline` is a high-level inference class that supports text, audio, vision, and multimodal tasks. It handles preprocessing the input and returns the appropriate output.\n\nInstantiate a pipeline and specify model to use for text generation. The model is downloaded and cached so you can easily reuse it again. Finally, pass some text to prompt the model.\n\nTo chat with a model, the usage pattern is the same. The only difference is you need to construct a chat history (the input to `Pipeline`) between you and the system.\n\n> [!TIP]\n> You can also chat with a model directly from the command line.\n> \n\nExpand the examples below to see how `Pipeline` works for different modalities and tasks.\n\nAutomatic speech recognition\n\nImage classification\n\nVisual question answering\n\nWhy should I use Transformers?\n\n1. Easy-to-use state-of-the-art models:\n\n1. Lower compute costs, smaller carbon footprint:\n\n1. Choose the right framework for every part of a models lifetime:\n\n1. Easily customize a model or an example to your needs:\n\nWhy shouldn't I use Transformers?\n\n- This library is not a modular toolbox of building blocks for neural nets. The code in the model files is not refactored with additional abstractions on purpose, so that researchers can quickly iterate on each of the models without diving into additional abstractions/files.\n- The training API is optimized to work with PyTorch models provided by Transformers. For generic machine learning loops, you should use another library like [Accelerate](https://huggingface.co/docs/accelerate).\n- The [example scripts](https://github.com/huggingface/transformers/tree/main/examples) are only *examples*. They may not necessarily work out-of-the-box on your specific use case and you'll need to adapt the code for it to work.\n\n100 projects using Transformers\n\nTransformers is more than a toolkit to use pretrained models, it's a community of projects built around it and the\nHugging Face Hub. We want Transformers to enable developers, researchers, students, professors, engineers, and anyone\nelse to build their dream projects.\n\nIn order to celebrate Transformers 100,000 stars, we wanted to put the spotlight on the\ncommunity with the [awesome-transformers](./awesome-transformers.md) page which lists 100\nincredible projects built with Transformers.\n\nIf you own or use a project that you believe should be part of the list, please open a PR to add it!\n\nExample models\n\nYou can test most of our models directly on their [Hub model pages](https://huggingface.co/models).\n\nExpand each modality below to see a few example models for various use cases.\n\nAudio\n\nComputer vision\n\nMultimodal\n\nNLP\n\nCitation\n\nWe now have a [paper](https://www.aclweb.org/anthology/2020.emnlp-demos.6/) you can cite for the  Transformers library:"}, {"name": "transformers", "tags": ["math", "ml", "web"], "summary": "State-of-the-art Machine Learning for JAX, PyTorch and TensorFlow", "text": "This library is used to implement state-of-the-art machine learning models using JAX, PyTorch, and TensorFlow, allowing developers to leverage pre-trained transformer architectures for a variety of tasks. With this library, developers can easily integrate powerful transformer-based models into their applications for natural language processing, computer vision, and other areas."}, {"name": "transforms3d", "tags": ["math"], "summary": "Functions for 3D coordinate transformations", "text": "############\nTransforms3d\n############\n\nCode to convert between various geometric transformations.\n\n* Composing rotations / zooms / shears / translations into affine matrix;\n* Decomposing affine matrix into rotations / zooms / shears / translations;\n* Conversions between different representations of rotations, including:\n\n  * 3x3 Rotation matrices;\n  * Euler angles;\n  * quaternions.\n\nWe have tried to document the algorithms carefully and write clear code in the\nhope that this code can be a teaching reference.  We document the math behind\nsome of the algorithms using `sympy `_ in\n``transforms3d/derivations``.  We would be very pleased if y'all would like to\nadd your own algorithms and derivations - please get a copy of the code from\nalgorithmically.  Feel free to use the github issue tracker and pull request\nsystem to ask for advice and support.\n\n*************\nDocumentation\n*************\n\nDocumentation for latest released version at\n\n****\nCode\n****\n\nSee https://github.com/matthew-brett/transforms3d\n\nReleased under the BSD two-clause license - see the file ``LICENSE`` in the\nsource distribution.\n\nMuch of the code comes from `transformations.py\n`_ by Christoph\nGohlke, also released under the BSD license.\n\nWe use Github actions to test the code automatically under Pythons 3.7 through\n3.10.\n\nWe depend on numpy >= 1.15.  You may be able to make it work on an earlier\nnumpy if you really needed that.\n\nThe latest released version is at https://pypi.python.org/pypi/transforms3d\n\n*******\nSupport\n*******\n\nPlease put up issues on the `transforms3d issue tracker\n`_."}, {"name": "transforms3d", "tags": ["math"], "summary": "Functions for 3D coordinate transformations", "text": "This library is used to perform various geometric transformations in 3D space, including composition and decomposition of affine matrices and conversions between different rotation representations. With this library, developers can easily manipulate 3D coordinates and rotations for tasks such as computer graphics, robotics, and engineering applications."}, {"name": "trimesh", "tags": ["math", "web"], "summary": "Import, export, process, analyze and view triangular meshes.", "text": "Basic Installation\n\nKeeping `trimesh` easy to install is a core goal, thus the *only* hard dependency is [`numpy`](http://www.numpy.org/). Installing other packages adds functionality but is not required. For the easiest install with just `numpy`:\n\nThe minimal install can load many supported formats (STL, PLY, OBJ, GLTF/GLB) into `numpy.ndarray` values. More functionality is available when [soft dependencies are installed](https://trimesh.org/install#dependency-overview), including convex hulls (`scipy`), graph operations (`networkx`), fast ray queries (`embreex`), vector path handling (`shapely` and `rtree`), XML formats like 3DXML/XAML/3MF (`lxml`), preview windows (`pyglet`), faster cache checks (`xxhash`), etc.\n\nTo install `trimesh` with the soft dependencies that generally install cleanly from binaries on Linux x86_64, MacOS ARM, and Windows x86_64 using `pip`:\n\nIf you are supporting a different platform or are freezing dependencies for an application we recommend you do not use extras, i.e. depend on `trimesh scipy` versus `trimesh[easy]`. Further information is available in the [advanced installation documentation](https://trimesh.org/install.html).\n\nQuick Start\n\nHere is an example of loading a mesh from file and colorizing its faces ([nicely formatted notebook version](https://trimesh.org/quick_start.html) of this example.\n\nFeatures\n\n* Import meshes from binary/ASCII STL, Wavefront OBJ, ASCII OFF, binary/ASCII PLY, GLTF/GLB 2.0, 3MF, XAML, 3DXML, etc.\n* Export meshes as GLB/GLTF, binary STL, binary PLY, ASCII OFF, OBJ, COLLADA, etc.\n* Import and export 2D or 3D vector paths with DXF or SVG files\n* Preview meshes using an OpenGL `pyglet` window, or in-line in jupyter or marimo notebooks using `three.js`\n* Automatic hashing from a subclassed numpy array for change tracking using MD5, zlib CRC, or xxhash, and internal caching of expensive values.\n* Calculate face adjacencies, face angles, vertex defects, convex hulls, etc.\n* Calculate cross sections for a 2D outline, or slice a mesh for a 3D remainder mesh, i.e. slicing for 3D-printing.\n* Split mesh based on face connectivity using networkx, or scipy.sparse\n* Calculate mass properties, including volume, center of mass, moment of inertia, principal components of inertia, etc. \n* Repair simple problems with triangle winding, normals, and quad/triangle holes\n* Compute rotation/translation/tessellation invariant identifier and find duplicate meshes\n* Check if a mesh is watertight, convex, etc.\n* Sample the surface of a mesh\n* Ray-mesh queries including location, triangle index, etc.\n* Boolean operations on meshes (intersection, union, difference) using Manifold3D or Blender.\n* Voxelize watertight meshes\n* Smooth watertight meshes using Laplacian smoothing algorithms (Classic, Taubin, Humphrey)\n* Subdivide faces of a mesh\n* Approximate minimum volume oriented bounding boxes and spheres for meshes.\n* Calculate nearest point on mesh surface and signed distance\n* Primitive objects (Box, Cylinder, Sphere, Extrusion) which are subclassed Trimesh objects and have all the same features (inertia, viewers, etc)\n* Simple scene graph and transform tree which can be rendered (pyglet window, three.js in a jupyter/marimo notebook or exported.\n* Many utility functions, like transforming points, unitizing vectors, aligning vectors, tracking numpy arrays for changes, grouping rows, etc.\n\nAdditional Notes"}, {"name": "trimesh", "tags": ["math", "web"], "summary": "Import, export, process, analyze and view triangular meshes.", "text": "This library is used to import, export, process, analyze, and view triangular meshes in various formats. It enables developers to efficiently load, manipulate, and visualize complex 3D geometric data using Python."}, {"name": "ultralytics-thop", "tags": ["math", "ml", "web"], "summary": "Ultralytics THOP package for fast computation of PyTorch model FLOPs and parameters.", "text": "THOP: PyTorch-OpCounter\n\nWelcome to the [THOP](https://github.com/ultralytics/thop) repository, your comprehensive solution for profiling [PyTorch](https://pytorch.org/) models by computing the number of Multiply-Accumulate Operations (MACs) and parameters. Developed by Ultralytics, this tool is essential for [deep learning](https://www.ultralytics.com/glossary/deep-learning-dl) practitioners aiming to evaluate model efficiency and performance, crucial aspects discussed in our [model training tips guide](https://docs.ultralytics.com/guides/model-training-tips/).\n\n(https://github.com/ultralytics/thop/actions/workflows/format.yml)\n(https://discord.com/invite/ultralytics)\n(https://community.ultralytics.com/)\n(https://reddit.com/r/ultralytics)\n\nDescription\n\nTHOP offers an intuitive API designed to profile PyTorch models by calculating the total number of MACs and parameters. This functionality is vital for assessing the computational efficiency and memory footprint of deep learning models, helping developers optimize performance for deployment, especially on [edge devices](https://www.ultralytics.com/glossary/edge-ai). Understanding these metrics is key to selecting the right model architecture, a topic explored in our [model comparison pages](https://docs.ultralytics.com/compare/).\n\nInstallation\n\nGet started with THOP quickly by installing it via pip:\n\n(https://pypi.org/project/ultralytics-thop/) (https://clickpy.clickhouse.com/dashboard/ultralytics-thop) (https://pypi.org/project/ultralytics-thop/)\n\nAlternatively, for the latest features and updates, install directly from the GitHub repository:\n\nThis ensures you have the most recent version, incorporating the latest improvements and bug fixes.\n\n\ufe0f How to Use\n\nBasic Usage\n\nProfiling a standard PyTorch model like [ResNet50](https://docs.pytorch.org/vision/main/models/generated/torchvision.models.resnet50.html) is straightforward. Import the necessary libraries, load your model and a sample input tensor, then use the `profile` function:\n\nDefine Custom Rules for Third-Party Modules\n\nIf your model includes custom or third-party modules not natively supported by THOP, you can define custom profiling rules using the `custom_ops` argument. This allows for accurate profiling even with complex or non-standard architectures, which is useful when working with models like those found in the [Ultralytics models section](https://docs.ultralytics.com/models/).\n\nImprove Output Readability\n\nFor clearer and more interpretable results, use the `thop.clever_format` function. This formats the raw MACs and parameter counts into human-readable strings (e.g., GigaMACs, MegaParams). This formatting helps in quickly understanding the scale of computational resources required, similar to the metrics provided in our [Ultralytics YOLOv8 documentation](https://docs.ultralytics.com/models/yolov8/).\n\nResults of Recent Models\n\nThe table below showcases the parameters and MACs for several popular [computer vision](https://www.ultralytics.com/glossary/computer-vision-cv) models, profiled using THOP. These benchmarks provide a comparative overview of model complexity and computational cost. You can reproduce these results by running the script located at `benchmark/evaluate_famous_models.py` in this repository. Comparing these metrics is essential for tasks like selecting models for [object detection](https://www.ultralytics.com/glossary/object-detection) or [image classification](https://www.ultralytics.com/glossary/image-classification). For more comparisons, see our [model comparison section](https://docs.ultralytics.com/compare/).\n\nModel\n----------------\nalexnet\nvgg11\nvgg11_bn\nvgg13\nvgg13_bn\nvgg16\nvgg16_bn\nvgg19\nvgg19_bn\nresnet18\nresnet34\nresnet50\nresnet101\nresnet152\nwide_resnet101_2\nwide_resnet50_2\n\nModel\n------------------\nresnext50_32x4d\nresnext101_32x8d\ndensenet121\ndensenet161\ndensenet169\ndensenet201\nsqueezenet1_0\nsqueezenet1_1\nmnasnet0_5\nmnasnet0_75\nmnasnet1_0\nmnasnet1_3\nmobilenet_v2\nshufflenet_v2_x0_5\nshufflenet_v2_x1_0\nshufflenet_v2_x1_5\nshufflenet_v2_x2_0\ninception_v3\n\nContribute\n\nWe actively welcome and encourage community contributions to make THOP even better! Whether it's adding support for new [PyTorch layers](https://docs.pytorch.org/docs/stable/nn.html), improving existing calculations, enhancing documentation, or fixing bugs, your input is valuable. Please see our [Contributing Guide](https://docs.ultralytics.com/help/contributing/) for detailed instructions on how to participate. Together, we can ensure THOP remains a state-of-the-art tool for the [machine learning](https://www.ultralytics.com/glossary/machine-learning-ml) community. Don't hesitate to share your feedback and suggestions!\n\nLicense\n\nTHOP is distributed under the [AGPL-3.0 License](https://www.gnu.org/licenses/agpl-3.0.en.html). This license promotes open collaboration and sharing of improvements. For complete details, please refer to the [LICENSE](https://github.com/ultralytics/thop/blob/main/LICENSE) file included in the repository. Understanding the license is important before integrating THOP into your projects, especially for commercial applications which may require an [Enterprise License](https://www.ultralytics.com/license).\n\nContact\n\nEncountered a bug or have a feature request? Please submit an issue through our [GitHub Issues](https://github.com/ultralytics/thop/issues) page. For general discussions, questions, and community support, join the vibrant Ultralytics community on our [Discord server](https://discord.com/invite/ultralytics). We look forward to hearing from you and collaborating!"}, {"name": "ultralytics-thop", "tags": ["math", "ml", "web"], "summary": "Ultralytics THOP package for fast computation of PyTorch model FLOPs and parameters.", "text": "This library is used to compute the FLOPs and parameters of PyTorch models, enabling developers to evaluate model efficiency and performance. With THOP, users can quickly profile their deep learning models and make informed decisions about model optimization and training."}, {"name": "ultralytics", "tags": ["cli", "data", "math", "ml", "ui"], "summary": "Ultralytics YOLO \ud83d\ude80 for SOTA object detection, multi-object tracking, instance segmentation, pose estimation and image classification.", "text": "Documentation\n\nSee below for quickstart installation and usage examples. For comprehensive guidance on training, validation, prediction, and deployment, refer to our full [Ultralytics Docs](https://docs.ultralytics.com/).\n\nInstall\n\nInstall the `ultralytics` package, including all [requirements](https://github.com/ultralytics/ultralytics/blob/main/pyproject.toml), in a [**Python>=3.8**](https://www.python.org/) environment with [**PyTorch>=1.8**](https://pytorch.org/get-started/locally/).\n\n(https://pypi.org/project/ultralytics/) (https://clickpy.clickhouse.com/dashboard/ultralytics) (https://pypi.org/project/ultralytics/)\n\nFor alternative installation methods, including [Conda](https://anaconda.org/conda-forge/ultralytics), [Docker](https://hub.docker.com/r/ultralytics/ultralytics), and building from source via Git, please consult the [Quickstart Guide](https://docs.ultralytics.com/quickstart/).\n\n(https://anaconda.org/conda-forge/ultralytics) (https://hub.docker.com/r/ultralytics/ultralytics) (https://hub.docker.com/r/ultralytics/ultralytics)\n\nUsage\n\nCLI\n\nYou can use Ultralytics YOLO directly from the Command Line Interface (CLI) with the `yolo` command:\n\nThe `yolo` command supports various tasks and modes, accepting additional arguments like `imgsz=640`. Explore the YOLO [CLI Docs](https://docs.ultralytics.com/usage/cli/) for more examples.\n\nPython\n\nUltralytics YOLO can also be integrated directly into your Python projects. It accepts the same [configuration arguments](https://docs.ultralytics.com/usage/cfg/) as the CLI:\n\nDiscover more examples in the YOLO [Python Docs](https://docs.ultralytics.com/usage/python/).\n\nModels\n\nUltralytics supports a wide range of YOLO models, from early versions like [YOLOv3](https://docs.ultralytics.com/models/yolov3/) to the latest [YOLO11](https://docs.ultralytics.com/models/yolo11/). The tables below showcase YOLO11 models pretrained on the [COCO](https://docs.ultralytics.com/datasets/detect/coco/) dataset for [Detection](https://docs.ultralytics.com/tasks/detect/), [Segmentation](https://docs.ultralytics.com/tasks/segment/), and [Pose Estimation](https://docs.ultralytics.com/tasks/pose/). Additionally, [Classification](https://docs.ultralytics.com/tasks/classify/) models pretrained on the [ImageNet](https://docs.ultralytics.com/datasets/classify/imagenet/) dataset are available. [Tracking](https://docs.ultralytics.com/modes/track/) mode is compatible with all Detection, Segmentation, and Pose models. All [Models](https://docs.ultralytics.com/models/) are automatically downloaded from the latest Ultralytics [release](https://github.com/ultralytics/assets/releases) upon first use.\n\nDetection (COCO)\n\nExplore the [Detection Docs](https://docs.ultralytics.com/tasks/detect/) for usage examples. These models are trained on the [COCO dataset](https://cocodataset.org/), featuring 80 object classes.\n\nModel\n------------------------------------------------------------------------------------\n[YOLO11n](https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11n.pt)\n[YOLO11s](https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11s.pt)\n[YOLO11m](https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11m.pt)\n[YOLO11l](https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11l.pt)\n[YOLO11x](https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11x.pt)\n\n- **mAPval** values refer to single-model single-scale performance on the [COCO val2017](https://cocodataset.org/) dataset. See [YOLO Performance Metrics](https://docs.ultralytics.com/guides/yolo-performance-metrics/) for details. Reproduce with `yolo val detect data=coco.yaml device=0`\n- **Speed** metrics are averaged over COCO val images using an [Amazon EC2 P4d](https://aws.amazon.com/ec2/instance-types/p4/) instance. CPU speeds measured with [ONNX](https://onnx.ai/) export. GPU speeds measured with [TensorRT](https://developer.nvidia.com/tensorrt) export. Reproduce with `yolo val detect data=coco.yaml batch=1 device=0|cpu`\n\nSegmentation (COCO)\n\nRefer to the [Segmentation Docs](https://docs.ultralytics.com/tasks/segment/) for usage examples. These models are trained on [COCO-Seg](https://docs.ultralytics.com/datasets/segment/coco/), including 80 classes.\n\nModel\n--------------------------------------------------------------------------------------------\n[YOLO11n-seg](https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11n-seg.pt)\n[YOLO11s-seg](https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11s-seg.pt)\n[YOLO11m-seg](https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11m-seg.pt)\n[YOLO11l-seg](https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11l-seg.pt)\n[YOLO11x-seg](https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11x-seg.pt)\n\n- **mAPval** values are for single-model single-scale on the [COCO val2017](https://cocodataset.org/) dataset. See [YOLO Performance Metrics](https://docs.ultralytics.com/guides/yolo-performance-metrics/) for details. Reproduce with `yolo val segment data=coco.yaml device=0`\n- **Speed** metrics are averaged over COCO val images using an [Amazon EC2 P4d](https://aws.amazon.com/ec2/instance-types/p4/) instance. CPU speeds measured with [ONNX](https://onnx.ai/) export. GPU speeds measured with [TensorRT](https://developer.nvidia.com/tensorrt) export. Reproduce with `yolo val segment data=coco.yaml batch=1 device=0|cpu`\n\nClassification (ImageNet)\n\nConsult the [Classification Docs](https://docs.ultralytics.com/tasks/classify/) for usage examples. These models are trained on [ImageNet](https://docs.ultralytics.com/datasets/classify/imagenet/), covering 1000 classes.\n\nModel\n--------------------------------------------------------------------------------------------\n[YOLO11n-cls](https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11n-cls.pt)\n[YOLO11s-cls](https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11s-cls.pt)\n[YOLO11m-cls](https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11m-cls.pt)\n[YOLO11l-cls](https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11l-cls.pt)\n[YOLO11x-cls](https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11x-cls.pt)\n\n- **acc** values represent model accuracy on the [ImageNet](https://www.image-net.org/) dataset validation set. Reproduce with `yolo val classify data=path/to/ImageNet device=0`\n- **Speed** metrics are averaged over ImageNet val images using an [Amazon EC2 P4d](https://aws.amazon.com/ec2/instance-types/p4/) instance. CPU speeds measured with [ONNX](https://onnx.ai/) export. GPU speeds measured with [TensorRT](https://developer.nvidia.com/tensorrt) export. Reproduce with `yolo val classify data=path/to/ImageNet batch=1 device=0|cpu`\n\nPose (COCO)\n\nSee the [Pose Estimation Docs](https://docs.ultralytics.com/tasks/pose/) for usage examples. These models are trained on [COCO-Pose](https://docs.ultralytics.com/datasets/pose/coco/), focusing on the 'person' class.\n\nModel\n----------------------------------------------------------------------------------------------\n[YOLO11n-pose](https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11n-pose.pt)\n[YOLO11s-pose](https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11s-pose.pt)\n[YOLO11m-pose](https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11m-pose.pt)\n[YOLO11l-pose](https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11l-pose.pt)\n[YOLO11x-pose](https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11x-pose.pt)"}, {"name": "ultralytics", "tags": ["cli", "data", "math", "ml", "ui"], "summary": "Ultralytics YOLO \ud83d\ude80 for SOTA object detection, multi-object tracking, instance segmentation, pose estimation and image classification.", "text": "- **mAPval** values are for single-model single-scale on the [COCO Keypoints val2017](https://docs.ultralytics.com/datasets/pose/coco/) dataset. See [YOLO Performance Metrics](https://docs.ultralytics.com/guides/yolo-performance-metrics/) for details. Reproduce with `yolo val pose data=coco-pose.yaml device=0`\n- **Speed** metrics are averaged over COCO val images using an [Amazon EC2 P4d](https://aws.amazon.com/ec2/instance-types/p4/) instance. CPU speeds measured with [ONNX](https://onnx.ai/) export. GPU speeds measured with [TensorRT](https://developer.nvidia.com/tensorrt) export. Reproduce with `yolo val pose data=coco-pose.yaml batch=1 device=0|cpu`\n\nOriented Bounding Boxes (DOTAv1)\n\nCheck the [OBB Docs](https://docs.ultralytics.com/tasks/obb/) for usage examples. These models are trained on [DOTAv1](https://docs.ultralytics.com/datasets/obb/dota-v2/#dota-v10/), including 15 classes.\n\nModel\n--------------------------------------------------------------------------------------------\n[YOLO11n-obb](https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11n-obb.pt)\n[YOLO11s-obb](https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11s-obb.pt)\n[YOLO11m-obb](https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11m-obb.pt)\n[YOLO11l-obb](https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11l-obb.pt)\n[YOLO11x-obb](https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11x-obb.pt)\n\n- **mAPtest** values are for single-model multiscale performance on the [DOTAv1 test set](https://captain-whu.github.io/DOTA/dataset.html). Reproduce by `yolo val obb data=DOTAv1.yaml device=0 split=test` and submit merged results to the [DOTA evaluation server](https://captain-whu.github.io/DOTA/evaluation.html).\n- **Speed** metrics are averaged over [DOTAv1 val images](https://docs.ultralytics.com/datasets/obb/dota-v2/#dota-v10) using an [Amazon EC2 P4d](https://aws.amazon.com/ec2/instance-types/p4/) instance. CPU speeds measured with [ONNX](https://onnx.ai/) export. GPU speeds measured with [TensorRT](https://developer.nvidia.com/tensorrt) export. Reproduce by `yolo val obb data=DOTAv1.yaml batch=1 device=0|cpu`\n\nIntegrations\n\nOur key integrations with leading AI platforms extend the functionality of Ultralytics' offerings, enhancing tasks like dataset labeling, training, visualization, and model management. Discover how Ultralytics, in collaboration with partners like [Weights & Biases](https://docs.ultralytics.com/integrations/weights-biases/), [Comet ML](https://docs.ultralytics.com/integrations/comet/), [Roboflow](https://docs.ultralytics.com/integrations/roboflow/), and [Intel OpenVINO](https://docs.ultralytics.com/integrations/openvino/), can optimize your AI workflow. Explore more at [Ultralytics Integrations](https://docs.ultralytics.com/integrations/).\n\nUltralytics HUB \n:-----------------------------------------------------------------------------------------------------------------------------:\nStreamline YOLO workflows: Label, train, and deploy effortlessly with [Ultralytics HUB](https://hub.ultralytics.com/). Try now!\n\nUltralytics HUB\n\nExperience seamless AI with [Ultralytics HUB](https://hub.ultralytics.com/), the all-in-one platform for data visualization, training YOLO models, and deployment\u2014no coding required. Transform images into actionable insights and bring your AI visions to life effortlessly using our cutting-edge platform and user-friendly [Ultralytics App](https://www.ultralytics.com/app-install). Start your journey for **Free** today!\n\nContribute\n\nWe thrive on community collaboration! Ultralytics YOLO wouldn't be the SOTA framework it is without contributions from developers like you. Please see our [Contributing Guide](https://docs.ultralytics.com/help/contributing/) to get started. We also welcome your feedback\u2014share your experience by completing our [Survey](https://www.ultralytics.com/survey?utm_source=github&utm_medium=social&utm_campaign=Survey). A huge **Thank You**  to everyone who contributes!\n\n(https://github.com/ultralytics/ultralytics/graphs/contributors)\n\nWe look forward to your contributions to help make the Ultralytics ecosystem even better!\n\nLicense\n\nUltralytics offers two licensing options to suit different needs:\n\n- **AGPL-3.0 License**: This [OSI-approved](https://opensource.org/license/agpl-v3) open-source license is perfect for students, researchers, and enthusiasts. It encourages open collaboration and knowledge sharing. See the [LICENSE](https://github.com/ultralytics/ultralytics/blob/main/LICENSE) file for full details.\n- **Ultralytics Enterprise License**: Designed for commercial use, this license allows for the seamless integration of Ultralytics software and AI models into commercial products and services, bypassing the open-source requirements of AGPL-3.0. If your use case involves commercial deployment, please contact us via [Ultralytics Licensing](https://www.ultralytics.com/license).\n\nContact\n\nFor bug reports and feature requests related to Ultralytics software, please visit [GitHub Issues](https://github.com/ultralytics/ultralytics/issues). For questions, discussions, and community support, join our active communities on [Discord](https://discord.com/invite/ultralytics), [Reddit](https://www.reddit.com/r/ultralytics/), and the [Ultralytics Community Forums](https://community.ultralytics.com/). We're here to help with all things Ultralytics!"}, {"name": "ultralytics", "tags": ["cli", "data", "math", "ml", "ui"], "summary": "Ultralytics YOLO \ud83d\ude80 for SOTA object detection, multi-object tracking, instance segmentation, pose estimation and image classification.", "text": "This library is used to implement state-of-the-art object detection, multi-object tracking, instance segmentation, pose estimation, and image classification tasks with high accuracy. It provides a comprehensive set of tools for training, validation, prediction, and deployment through its extensive documentation and installation options."}, {"name": "umap-learn", "tags": ["data", "math", "ml", "ui", "visualization", "web"], "summary": "Uniform Manifold Approximation and Projection", "text": ".. -*- mode: rst -*-\n\n.. image:: doc/logo_large.png\n  :width: 600\n  :alt: UMAP logo\n  :align: center\n\npypi_version\n\nconda_version\n\nLicense\n\nDocs\n\n.. |pypi_version| image:: https://img.shields.io/pypi/v/umap-learn.svg\n.. _pypi_version: https://pypi.python.org/pypi/umap-learn/\n\n.. _pypi_downloads: https://pepy.tech/project/umap-learn\n\n.. _conda_version: https://anaconda.org/conda-forge/umap-learn\n\n.. _conda_downloads: https://anaconda.org/conda-forge/umap-learn\n\n.. |License| image:: https://img.shields.io/pypi/l/umap-learn.svg\n.. _License: https://github.com/lmcinnes/umap/blob/master/LICENSE.txt\n\n.. |build_status| image:: https://dev.azure.com/TutteInstitute/build-pipelines/_apis/build/status/lmcinnes.umap?branchName=master\n.. _build_status: https://dev.azure.com/TutteInstitute/build-pipelines/_build/latest?definitionId=2&branchName=master\n\n.. _Coverage: https://coveralls.io/github/lmcinnes/umap\n\n.. |joss_paper| image:: http://joss.theoj.org/papers/10.21105/joss.00861/status.svg\n.. _joss_paper: https://doi.org/10.21105/joss.00861\n\n====\nUMAP\n====\n\nUniform Manifold Approximation and Projection (UMAP) is a dimension reduction\ntechnique that can be used for visualisation similarly to t-SNE, but also for\ngeneral non-linear dimension reduction. The algorithm is founded on three\nassumptions about the data:\n\n1. The data is uniformly distributed on a Riemannian manifold;\n2. The Riemannian metric is locally constant (or can be approximated as such);\n3. The manifold is locally connected.\n\nFrom these assumptions it is possible to model the manifold with a fuzzy\ntopological structure. The embedding is found by searching for a low dimensional\nprojection of the data that has the closest possible equivalent fuzzy\ntopological structure.\n\nThe details for the underlying mathematics can be found in\n`our paper on ArXiv `_:\n\nMcInnes, L, Healy, J, *UMAP: Uniform Manifold Approximation and Projection\nfor Dimension Reduction*, ArXiv e-prints 1802.03426, 2018\n\nA broader introduction to UMAP targetted the scientific community can be found \nin our `paper published in Nature Review Methods Primers  `_:\n\nHealy, J., McInnes, L. *Uniform manifold approximation and projection*. Nat Rev Methods \nPrimers 4, 82 (2024).\n\nA read only version of this paper can accessed via `link `_\n\nThe important thing is that you don't need to worry about that\u2014you can use\nUMAP right now for dimension reduction and visualisation as easily as a drop\nin replacement for scikit-learn's t-SNE.\n\nDocumentation is `available via Read the Docs `_.\n\n**New: this package now also provides support for densMAP.** The densMAP algorithm augments UMAP\nto preserve local density information in addition to the topological structure of the data.\nDetails of this method are described in the following `paper `_:\n\nNarayan, A, Berger, B, Cho, H, *Assessing Single-Cell Transcriptomic Variability\nthrough Density-Preserving Data Visualization*, Nature Biotechnology, 2021\n\n----------\nInstalling\n----------\n\nUMAP depends upon ``scikit-learn``, and thus ``scikit-learn``'s dependencies\nsuch as ``numpy`` and ``scipy``. UMAP adds a requirement for ``numba`` for\nperformance reasons. The original version used Cython, but the improved code\nclarity, simplicity and performance of Numba made the transition necessary.\n\nRequirements:\n\n* Python 3.6 or greater\n* numpy\n* scipy\n* scikit-learn\n* numba\n* tqdm\n* `pynndescent `_\n\nRecommended packages:\n\n* For plotting\n   * matplotlib\n   * datashader\n   * holoviews\n* for Parametric UMAP\n   * tensorflow > 2.0.0\n\n**Install Options**\n\nConda install, via the excellent work of the conda-forge team:\n\n.. code:: bash\n\nThe conda-forge packages are available for Linux, OS X, and Windows 64 bit.\n\nPyPI install, presuming you have numba and sklearn and all its requirements\n(numpy and scipy) installed:\n\n.. code:: bash\n\nIf you wish to use the plotting functionality you can use\n\n.. code:: bash\n\nto install all the plotting dependencies."}, {"name": "umap-learn", "tags": ["data", "math", "ml", "ui", "visualization", "web"], "summary": "Uniform Manifold Approximation and Projection", "text": "If you wish to use Parametric UMAP, you need to install Tensorflow, which can be\ninstalled either using the instructions at https://www.tensorflow.org/install\n(recommended) or using\n\n.. code:: bash\n\nfor a CPU-only version of Tensorflow.\n\nIf you're on an x86 processor, you can also optionally install `tbb`, which will\nprovide additional CPU optimizations:\n\n.. code:: bash\n\nIf pip is having difficulties pulling the dependencies then we'd suggest installing\nthe dependencies manually using anaconda followed by pulling umap from pip:\n\n.. code:: bash\n\nFor a manual install get this package:\n\n.. code:: bash\n\nOptionally, install the requirements through Conda:\n\n.. code:: bash\n\nThen install the package\n\n.. code:: bash\n\n---------------\nHow to use UMAP\n---------------\n\nThe umap package inherits from sklearn classes, and thus drops in neatly\nnext to other sklearn transformers with an identical calling API.\n\n.. code:: python\n\nThere are a number of parameters that can be set for the UMAP class; the\nmajor ones are as follows:\n\n-  ``n_neighbors``: This determines the number of neighboring points used in\n\n-  ``min_dist``: This controls how tightly the embedding is allowed compress\n\n-  ``metric``: This determines the choice of metric used to measure distance\n\nAn example of making use of these options:\n\n.. code:: python\n\nUMAP also supports fitting to sparse matrix data. For more details\nplease see `the UMAP documentation `_\n\n----------------\nBenefits of UMAP\n----------------\n\nUMAP has a few signficant wins in its current incarnation.\n\nFirst of all UMAP is *fast*. It can handle large datasets and high\ndimensional data without too much difficulty, scaling beyond what most t-SNE\npackages can manage. This includes very high dimensional sparse datasets. UMAP\nhas successfully been used directly on data with over a million dimensions.\n\nSecond, UMAP scales well in embedding dimension\u2014it isn't just for\nvisualisation! You can use UMAP as a general purpose dimension reduction\ntechnique as a preliminary step to other machine learning tasks. With a\nlittle care it partners well with the `hdbscan\n`_ clustering library (for\nmore details please see `Using UMAP for Clustering\n`_).\n\nThird, UMAP often performs better at preserving some aspects of global structure\nof the data than most implementations of t-SNE. This means that it can often\nprovide a better \"big picture\" view of your data as well as preserving local neighbor\nrelations.\n\nFourth, UMAP supports a wide variety of distance functions, including\nnon-metric distance functions such as *cosine distance* and *correlation\ndistance*. You can finally embed word vectors properly using cosine distance!\n\nFifth, UMAP supports adding new points to an existing embedding via\nthe standard sklearn ``transform`` method. This means that UMAP can be\nused as a preprocessing transformer in sklearn pipelines.\n\nSixth, UMAP supports supervised and semi-supervised dimension reduction.\nThis means that if you have label information that you wish to use as\nextra information for dimension reduction (even if it is just partial\nlabelling) you can do that\u2014as simply as providing it as the ``y``\nparameter in the fit method."}, {"name": "umap-learn", "tags": ["data", "math", "ml", "ui", "visualization", "web"], "summary": "Uniform Manifold Approximation and Projection", "text": "Seventh, UMAP supports a variety of additional experimental features including: an\n\"inverse transform\" that can approximate a high dimensional sample that would map to\na given position in the embedding space; the ability to embed into non-euclidean\nspaces including hyperbolic embeddings, and embeddings with uncertainty; very\npreliminary support for embedding dataframes also exists.\n\nFinally, UMAP has solid theoretical foundations in manifold learning\n(see `our paper on ArXiv `_).\nThis both justifies the approach and allows for further\nextensions that will soon be added to the library.\n\n------------------------\nPerformance and Examples\n------------------------\n\nUMAP is very efficient at embedding large high dimensional datasets. In\nparticular it scales well with both input dimension and embedding dimension.\nFor the best possible performance we recommend installing the nearest neighbor\ncomputation library `pynndescent `_ .\nUMAP will work without it, but if installed it will run faster, particularly on\nmulticore machines.\n\nFor a problem such as the 784-dimensional MNIST digits dataset with\n70000 data samples, UMAP can complete the embedding in under a minute (as\ncompared with around 45 minutes for scikit-learn's t-SNE implementation).\nDespite this runtime efficiency, UMAP still produces high quality embeddings.\n\nThe obligatory MNIST digits dataset, embedded in 42\nseconds (with pynndescent installed and after numba jit warmup)\nusing a 3.1 GHz Intel Core i7 processor (n_neighbors=10, min_dist=0.001):\n\n.. image:: images/umap_example_mnist1.png\n\nThe MNIST digits dataset is fairly straightforward, however. A better test is\nthe more recent \"Fashion MNIST\" dataset of images of fashion items (again\n70000 data sample in 784 dimensions). UMAP\nproduced this embedding in 49 seconds (n_neighbors=5, min_dist=0.1):\n\n.. image:: images/umap_example_fashion_mnist1.png\n\nThe UCI shuttle dataset (43500 sample in 8 dimensions) embeds well under\n*correlation* distance in 44 seconds (note the longer time\nrequired for correlation distance computations):\n\n.. image:: images/umap_example_shuttle.png\n\nThe following is a densMAP visualization of the MNIST digits dataset with 784 features\nbased on the same parameters as above (n_neighbors=10, min_dist=0.001). densMAP reveals\nthat the cluster corresponding to digit 1 is noticeably denser, suggesting that\nthere are fewer degrees of freedom in the images of 1 compared to other digits.\n\n.. image:: images/densmap_example_mnist.png\n\n--------\nPlotting\n--------\n\nUMAP includes a subpackage ``umap.plot`` for plotting the results of UMAP embeddings.\nThis package needs to be imported separately since it has extra requirements\n(matplotlib, datashader and holoviews). It allows for fast and simple plotting and\nattempts to make sensible decisions to avoid overplotting and other pitfalls. An\nexample of use:\n\n.. code:: python\n\nThe plotting package offers basic plots, as well as interactive plots with hover\ntools and various diagnostic plotting options. See the documentation for more details.\n\n---------------\nParametric UMAP\n---------------\n\nParametric UMAP provides support for training a neural network to learn a UMAP based\ntransformation of data. This can be used to support faster inference of new unseen\ndata, more robust inverse transforms, autoencoder versions of UMAP and\nsemi-supervised classification (particularly for data well separated by UMAP and very\nlimited amounts of labelled data). See the\n`documentation of Parametric UMAP `_\nor the\n`example notebooks `_\nfor more."}, {"name": "umap-learn", "tags": ["data", "math", "ml", "ui", "visualization", "web"], "summary": "Uniform Manifold Approximation and Projection", "text": "-------\ndensMAP\n-------\n\nThe densMAP algorithm augments UMAP to additionally preserve local density information\nin addition to the topological structure captured by UMAP. One can easily run densMAP\nusing the umap package by setting the ``densmap`` input flag:\n\n.. code:: python\n\nThis functionality is built upon the densMAP `implementation `_ provided by the developers\nof densMAP, who also contributed to integrating densMAP into the umap package.\n\ndensMAP inherits all of the parameters of UMAP. The following is a list of additional\nparameters that can be set for densMAP:\n\n- ``dens_frac``: This determines the fraction of epochs (a value between 0 and 1) that will include the density-preservation term in the optimization objective. This parameter is set to 0.3 by default. Note that densMAP switches density optimization on after an initial phase of optimizing the embedding using UMAP.\n\n- ``dens_lambda``: This determines the weight of the density-preservation objective. Higher values prioritize density preservation, and lower values (closer to zero) prioritize the UMAP objective. Setting this parameter to zero reduces the algorithm to UMAP. Default value is 2.0.\n\n- ``dens_var_shift``: Regularization term added to the variance of local densities in the embedding for numerical stability. We recommend setting this parameter to 0.1, which consistently works well in many settings.\n\n- ``output_dens``: When this flag is True, the call to ``fit_transform`` returns, in addition to the embedding, the local radii (inverse measure of local density defined in the `densMAP paper `_) for the original dataset and for the embedding. The output is a tuple ``(embedding, radii_original, radii_embedding)``. Note that the radii are log-transformed. If False, only the embedding is returned. This flag can also be used with UMAP to explore the local densities of UMAP embeddings. By default this flag is False.\n\nFor densMAP we recommend larger values of ``n_neighbors`` (e.g. 30) for reliable estimation of local density.\n\nAn example of making use of these options (based on a subsample of the mnist_784 dataset):\n\n.. code:: python\n\nSee `the documentation `_ for more details.\n\n---------------------------------\nInteractive UMAP with Nomic Atlas\n---------------------------------\n\n.. image:: https://assets.nomicatlas.com/mnist-training-embeddings-umap-short.gif\n   :width: 600\n   :alt: MNIST UMAP visualization in Nomic Atlas\n\nFor interactive exploration of UMAP embeddings, especially for visualizing large datasets data over time/training epochs, you can use `Nomic Atlas `_. Nomic Atlas is a platform for embedding generation, visualization, analysis, and retrieval that directly integrates UMAP as one of its projection models.\n\nUsing Nomic Atlas with UMAP is straightforward:\n\n.. code:: python\n\nNomic Atlas provides:\n\n* In-browser analysis of your UMAP data with the `Atlas Analyst `_\n* Vector search over your UMAP data using the `Nomic API `_\n* Interactive features like zooming, recoloring, searching, and filtering in the `Nomic Atlas data map `_\n* Scalability for millions of data points\n* Rich information display on hover\n* Shareable UMAPs via URL links to your embeddings and data maps in Atlas\n\n----------------\nHelp and Support\n----------------"}, {"name": "umap-learn", "tags": ["data", "math", "ml", "ui", "visualization", "web"], "summary": "Uniform Manifold Approximation and Projection", "text": "Documentation is at `Read the Docs `_.\nThe documentation `includes a FAQ `_ that\nmay answer your questions. If you still have questions then please\n`open an issue `_\nand I will try to provide any help and guidance that I can.\n\n--------\nCitation\n--------\n\nIf you make use of this software for your work we would appreciate it if you\nwould cite the paper from the Journal of Open Source Software:\n\n.. code:: bibtex\n\nIf you would like to cite this algorithm in your work the ArXiv paper is the\ncurrent reference:\n\n.. code:: bibtex\n\n@article{2018arXivUMAP,\n   }\n\nIf you found the Nature Primer introduction useful please cite the following reference:\n\n.. code:: bibtex\n\nAdditionally, if you use the densMAP algorithm in your work please cite the following reference:\n\n.. code:: bibtex\n\nIf you use the Parametric UMAP algorithm in your work please cite the following reference:\n\n.. code:: bibtex\n\n-------\nLicense\n-------\n\nThe umap package is 3-clause BSD licensed.\n\nWe would like to note that the umap package makes heavy use of\nNumFOCUS sponsored projects, and would not be possible without\ntheir support of those projects, so please `consider contributing to NumFOCUS `_.\n\n------------\nContributing\n------------\n\nContributions are more than welcome! There are lots of opportunities\nfor potential projects, so please get in touch if you would like to\nhelp out. Everything from code to notebooks to\nexamples and documentation are all *equally valuable* so please don't feel\nyou can't contribute. To contribute please\n`fork the project `_\nmake your changes and\nsubmit a pull request. We will do our best to work through any issues with\nyou and get your code merged into the main branch."}, {"name": "umap-learn", "tags": ["data", "math", "ml", "ui", "visualization", "web"], "summary": "Uniform Manifold Approximation and Projection", "text": "This library is used to perform dimensionality reduction and visualization of high-dimensional data using the Uniform Manifold Approximation and Projection (UMAP) algorithm. With umap-learn, developers can embed complex datasets in two or three dimensions for easy exploration and analysis."}, {"name": "uncertainties", "tags": ["math", "web"], "summary": "calculations with values with uncertainties, error propagation", "text": "uncertainties\n=============\n\n.. image:: https://img.shields.io/pypi/v/uncertainties.svg\n   :target: https://pypi.org/project/uncertainties/\n\n   :target: https://pepy.tech/project/uncertainties\n\n   :target: https://codecov.io/gh/lmfit/uncertainties/\n.. image:: https://img.shields.io/github/actions/workflow/status/lmfit/uncertainties/python-package.yml?logo=github%20actions\n   :target: https://github.com/lmfit/uncertainties/actions/workflows/python-package.yml\n\nThe ``uncertainties`` package allows calculations with values that have\nuncertaintes, such as (2 +/- 0.1)*2 = 4 +/- 0.2.  ``uncertainties`` takes the\npain and complexity out of error propagation and calculations of values with\nuncertainties.  For more information, see https://uncertainties.readthedocs.io/\n\nBasic examples\n--------------\n\n.. code-block:: python\n\nMain features\n-------------\n\n- **Transparent calculations with uncertainties**: Little or\n  no modification of existing code is needed to convert calculations of floats\n  to calculations of values with uncertainties.\n\n- **Correlations** between expressions are correctly taken into\n  account.  Thus, ``x-x`` is exactly zero.\n\n- **Most  mathematical operations** are supported, including most\n  functions from the standard math_ module (sin,...).  Comparison\n  operators (``>``, ``==``, etc.) are supported too.\n\n- Many **fast operations on arrays and matrices** of numbers with\n  uncertainties are supported.\n\n- **Extensive support for printing** numbers with uncertainties\n  (including LaTeX support and pretty-printing).\n\n- Most uncertainty calculations are performed **analytically**.\n\n- This module also gives access to the **derivatives** of any\n  mathematical expression (they are used by `error\n  propagation theory`_, and are thus automatically calculated by this\n  module).\n\nInstallation or upgrade\n-----------------------\n\nTo install `uncertainties`, use::\n\nTo upgrade from an older version, use::\n\nFurther details are in the `on-line documentation\n`_.\n\nGit branches\n------------\n\nThe GitHub ``master`` branch is the latest development version, and is intended\nto be a stable pre-release version. It will be experimental, but should pass\nall tests..  Tagged releases will be available on GitHub, and correspond to the\nreleases to PyPI.  The GitHub ``gh-pages`` branch will contain a stable test version\nof the documentation that can be viewed at\n``_.  Other Github branches should be\ntreated as unstable and in-progress development branches.\n\nLicense\n-------\n\nThis package and its documentation are released under the `Revised BSD\nLicense `_.\n\nHistory\n-------\n\n..\n   Note from Eric Lebigot: I would like the origin of the package to\n   remain documented for its whole life. Thanks!\n\nThis package was created back around 2009 by `Eric O. LEBIGOT `_.\n\nOwnership of the package was taken over by the `lmfit GitHub organization `_ in 2024.\n\n.. _IPython: https://ipython.readthedocs.io/en/stable/\n.. _math: https://docs.python.org/library/math.html\n.. _error propagation theory: https://en.wikipedia.org/wiki/Propagation_of_uncertainty\n.. _main website: https://uncertainties.readthedocs.io/"}, {"name": "uncertainties", "tags": ["math", "web"], "summary": "calculations with values with uncertainties, error propagation", "text": "This library is used to perform calculations that take into account the uncertainties associated with input values, simplifying error propagation and providing reliable results. With this library, developers can easily calculate the uncertainty of composite values by combining quantities with errors."}, {"name": "unsloth", "tags": ["cli", "data", "math", "ml", "ui"], "summary": "2-5X faster training, reinforcement learning & finetuning", "text": "Train gpt-oss, DeepSeek, Gemma, Qwen & Llama 2x faster with 70% less VRAM!\n\nTrain for Free\n\nNotebooks are beginner friendly. Read our [guide](https://docs.unsloth.ai/get-started/fine-tuning-guide). Add dataset, run, then export your trained model to GGUF, llama.cpp, Ollama, vLLM, SGLang or Hugging Face.\n\nModel\n-----------\n**gpt-oss (20B)**\n**Mistral Ministral 3 (3B)**\n**gpt-oss (20B): GRPO**\n**Qwen3: Advanced GRPO**\n**Qwen3-VL (8B): GSPO**\n**Gemma 3 (270M)**\n**Gemma 3n (4B)**\n**DeepSeek-OCR (3B)**\n**Llama 3.1 (8B) Alpaca**\n**Llama 3.2 Conversational**\n**Orpheus-TTS (3B)**\n\n\u26a1 Quickstart\n\nLinux or WSL\n\nWindows\nFor Windows, `pip install unsloth` works only if you have Pytorch installed. Read our [Windows Guide](https://docs.unsloth.ai/get-started/installing-+-updating/windows-installation).\n\nDocker\nUse our official [Unsloth Docker image](https://hub.docker.com/r/unsloth/unsloth)  container. Read our [Docker Guide](https://docs.unsloth.ai/get-started/install-and-update/docker).\n\nBlackwell & DGX Spark\nFor RTX 50x, B200, 6000 GPUs: `pip install unsloth`. Read our [Blackwell Guide](https://docs.unsloth.ai/basics/training-llms-with-blackwell-rtx-50-series-and-unsloth) and [DGX Spark Guide](https://docs.unsloth.ai/new/fine-tuning-llms-with-nvidia-dgx-spark-and-unsloth) for more details.\n\nUnsloth News\n\nClick for more news\n\nLinks and Resources\nType\n-------------------------------\n&nbsp; **r/unsloth Reddit**\n **Documentation & Wiki**\n&nbsp; **Twitter (aka X)**\n **Installation**\n **Our Models**\n\ufe0f **Blog**\n\n\u2b50 Key Features\n- Supports **full-finetuning**, pretraining, 4b-bit, 16-bit and **FP8** training\n- Supports **all models** including [TTS](https://docs.unsloth.ai/basics/text-to-speech-tts-fine-tuning), multimodal, [BERT](https://docs.unsloth.ai/get-started/unsloth-notebooks#other-important-notebooks) and more! Any model that works in transformers, works in Unsloth.\n- The most efficient library for [Reinforcement Learning (RL)](https://docs.unsloth.ai/get-started/reinforcement-learning-rl-guide), using 80% less VRAM. Supports GRPO, GSPO, DrGRPO, DAPO etc.\n- **0% loss in accuracy** - no approximation methods - all exact.\n- Supports NVIDIA (since 2018), [AMD](https://docs.unsloth.ai/get-started/install-and-update/amd) and Intel GPUs. Minimum CUDA Capability 7.0 (V100, T4, Titan V, RTX 20, 30, 40x, A100, H100, L40 etc)\n- Works on **Linux**, WSL and **Windows**\n- All kernels written in [OpenAI's Triton](https://openai.com/index/triton/) language. Manual backprop engine.\n- If you trained a model with Unsloth, you can use this cool sticker! &nbsp;\n\nInstall Unsloth\nYou can also see our docs for more detailed installation and updating instructions [here](https://docs.unsloth.ai/get-started/installing-+-updating).\n\nUnsloth supports Python 3.13 or lower.\n\nPip Installation\n**Install with pip (recommended) for Linux devices:**\n\n**To update Unsloth:**\n\nSee [here](#advanced-pip-installation) for advanced pip install instructions.\n\nWindows Installation\n\n1. **Install NVIDIA Video Driver:**\n  You should install the latest driver for your GPU. Download drivers here: [NVIDIA GPU Driver](https://www.nvidia.com/Download/index.aspx).\n\n3. **Install Visual Studio C++:**\n   You will need Visual Studio, with C++ installed. By default, C++ is not installed with [Visual Studio](https://visualstudio.microsoft.com/vs/community/), so make sure you select all of the C++ options. Also select options for Windows 10/11 SDK. For detailed instructions with options, see [here](https://docs.unsloth.ai/get-started/installing-+-updating).\n\n5. **Install CUDA Toolkit:**\n   Follow the instructions to install [CUDA Toolkit](https://developer.nvidia.com/cuda-toolkit-archive).\n\n6. **Install PyTorch:**\n   You will need the correct version of PyTorch that is compatible with your CUDA drivers, so make sure to select them carefully.\n   [Install PyTorch](https://pytorch.org/get-started/locally/).\n\n7. **Install Unsloth:**\n\nNotes\nTo run Unsloth directly on Windows:\n- Install Triton from this Windows fork and follow the instructions [here](https://github.com/woct0rdho/triton-windows) (be aware that the Windows fork requires PyTorch >= 2.4 and CUDA 12)\n- In the `SFTConfig`, set `dataset_num_proc=1` to avoid a crashing issue:\n\nAdvanced/Troubleshooting\n\nFor **advanced installation instructions** or if you see weird errors during installations:\n\nFirst try using an isolated environment via then `pip install unsloth`"}, {"name": "unsloth", "tags": ["cli", "data", "math", "ml", "ui"], "summary": "2-5X faster training, reinforcement learning & finetuning", "text": "1. Install `torch` and `triton`. Go to https://pytorch.org to install it. For example `pip install torch torchvision torchaudio triton`\n2. Confirm if CUDA is installed correctly. Try `nvcc`. If that fails, you need to install `cudatoolkit` or CUDA drivers.\n3. Install `xformers` manually via:\n\n5. For GRPO runs, you can try installing `vllm` and seeing if `pip install vllm` succeeds.\n6. Double check that your versions of Python, CUDA, CUDNN, `torch`, `triton`, and `xformers` are compatible with one another. The [PyTorch Compatibility Matrix](https://github.com/pytorch/pytorch/blob/main/RELEASE.md#release-compatibility-matrix) may be useful. \n5. Finally, install `bitsandbytes` and check it with `python -m bitsandbytes`\n\nConda Installation (Optional)\n`\u26a0\ufe0fOnly use Conda if you have it. If not, use Pip`. Select either `pytorch-cuda=11.8,12.1` for CUDA 11.8 or CUDA 12.1. We support `python=3.10,3.11,3.12`.\n\nIf you're looking to install Conda in a Linux environment, read here, or run the below\n\nAdvanced Pip Installation\n`\u26a0\ufe0fDo **NOT** use this if you have Conda.` Pip is a bit more complex since there are dependency issues. The pip command is different for `torch 2.2,2.3,2.4,2.5,2.6,2.7,2.8,2.9` and CUDA versions.\n\nFor other torch versions, we support `torch211`, `torch212`, `torch220`, `torch230`, `torch240`, `torch250`, `torch260`, `torch270`, `torch280`, `torch290` and for CUDA versions, we support `cu118` and `cu121` and `cu124`. For Ampere devices (A100, H100, RTX3090) and above, use `cu118-ampere` or `cu121-ampere` or `cu124-ampere`.\n\nFor example, if you have `torch 2.4` and `CUDA 12.1`, use:\n\nAnother example, if you have `torch 2.9` and `CUDA 13.0`, use:\n\nAnd other examples:\n\nOr, run the below in a terminal to get the **optimal** pip installation command:\n\nOr, run the below manually in a Python REPL:\n\nDocker Installation\nYou can use our pre-built Docker container with all dependencies to use Unsloth instantly with no setup required.\n[Read our guide](https://docs.unsloth.ai/get-started/install-and-update/docker).\n\nThis container requires installing [NVIDIA's Container Toolkit](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html).\n\nAccess Jupyter Lab at `http://localhost:8888` and start fine-tuning!\n\nDocumentation\n- Go to our official [Documentation](https://docs.unsloth.ai) for [running models](https://docs.unsloth.ai/basics/running-and-saving-models), [saving to GGUF](https://docs.unsloth.ai/basics/running-and-saving-models/saving-to-gguf), [checkpointing](https://docs.unsloth.ai/basics/finetuning-from-last-checkpoint), [evaluation](https://docs.unsloth.ai/get-started/fine-tuning-llms-guide#evaluation) and more!\n- Read our Guides for: [Fine-tuning](https://docs.unsloth.ai/get-started/fine-tuning-llms-guide), [Reinforcement Learning](https://docs.unsloth.ai/get-started/reinforcement-learning-rl-guide), [Text-to-Speech (TTS)](https://docs.unsloth.ai/basics/text-to-speech-tts-fine-tuning), [Vision](https://docs.unsloth.ai/basics/vision-fine-tuning) and [any model](https://docs.unsloth.ai/models/tutorials-how-to-fine-tune-and-run-llms).\n- We support Huggingface's transformers, TRL, Trainer, Seq2SeqTrainer and Pytorch code.\n\nUnsloth example code to fine-tune gpt-oss-20b:\n\nReinforcement Learning\n[RL](https://docs.unsloth.ai/get-started/reinforcement-learning-rl-guide) including [GRPO](https://docs.unsloth.ai/get-started/reinforcement-learning-rl-guide#training-with-grpo), [GSPO](https://docs.unsloth.ai/get-started/reinforcement-learning-rl-guide/gspo-reinforcement-learning), **FP8** traning, DrGRPO, DAPO, PPO, Reward Modelling, Online DPO all work with Unsloth.\nRead our [Reinforcement Learning Guide](https://docs.unsloth.ai/get-started/reinforcement-learning-rl-guide) or our [advanced RL docs](https://docs.unsloth.ai/get-started/reinforcement-learning-rl-guide/advanced-rl-documentation) for batching, generation & training parameters.\n\nList of RL notebooks:\n\nPerformance Benchmarking\n- For our most detailed benchmarks, read our [Llama 3.3 Blog](https://unsloth.ai/blog/llama3-3).\n- Benchmarking of Unsloth was also conducted by [Hugging Face](https://huggingface.co/blog/unsloth-trl).\n\nWe tested using the Alpaca  Dataset, a batch size of 2, gradient accumulation steps of 4, rank = 32, and applied QLoRA on all linear layers (q, k, v, o, gate, up, down):\n  \nModel\n----------------\nLlama 3.3 (70B)\nLlama 3.1 (8B)\n\nContext length benchmarks"}, {"name": "unsloth", "tags": ["cli", "data", "math", "ml", "ui"], "summary": "2-5X faster training, reinforcement learning & finetuning", "text": "Llama 3.1 (8B) max. context length\nWe tested Llama 3.1 (8B) Instruct and did 4bit QLoRA on all linear layers (Q, K, V, O, gate, up and down) with rank = 32 with a batch size of 1. We padded all sequences to a certain maximum sequence length to mimic long context finetuning workloads.\nGPU VRAM\n----------\n8 GB\n12 GB\n16 GB\n24 GB\n40 GB\n48 GB\n80 GB\n\nLlama 3.3 (70B) max. context length\nWe tested Llama 3.3 (70B) Instruct on a 80GB A100 and did 4bit QLoRA on all linear layers (Q, K, V, O, gate, up and down) with rank = 32 with a batch size of 1. We padded all sequences to a certain maximum sequence length to mimic long context finetuning workloads.\n\nGPU VRAM\n----------\n48 GB\n80 GB\n\nCitation\n\nYou can cite the Unsloth repo as follows:\n\nThank You to\n- And of course for every single person who has contributed or has used Unsloth!"}, {"name": "unsloth", "tags": ["cli", "data", "math", "ml", "ui"], "summary": "2-5X faster training, reinforcement learning & finetuning", "text": "This library is used to significantly accelerate the training process for various large language models, enabling developers to train models 2-5X faster and with 70% less VRAM. Developers can leverage this library to efficiently fine-tune and deploy a range of pre-trained models across different applications and use cases."}, {"name": "unstructured-inference", "tags": ["math", "ml", "ui", "web"], "summary": "A library for performing inference using trained models.", "text": "Installation\n\nPackage\n\nRun `pip install unstructured-inference`.\n\nDetectron2\n\n[Detectron2](https://github.com/facebookresearch/detectron2) is required for using models from the [layoutparser model zoo](#using-models-from-the-layoutparser-model-zoo) \nbut is not automatically installed with this package. \nFor MacOS and Linux, build from source with:\n\nOther install options can be found in the \n[Detectron2 installation guide](https://detectron2.readthedocs.io/en/latest/tutorials/install.html).\n\nWindows is not officially supported by Detectron2, but some users are able to install it anyway. \nSee discussion [here](https://layout-parser.github.io/tutorials/installation#for-windows-users) for \ntips on installing Detectron2 on Windows.\n\nRepository\n\nTo install the repository for development, clone the repo and run `make install` to install dependencies.\nRun `make help` for a full list of install options.\n\nGetting Started\n\nTo get started with the layout parsing model, use the following commands:\n\nOnce the model has detected the layout and OCR'd the document, the text extracted from the first \npage of the sample document will be displayed.\nYou can convert a given element to a `dict` by running the `.to_dict()` method.\n\nModels\n\nThe inference pipeline operates by finding text elements in a document page using a detection model, then extracting the contents of the elements using direct extraction (if available), OCR, and optionally table inference models.\n\nWe offer several detection models including [Detectron2](https://github.com/facebookresearch/detectron2) and [YOLOX](https://github.com/Megvii-BaseDetection/YOLOX).\n\nUsing a non-default model\n\nWhen doing inference, an alternate model can be used by passing the model object to the ingestion method via the `model` parameter. The `get_model` function can be used to construct one of our out-of-the-box models from a keyword, e.g.:\n\nUsing your own model\n\nAny detection model can be used for in the `unstructured_inference` pipeline by wrapping the model in the `UnstructuredObjectDetectionModel` class. To integrate with the `DocumentLayout` class, a subclass of `UnstructuredObjectDetectionModel` must have a `predict` method that accepts a `PIL.Image.Image` and returns a list of `LayoutElement`s, and an `initialize` method, which loads the model and prepares it for inference.\n\nSecurity Policy\n\nSee our [security policy](https://github.com/Unstructured-IO/unstructured-inference/security/policy) for\ninformation on how to report security vulnerabilities.\n\nLearn more\n\nSection\n-\n[Unstructured Community Github](https://github.com/Unstructured-IO/community)\n[Unstructured Github](https://github.com/Unstructured-IO)\n[Company Website](https://unstructured.io)"}, {"name": "unstructured-inference", "tags": ["math", "ml", "ui", "web"], "summary": "A library for performing inference using trained models.", "text": "This library is used to perform inference using trained models from various sources, including the layoutparser model zoo. By leveraging this library, developers can easily integrate and run pre-trained models in their applications."}, {"name": "unstructured", "tags": ["data", "dev", "math", "ml", "ui", "web"], "summary": "A library that prepares raw documents for downstream ML tasks.", "text": "Try the Unstructured Platform Product\n\nReady to move your data processing pipeline to production, and take advantage of advanced features? Check out [Unstructured Platform](https://unstructured.io/enterprise). In addition to better processing performance, take advantage of chunking, embedding, and image and table enrichment generation, all from a low code UI or an API. [Request a demo](https://unstructured.io/contact) from our sales team to learn more about how to get started.\n\n:eight_pointed_black_star: Quick Start\n\nThere are several ways to use the `unstructured` library:\n* [Run the library in a container](https://github.com/Unstructured-IO/unstructured#run-the-library-in-a-container) or\n* Install the library\n* For installation with `conda` on Windows system, please refer to the [documentation](https://unstructured-io.github.io/unstructured/installing.html#installation-with-conda-on-windows)\n\nRun the library in a container\n\nThe following instructions are intended to help you get up and running using Docker to interact with `unstructured`.\nSee [here](https://docs.docker.com/get-docker/) if you don't already have docker installed on your machine.\n\nNOTE: we build multi-platform images to support both x86_64 and Apple silicon hardware. `docker pull` should download the corresponding image for your architecture, but you can specify with `--platform` (e.g. `--platform linux/amd64`) if needed.\n\nWe build Docker images for all pushes to `main`. We tag each image with the corresponding short commit hash (e.g. `fbc7a69`) and the application version (e.g. `0.5.5-dev1`). We also tag the most recent image with `latest`. To leverage this, `docker pull` from our image repository.\n\nOnce pulled, you can create a container from this image and shell to it.\n\nYou can also build your own Docker image. Note that the base image is `wolfi-base`, which is\nupdated regularly. If you are building the image locally, it is possible `docker-build` could\nfail due to upstream changes in `wolfi-base`.\n\nIf you only plan on parsing one type of data you can speed up building the image by commenting out some\nof the packages/requirements necessary for other data types. See Dockerfile to know which lines are necessary\nfor your use case.\n\nOnce in the running container, you can try things directly in Python interpreter's interactive mode.\n\nInstalling the library\nUse the following instructions to get up and running with `unstructured` and test your\ninstallation.\n\n- Install the Python SDK to support all document types with `pip install \"unstructured[all-docs]\"`\n  - For plain text files, HTML, XML, JSON and Emails that do not require any extra dependencies, you can run `pip install unstructured`\n  - To process other doc types, you can install the extras required for those documents, such as `pip install \"unstructured[docx,pptx]\"`\n- Install the following system dependencies if they are not already available on your system.\n  Depending on what document types you're parsing, you may not need all of these.\n\n- For suggestions on how to install on the Windows and to learn about dependencies for other features, see the\n  installation documentation [here](https://unstructured-io.github.io/unstructured/installing.html).\n\nAt this point, you should be able to run the following code:\n\nInstallation Instructions for Local Development\n\nThe following instructions are intended to help you get up and running with `unstructured`\nlocally if you are planning to contribute to the project."}, {"name": "unstructured", "tags": ["data", "dev", "math", "ml", "ui", "web"], "summary": "A library that prepares raw documents for downstream ML tasks.", "text": "* Using `pyenv` to manage virtualenv's is recommended but not necessary\n\t* Mac install instructions. See [here](https://github.com/Unstructured-IO/community#mac--homebrew) for more detailed instructions.\n\t\t* `brew install pyenv-virtualenv`\n\t  * `pyenv install 3.10`\n  * Linux instructions are available [here](https://github.com/Unstructured-IO/community#linux).\n\n* Create a virtualenv to work in and activate it, e.g. for one named `unstructured`:\n\n`pyenv  virtualenv 3.10 unstructured` \n\t`pyenv activate unstructured`\n\n* Run `make install`\n\n* Optional:\n  * To install models and dependencies for processing images and PDFs locally, run `make install-local-inference`.\n  * For processing image files, `tesseract` is required. See [here](https://tesseract-ocr.github.io/tessdoc/Installation.html) for installation instructions.\n  * For processing PDF files, `tesseract` and `poppler` are required. The [pdf2image docs](https://pdf2image.readthedocs.io/en/latest/installation.html) have instructions on installing `poppler` across various platforms.\n\nAdditionally, if you're planning to contribute to `unstructured`, we provide you an optional `pre-commit` configuration\nfile to ensure your code matches the formatting and linting standards used in `unstructured`.\nIf you'd prefer not to have code changes auto-tidied before every commit, you can use  `make check` to see\nwhether any linting or formatting changes should be applied, and `make tidy` to apply them.\n\nIf using the optional `pre-commit`, you'll just need to install the hooks with `pre-commit install` since the\n`pre-commit` package is installed as part of `make install` mentioned above. Finally, if you decided to use `pre-commit`\nyou can also uninstall the hooks with `pre-commit uninstall`.\n\nIn addition to develop in your local OS we also provide a helper to use docker providing a development environment:\n\nThis starts a docker container with your local repo mounted to `/mnt/local_unstructured`. This docker image allows you to develop without worrying about your OS's compatibility with the repo and its dependencies.\n\n:clap: Quick Tour\n\nDocumentation\nFor more comprehensive documentation, visit https://docs.unstructured.io . You can also learn\nmore about our other products on the documentation page, including our SaaS API.\n\nHere are a few pages from the [Open Source documentation page](https://docs.unstructured.io/open-source/introduction/overview)\nthat are helpful for new users to review:\n\nPDF Document Parsing Example\nThe following examples show how to get started with the `unstructured` library. The easiest way to parse a document in unstructured is to use the `partition` function. If you use `partition` function, `unstructured` will detect the file type and route it to the appropriate file-specific partitioning function. If you are using the `partition` function, you may need to install additional dependencies per doc type.\nFor example, to install docx dependencies you need to run `pip install \"unstructured[docx]\"`.\nSee our  [installation guide](https://docs.unstructured.io/open-source/installation/full-installation) for more details.\n\nRun `print(\"\\n\\n\".join([str(el) for el in elements]))` to get a string representation of the\noutput, which looks like:\n\nSee the [partitioning](https://docs.unstructured.io/open-source/core-functionality/partitioning)\nsection in our documentation for a full list of options and instructions on how to use\nfile-specific partitioning functions.\n\n:guardsman: Security Policy\n\nSee our [security policy](https://github.com/Unstructured-IO/unstructured/security/policy) for\ninformation on how to report security vulnerabilities.\n\n:bug: Reporting Bugs"}, {"name": "unstructured", "tags": ["data", "dev", "math", "ml", "ui", "web"], "summary": "A library that prepares raw documents for downstream ML tasks.", "text": "Encountered a bug? Please create a new [GitHub issue](https://github.com/Unstructured-IO/unstructured/issues/new/choose) and use our bug report template to describe the problem. To help us diagnose the issue, use the `python scripts/collect_env.py` command to gather your system's environment information and include it in your report. Your assistance helps us continuously improve our software - thank you!\n\n:books: Learn more\n\nSection\n-\n[Company Website](https://unstructured.io)\n[Documentation](https://docs.unstructured.io/)\n[Batch Processing](https://github.com/Unstructured-IO/unstructured-ingest)\n\n:chart_with_upwards_trend: Analytics\n\nThis library includes a very lightweight analytics \"ping\" when the library is loaded, however you can opt out of this data collection by setting the environment variable `DO_NOT_TRACK=true` before executing any `unstructured` code. To learn more about how we collect and use this data, please read our [Privacy Policy](https://unstructured.io/privacy-policy)."}, {"name": "unstructured", "tags": ["data", "dev", "math", "ml", "ui", "web"], "summary": "A library that prepares raw documents for downstream ML tasks.", "text": "This library is used to prepare raw documents for downstream machine learning tasks by providing features like chunking, embedding, and image/table enrichment. It enables developers to streamline their data processing pipelines and improve performance through a low-code UI or API."}, {"name": "usaddress", "tags": ["cli", "dev", "math", "ui", "web"], "summary": "Parse US addresses using conditional random fields", "text": "Tools built with usaddress\n\n[Parserator API](https://parserator.datamade.us/)\nA RESTful API built on top of usaddress for programmers who don't use python. Requires an API key and the first 1,000 parses are free.\n\n[Parserator Google Sheets App](https://workspace.google.com/u/0/marketplace/app/parserator_parse_and_split_addresses/945974620840)\nParserator: Parse and Split Addresses allows you to easily split addresses into separate columns by street, city, state, zipcode and more right in Google Sheets.\n\nHow to use the usaddress python library\n\n1. Install usaddress with [pip](https://pip.readthedocs.io/en/latest/quickstart.html), a tool for installing and managing python packages ([beginner's guide here](http://www.dabapps.com/blog/introduction-to-pip-and-virtualenv-python/)).\n\n  In the terminal,\n  \n  \n2. Parse some addresses!\n\n  \n\n  Note that `parse` and `tag` are different methods:\n\nHow to use this development code (for the nerds)\nusaddress uses [parserator](https://github.com/datamade/parserator), a library for making and improving probabilistic parsers - specifically, parsers that use [python-crfsuite](https://github.com/tpeng/python-crfsuite)'s implementation of conditional random fields. Parserator allows you to train the usaddress parser's model (a .crfsuite settings file) on labeled training data, and provides tools for adding new labeled training data.\n\nBuilding & testing the code in this repo\n\nTo build a development version of usaddress on your machine, run the following code in your command line:\n  \n\nThen run the testing suite to confirm that everything is working properly:\n\n   \n   \nHaving trouble building the code? [Open an issue](https://github.com/datamade/usaddress/issues/new) and we'd be glad to help you troubleshoot.\n\nAdding new training data\n\nIf usaddress is consistently failing on particular address patterns, you can adjust the parser's behavior by adding new training data to the model. [Follow our guide in the training directory](./training/README.md), and be sure to make a pull request so that we can incorporate your contribution into our next release!\n\nImportant links\n\n* Web Interface: https://parserator.datamade.us/usaddress\n* Python Package Distribution: https://pypi.python.org/pypi/usaddress\n* Python Package Documentation: https://usaddress.readthedocs.io/\n* API Documentation: https://parserator.datamade.us/api-docs\n* Repository: https://github.com/datamade/usaddress\n* Issues: https://github.com/datamade/usaddress/issues\n* Blog post: http://datamade.us/blog/parsing-addresses-with-usaddress\n\nTeam\n\n* [Forest Gregg](https://github.com/fgregg), DataMade\n* [Cathy Deng](https://github.com/cathydeng), DataMade\n* [Miroslav Batchkarov](http://mbatchkarov.github.io), University of Sussex\n* [Jean Cochrane](https://github.com/jeancochrane), DataMade\n\nBad Parses / Bugs\n\nReport issues in the [issue tracker](https://github.com/datamade/usaddress/issues)\n\nIf an address was parsed incorrectly, please let us know! You can either [open an issue](https://github.com/datamade/usaddress/issues/new) or (if you're adventurous) [add new training data to improve the parser's model.](./training/README.md) When possible, please send over a few real-world examples of similar address patterns, along with some info about the source of the data - this will help us train the parser and improve its performance.\n\nIf something in the library is not behaving intuitively, it is a bug, and should be reported.\n\nNote on Patches/Pull Requests\n \n* Fork the project.\n* Make your feature addition or bug fix.\n* Send us a pull request. Bonus points for topic branches!\n\nCopyright\n\nCopyright (c) 2025 Atlanta Journal Constitution. Released under the [MIT License](./LICENSE)."}, {"name": "usaddress", "tags": ["cli", "dev", "math", "ui", "web"], "summary": "Parse US addresses using conditional random fields", "text": "This library is used to parse US addresses using conditional random fields, allowing developers to accurately extract address components such as street, city, state, and ZIP code. This enables efficient data processing and analysis of geospatial information in various applications."}, {"name": "vector-quantize-pytorch", "tags": ["data", "math", "ml"], "summary": "Vector Quantization - Pytorch", "text": "Vector Quantization - Pytorch\n\nA vector quantization library originally transcribed from Deepmind's tensorflow implementation, made conveniently into a package. It uses exponential moving averages to update the dictionary.\n\nVQ has been successfully used by Deepmind and OpenAI for high quality generation of images (VQ-VAE-2) and music (Jukebox).\n\nInstall\n\nUsage\n\nResidual VQ\n\nThis paper proposes to use multiple vector quantizers to recursively quantize the residuals of the waveform. You can use this with the `ResidualVQ` class and one extra initialization parameter.\n\nFurthermore, this paper uses Residual-VQ to construct the RQ-VAE, for generating high resolution images with more compressed codes.\n\nThey make two modifications. The first is to share the codebook across all quantizers. The second is to stochastically sample the codes rather than always taking the closest match. You can use both of these features with two extra keyword arguments.\n\nA recent paper further proposes to do residual VQ on groups of the feature dimension, showing equivalent results to Encodec while using far fewer codebooks. You can use it by importing `GroupedResidualVQ`\n\nInitialization\n\nThe SoundStream paper proposes that the codebook should be initialized by the kmeans centroids of the first batch. You can easily turn on this feature with one flag `kmeans_init = True`, for either `VectorQuantize` or `ResidualVQ` class\n\nGradient Computation\n\nVQ-VAEs are traditionally trained with the straight-through estimator (STE). During the backwards pass, the gradient flows _around_ the VQ layer rather than _through_ it. The rotation trick paper proposes to transform the gradient _through_ the VQ layer so the relative angle and magnitude between the input vector and quantized output are encoded into the gradient. You can enable or disable this feature with  in the  class.\n\nIncreasing codebook usage\n\nThis repository will contain a few techniques from various papers to combat \"dead\" codebook entries, which is a common problem when using vector quantizers.\n\nLower codebook dimension\n\nThe Improved VQGAN paper proposes to have the codebook kept in a lower dimension. The encoder values are projected down before being projected back to high dimensional after quantization. You can set this with the `codebook_dim` hyperparameter.\n\nCosine similarity\n\nThe Improved VQGAN paper also proposes to l2 normalize the codes and the encoded vectors, which boils down to using cosine similarity for the distance. They claim enforcing the vectors on a sphere leads to improvements in code usage and downstream reconstruction. You can turn this on by setting `use_cosine_sim = True`\n\nExpiring stale codes\n\nFinally, the SoundStream paper has a scheme where they replace codes that have hits below a certain threshold with randomly selected vector from the current batch. You can set this threshold with `threshold_ema_dead_code` keyword.\n\nOrthogonal regularization loss\n\nVQ-VAE / VQ-GAN is quickly gaining popularity. A recent paper proposes that when using vector quantization on images, enforcing the codebook to be orthogonal leads to translation equivariance of the discretized codes, leading to large improvements in downstream text to image generation tasks."}, {"name": "vector-quantize-pytorch", "tags": ["data", "math", "ml"], "summary": "Vector Quantization - Pytorch", "text": "You can use this feature by simply setting the `orthogonal_reg_weight` to be greater than `0`, in which case the orthogonal regularization will be added to the auxiliary loss outputted by the module.\n\nMulti-headed VQ\n\nThere has been a number of papers that proposes variants of discrete latent representations with a multi-headed approach (multiple codes per feature). I have decided to offer one variant where the same codebook is used to vector quantize across the input dimension `head` times.\n\nYou can also use a more proven approach (memcodes) from NWT paper\n\nRandom Projection Quantizer\n\nThis paper first proposed to use a random projection quantizer for masked speech modeling, where signals are projected with a randomly initialized matrix and then matched with a random initialized codebook. One therefore does not need to learn the quantizer. This technique was used by Google's Universal Speech Model to achieve SOTA for speech-to-text modeling.\n\nUSM further proposes to use multiple codebook, and the masked speech modeling with a multi-softmax objective. You can do this easily by setting `num_codebooks` to be greater than 1\n\nThis repository should also automatically synchronizing the codebooks in a multi-process setting. If somehow it isn't, please open an issue. You can override whether to synchronize codebooks or not by setting `sync_codebook = True | False`\n\nSim VQ\n\nA new paper proposes a scheme where the codebook is frozen, and the codes are implicitly generated through a linear projection. The authors claim this setup leads to less codebook collapse as well as easier convergence. I have found this to perform even better when paired with rotation trick from Fifty et al., and expanding the linear projection to a small one layer MLP. You can experiment with it as so\n\nUpdate: hearing mixed results\n\nFor the residual flavor, just import `ResidualSimVQ` instead\n\nFinite Scalar Quantization\n\nVQ\n------------------\nQuantization\nGradients\nAuxiliary Losses\nTricks\nParameters\n\n[This](https://arxiv.org/abs/2309.15505) work out of Google Deepmind aims to vastly simplify the way vector quantization is done for generative modeling, removing the need for commitment losses, EMA updating of the codebook, as well as tackle the issues with codebook collapse or insufficient utilization. They simply round each scalar into discrete levels with straight through gradients; the codes become uniform points in a hypercube.\n\nThanks goes out to [@sekstini](https://github.com/sekstini) for porting over this implementation in record time!\n\nAn improvised Residual FSQ, for an attempt to improve audio encoding.\n\nCredit goes to [@sekstini](https://github.com/sekstini) for originally incepting the idea [here](https://github.com/lucidrains/vector-quantize-pytorch/pull/74#issuecomment-1742048597)\n\nLookup Free Quantization\n\nThe research team behind MagViT has released new SOTA results for generative video modeling. A core change between v1 and v2 include a new type of quantization, look-up free quantization (LFQ), which eliminates the codebook and embedding lookup entirely.\n\nThis paper presents a simple LFQ quantizer of using independent binary latents. Other implementations of LFQ exist. However, the team shows that MAGVIT-v2 with LFQ significantly improves on the ImageNet benchmark. The differences between LFQ and 2-level FSQ includes entropy regularizations as well as maintained commitment loss."}, {"name": "vector-quantize-pytorch", "tags": ["data", "math", "ml"], "summary": "Vector Quantization - Pytorch", "text": "Developing a more advanced method of LFQ quantization without codebook-lookup could revolutionize generative modeling.\n\nYou can use it simply as follows. Will be dogfooded at MagViT2 pytorch port\n\nYou can also pass in video features as `(batch, feat, time, height, width)` or sequences as `(batch, seq, feat)`\n\nOr support multiple codebooks\n\nAn improvised Residual LFQ, to see if it can lead to an improvement for audio compression.\n\nLatent Quantization\n\nDisentanglement is essential for representation learning as it promotes interpretability, generalization, improved learning, and robustness. It aligns with the goal of capturing meaningful and independent features of the data, facilitating more effective use of learned representations across various applications. For better disentanglement, the challenge is to disentangle underlying variations in a dataset without explicit ground truth information. This work introduces a key inductive bias aimed at encoding and decoding within an organized latent space. The strategy incorporated encompasses discretizing the latent space by assigning discrete code vectors through the utilization of an individual learnable scalar codebook for each dimension. This methodology enables their models to surpass robust prior methods effectively.\n\nBe aware they had to use a very high weight decay for the results in this paper.\n\nYou can also pass in video features as `(batch, feat, time, height, width)` or sequences as `(batch, seq, feat)`\n\nOr support multiple codebooks\n\nCitations"}, {"name": "vector-quantize-pytorch", "tags": ["data", "math", "ml"], "summary": "Vector Quantization - Pytorch", "text": "This library is used to implement vector quantization in PyTorch, enabling developers to compress and generate high-quality image and audio data through techniques like VQ-VAE-2 and RQ-VAE. With this library, developers can utilize vector quantization methods to achieve efficient compression of complex data types, such as images and music."}, {"name": "vector", "tags": ["math", "web"], "summary": "Vector classes and utilities", "text": "Vector: arrays of 2D, 3D, and Lorentz vectors\n\n[![PyPI platforms][pypi-platforms]][pypi-link]\n[![PyPI version][pypi-version]][pypi-link]\n[![Conda latest release][conda-version]][conda-link]\n\nInstallation\n\nYou can install Vector with [pip](https://pypi.org/project/vector/) and [conda](https://anaconda.org/conda-forge/vector).\n\nIntroduction\n\nVector is a Python library for 2D and 3D spatial vectors, as well as 4D space-time vectors. It is especially intended for performing geometric calculations on _arrays of vectors_, rather than one vector at a time in a Python for loop.\n\nVector is part of the [Scikit-HEP project](https://scikit-hep.org/), High Energy Physics (HEP) tools in Python.\n\nCoordinate systems\n\nVectors may be expressed in any of these coordinate systems:\n\n- the azimuthal plane may be Cartesian `x` `y` or polar `rho` ($\\rho$) `phi` ($\\phi$)\n- the longitudinal axis may be Cartesian `z`, polar `theta` ($\\theta$), or pseudorapidity `eta` ($\\eta$)\n- the temporal component for space-time vectors may be Cartesian `t` or proper time `tau` ($\\tau$)\n\nin any combination. (That is, 4D vectors have 2\u00d73\u00d72 = 12 distinct coordinate systems.)\n\nBackends\n\nVectors may be included in any of these data types:\n\nEach of these \"backends\" provides the same suite of properties and methods, through a common \"compute\" library.\n\nIntegrations\n\nOptionally, the vector package provides integration with other libraries. Currently, this includes:\n\n- [PyTree integrations](https://vector.readthedocs.io/en/latest/src/pytree.html) using the [optree](https://github.com/metaopt/optree) package.\n\nGeometric versus momentum\n\nFinally, vectors come in two flavors:\n\n- geometric: only one name for each property or method\n- momentum: same property or method can be accessed with several synonyms, such as `pt` ($p_T$, transverse momentum) for the azimuthal magnitude `rho` ($\\rho$) and `energy` and `mass` for the Cartesian time `t` and proper time `tau` ($\\tau$).\n\nFamiliar conventions\n\nNames and coordinate conventions were chosen to align with [ROOT](https://root.cern/)'s [TLorentzVector](https://root.cern.ch/doc/master/classTLorentzVector.html) and [Math::LorentzVector](https://root.cern.ch/doc/master/classROOT_1_1Math_1_1LorentzVector.html), as well as [scikit-hep/math](https://github.com/scikit-hep/scikit-hep/tree/master/skhep/math), [uproot-methods TLorentzVector](https://github.com/scikit-hep/uproot3-methods/blob/master/uproot3_methods/classes/TLorentzVector.py), [henryiii/hepvector](https://github.com/henryiii/hepvector), and [coffea.nanoevents.methods.vector](https://coffea-hep.readthedocs.io/en/latest/modules/coffea.nanoevents.methods.vector.html).\n\nGetting help\n\nContributing to Vector\n\nIf you want to contribute to Vector, [pull requests](https://github.com/scikit-hep/vector/pulls) are welcome!\n\nPlease install the latest version of the `main` branch from source or a fork:\n\nRefer to [CONTRIBUTING.md](https://github.com/scikit-hep/vector/blob/main/.github/CONTRIBUTING.md) for more.\n\nCiting Vector\n\nDocumentation\n\nTutorials\n\nVector constructors\n\nVector functions\n\nIntegrations\n\n- [PyTree integration API](https://vector.readthedocs.io/en/latest/src/pytree_api.html)\n\nMore ways to learn\n\n- [Papers and talks](https://vector.readthedocs.io/en/latest/src/talks.html)\n\nContributors\n\nThanks goes to these wonderful people ([emoji key](https://allcontributors.org/docs/en/emoji-key)):\n\n  \n  \n\nThis project follows the\n[all-contributors](https://github.com/all-contributors/all-contributors)\nspecification. Contributions of any kind welcome! See\n[CONTRIBUTING.md](./.github/CONTRIBUTING.md) for information on setting up a\ndevelopment environment.\n\nAcknowledgements\n\nThis library was primarily developed by Saransh Chopra, Henry Schreiner, Jim Pivarski, Eduardo Rodrigues, and Jonas Eschle.\n\nSupport for this work was provided by the National Science Foundation cooperative agreement [OAC-1836650](https://www.nsf.gov/awardsearch/showAward?AWD_ID=1836650) and [PHY-2323298](https://www.nsf.gov/awardsearch/showAward?AWD_ID=2323298) (IRIS-HEP) and [OAC-1450377](https://www.nsf.gov/awardsearch/showAward?AWD_ID=1450377) (DIANA/HEP). Any opinions, findings, conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the National Science Foundation."}, {"name": "vector", "tags": ["math", "web"], "summary": "Vector classes and utilities", "text": "This library is used to create and manipulate arrays of 2D, 3D, and Lorentz vectors for geometric calculations. Developers can use it to perform complex spatial computations on collections of vectors with ease, making it suitable for applications in High Energy Physics (HEP) and other fields."}, {"name": "vllm", "tags": ["dev", "math", "ml", "web"], "summary": "A high-throughput and memory-efficient inference and serving engine for LLMs", "text": "About\n\nvLLM is a fast and easy-to-use library for LLM inference and serving.\n\nOriginally developed in the [Sky Computing Lab](https://sky.cs.berkeley.edu) at UC Berkeley, vLLM has evolved into a community-driven project with contributions from both academia and industry.\n\nvLLM is fast with:\n\n- State-of-the-art serving throughput\n- Efficient management of attention key and value memory with [**PagedAttention**](https://blog.vllm.ai/2023/06/20/vllm.html)\n- Continuous batching of incoming requests\n- Fast model execution with CUDA/HIP graph\n- Quantizations: [GPTQ](https://arxiv.org/abs/2210.17323), [AWQ](https://arxiv.org/abs/2306.00978), [AutoRound](https://arxiv.org/abs/2309.05516), INT4, INT8, and FP8\n- Optimized CUDA kernels, including integration with FlashAttention and FlashInfer\n- Speculative decoding\n- Chunked prefill\n\nvLLM is flexible and easy to use with:\n\n- Seamless integration with popular Hugging Face models\n- High-throughput serving with various decoding algorithms, including *parallel sampling*, *beam search*, and more\n- Tensor, pipeline, data and expert parallelism support for distributed inference\n- Streaming outputs\n- OpenAI-compatible API server\n- Support for NVIDIA GPUs, AMD CPUs and GPUs, Intel CPUs and GPUs, PowerPC CPUs, Arm CPUs, and TPU. Additionally, support for diverse hardware plugins such as Intel Gaudi, IBM Spyre and Huawei Ascend.\n- Prefix caching support\n- Multi-LoRA support\n\nvLLM seamlessly supports most popular open-source models on HuggingFace, including:\n\n- Transformer-like LLMs (e.g., Llama)\n- Mixture-of-Expert LLMs (e.g., Mixtral, Deepseek-V2 and V3)\n- Embedding Models (e.g., E5-Mistral)\n- Multi-modal LLMs (e.g., LLaVA)\n\nFind the full list of supported models [here](https://docs.vllm.ai/en/latest/models/supported_models.html).\n\nGetting Started\n\nInstall vLLM with `pip` or [from source](https://docs.vllm.ai/en/latest/getting_started/installation/gpu/index.html#build-wheel-from-source):\n\nVisit our [documentation](https://docs.vllm.ai/en/latest/) to learn more.\n\nContributing\n\nWe welcome and value any contributions and collaborations.\nPlease check out [Contributing to vLLM](https://docs.vllm.ai/en/latest/contributing/index.html) for how to get involved.\n\nSponsors\n\nvLLM is a community project. Our compute resources for development and testing are supported by the following organizations. Thank you for your support!\n\nCash Donations:\n\n- a16z\n- Dropbox\n- Sequoia Capital\n- Skywork AI\n- ZhenFund\n\nCompute Resources:\n\n- Alibaba Cloud\n- AMD\n- Anyscale\n- Arm\n- AWS\n- Crusoe Cloud\n- Databricks\n- DeepInfra\n- Google Cloud\n- IBM\n- Intel\n- Lambda Lab\n- Nebius\n- Novita AI\n- NVIDIA\n- Red Hat\n- Replicate\n- Roblox\n- RunPod\n- Trainy\n- UC Berkeley\n- UC San Diego\n- Volcengine\n\nSlack Sponsor: Anyscale\n\nWe also have an official fundraising venue through [OpenCollective](https://opencollective.com/vllm). We plan to use the fund to support the development, maintenance, and adoption of vLLM.\n\nCitation\n\nIf you use vLLM for your research, please cite our [paper](https://arxiv.org/abs/2309.06180):\n\nContact Us\n\n- For collaborations and partnerships, please contact us at [vllm-questions@lists.berkeley.edu](mailto:vllm-questions@lists.berkeley.edu)\n\nMedia Kit"}, {"name": "vllm", "tags": ["dev", "math", "ml", "web"], "summary": "A high-throughput and memory-efficient inference and serving engine for LLMs", "text": "This library is used to accelerate the inference and serving of Large Language Models (LLMs) by providing high-throughput performance and memory efficiency. With vLLM, developers can optimize their LLM-based applications for production environments with features like state-of-the-art throughput, efficient attention management, and optimized CUDA kernels."}, {"name": "vtk", "tags": ["math", "visualization"], "summary": "VTK is an open-source toolkit for 3D computer graphics, image processing, and visualization", "text": "![VTK - The Visualization Toolkit][vtk-banner]\n\nIntroduction\n============\n\nVTK is an open-source software system for image processing, 3D\ngraphics, volume rendering and visualization. VTK includes many\nadvanced algorithms (e.g., surface reconstruction, implicit modeling,\ndecimation) and rendering techniques (e.g., hardware-accelerated\nvolume rendering, LOD control).\n\nVTK is used by academicians for teaching and research; by government\nresearch institutions such as Los Alamos National Lab in the US or\nCINECA in Italy; and by many commercial firms who use VTK to build or\nextend products.\n\nThe origin of VTK is with the textbook \"The Visualization Toolkit, an\nObject-Oriented Approach to 3D Graphics\" originally published by\nPrentice Hall and now published by Kitware, Inc. (Third Edition ISBN\n1-930934-07-6). VTK has grown (since its initial release in 1994) to a\nworld-wide user base in the commercial, academic, and research\ncommunities.\n\nLearning Resources\n==================\n\n* General information is available at the [VTK Homepage][vtk-homepage].\n\n* Community discussion takes place on the [VTK Discourse][vtk-discourse] forum.\n\n* Commercial [support and training][kitware-support]\n  are available from [Kitware][].\n\n* Doxygen-generated nightly reference documentation is\n  available [online][vtk-doxygen].\n\n* There is now a large collection of [VTK Examples][vtk-examples] that\n  showcase VTK features and provide a useful learning resource.\n\nReporting Bugs\n==============\n\nIf you have found a bug:\n\n1. If you have a patch, please read the [CONTRIBUTING.md][vtk-contributing] document.\n\n2. Otherwise, please join the [VTK Discourse][vtk-discourse] forum and ask\n   about the expected and observed behaviors to determine if it is\n   really a bug.\n\n3. Finally, if the issue is not resolved by the above steps, open\n   an entry in the [VTK Issue Tracker][vtk-issues].\n\nRequirements\n============\n\nIn general VTK tries to be as portable as possible; the specific configurations below are known to work and tested.\n\nVTK supports the following compilers:\n\n1. GCC 8.0 or newer\n2. Clang 5.0 or newer\n3. Apple Clang 10.0 or newer\n4. Microsoft Visual Studio 2017 or newer\n5. Intel 19.0 or newer\n\nVTK supports the following operating systems:\n\n1. Windows Vista or newer\n2. Mac OS X 10.7 or newer\n3. Linux (ex: Ubuntu 12.04 or newer, Debian 4 or newer)\n\nBuilding\n========\n\nSee [build.md][vtk-build] (in Documentation/dev/) for build instructions.\n\nContributing\n============\n\nSee [CONTRIBUTING.md][vtk-contributing] for instructions to contribute.\n\nLicense\n=======\n\nVTK is distributed under the OSI-approved BSD 3-clause License.\nSee [Copyright.txt][vtk-copyright] for details."}, {"name": "vtk", "tags": ["math", "visualization"], "summary": "VTK is an open-source toolkit for 3D computer graphics, image processing, and visualization", "text": "This library is used to build and extend 3D graphics and visualization applications for image processing, volume rendering, and other advanced tasks. With VTK, developers can create complex visualizations, simulations, and interactive 3D models with hardware-accelerated performance."}, {"name": "wandb", "tags": ["math", "ml", "web"], "summary": "A CLI and library for interacting with the Weights & Biases API.", "text": "Documentation\n\nSee the [W&B Developer Guide](https://docs.wandb.ai/?utm_source=github&utm_medium=code&utm_campaign=wandb&utm_content=documentation) and [API Reference Guide](https://docs.wandb.ai/training/api-reference#api-overview?utm_source=github&utm_medium=code&utm_campaign=wandb&utm_content=documentation) for a full technical description of the W&B platform.\n\n&nbsp;\n\nQuickstart\n\nInstall W&B to track, visualize, and manage machine learning experiments of any size.\n\nInstall the wandb library\n\nSign up and create an API key\n\nSign up for a [W&B account](https://wandb.ai/login?utm_source=github&utm_medium=code&utm_campaign=wandb&utm_content=quickstart). Optionally, use the `wandb login` CLI to configure an API key on your machine. You can skip this step -- W&B will prompt you for an API key the first time you use it.\n\nCreate a machine learning training experiment\n\nIn your Python script or notebook, initialize a W&B run with `wandb.init()`.\nSpecify hyperparameters and log metrics and other information to W&B.\n\nVisit [wandb.ai/home](https://wandb.ai/home) to view recorded metrics such as accuracy and loss and how they changed during each training step. Each run object appears in the Runs column with generated names.\n\n&nbsp;\n\nIntegrations\n\nW&B [integrates](https://docs.wandb.ai/models/integrations) with popular ML frameworks and libraries making it fast and easy to set up experiment tracking and data versioning inside existing projects.\n\nFor developers adding W&B to a new framework, follow the [W&B Developer Guide](https://docs.wandb.ai/models/integrations/add-wandb-to-any-library).\n\n&nbsp;\n\nW&B Hosting Options\n\nWeights & Biases is available in the cloud or installed on your private infrastructure. Set up a W&B Server in a production environment in one of three ways:\n\n1. [Multi-tenant Cloud](https://docs.wandb.ai/platform/hosting/hosting-options/multi_tenant_cloud?utm_source=github&utm_medium=code&utm_campaign=wandb&utm_content=hosting): Fully managed platform deployed in W&B\u2019s Google Cloud Platform (GCP) account in GCP\u2019s North America regions.\n2. [Dedicated Cloud](https://docs.wandb.ai/platform/hosting/hosting-options/dedicated_cloud?utm_source=github&utm_medium=code&utm_campaign=wandb&utm_content=hosting): Single-tenant, fully managed platform deployed in W&B\u2019s AWS, GCP, or Azure cloud accounts. Each Dedicated Cloud instance has its own isolated network, compute and storage from other W&B Dedicated Cloud instances.\n3. [Self-Managed](https://docs.wandb.ai/platform/hosting/hosting-options/self-managed?utm_source=github&utm_medium=code&utm_campaign=wandb&utm_content=hosting): Deploy W&B Server on your AWS, GCP, or Azure cloud account or within your on-premises infrastructure.\n\nSee the [Hosting documentation](https://docs.wandb.ai/guides/hosting?utm_source=github&utm_medium=code&utm_campaign=wandb&utm_content=hosting) in the W&B Developer Guide for more information.\n\n&nbsp;\n\nPython Version Support\n\nWe are committed to supporting our minimum required Python version for _at least_ six months after its official end-of-life (EOL) date, as defined by the Python Software Foundation. You can find a list of Python EOL dates [here](https://devguide.python.org/versions/).\n\nWhen we discontinue support for a Python version, we will increment the library\u2019s minor version number to reflect this change.\n\n&nbsp;\n\nContribution guidelines\n\nWeights & Biases \ufe0f open source, and we welcome contributions from the community! See the [Contribution guide](https://github.com/wandb/wandb/blob/main/CONTRIBUTING.md) for more information on the development workflow and the internals of the wandb library. For wandb bugs and feature requests, visit [GitHub Issues](https://github.com/wandb/wandb/issues) or contact support@wandb.com.\n\n&nbsp;\n\nW&B Community\n\nBe a part of the growing W&B Community and interact with the W&B team in our [Discord](https://wandb.me/discord). Stay connected with the latest ML updates and tutorials with [W&B Fully Connected](https://wandb.ai/fully-connected).\n\n&nbsp;\n\nLicense\n\n[MIT License](https://github.com/wandb/wandb/blob/main/LICENSE)"}, {"name": "wandb", "tags": ["math", "ml", "web"], "summary": "A CLI and library for interacting with the Weights & Biases API.", "text": "This library is used to track, visualize, and manage machine learning experiments of any size through integration with the Weights & Biases API. It enables developers to easily sign up, create API keys, and configure their accounts for seamless interaction with the W&B platform."}, {"name": "weasel", "tags": ["math"], "summary": "Weasel: A small and easy workflow system", "text": "Weasel: A small and easy workflow system\n\nWeasel lets you manage and share **end-to-end workflows** for\ndifferent **use cases and domains**, and orchestrate training, packaging and\nserving your custom pipelines. You can start off by cloning a pre-defined\nproject template, adjust it to fit your needs, load in your data, train a\npipeline, export it as a Python package, upload your outputs to a remote storage\nand share your results with your team. Weasel can be used via the\n[`weasel`](https://github.com/explosion/weasel/blob/main/docs/cli.md) command and we provide templates in our\n[`projects`](https://github.com/explosion/projects) repo.\n\nExample: Get started with a project template\n\nThe easiest way to get started is to clone a project template and run it \u2013\u00a0for\nexample, this [end-to-end template](https://github.com/explosion/projects/tree/v3/pipelines/tagger_parser_ud)\nthat lets you train a spaCy **part-of-speech\ntagger** and **dependency parser** on a Universal Dependencies treebank.\n\n> **Note**\n>\n> Our [`projects`](https://github.com/explosion/projects) repo includes various\n> project templates for different NLP tasks, models, workflows and integrations\n> that you can clone and run. The easiest way to get started is to pick a\n> template, clone it and start modifying it!\n\nDocumentation\n\nGet started with the documentation:\n\nMigrating from spaCy Projects\n\nWeasel is a standalone replacement for spaCy Projects.\nThere are a few backward incompatibilities that you should be aware of:\n\n- The `SPACY_CONFIG_OVERRIDES` environment variable is no longer checked.\n  You can set configuration overrides using `WEASEL_CONFIG_OVERRIDES`.\n- Support for the `spacy_version` configuration key has been dropped.\n- Support for the `check_requirements` configuration key has been dropped.\n- Support for `SPACY_PROJECT_USE_GIT_VERSION` environment variable has been dropped.\n- Error codes are now Weasel-specific, and do not follow spaCy error codes.\n\nWeasel checks for the first three incompatibilities and will issue a\nwarning if you're using it with spaCy-specific configuration options."}, {"name": "weasel", "tags": ["math"], "summary": "Weasel: A small and easy workflow system", "text": "This library is used to manage and share end-to-end workflows for various use cases and domains, allowing developers to orchestrate training, packaging, and serving custom pipelines. With Weasel, developers can easily create, train, and deploy their own pipelines using a set of pre-defined project templates and command-line interface."}, {"name": "webrtcvad-wheels", "tags": ["math", "ml", "ui", "web"], "summary": "Python interface to the Google WebRTC Voice Activity Detector (VAD) [released with binary wheels!]", "text": ".. image:: https://img.shields.io/pypi/v/webrtcvad-wheels.svg\n.. image:: https://img.shields.io/pypi/pyversions/webrtcvad-wheels.svg\n.. image:: https://img.shields.io/pypi/wheel/webrtcvad-wheels.svg\n.. image:: https://img.shields.io/pypi/dm/webrtcvad-wheels.svg?logo=python\n\npy-webrtcvad-wheels\n===================\n\nThis is a python interface to the WebRTC Voice Activity Detector (VAD).\nIt is forked from\n`wiseman/py-webrtcvad `_ to\nprovide updated releases with **binary wheels for Windows, macOS, and\nLinux**. Also includes **additional fixes and improvements**.\n\nA `VAD `_\nclassifies a piece of audio data as being voiced or unvoiced. It can\nbe useful for telephony and speech recognition.\n\nThe VAD that Google developed for the `WebRTC `_\nproject is reportedly one of the best available, being fast, modern\nand free.\n\nHow to use it\n-------------\n\n0. Install the webrtcvad module::\n\n1. Create a ``Vad`` object::\n\n2. Optionally, set its aggressiveness mode, which is an integer\n   between 0 and 3 (inclusive). 0 is the least aggressive about filtering out\n   non-speech, 3 is the most aggressive. (You can also set the mode\n   when you create the VAD, e.g. ``vad = webrtcvad.Vad(3)``; the default is ``0``)::\n\n3. Give it a short segment (\"frame\") of audio. The WebRTC VAD only\n   accepts 16-bit mono PCM audio, sampled at 8000, 16000, 32000 or 48000 Hz.\n   A frame must be either 10, 20, or 30 ms in duration::\n\nSee `example.py\n`_ for\na more detailed example that will process a .wav file, find the voiced\nsegments, and write each one as a separate .wav.\n\nHow to run unit tests\n---------------------\n\nTo run unit tests::\n\nHistory\n-------\n\n2.0.14\n\n* Add RISC-V support (but no wheels yet). Thanks, `hack3ric `_!\n* Add loongarch64 support (but no wheels yet). Thanks, `zhangwenlong8911 `_!\n\n2.0.13\n\n* Add tests for memory leaks.\n* Fix memory leak in constructing `Vad` objects. Thanks, `manipopopo `_!\n\n2.0.12\n\n* Add Python 3.12 & 3.13 builds.\n* Fix `pkg_resources` usage for Python 3.12+.\n\n2.0.11.post1\n\n* Force build of new wheels.\n\n2.0.11\n\n* Fix out-of-bounds memory read in WebRtcVad_FindMinimum.\n* Add Python 3.10 & 3.11 builds.\n* Add PPC support & builds.\n* Implement CI/CD with GitHub Actions instead.\n\n2.0.10.post2\n\n* Revert updating to the latest webrtcvad upstream version, as it breaks build.\n* Tweak CI/CD configuration.\n* Add Python 3.9 build.\n\n2.0.10.post1\n\n* Merge various changes from upstream.\n* Implement CI/CD with Travis CI.\n\nFORK\n\n2.0.10\n\n* Fixed memory leak. Thank you, `bond005 `_!\n\n2.0.9\n\n* Improved example code. Added WebRTC license.\n\n2.0.8\n\n* Fixed Windows compilation errors. Thank you, `xiongyihui `_!"}, {"name": "webrtcvad-wheels", "tags": ["math", "ml", "ui", "web"], "summary": "Python interface to the Google WebRTC Voice Activity Detector (VAD) [released with binary wheels!]", "text": "This library is used to detect voice activity in audio data using the WebRTC Voice Activity Detector (VAD) algorithm, allowing developers to classify audio segments as voiced or unvoiced. With this library, developers can easily integrate voice detection capabilities into their applications, enabling features like speech recognition and telephony functionality."}, {"name": "x-transformers", "tags": ["math", "ml"], "summary": "X-Transformers", "text": "x-transformers\n\nA concise but fully-featured transformer, complete with a set of promising e**x**perimental features from various papers.\n\nInstall\n\nUsage\n\nFull encoder / decoder\n\nDecoder-only (GPT-like)\n\nGPT3 would be approximately the following (but you wouldn't be able to run it anyways)\n\nEncoder-only (BERT-like)\n\nState of the art image classification (SimpleViT)\n\nImage -> caption\n\nPaLI, state of the art language-vision model\n\nDropouts\n\nFeatures\n\nFlash Attention\n\nWhat originally started off as a short paper from Markus Rabe culminated as a practical fused attention CUDA kernel, named Flash Attention by Tri Dao.\n\nThe technique processes the attention matrix in tiles, only keeping track of the running softmax and exponentiated weighted sums. By recomputing on the backwards pass in a tiled fashion, one is able to keep the memory linear with respect to sequence length. This allows a lot of recent models  to be able to reach for longer context lengths without worrying about the memory bottleneck.\n\nOther engineering decisions made by Tri Dao led to its enormous success, namely minimizing HBM accesses so that both the forwards and backwards outperform naive attention. In other words, flash attention is not only more memory efficient, but faster as well, making it a necessity for training transformers.\n\nMetaAI has recently added the ability to use Tri Dao's CUDA kernel through the scaled_dot_product_attention function in Pytorch 2.0. (They also have a `mem_efficient` attention, which is identical to flash attention design, just that the tiles are traversed differently)\n\nLlama was trained using Flash Attention. The only reason to avoid it is if you require operating on the attention matrix (dynamic positional bias, talking heads, residual attention).\n\nYou can use it in this repository by setting `attn_flash` to `True` and enjoy the immediate memory savings and increase in speed.\n\nex.\n\nAugmenting Self-attention with Persistent Memory\n\nProposes adding learned memory key / values prior to attention. They were able to remove feedforwards altogether and attain similar performance to the original transformers. I have found that keeping the feedforwards and adding the memory key / values leads to even better performance.\n\nMemory Transformers\n\nProposes adding learned tokens, akin to CLS tokens, named memory tokens, that is passed through the attention layers alongside the input tokens. This setting is compatible with both encoder and decoder training.\n\nUpdate: MetaAI researchers have found that adding memory tokens (they call them register tokens), alleviates outliers (which is suspected now to be a pathology of attention networks unable to attend to nothing).\n\nUpdate 2: a hybrid architecture out of Nvidia named Hymba used memory tokens successfully in the autoregressive case, termed meta tokens in their paper.\n\nUpdate 3: further corroborated by a paper trying to extend memory in attention networks, termed persistent memory\n\nTransformers Without Tears\n\nThey experiment with alternatives to Layer normalization and found one that is both effective and simpler. Researchers have shared with me this leads to faster convergence."}, {"name": "x-transformers", "tags": ["math", "ml"], "summary": "X-Transformers", "text": "You can also use the l2 normalized embeddings proposed as part of `fixnorm`. I have found it leads to improved convergence, when paired with small initialization (proposed by BlinkDL). The small initialization will be taken care of as long as `l2norm_embed` is set to `True`\n\nAlong the same lines of l2 normalized embeddings, Huggingface's 175B parameter BLOOM also places a layernorm right after the embeddings and just before the tokens enter the attention layers. This was corroborated by Yandex's 100B parameter YaLM to stabilize training.\n\nIt is recommended you either have either `l2norm_embed` or `post_emb_norm` set to `True` but not both, as they probably serve the same purpose.\n\nRoot Mean Square Layer Normalization\n\nThe authors propose to replace layer normalization with a simpler alternative, without mean centering and the learned bias. An investigative paper found this to be the best performing normalization variant. It was also used in Deepmind's latest large language models, Retro and Gopher.\n\n*July 2023* A linear attention paper has experiments to show that removing the learned multiplicative gamma led to no performance degradation. This simplifies the RMS normalization to a satisfying `l2norm(x) * sqrt(dim)`.\n\nGLU Variants Improve Transformer\n\nNoam Shazeer paper that explores gating in the feedforward, finding that simple gating with GELU leads to significant improvements. This variant also showed up in the latest mT5 architecture. You should always turn this on (I may eventually turn it on by default).\n\nThe PaLM language model also chose to use the Swish GLU variant. You can turn this on by setting two flags\n\npython\nimport torch\nfrom x_transformers import TransformerWrapper, Decoder, Encoder\n\nmodel = TransformerWrapper(\n)\npython\nimport torch\nfrom x_transformers import TransformerWrapper, Decoder, Encoder\n\nmodel = TransformerWrapper(\n)\npython\nimport torch\nfrom x_transformers import TransformerWrapper, Decoder\n\nmodel = TransformerWrapper(\n)\npython\nmodel = TransformerWrapper(\n)\npython\nimport torch\nfrom x_transformers import TransformerWrapper, Decoder\n\nmodel = TransformerWrapper(\n)\npython\nimport torch\nfrom x_transformers import TransformerWrapper, Decoder\n\nmodel = TransformerWrapper(\n)\npython\nimport torch\nfrom x_transformers import TransformerWrapper, Decoder\n\nmodel = TransformerWrapper(\n)\npython\nimport torch\nfrom x_transformers import TransformerWrapper, Encoder\n\nmodel = TransformerWrapper(\n)\npython\nimport torch\nfrom x_transformers import TransformerWrapper, Encoder\n\nmodel = TransformerWrapper(\n)\npython\nimport torch\nfrom x_transformers import TransformerWrapper, Encoder\n\nmodel = TransformerWrapper(\n)\npython\nimport torch\nfrom x_transformers import TransformerWrapper, Encoder\n\nmodel = TransformerWrapper(\n)\npython\nimport torch\nfrom x_transformers import TransformerWrapper, Decoder\n\nmodel = TransformerWrapper(\n)\npython\nimport torch\nfrom x_transformers import TransformerWrapper, Encoder\n\nmodel = TransformerWrapper(\n)\npython\nimport torch\nfrom x_transformers import TransformerWrapper, Decoder\n\nmodel = TransformerWrapper(\n)\npython\nimport torch\nfrom x_transformers import TransformerWrapper, Encoder\n\nmodel = TransformerWrapper(\n)\npython\nimport torch\nfrom x_transformers import XTransformer\n\nmodel = XTransformer(\n)\npython\nimport torch\nfrom x_transformers import TransformerWrapper, Decoder\n\nmodel_xl = TransformerWrapper(\n)\n\nseg1 = torch.randint(0, 20000, (1, 512))\nseg2 = torch.randint(0, 20000, (1, 512))\nseg3 = torch.randint(0, 20000, (1, 512))\n\nlogits1, mems1  = model_xl(seg1, return_mems = True)\nlogits2, mems2  = model_xl(seg2, mems = mems1, return_mems = True)\nlogits3, mems3  = model_xl(seg3, mems = mems2, return_mems = True)\npython"}, {"name": "x-transformers", "tags": ["math", "ml"], "summary": "X-Transformers", "text": "pass in the above model_xl\n\nxl_wrapper = XLAutoregressiveWrapper(model_xl)\n\nseg = torch.randint(0, 20000, (1, 4096)).cuda()  # sequence exceeding max length, automatically segmented and memory managed\n\nloss = xl_wrapper(seg)\nloss.backward()\n\nthen, after much training\n\nprime = seg[:, :1024]   # if prime exceeds max length, memory will be caught up before generating\n\ngenerated = xl_wrapper.generate(prime, 4096)  # (1, 4096)\npython\nimport torch\nfrom x_transformers import TransformerWrapper, Decoder\n\nmodel_xl = TransformerWrapper(\n)\n\nseg1 = torch.randint(0, 20000, (1, 512))\nseg2 = torch.randint(0, 20000, (1, 512))\nseg3 = torch.randint(0, 20000, (1, 512))\n\nlogits1, mems1  = model_xl(seg1, return_mems = True)\nlogits2, mems2  = model_xl(seg2, mems = mems1, return_mems = True) # mems1 of layer N are automatically routed to the layer N-1\npython\nimport torch\nfrom x_transformers import TransformerWrapper, Decoder\n\nmodel = TransformerWrapper(\n)\npython\nimport torch\nfrom x_transformers import TransformerWrapper, Decoder\n\nmodel = TransformerWrapper(\n)\npython\nimport torch\nfrom x_transformers import TransformerWrapper, Decoder\n\nmodel = TransformerWrapper(\n)\npython\nimport torch\nfrom x_transformers import TransformerWrapper, Decoder\n\nmodel = TransformerWrapper(\n)\npython\nimport torch\nfrom x_transformers import TransformerWrapper, Decoder\n\nmodel = TransformerWrapper(\n)\npython\nimport torch\nfrom x_transformers import TransformerWrapper, Decoder\n\nmodel = TransformerWrapper(\n)\npython\nimport torch\nfrom x_transformers import TransformerWrapper, Decoder\n\nmodel = TransformerWrapper(\n)\npython\nimport torch\nfrom x_transformers import TransformerWrapper, Decoder\n\nmodel = TransformerWrapper(\n)\n\nx = torch.randint(0, 20000, (1, 1024))\nmodel(x)\npython\nimport torch\nfrom x_transformers import TransformerWrapper, Decoder\n\nmodel = TransformerWrapper(\n)\n\nx = torch.randint(0, 20000, (1, 1024))\nmodel(x)\npython\nimport torch\nfrom x_transformers import TransformerWrapper, Decoder\n\nmodel = TransformerWrapper(\n)\n\nx = torch.randint(0, 20000, (1, 1024))\nmodel(x)\npython\nimport torch\nfrom x_transformers import TransformerWrapper, Decoder\n\nmodel = TransformerWrapper(\n)\n\nx = torch.randint(0, 20000, (1, 1024))\nmodel(x)\npython\nimport torch\nfrom x_transformers import TransformerWrapper, Decoder\n\nmodel = TransformerWrapper(\n)\n\nx = torch.randint(0, 20000, (1, 1024))\nmodel(x)\npython\nimport torch\nfrom x_transformers import TransformerWrapper, Decoder\n\nmodel = TransformerWrapper(\n)\n\nx = torch.randint(0, 20000, (1, 1024))\nmodel(x)\npython\nimport torch\nfrom x_transformers import TransformerWrapper, Decoder\n\nmodel = TransformerWrapper(\n)\n\nx = torch.randint(0, 20000, (1, 1024))\nmodel(x)\npython\nimport torch\nfrom x_transformers import TransformerWrapper, Decoder\n\nmodel = TransformerWrapper(\n)\n\nx = torch.randint(0, 256, (1, 1024))\nmodel(x)\npython\nimport torch\nfrom x_transformers import TransformerWrapper, Decoder\n\nmodel = TransformerWrapper(\n)\n\nx = torch.randint(0, 20000, (1, 1024))\nmodel(x)\npython\nimport torch\nfrom x_transformers import TransformerWrapper, Decoder, AutoregressiveWrapper\n\nmodel = TransformerWrapper(\n)\n\nmodel = AutoregressiveWrapper(\n).cuda()\n\nmock data\n\nx = torch.randint(0, 20000, (1, 1024)).cuda()\n\nderive cross entropy loss, masking all taken care of\n\nloss = model(x)\nloss.backward()\npython\nimport torch\nfrom x_transformers import Encoder, CrossAttender\n\nenc = Encoder(dim = 512, depth = 6)\nmodel = CrossAttender(dim = 512, depth = 6)\n\nnodes = torch.randn(1, 1, 512)\nnode_masks = torch.ones(1, 1).bool()\n\nneighbors = torch.randn(1, 5, 512)\nneighbor_masks = torch.ones(1, 5).bool()\n\nencoded_neighbors = enc(neighbors, mask = neighbor_masks)\nmodel(nodes, context = encoded_neighbors, mask = node_masks, context_mask = neighbor_masks) # (1, 1, 512)\n\npython\nimport torch\nfrom x_transformers import ContinuousTransformerWrapper, Decoder\n\nmodel = ContinuousTransformerWrapper(\n)\n\nx = torch.randn((1, 1024, 32))\nmask = torch.ones(1, 1024).bool()"}, {"name": "x-transformers", "tags": ["math", "ml"], "summary": "X-Transformers", "text": "model(x, mask = mask) # (1, 1024, 100)\npython\nimport torch\nfrom x_transformers import ContinuousTransformerWrapper, Decoder\nfrom x_transformers import ContinuousAutoregressiveWrapper\n\nmodel = ContinuousTransformerWrapper(\n)\n\nwrap it with the continuous autoregressive wrapper\n\nmodel = ContinuousAutoregressiveWrapper(model)\n\nmock data\n\nx = torch.randn((1, 1024, 777))\nmask = torch.ones(1, 1024).bool()\n\ntrain on a lot of data above\n\nloss = model(x, mask = mask)\nloss.backward\n\nthen generate\n\nstart_emb = torch.randn(1, 777)\ngenerated = model.generate(start_emb, 17) # (17, 777)\npython\nimport torch\n\nfrom x_transformers import (\n)\n\nmodel = XValTransformerWrapper(\n)\n\nwrap it with the xval autoregressive wrapper\n\nmodel = XValAutoregressiveWrapper(model)\n\nmock data\n\nids = torch.randint(0, 4, (1, 777))\nnums = torch.randn(1, 777)\n\ntrain on a lot of data above\n\nloss = model(ids, nums)\nloss.backward()\n\nthen generate\n\nstart_ids = torch.randint(0, 4, (1, 1))\nstart_nums = torch.randn(1, 1)\n\nids_out, num_out, is_number_mask = model.generate(start_ids, start_nums, 17)\n\n(1, 17), (1, 17), (1, 17)\n\ndiscrete, continuous, mask for discrete / continuous\nbibtex\n@misc{vaswani2017attention,\n}\nbibtex\n@article{DBLP:journals/corr/abs-1907-01470,\n}\nbibtex\n@article{1910.05895,\n}\nbibtex\n@misc{shazeer2020glu,\n}\nbibtex\n@inproceedings{Zoph2022STMoEDS,\n}\nbibtex\n@misc{bhojanapalli2020lowrank,\n}\nbibtex\n@misc{burtsev2020memory,\n}\nbibtex\n@misc{zhao2019explicit,\n}\nbibtex\n@misc{correia2019adaptively,\n}\nbibtex\n@misc{shazeer2020talkingheads,\n}\nbibtex\n@misc{press2020improving,\n}\nbibtex\n@misc{lu2019understanding,\n}\nbibtex\n@misc{ke2020rethinking,\n}\nbibtex\n@misc{dosovitskiy2020image,\n}\nbibtex\n@misc{huang2019attention,\n}\nbibtex\n@misc{raffel2020exploring,\n}\nbibtex\n@inproceedings{martins-etal-2020-sparse,\n}\nbibtex\n@misc{he2020realformer,\n}\nbibtex\n@misc{carion2020endtoend,\n}\nbibtex\n@misc{press2021ALiBi,\n}\nbibtex\n@misc{parisotto2019stabilizing,\n}\nbibtex\n@misc{narang2021transformer,\n}\nbibtex\n@misc{zhang2019root,\n}\nbibtex\n@inproceedings{Qin2023ScalingTT,\n}\nbibtex\n@misc{su2021roformer,\n}\nbibtex\n@inproceedings{Yang2025RopeTN,\n}\nbibtex\n@inproceedings{Chen2023ExtendingCW,\n}\nbibtex\n@inproceedings{Sun2022ALT,\n  title     = {A Length-Extrapolatable Transformer},\n  author    = {Yutao Sun and Li Dong and Barun Patra and Shuming Ma and Shaohan Huang and Alon Benhaim and Vishrav Chaudhary and Xia Song and Furu Wei},\n  year      = {2022}\n}\nbibtex\n@Article{AlphaFold2021,\n}\nbibtex\n@software{peng_bo_2021_5196578,\n}\nbibtex\n@misc{csord\u00e1s2021devil,\n}\nbibtex\n@misc{so2021primer,\n}\nbibtex\n@misc{ding2021erniedoc,\n}\nbibtex\n@misc{ding2021cogview,\n}\nbibtex\n@inproceedings{anonymous2022normformer,\n}\nbibtex\n@misc{henry2020querykey,\n}\nbibtex\n@misc{liu2021swin,\n}\nbibtex\n@article{Haviv2022TransformerLM,\n}\nbibtex\n@article{chowdhery2022PaLM,\n}\nbibtex\n@article{Shazeer2019FastTD,\n}\nbibtex\n@article{Ainslie2023GQATG,\n}\nbibtex\n@article{Liu2022FCMFC,\n}\nbibtex\n@inproceedings{Huang2016DeepNW,\n}\nbibtex\n@inproceedings{Hua2022TransformerQI,\n}\nbibtex\n@article{Chang2022MaskGITMG,\n}\nbibtex\n@article{Lezama2022ImprovedMI,\n}\nbibtex\n@misc{https://doi.org/10.48550/arxiv.2302.01327,\n}\nbibtex\n@inproceedings{dao2022flashattention,\n}\nbibtex\n@inproceedings{Dehghani2023ScalingVT,\n}\nbibtex\n@article{Beyer2022BetterPV,\n}\nbibtex\n@article{Kazemnejad2023TheIO,\n}\nbibtex\n@misc{bloc97-2023\n}\nbibtex\n@inproceedings{Zoph2022STMoEDS,\n}\nbibtex\n@article{Lan2019ALBERTAL,\n}\nbibtex\n@inproceedings{Li2022ContrastiveDO,\n}\nbibtex\n@inproceedings{OBrien2023ContrastiveDI,\n}\nbibtex\n@inproceedings{Darcet2023VisionTN,\n}\nbibtex\n@article{Bondarenko2023QuantizableTR,\n}\nbibtex\n@inproceedings{Golkar2023xValAC,\n}\nbibtex\n@article{Wang2022DeepNetST,\n}\nbibtex\n@article{Rafailov2023DirectPO,\n}\nbibtex\n@misc{xAI2024Grok,\n}\nbibtex\n@inproceedings{Golovneva2024ContextualPE,\n}\nbibtex\n@article{Peebles2022ScalableDM,\n}\nbibtex\n@misc{Rubin2024,\n}\nbibtex\n@article{Mesnard2024GemmaOM,\n}\nbibtex\n@article{Nguyen2024MinPS,\n}\nbibtex\n@article{Bao2022AllAW,\n}\nbibtex\n@article{Jumper2021HighlyAP,\n}\nbibtex\n@article{Yang2017BreakingTS,\n}\nbibtex\n@inproceedings{Kanai2018SigsoftmaxRO,\nbibtex\n@article{Kim2020TheLC,\n}\nbibtex\n@inproceedings{Ramapuram2024TheoryAA,\n}\nbibtex\n@inproceedings{Leviathan2024SelectiveAI,\n}\nbibtex\n@article{Bai2019DeepEM,\n}\nbibtex\n@article{Wu2021MuseMorphoseFA,\n}\nbibtex\n@inproceedings{Zhou2024ValueRL,\n}\nbibtex\n@inproceedings{anonymous2024forgetting,\n}\nbibtex\n@inproceedings{anonymous2024from,\n}\nbibtex\n@inproceedings{Duvvuri2024LASERAW,\n}\nbibtex\n@article{Zhu2024HyperConnections,\n}\nbibtex\n@inproceedings{anonymous2024hymba,\n}\nbibtex\n@article{Shao2024DeepSeekV2AS,\n}\nbibtex\n@inproceedings{Gerasimov2025YouDN,\n}\nbibtex\n@inproceedings{Hu2024TheBS,\n}\nbibtex\n@article{Charpentier2024GPTOB,\n}\nbibtex\n@inproceedings{Zhu2025TransformersWN,\n}\nbibtex\n@article{Pagnoni2024ByteLT,\n}\nbibtex\n@misc{Jordan2024,\n}\nbibtex\n@inproceedings{Assran2025VJEPA2S,\n}\nbibtex\n@misc{bloem2025universalpretrainingiteratedrandom,\n}\nbibtex\n@misc{openai_gpt_oss,\n}\nbibtex\n@article{Sahoo2024SimpleAE,\n}\nbibtex\n@misc{kimiteam2025kimik2openagentic,\n}\nbibtex\n@misc{zhao2023learningfinegrainedbimanualmanipulation,\n}\nbibtex\n@misc{jordan2024muon,\n  author    = {Keller Jordan and Yuchen Jin and Vlado Boza and Jiacheng You and Franz Cesista and Laker Newhouse and Jeremy Bernstein},\n  title     = {Muon: An optimizer for hidden layers in neural networks},\n  year      = {2024},\n  url       = {https://kellerjordan.github.io/posts/muon/}\n}\nbibtex\n@misc{wang2025muonoutperformsadamtailend,\n}\nbibtex\n@misc{yan2017hierarchicalmultiscaleattentionnetworks,\n}\nbibtex\n@misc{lv2025expressiveattentionnegativeweights,\n}\nbibtex\n@inproceedings{Fleuret2025TheFT,\n}\nbibtex\n@inproceedings{anonymous2025beliefformer,\n}\nbibtex\n@misc{chen2025strongernormalizationfreetransformers,\n}\nbibtex\n@misc{gopalakrishnan2025decouplingwhatwherepolar,\n}\n```"}, {"name": "x-transformers", "tags": ["math", "ml"], "summary": "X-Transformers", "text": "*solve intelligence... then use that to solve everything else.* - Demis Hassabis"}, {"name": "x-transformers", "tags": ["math", "ml"], "summary": "X-Transformers", "text": "This library is used to provide a concise yet feature-rich transformer implementation with experimental features from various papers, enabling developers to create state-of-the-art models like GPT3, BERT, and PaLI with ease. With this library, developers can achieve cutting-edge performance on tasks such as image classification, language-vision, and text generation."}, {"name": "xformers", "tags": ["math", "ml"], "summary": "XFormers: A collection of composable Transformer building blocks.", "text": "XFormers: A collection of composable Transformer building blocks.XFormers aims at being able to reproduce most architectures in the Transformer-family SOTA,defined as compatible and combined building blocks as opposed to monolithic models"}, {"name": "xformers", "tags": ["math", "ml"], "summary": "XFormers: A collection of composable Transformer building blocks.", "text": "This library is used to compose various building blocks for creating different architectures within the Transformer family, enabling developers to easily combine or modify these components. With xformers, developers can quickly reproduce state-of-the-art (SOTA) models in the Transformer family by selecting and combining compatible building blocks."}, {"name": "ydata-profiling", "tags": ["cli", "data", "math", "ui", "visualization", "web"], "summary": "Generate profile report for pandas DataFrame", "text": "ydata-profiling\n\n(https://github.com/ydataai/pandas-profiling/actions/workflows/tests.yml)\n(https://pypi.python.org/pypi/ydata-profiling/)\n(https://pypi.org/project/ydata-profiling/)\n(https://codecov.io/gh/ydataai/pandas-profiling)\n(https://github.com/ydataai/pandas-profiling/releases)\n(https://pypi.org/project/ydata-profiling/)\n(https://github.com/python/black)\n\nDocumentation\n  Discord\n  Stack Overflow\n  Latest changelog\n\nDo you like this project? Show us your love and give feedback!\n\n`ydata-profiling` primary goal is to provide a one-line Exploratory Data Analysis (EDA) experience in a consistent and fast solution. Like pandas `df.describe()` function, that is so handy, ydata-profiling delivers an extended analysis of a DataFrame while allowing the data analysis to be exported in different formats such as **html** and **json**.\n\nThe package outputs a simple and digested analysis of a dataset, including **time-series** and **text**.\n\n> **Looking for a scalable solution that can fully integrate with your database systems?**\n> Leverage YData Fabric Data Catalog to connect to different databases and storages (Oracle, snowflake, PostGreSQL, GCS, S3, etc.) and leverage an interactive and guided profiling experience in Fabric. Check out the [Community Version](http://ydata.ai/register?utm_source=ydata-profiling&utm_medium=documentation&utm_campaign=YData%20Fabric%20Community).\n\n\u25b6\ufe0f Quickstart\n\nInstall\n\nor\n\nStart profiling\n\nStart by loading your pandas `DataFrame` as you normally would, e.g. by using:\n\nTo generate the standard profiling report, merely run:\n\nKey features\n\n- **Type inference**: automatic detection of columns' data types (*Categorical*, *Numerical*, *Date*, etc.)\n- **Warnings**: A summary of the problems/challenges in the data that you might need to work on (*missing data*, *inaccuracies*, *skewness*, etc.)\n- **Univariate analysis**: including descriptive statistics (mean, median, mode, etc) and informative visualizations such as distribution histograms\n- **Multivariate analysis**: including correlations, a detailed analysis of missing data, duplicate rows, and visual support for variables pairwise interaction\n- **Time-Series**: including different statistical information relative to time dependent data such as auto-correlation and seasonality, along ACF and PACF plots.\n- **Text analysis**: most common categories (uppercase, lowercase, separator), scripts (Latin, Cyrillic) and blocks (ASCII, Cyrilic)\n- **File and Image analysis**: file sizes, creation dates, dimensions, indication of truncated images and existence of EXIF metadata\n- **Compare datasets**: one-line solution to enable a fast and complete report on the comparison of datasets\n- **Flexible output formats**: all analysis can be exported to an HTML report that can be easily shared with different parties, as JSON for an easy integration in automated systems and as a widget in a Jupyter Notebook.\n\nThe report contains three additional sections:\n\n- **Overview**: mostly global details about the dataset (number of records, number of variables, overall missigness and duplicates, memory footprint)\n- **Alerts**: a comprehensive and automatic list of potential data quality issues (high correlation, skewness, uniformity, zeros, missing values, constant values, between others)\n- **Reproduction**: technical details about the analysis (time, version and configuration)\n\nLatest features\n\nSpark\n\nSpark support has been released, but we are always looking for an extra pair of hands .\n[Check current work in progress!](https://github.com/ydataai/ydata-profiling/projects/3).\n\nUse cases\nYData-profiling can be used to deliver a variety of different use-case. The documentation includes guides, tips and tricks for tackling them:\n\nUse case\n----------\n[Comparing datasets](https://docs.profiling.ydata.ai/latest/features/comparing_datasets)\n[Profiling a Time-Series dataset](https://docs.profiling.ydata.ai/latest/features/time_series_datasets)\n[Profiling large datasets](https://docs.profiling.ydata.ai/latest/features/big_data)\n[Handling sensitive data](https://docs.profiling.ydata.ai/latest/features/sensitive_data)\n[Dataset metadata and data dictionaries](https://docs.profiling.ydata.ai/latest/features/metadata)\n[Customizing the report's appearance](https://docs.profiling.ydata.ai/latest/features/custom_reports)\n[Profiling Databases](https://docs.profiling.ydata.ai/latest/features/collaborative_data_profiling)\n\nUsing inside Jupyter Notebooks"}, {"name": "ydata-profiling", "tags": ["cli", "data", "math", "ui", "visualization", "web"], "summary": "Generate profile report for pandas DataFrame", "text": "There are two interfaces to consume the report inside a Jupyter notebook: through widgets and through an embedded HTML report.\n\nThe above is achieved by simply displaying the report as a set of widgets. In a Jupyter Notebook, run:\n\nThe HTML report can be directly embedded in a cell in a similar fashion:\n\nExporting the report to a file\n\nTo generate a HTML report file, save the `ProfileReport` to an object and use the `to_file()` function:\n\nAlternatively, the report's data can be obtained as a JSON file:\n\nUsing in the command line\n\nFor standard formatted CSV files (which can be read directly by pandas without additional settings), the `ydata_profiling` executable can be used in the command line. The example below generates a report named *Example Profiling Report*, using a configuration file called `default.yaml`, in the file `report.html` by processing a `data.csv` dataset.\n\nAdditional details on the CLI are available [on the documentation](https://ydata-profiling.ydata.ai/docs/master/pages/getting_started/quickstart.html#command-line-usage).\n\nExamples\n\nThe following example reports showcase the potentialities of the package across a wide range of dataset and data types:\n\n* [Census Income](https://ydata-profiling.ydata.ai/examples/master/census/census_report.html) (US Adult Census data relating income with other demographic properties)\n* [NASA Meteorites](https://ydata-profiling.ydata.ai/examples/master/meteorites/meteorites_report.html) (comprehensive set of meteorite landing - object properties and locations) (https://colab.research.google.com/github/ydataai/pandas-profiling/blob/master/examples/meteorites/meteorites_cloud.ipynb) (https://mybinder.org/v2/gh/ydataai/pandas-profiling/master?filepath=examples%2Fmeteorites%2Fmeteorites%5Fcloud.ipynb)\n* [Titanic](https://ydata-profiling.ydata.ai/examples/master/titanic/titanic_report.html) (the \"Wonderwall\" of datasets) (https://colab.research.google.com/github/ydataai/pandas-profiling/blob/master/examples/titanic/titanic_cloud.ipynb) (https://mybinder.org/v2/gh/ydataai/pandas-profiling/master?filepath=examples%2Ftitanic%2Ftitanic%5Fcloud.ipynb)\n* [NZA](https://ydata-profiling.ydata.ai/examples/master/nza/nza_report.html) (open data from the Dutch Healthcare Authority)\n* [Stata Auto](https://ydata-profiling.ydata.ai/examples/master/stata_auto/stata_auto_report.html) (1978 Automobile data)\n* [Colors](https://ydata-profiling.ydata.ai/examples/master/colors/colors_report.html) (a simple colors dataset)\n* [Vektis](https://ydata-profiling.ydata.ai/examples/master/vektis/vektis_report.html) (Vektis Dutch Healthcare data)\n* [UCI Bank Dataset](https://ydata-profiling.ydata.ai/examples/master/bank_marketing_data/uci_bank_marketing_report.html) (marketing dataset from a bank)\n* [Russian Vocabulary](https://ydata-profiling.ydata.ai/examples/master/features/russian_vocabulary.html) (100 most common Russian words, showcasing unicode text analysis)\n* [Website Inaccessibility](https://ydata-profiling.ydata.ai/examples/master/features/website_inaccessibility_report.html) (website accessibility analysis, showcasing support for URL data)\n* [Orange prices](https://ydata-profiling.ydata.ai/examples/master/features/united_report.html) and \n* [Coal prices](https://ydata-profiling.ydata.ai/examples/master/features/flatly_report.html) (simple pricing evolution datasets, showcasing the theming options)\n* [USA Air Quality](https://github.com/ydataai/pandas-profiling/tree/master/examples/usaairquality) (Time-series air quality dataset EDA example)\n* [HCC](https://github.com/ydataai/pandas-profiling/tree/master/examples/hcc) (Open dataset from healthcare, showcasing compare between two sets of data, before and after preprocessing)\n\n\ufe0f Installation\nAdditional details, including information about widget support, are available [on the documentation](https://ydata-profiling.ydata.ai/docs/master/pages/getting_started/installation.html).\n\nUsing pip\n(https://pepy.tech/project/ydata-profiling)\n(https://pepy.tech/project/ydata-profiling/month)\n(https://pypi.org/project/ydata-profiling/)\n\nYou can install using the `pip` package manager by running:\n\nExtras\n\nThe package declares \"extras\", sets of additional dependencies.\n\n* `[notebook]`: support for rendering the report in Jupyter notebook widgets.\n* `[unicode]`: support for more detailed Unicode analysis, at the expense of additional disk space.\n* `[pyspark]`: support for pyspark for big dataset analysis\n\nInstall these with e.g.\n\nUsing conda\n(https://anaconda.org/conda-forge/pandas-profiling)\n(https://anaconda.org/conda-forge/pandas-profiling)\n\nYou can install using the `conda` package manager by running:\n\nFrom source (development)\n\nDownload the source code by cloning the repository or click on [Download ZIP](https://github.com/ydataai/pandas-profiling/archive/master.zip) to download the latest stable version.\n\nInstall it by navigating to the proper directory and running:\n\nThe profiling report is written in HTML and CSS, which means a modern browser is required.\n\nYou need [Python 3](https://python3statement.github.io/) to run the package. Other dependencies can be found in the requirements files:\n\nFilename\n----------\n[requirements.txt](https://github.com/ydataai/pandas-profiling/blob/master/requirements.txt)\n[requirements-dev.txt](https://github.com/ydataai/pandas-profiling/blob/master/requirements-dev.txt)\n[requirements-test.txt](https://github.com/ydataai/pandas-profiling/blob/master/requirements-test.txt)\n[setup.py](https://github.com/ydataai/pandas-profiling/blob/master/setup.py)\n\nIntegrations\n\nTo maximize its usefulness in real world contexts, `ydata-profiling` has a set of implicit and explicit integrations with a variety of other actors in the Data Science ecosystem:"}, {"name": "ydata-profiling", "tags": ["cli", "data", "math", "ui", "visualization", "web"], "summary": "Generate profile report for pandas DataFrame", "text": "Integration type\n---\n[Other DataFrame libraries](https://docs.profiling.ydata.ai/latest/integrations/other_dataframe_libraries)\n[Great Expectations](https://ydata-profiling.ydata.ai/docs/master/pages/integrations/great_expectations.html)\n[Interactive applications](https://docs.profiling.ydata.ai/latest/integrations/interactive_applications)\n[Pipelines](https://ydata-profiling.ydata.ai/docs/master/pages/integrations/pipelines.html)\n[Cloud services](https://ydata-profiling.ydata.ai/docs/master/pages/integrations/cloud_services.html)\n[IDEs](https://ydata-profiling.ydata.ai/docs/master/pages/integrations/ides.html)\n\nSupport\nNeed help? Want to share a perspective? Report a bug? Ideas for collaborations? Reach out via the following channels:\n\n> **Need Help?**\n> Get your questions answered with a product owner by [booking a Pawsome chat](https://meetings.hubspot.com/fabiana-clemente)!\n\n>  Before reporting an issue on GitHub, check out [Common Issues](https://docs.profiling.ydata.ai/latest/support-contribution/common_issues).\n\nContributing\nLearn how to get involved in the [Contribution Guide](https://ydata-profiling.ydata.ai/docs/master/pages/support_contrib/contribution_guidelines.html).\n\nA low-threshold place to ask questions or start contributing is the [Data Centric AI Community's Discord](https://tiny.ydata.ai/dcai-ydata-profiling).\n\nA big thank you to all our amazing contributors!\n\nContributors wall made with [contrib.rocks](https://contrib.rocks)."}, {"name": "ydata-profiling", "tags": ["cli", "data", "math", "ui", "visualization", "web"], "summary": "Generate profile report for pandas DataFrame", "text": "This library is used to generate comprehensive profile reports for pandas DataFrames with a single line of code, enabling easy Exploratory Data Analysis. It provides a fast and consistent solution for extended analysis of a DataFrame, allowing data analysis to be exported in different formats."}, {"name": "yellowbrick", "tags": ["data", "math", "ml", "visualization", "web"], "summary": "A suite of visual analysis and diagnostic tools for machine learning.", "text": "Yellowbrick\n\n(https://www.scikit-yb.org/)\n\nYellowbrick is a suite of visual analysis and diagnostic tools designed to facilitate machine learning with scikit-learn. The library implements a new core API object, the `Visualizer` that is an scikit-learn estimator &mdash; an object that learns from data. Similar to transformers or models, visualizers learn from data by creating a visual representation of the model selection workflow.\n\nVisualizer allow users to steer the model selection process, building intuition around feature engineering, algorithm selection and hyperparameter tuning. For instance, they can help diagnose common problems surrounding model complexity and bias, heteroscedasticity, underfit and overtraining, or class balance issues. By applying visualizers to the model selection workflow, Yellowbrick allows you to steer predictive models toward more successful results, faster.\n\nThe full documentation can be found at [scikit-yb.org](https://scikit-yb.org/) and includes a [Quick Start Guide](https://www.scikit-yb.org/en/latest/quickstart.html) for new users.\n\nVisualizers\n\nVisualizers are estimators &mdash; objects that learn from data &mdash; whose primary objective is to create visualizations that allow insight into the model selection process. In scikit-learn terms, they can be similar to transformers when visualizing the data space or wrap a model estimator similar to how the `ModelCV` (e.g. [`RidgeCV`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RidgeCV.html), [`LassoCV`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LassoCV.html)) methods work. The primary goal of Yellowbrick is to create a sensical API similar to scikit-learn. Some of our most popular visualizers include:\n\nClassification Visualization\n\n- **Classification Report**: a visual classification report that displays a model's precision, recall, and F1 per-class scores as a heatmap\n- **Confusion Matrix**: a heatmap view of the confusion matrix of pairs of classes in multi-class classification\n- **Discrimination Threshold**: a visualization of the precision, recall, F1-score, and queue rate with respect to the discrimination threshold of a binary classifier\n- **Precision-Recall Curve**: plot the precision vs recall scores for different probability thresholds\n- **ROCAUC**: graph the receiver operator characteristic (ROC) and area under the curve (AUC)\n\nClustering Visualization\n\n- **Intercluster Distance Maps**: visualize the relative distance and size of clusters\n- **KElbow Visualizer**: visualize cluster according to the specified scoring function, looking for the \"elbow\" in the curve.\n- **Silhouette Visualizer**: select `k` by visualizing the silhouette coefficient scores of each cluster in a single model\n\nFeature Visualization\n\n- **Manifold Visualization**: high-dimensional visualization with manifold learning\n- **Parallel Coordinates**: horizontal visualization of instances\n- **PCA Projection**: projection of instances based on principal components\n- **RadViz Visualizer**: separation of instances around a circular plot\n- **Rank Features**: single or pairwise ranking of features to detect relationships\n\nModel Selection Visualization\n\n- **Cross Validation Scores**: display the cross-validated scores as a bar chart with the average score plotted as a horizontal line\n- **Feature Importances**: rank features based on their in-model performance\n- **Learning Curve**: show if a model might benefit from more data or less complexity\n- **Recursive Feature Elimination**: find the best subset of features based on importance\n- **Validation Curve**: tune a model with respect to a single hyperparameter\n\nRegression Visualization\n\n- **Alpha Selection**: show how the choice of alpha influences regularization\n- **Cook's Distance**: show the influence of instances on linear regression\n- **Prediction Error Plots**: find model breakdowns along the domain of the target\n- **Residuals Plot**: show the difference in residuals of training and test data\n\nTarget Visualization\n\n- **Balanced Binning Reference**: generate a histogram with vertical lines showing the recommended value point to the bin data into evenly distributed bins\n- **Class Balance**: show the relationship of the support for each class in both the training and test data by displaying how frequently each class occurs as a bar graph the frequency of the classes' representation in the dataset\n- **Feature Correlation**: visualize the correlation between the dependent variables and the target\n\nText Visualization\n\n- **Dispersion Plot**: visualize how key terms are dispersed throughout a corpus\n- **PosTag Visualizer**: plot the counts of different parts-of-speech throughout a tagged corpus\n- **Token Frequency Distribution**: visualize the frequency distribution of terms in the corpus\n- **t-SNE Corpus Visualization**: uses stochastic neighbor embedding to project documents\n- **UMAP Corpus Visualization**: plot similar documents closer together to discover clusters\n\n... and more! Yellowbrick is adding new visualizers all the time so be sure to check out our [examples gallery]https://github.com/DistrictDataLabs/yellowbrick/tree/develop/examples) &mdash; or even the [develop](https://github.com/districtdatalabs/yellowbrick/tree/develop) branch &mdash; and feel free to contribute your ideas for new Visualizers!\n\nAffiliations\n(https://www.districtdatalabs.com/) (https://numfocus.org/)"}, {"name": "yellowbrick", "tags": ["data", "math", "ml", "visualization", "web"], "summary": "A suite of visual analysis and diagnostic tools for machine learning.", "text": "This library is used to facilitate machine learning with scikit-learn by providing a suite of visual analysis and diagnostic tools to steer the model selection process. It enables developers to build intuition around feature engineering, algorithm selection, and hyperparameter tuning through visual representations of the model selection workflow."}, {"name": "yfinance", "tags": ["math", "web"], "summary": "Download market data from Yahoo! Finance API", "text": "Download market data from Yahoo! Finance's API\n\n**yfinance** offers a Pythonic way to fetch financial & market data from [Yahoo!\u24c7 finance](https://finance.yahoo.com).\n\n---\n\n> [!IMPORTANT]  \n> **Yahoo!, Y!Finance, and Yahoo! finance are registered trademarks of Yahoo, Inc.**\n>\n> yfinance is **not** affiliated, endorsed, or vetted by Yahoo, Inc. It's an open-source tool that uses Yahoo's publicly available APIs, and is intended for research and educational purposes.\n> \n> **You should refer to Yahoo!'s terms of use** ([here](https://policies.yahoo.com/us/en/yahoo/terms/product-atos/apiforydn/index.htm), [here](https://legal.yahoo.com/us/en/yahoo/terms/otos/index.html), and [here](https://policies.yahoo.com/us/en/yahoo/terms/index.htm)) **for details on your rights to use the actual data downloaded.\n>\n> Remember - the Yahoo! finance API is intended for personal use only.**\n\n---\n\n> [!TIP]\n> THE NEW DOCUMENTATION WEBSITE IS NOW LIVE! \n> \n> Visit [**ranaroussi.github.io/yfinance**](https://ranaroussi.github.io/yfinance)\n\n---\n\nMain components\n\n- `Ticker`: single ticker data\n- `Tickers`: multiple tickers' data\n- `download`: download market data for multiple tickers\n- `Market`: get information about a market\n- `WebSocket` and `AsyncWebSocket`: live streaming data\n- `Search`: quotes and news from search\n- `Sector` and `Industry`: sector and industry information\n- `EquityQuery` and `Screener`: build query to screen market\n\nInstallation\n\nInstall `yfinance` from PYPI using `pip`:\n\n[yfinance relies on the community to investigate bugs and contribute code. Here's how you can help.](CONTRIBUTING.md)\n\n---\n\n---\n\nLegal Stuff\n\n**yfinance** is distributed under the **Apache Software License**. See\nthe [LICENSE.txt](./LICENSE.txt) file in the release for details.\n\nAGAIN - yfinance is **not** affiliated, endorsed, or vetted by Yahoo, Inc. It's\nan open-source tool that uses Yahoo's publicly available APIs, and is\nintended for research and educational purposes. You should refer to Yahoo!'s terms of use\n([here](https://policies.yahoo.com/us/en/yahoo/terms/product-atos/apiforydn/index.htm),\n[here](https://legal.yahoo.com/us/en/yahoo/terms/otos/index.html), and\n[here](https://policies.yahoo.com/us/en/yahoo/terms/index.htm)) for\ndetails on your rights to use the actual data downloaded.\n\n---\n\nP.S.\n\nPlease drop me a note with any feedback you have.\n\n**Ran Aroussi**"}, {"name": "yfinance", "tags": ["math", "web"], "summary": "Download market data from Yahoo! Finance API", "text": "This library is used to download market data from Yahoo! Finance's API for research and educational purposes, allowing developers to easily access financial and market information. With this library, developers can fetch historical stock prices, real-time quotes, and other market data in a straightforward and Pythonic way."}]